# Mixed Effects Models

```{r include=FALSE}
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
library(multilevelmod)

theme_set(
  theme_light() +
    theme(
      plot.caption = element_text(hjust = 0),
      strip.background = element_rect(fill="gray90", color="gray60"),
      strip.text = element_text(color = "gray20")
    )
)
```

Linear mixed effects models (also known as multilevel models) are used for cases in which there are multiple observations per participant, a violation of the linear regression independence assumption. Repeated measures ANOVA is one way to analyze this kind of data, but it has several shortcomings. It does not control for covariates well, does not handle missing observations within a subject, requires a continuous outcome on categorical predictors, and does not estimate the magnitude or direction of effects [@Brown2021]. Linear mixed effects models address all of the shortcomings of repeated measures ANOVA, and can be generalized into categorical outcomes a well.

Mixed-effects models are called “mixed” because they simultaneously model fixed and random effects. Fixed effects are population-level effects that should persist across experiments. Random effects are level effects of some grouping factor such as participants. Random effects are included in mixed-effects models to account for the fact that the behavior of particular participants or items may differ from the average trend for unspecified reasons. Including random effects resolves the non-independence problem. The fixed-intercept estimate represents the average intercept, and random intercepts allow each predictor level to deviate from this average. Mixed-effects models account for differences in variability with fixed and random slopes. These deviations are assumed to follow a normal distribution with a mean of zero and a variance that is estimated by the model.

Mixed-effects models estimate additional parameters: correlations among random effects. For example, a model with by-participant random intercepts and slopes for a control variable, mixed-effects models estimate the correlations among the random intercepts and slopes. A negative correlation would suggest that individuals with higher intercepts (higher outcome values) tend to have lower slopes (control variable effects). Correlations can be informative about possible ceiling or floor effects. A negative correlation suggests that the control variable effect lessons for higher outcomes, meaning their is a ceiling that the process is pushing up against.

## Linear Mixed Effects {#lme}

<!--Logistic regression estimates the probability that a categorical dependent variable is a particular level. The dependent variable levels can be binomial, multinomial, or ordinal. The *binary* logistic regression model is

$$y_i = \mathrm{logit}(\pi_i) = \log \left( \frac{\pi_i}{1 - \pi_i} \right) = X_i \beta$$

where $\pi_i$ is the response $i$'s binary level probability. The model predicts the *log odds* of the level. Why do this? The range of outcomes need to be between 0 and 1, and a sigmoid function, $f(y) = 1 / \left(1 + e^y \right)$, does that. If the _log odds_ of the level equals $X_i\beta$, then the _odds_ of the level equals $e^{X\beta}$. You can solve for $\pi_i$ to get $\pi = \mathrm{odds} / (\mathrm{odds} + 1)$. Substituting, 

$$\pi_i = \frac{\exp(y_i)}{1 + \exp(y_i)} = \frac{e^{X_i\beta}}{1 + e^{X_i\beta}}$$

which you can simplify to $\pi_i = 1 / (1 + e^{-X_i\beta})$, a sigmoid function. The upshot is $X\beta$ is the functional relationship between the independent variables and _a function of the response_, not the response itself.

The model parameters are estimated either by _iteratively reweighted least squares optimization_ or by _maximum likelihood estimation_ (MLE). MLE maximizes the probability produced by a set of parameters $\beta$ given a data set and probability distribution.^[Notes from [Machine Learning Mastery](https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/).] In logistic regression the probability distribution is the binomial and each observation is the outcome of a single Bernoulli trial. 

$$L(\beta; y, X) = \prod_{i=1}^n \pi_i^{y_i}(1 - \pi_i)^{(1-y_i)} = \prod_{i=1}^n\frac{\exp(y_i X_i \beta)}{1 + \exp(X_i \beta)}.$$

In practice, multiplying many small probabilities can be unstable, so MLE optimizes the log likelihood instead. 

$$\begin{align}
l(\beta; y, X) &= \sum_{i = 1}^n \left(y_i \log(\pi_i) + (1 - y_i) \log(1 - \pi_i) \right) \\
               &= \sum_{i = 1}^n \left(y_i X_i \beta - \log(1 + e^{X_i\beta}) \right)
\end{align}$$

Sometimes you will see the _cost function_ optimized. The cost function is the negative of of the log likelihood function.

**Assumptions**

The binomial logistic regression model requires a dichotomous dependent variable and independent observations. The sample size should be large, at least 10 observations per dependent variable level and independent variable. There are three conditions related to the data distribution: i) the logit transformation must be linearly related to any continuous independent variables, ii) there must be no multicollinearity, and iii) there must be no influential outliers.

Be aware of over-dispersion, a common issue with GLM. For a binomial logistic regression, the response variable should be distributed $y_i \sim \mathrm{Bin}(n_i, \pi_i)$ with $\mu_i = n_i \pi_i$ and $\sigma^2 = \pi (1 - \pi)$. Over-dispersion means the data shows evidence of variance greater than $\sigma^2$.
-->

### Case Study {.unnumbered #lme1}

This case study uses data from @Brown2021's tutorial [supplemental materials](https://osf.io/v6qag/). A study presented 53 participants with 543 isolated words (`stim`), some in auditory `modality` and some in audiovisual modality. Participants (`PID`) repeated the words while performing an unrelated response time (`RT`) task in a tactile modality. We know audiovisual is better than audio alone for speech recognition, but what is the affect on a secondary task? 

The initial summary table suggests audiovisual stimulation _slows_ response times for the secondary task by about 10%.

```{r}
cs1 <- list()

cs1$dat <- readr::read_csv("./input/mixed-effects-rt.csv", col_types = "cdcc") %>%
  mutate(modality = factor(modality, levels = c("Audio-only", "Audiovisual")))

skimr::skim(cs1$dat)

cs1$dat %>%
  gtsummary::tbl_summary(
    by = modality,
    include = -c(PID, stim),
    statistic = list(gtsummary::all_continuous() ~ "{mean}, {sd}")
  )
```

<br>

```{r}
bind_rows(
  Participants = cs1$dat %>% summarize(.by = c(modality, PID), M = mean(RT)),
  Stimulis = cs1$dat %>% summarize(.by = c(modality, stim), M = mean(RT)),
  .id = "Factor"
) %>%
  mutate(grp = coalesce(PID, stim)) %>%
  ggplot(aes(x = modality)) +
  geom_line(aes(y = M, group = grp), alpha = .3) +
  facet_wrap(facets = vars(Factor)) +
  labs(title = "Modality Effect on Mean Response Times by Participant, Stimulas")
```

If your model is predictive rather than inferential, split the data into training/testing data sets.

```{r collapse=TRUE}
# For reproducibility
set.seed(123)

(x <- initial_split(cs1$dat, prop = 0.7, strata = modality))

cs1$dat_training <- training(x)
dim(cs1$dat_training)

cs1$dat_testing <- testing(x)
dim(cs1$dat_testing)
```

### Fit the Model {-}

Model modality as a fixed effect because you expect an average relationship between modality and response times that persists across participants and words. Model participants and words as random effects because they are randomly sampled and you expect variability within those populations. You can see that in the by-participant and by-stimulas plots in the prior section.

The basic syntax for a mixed-effects model for a single independent predictor and random intercepts for participants is

```{r eval=FALSE}
lmer4::lmer(RT ~ modality + (1|PID) + (1|stim), data = dat)
```

The pipe indicates the value to the left varies by the variable to the right. 1 means the intercept. Our model includes two random intercepts, the participant and the word.

You can model an interaction between the fixed effect variable(s) and random effects variable(s) by change the random intercept to the fixed effect variable so both the intercept and the slope can vary. The 1 inside the parentheses below is for clarity only - it's implied in the formulation. However, if you want to remove the estimated correlation between two random effects, change the 1 to zero. [@Brown2021] explains that you would then have to add the variable back to get the random intercept, so you could instead have `(0 + modality|PID) + (1|PID)`. 

```{r eval=FALSE}
lmer4::lmer(RT ~ modality + (1 + modality|PID) + (1 + modality|stim), data = dat)
```

Fit the model using the tidymodels framework. If you want to continue using the classic methodology, the lme4 object is inside the tidymodels fit. The model fit returns a brief summary with the coefficients and model diagnostics.

:::rmdnote
If the model fit encounters convergence issues, @Brown2021 suggests using `afex::all_fit()` to test various optimizers. Of course, you should consider the model structure and data issues first.
:::

```{r}
cs1$model <- linear_reg() %>% set_engine("lmer") 

cs1$fmla <- formula(RT ~ modality + (1 + modality|PID) + (1 + modality|stim))

cs1$fit <- cs1$model %>% fit(cs1$fmla, data = cs1$dat)

# The fit object returned by lmer(). You'll need this for interpretation and 
# checking assumptions.
cs1$result <- cs1$fit %>% extract_fit_engine()

# If you are fitting a predictive model, use the training set.
cs1$fit_training <- cs1$model %>% fit(cs1$fmla, data = cs1$dat_training)

cs1$result %>% summary()
```

### Interpretation {-}

The summary object shows the coefficients, but it's easier to get at them by tidying the fit. Note that `tidy()` is in the **broom.mixed** package.

```{r}
(cs1$tidy_fit <- broom.mixed::tidy(cs1$fit))
```

The `r cs1$tidy_fit %>% filter(term == "(Intercept)") %>% pull(estimate) %>% comma(1)` ms intercept is the average response time for the reference case, audio modality and audiovisual modality increased response times by `r cs1$tidy_fit %>% filter(term == "modalityAudiovisual") %>% pull(estimate) %>% comma(1)` ms over the reference case. 

The random-effects estimates are the variation of response times around the fixed intercept and slope for each predictor. Response times varied around `r cs1$tidy_fit %>% filter(term == "(Intercept)") %>% pull(estimate) %>% comma(1)` ms by `r cs1$tidy_fit %>% filter(term == "sd__(Intercept)", group == "stim") %>% pull(estimate) %>% comma(1)` ms for `stim` and `r cs1$tidy_fit %>% filter(term == "sd__(Intercept)", group == "PID") %>% pull(estimate) %>% comma(1)` ms for PID. The modality effect on response times varied around `r cs1$tidy_fit %>% filter(term == "modalityAudiovisual") %>% pull(estimate) %>% comma(1)` ms by `r cs1$tidy_fit %>% filter(term == "sd__modalityAudiovisual", group == "stim") %>% pull(estimate) %>% comma(1)` ms for `stim` and `r cs1$tidy_fit %>% filter(term == "sd__modalityAudiovisual", group == "PID") %>% pull(estimate) %>% comma(1)` ms for PID. That last point means a participant whose slope is 1 SD below the mean would have a slope near zero - a modality effect of zero.

The `r cs1$tidy_fit %>% filter(term == "cor__(Intercept).modalityAudiovisual", group == "stim") %>% pull(estimate) %>% comma(.01)` correlation between the stimuli and fixed-effect intercepts indicates that stimuli with longer response times in the audio modality tended to have more positive slopes (stronger positive audiovisual effect). The `r cs1$tidy_fit %>% filter(term == "cor__(Intercept).modalityAudiovisual", group == "PID") %>% pull(estimate) %>% comma(.01)` correlation between the participant and fixed-effect intercepts indicates that participants with longer response times in the audio modality tended to have a less positive slope (weaker positive, or possibly even a negative, audiovisual effect).

### Model Assumptions {-}

The linear regression model assumes the relationship between the predictors and the response is linear and the residuals are independent random variables normally distributed with mean zero and constant variance. Additionally, you will want to check for multicollinearity in the predictors because it can produce unreliable coefficient estimates and predicted values. The `plot()` function produces a set of diagnostic plots to test the assumptions.

The **Residuals vs Fitted** plot, $e \sim \hat{Y}$, tests the linearity and equal error variances assumptions. The plot also identifies outliers. The polynomial trend line should show that the residuals vary around $e = 0$ in a straight horizontal line (linearity). The residuals should have random scatter in a band of constant width around 0, and no fan shape at the low and high ends (equal variances). All tests and intervals are sensitive to the equal variances condition. The plot also reveals multicollinearity. If the residuals and fitted values are correlated, multicollinearity may be a problem.

```{r warning=FALSE, message=FALSE}
# par(mfrow = c(2, 2))
plot(cs1$result, labels.id = NULL)
```

### Evaluate the Fit {-}

Use likelihood ratio test to determine whether predictors affect the response variable. Compare the fit to a model without the predictor of interest. The likelihood ratio test is usually performed with the anova function, but there is a better way. `afex::mixed(method = "LRT")` performs the test for all fixed effects variables in the model. In this case study we have only one.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
(cs1$lrt <- afex::mixed(cs1$fmla, data = cs1$dat, method = "LRT"))
```

The likelihood-ratio test indicated that the model including modality provided a better fit for the data than a model without it, $\chi^2$(1) = `r cs1$lrt$anova_table$Chisq %>% comma(.1)`, *p* < .001.

### Reporting {-}

> A likelihood-ratio test indicated that the model including modality provided a better fit for the data than a model without it, $\chi^2$(1) = `r cs1$lrt$anova_table$Chisq %>% comma(.1)`, *p* < .001. Examination of the summary output for the full model indicated that response times were on average an estimated `r cs1$tidy_fit %>% filter(term == "modalityAudiovisual") %>% pull(estimate) %>% comma(.1)` ms slower in the audiovisual relative to the audio-only condition($\hat{\beta}$ = `r cs1$tidy_fit %>% filter(term == "modalityAudiovisual") %>% pull(estimate) %>% comma(.1)`, *SE* = `r cs1$tidy_fit %>% filter(term == "modalityAudiovisual") %>% pull(std.error) %>% comma(.1)`, *t* = `r cs1$tidy_fit %>% filter(term == "modalityAudiovisual") %>% pull(statistic) %>% comma(.1)`).

```{r message=FALSE}
gtsummary::tbl_regression(cs1$fit)
```

