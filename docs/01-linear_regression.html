<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Ordinary Least Squares – Supervised Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02-generalized_linear_models.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-linear_regression.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Supervised Machine Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intro</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-linear_regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-generalized_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Generalized Linear Models (GLM)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-linear_mixed_effects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Mixed Effects Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-nonlinear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Non-linear Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-decision_trees.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Decision Trees</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-support_vector_machines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-bayesian_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-emmeans.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Estimated Marginal Means</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#parameter-estimation" id="toc-parameter-estimation" class="nav-link active" data-scroll-target="#parameter-estimation"><span class="header-section-number">1.1</span> Parameter Estimation</a>
  <ul>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  </ul></li>
  <li><a href="#model-assumptions" id="toc-model-assumptions" class="nav-link" data-scroll-target="#model-assumptions"><span class="header-section-number">1.2</span> Model Assumptions</a>
  <ul>
  <li><a href="#multicollinearity" id="toc-multicollinearity" class="nav-link" data-scroll-target="#multicollinearity"><span class="header-section-number">1.2.1</span> Multicollinearity</a></li>
  </ul></li>
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction"><span class="header-section-number">1.3</span> Prediction</a></li>
  <li><a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference"><span class="header-section-number">1.4</span> Inference</a>
  <ul>
  <li><a href="#t-test" id="toc-t-test" class="nav-link" data-scroll-target="#t-test"><span class="header-section-number">1.4.1</span> <em>t</em>-Test</a></li>
  <li><a href="#f-test" id="toc-f-test" class="nav-link" data-scroll-target="#f-test"><span class="header-section-number">1.4.2</span> <em>F</em>-Test</a></li>
  </ul></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation"><span class="header-section-number">1.5</span> Interpretation</a></li>
  <li><a href="#model-validation" id="toc-model-validation" class="nav-link" data-scroll-target="#model-validation"><span class="header-section-number">1.6</span> Model Validation</a>
  <ul>
  <li><a href="#accuracy-metrics" id="toc-accuracy-metrics" class="nav-link" data-scroll-target="#accuracy-metrics"><span class="header-section-number">1.6.1</span> Accuracy Metrics</a></li>
  <li><a href="#r-squared" id="toc-r-squared" class="nav-link" data-scroll-target="#r-squared"><span class="header-section-number">1.6.2</span> R-Squared</a>
  <ul class="collapse">
  <li><a href="#rmse-rse-mae" id="toc-rmse-rse-mae" class="nav-link" data-scroll-target="#rmse-rse-mae"><span class="header-section-number">1.6.2.1</span> RMSE, RSE, MAE</a></li>
  <li><a href="#aic-bic" id="toc-aic-bic" class="nav-link" data-scroll-target="#aic-bic"><span class="header-section-number">1.6.2.2</span> AIC, BIC</a></li>
  </ul></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation"><span class="header-section-number">1.6.3</span> Cross-Validation</a>
  <ul class="collapse">
  <li><a href="#validation-set" id="toc-validation-set" class="nav-link" data-scroll-target="#validation-set"><span class="header-section-number">1.6.3.1</span> Validation Set</a></li>
  <li><a href="#loocv" id="toc-loocv" class="nav-link" data-scroll-target="#loocv"><span class="header-section-number">1.6.3.2</span> LOOCV</a></li>
  <li><a href="#k-fold-cross-validation" id="toc-k-fold-cross-validation" class="nav-link" data-scroll-target="#k-fold-cross-validation"><span class="header-section-number">1.6.3.3</span> K-fold Cross-Validation</a></li>
  <li><a href="#repeated-k-fold-cv" id="toc-repeated-k-fold-cv" class="nav-link" data-scroll-target="#repeated-k-fold-cv"><span class="header-section-number">1.6.3.4</span> Repeated K-fold CV</a></li>
  <li><a href="#bootstrapping" id="toc-bootstrapping" class="nav-link" data-scroll-target="#bootstrapping"><span class="header-section-number">1.6.3.5</span> Bootstrapping</a></li>
  </ul></li>
  <li><a href="#gain-curve" id="toc-gain-curve" class="nav-link" data-scroll-target="#gain-curve"><span class="header-section-number">1.6.4</span> Gain Curve</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>These notes rely on PSU<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, STHDA<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, and <span class="citation" data-cites="Molner2020">(<a href="10-references.html#ref-Molner2020" role="doc-biblioref">Molnar 2020</a>)</span>,</p>
<p>The population regression model <span class="math inline">\(E(Y) = X \beta\)</span> summarizes the trend between the predictors and the mean responses. The individual responses are assumed to be normally distributed about the population regression, <span class="math inline">\(y_i = X_i \beta + \epsilon_i\)</span> with varying mean, but constant variance, <span class="math inline">\(y_i \sim N(\mu_i, \sigma^2).\)</span> Equivalently, the model presumes a linear relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span> with residuals <span class="math inline">\(\epsilon\)</span> that are independent normal random variables with mean zero and constant variance <span class="math inline">\(\sigma^2\)</span>. Estimate the population regression model coefficients as <span class="math inline">\(\hat{y} = X \hat{\beta}\)</span>, and the population variance as <span class="math inline">\(\hat{\sigma}^2\)</span>. The most common method of estimating the <span class="math inline">\(\beta\)</span> coefficients and <span class="math inline">\(\sigma\)</span> is ordinary least squares (OLS). OLS minimizes the sum of squared residuals from a random sample. The predicted values vary about the actual value, <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>, where <span class="math inline">\(\hat{y}_i = X_i \hat{\beta}\)</span>.</p>
<p>The OLS model is the best linear unbiased estimator (BLUE) if the residuals are independent random variables normally distributed with mean zero and constant variance. Recall these conditions with the LINE pneumonic: <strong>L</strong>inear, <strong>I</strong>ndependent, <strong>N</strong>ormal, and <strong>E</strong>qual.</p>
<p><strong>Linearity</strong>. The explanatory variables are each linearly related to the response variable: <span class="math inline">\(E(\epsilon | X_j) = 0\)</span>.</p>
<p><strong>Independence</strong>. The residuals are unrelated to each other. Independence is violated by repeated measurements and temporal regressors.</p>
<p><strong>Normality</strong>. The residuals are normally distributed: <span class="math inline">\(\epsilon|X \sim N(0, \sigma^2I)\)</span>.</p>
<p><strong>Equal Variances</strong>. The variance of the residuals is constant (homoscedasticity): <span class="math inline">\(E(\epsilon \epsilon' | X) = \sigma^2I\)</span></p>
<p>Additionally, there should be little <strong>multicollinearity</strong> among the explanatory variables.</p>
<p>If the modeling objective is inference, fit the model with the full data set, evaluate the model fit, and interpret the parameter estimates. If the objective is prediction, fit the model with a partitioned train/test data set, cross-validate the fit with the test set, then deploy the model. Both are covered below.</p>
<section id="parameter-estimation" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="parameter-estimation"><span class="header-section-number">1.1</span> Parameter Estimation</h2>
<p>There are two model parameters to estimate: <span class="math inline">\(\hat{\beta}\)</span> estimates the coefficient vector <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\hat{\sigma}\)</span> estimates the variance of the residuals along the regression line. Derive <span class="math inline">\(\hat{\beta}\)</span> by minimizing the sum of squared residuals <span class="math inline">\(SSE = (y - X \hat{\beta})' (y - X \hat{\beta})\)</span>. The result is</p>
<p><span class="math display">\[\hat{\beta} = (X'X)^{-1}X'y.\]</span></p>
<p>The residual standard error (RSE) estimates the sample deviation around the population regression line. <em>(Think of each value of <span class="math inline">\(X\)</span> along the regression line as a subpopulation with mean <span class="math inline">\(y_i\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. This variance is assumed to be the same for all <span class="math inline">\(X\)</span>.)</em></p>
<p><span class="math display">\[\hat{\sigma} = \sqrt{(n-k-1)^{-1} e'e}.\]</span></p>
<p>The standard error for the coefficient estimators is the square root of the error variance divided by <span class="math inline">\((X'X)\)</span>.</p>
<p><span class="math display">\[SE(\hat{\beta}) = \sqrt{\hat{\sigma}^2 (X'X)^{-1}}.\]</span></p>
<p>Linear regression is related to correlation. The coefficient estimator for <span class="math inline">\(\beta_1\)</span> in simple linear regression is equal to the Pearson correlation coefficient multiplied by the ratio of the standard deviations of <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\hat{\beta}_1 &amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2} \\
&amp;= \frac{Cov(X,Y)}{Var(X)} \\
&amp;= r \cdot \frac{\sigma_Y}{\sigma_X}
\end{align}
\]</span></p>
<p>The Pearson correlation is the ratio of the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and product of their standard deviations.</p>
<p><span class="math display">\[
\begin{align}
r &amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n(x_i - \bar{x})^2}  \sqrt{\sum_{i=1}^n(y_i - \bar{y})^2}} \\
&amp;= \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}
\end{align}
\]</span></p>
<p>The formula for <span class="math inline">\(\hat{\beta}\)</span> is almost identical to that of <span class="math inline">\(r\)</span> with the exception that the denominator is the variance of <span class="math inline">\(X\)</span> rather than the product of the standard deviations. The Pearson correlation is the <em>standardized</em> linear relationship. Pearson correlation measures the strength and direction of the linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> in a standardized form.</p>
<ul>
<li>If <span class="math inline">\(r = 0\)</span>, then <span class="math inline">\(\beta_1 = 0\)</span> and there is no linear relationship.</li>
<li>if <span class="math inline">\(r = 1\)</span> or <span class="math inline">\(r = -1\)</span>, then the relationship is perfectly linear, and <span class="math inline">\(\beta_1\)</span> measures the relative <em>scale</em> of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>.</li>
</ul>
<section id="example" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="example">Example</h3>
<p>Dataset <code>mtcars</code> contains response variable fuel consumption <code>mpg</code> and 10 aspects of automobile design and performance for 32 automobiles. What is the relationship between the response and its predictors?</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"mtcars"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>mt_cars <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">vs =</span> <span class="fu">factor</span>(vs, <span class="at">levels =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"V"</span>, <span class="st">"S"</span>)),</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">am =</span> <span class="fu">factor</span>(am, <span class="at">levels =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"automatic"</span>, <span class="st">"manual"</span>))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(mt_cars)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 32
Columns: 11
$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…
$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…
$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…
$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…
$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…
$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…
$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…
$ vs   &lt;fct&gt; V, V, S, S, V, S, V, S, S, S, S, V, V, V, V, V, V, S, S, S, S, V,…
$ am   &lt;fct&gt; manual, manual, manual, automatic, automatic, automatic, automati…
$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…
$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…</code></pre>
</div>
</div>
<p>The correlation matrix of the numeric variables shows <code>wt</code> has the strongest association with <code>mpg</code> (<em>r</em> = -0.87) followed by <code>disp</code> (<em>r</em> = -0.85) and <code>hp</code> (<em>r</em> = -0.78). <code>drat</code> is moderately correlated (<em>r</em> = 0.68), and <code>qsec</code> is weakly correlated (<em>r</em> = 0.42). Many predictors are strongly correlated with each other.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>mt_cars <span class="sc">%&gt;%</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cor</span>() <span class="sc">%&gt;%</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  corrplot<span class="sc">::</span><span class="fu">corrplot</span>(<span class="at">type =</span> <span class="st">"upper"</span>, <span class="at">method =</span> <span class="st">"number"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Boxplots of the categorical variables reveal differences in levels.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>mt_cars <span class="sc">%&gt;%</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(mpg, <span class="fu">where</span>(is.factor)) <span class="sc">%&gt;%</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.factor), as.character)) <span class="sc">%&gt;%</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="sc">-</span>mpg) <span class="sc">%&gt;%</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> mpg)) <span class="sc">+</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(name), <span class="at">scales =</span> <span class="st">"free_x"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Fit a population model to the predictors.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mt_cars)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ ., data = mt_cars)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.4506 -1.6044 -0.1196  1.2193  4.6271 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) 12.30337   18.71788   0.657   0.5181  
cyl         -0.11144    1.04502  -0.107   0.9161  
disp         0.01334    0.01786   0.747   0.4635  
hp          -0.02148    0.02177  -0.987   0.3350  
drat         0.78711    1.63537   0.481   0.6353  
wt          -3.71530    1.89441  -1.961   0.0633 .
qsec         0.82104    0.73084   1.123   0.2739  
vsS          0.31776    2.10451   0.151   0.8814  
ammanual     2.52023    2.05665   1.225   0.2340  
gear         0.65541    1.49326   0.439   0.6652  
carb        -0.19942    0.82875  -0.241   0.8122  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.65 on 21 degrees of freedom
Multiple R-squared:  0.869, Adjusted R-squared:  0.8066 
F-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07</code></pre>
</div>
</div>
<p><code>summary()</code> shows <span class="math inline">\(\hat{\beta}\)</span> as <code>Estimate</code>, <span class="math inline">\(SE({\hat{\beta}})\)</span> as <code>Std. Error</code>, and <span class="math inline">\(\hat{\sigma}\)</span> as <code>Residual standard error</code>. You can manually perform these calculations using matrix algebra<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. Recall, <span class="math inline">\(\hat{\beta} = (X'X)^{-1}X'y\)</span>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(fit)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> mt_cars<span class="sc">$</span>mpg</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>(beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                   [,1]
(Intercept) 12.30337416
cyl         -0.11144048
disp         0.01333524
hp          -0.02148212
drat         0.78711097
wt          -3.71530393
qsec         0.82104075
vsS          0.31776281
ammanual     2.52022689
gear         0.65541302
carb        -0.19941925</code></pre>
</div>
</div>
<p>The residual standard error is <span class="math inline">\(\hat{\sigma} = \sqrt{(n-k-1)^{-1} \hat{e}'\hat{e}}\)</span>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X) <span class="sc">-</span> <span class="dv">1</span>  <span class="co"># exclude the intercept term</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_hat</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>sse <span class="ot">&lt;-</span> <span class="fu">sum</span>((y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>(dof <span class="ot">&lt;-</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 21</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>(rse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(sse <span class="sc">/</span> dof))</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2.650197</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The standard errors of the coefficients are <span class="math inline">\(SE(\hat{\beta}) = \sqrt{\hat{\sigma}^2 (X'X)^{-1}}\)</span>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>(se_beta_hat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(rse<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)         cyl        disp          hp        drat          wt 
18.71788443  1.04502336  0.01785750  0.02176858  1.63537307  1.89441430 
       qsec         vsS    ammanual        gear        carb 
 0.73084480  2.10450861  2.05665055  1.49325996  0.82875250 </code></pre>
</div>
</div>
</section>
</section>
<section id="model-assumptions" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="model-assumptions"><span class="header-section-number">1.2</span> Model Assumptions</h2>
<p>The linear regression model assumes the relationship between the predictors and the response is linear and the residuals are independent random variables normally distributed with mean zero and constant variance. Additionally, you will want to check for multicollinearity in the predictors because it can produce unreliable coefficient estimates and predicted values. The <code>plot()</code> function produces a set of diagnostic plots to test the assumptions.</p>
<p>Use the <strong>Residuals vs Fitted</strong> plot, <span class="math inline">\(e \sim \hat{Y}\)</span>, to test the linearity and equal error variances assumptions. The plot also identifies outliers. The polynomial trend line should show that the residuals vary around <span class="math inline">\(e = 0\)</span> in a straight horizontal line (linearity). The residuals should have random scatter in a band of constant width around 0, and no fan shape at the low and high ends (equal variances). All tests and intervals are sensitive to the equal variances condition. The plot also reveals multicollinearity. If the residuals and fitted values are correlated, multicollinearity may be a problem.</p>
<p>Use the <strong>Q-Q Residuals</strong> plot (aka, residuals normal probability plot) to test the normality assumption. It plots the theoretical percentiles of the normal distribution versus the observed sample percentiles. It should be approximately linear with no bow-shaped deviations. Sometimes this normality check fails when the linearity check fails, so check for linearity first. Parameter estimation is not sensitive to the normality assumption, but prediction intervals are.</p>
<p>Use the <strong>Scale-Location</strong> plot, <span class="math inline">\(\sqrt{e / sd(e)} \sim \hat{y}\)</span>, to test for equal variance (aka, homoscedasticity). The square root of the absolute value of the residuals should be spread equally along a horizontal line.</p>
<p>The <strong>Residuals vs Leverage</strong> plot identifies influential observations. The standardized residuals should fall within the 95% probability band.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit, <span class="at">labels.id =</span> <span class="cn">NULL</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>These diagnostics are usually sufficient. If any of the diagnostics fail, you can re-specify the model or transform the data.</p>
<p>Consider the linearity assumption. The explanatory variables should <em>each</em> be linearly related to the response variable: <span class="math inline">\(E(\epsilon | X_j) = 0\)</span>. The <strong>Residuals vs Fitted</strong> plot tested this overall. A curved pattern in the residuals indicates a curvature in the relationship between the response and the predictor that is not explained by the model. There are other tests of linearity. The full list:</p>
<ul>
<li>Residuals vs fits plot <span class="math inline">\((e \sim \hat{Y})\)</span> should randomly vary around 0.</li>
<li>Observed vs fits plot <span class="math inline">\((Y \sim \hat{Y})\)</span> should be symmetric along the 45-degree line.<br>
</li>
<li>Each <span class="math inline">\((Y \sim X_j )\)</span> plot should have correlation <span class="math inline">\(\rho \sim 1\)</span>.<br>
</li>
<li>Each <span class="math inline">\((e \sim X_j)\)</span> plot should exhibit no pattern.</li>
</ul>
<p>The diagnostic plots above look pretty good except for some evidence of heteroskedasticity at the upper end of predicted values. You might drill into the linearity condition to start. Start with plots of <span class="math inline">\((Y \sim X_j )\)</span> - are the correlation coefficients ~1? Yes, all but <code>qsec</code> have correlations over .6.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> mt_cars <span class="sc">%&gt;%</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>mpg)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>x_cor <span class="ot">&lt;-</span> x <span class="sc">%&gt;%</span> </span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">.by =</span> name) <span class="sc">%&gt;%</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">val_corr =</span> <span class="fu">map_dbl</span>(data, <span class="sc">~</span><span class="fu">cor</span>(.<span class="sc">$</span>mpg, .<span class="sc">$</span>value)))</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>x <span class="sc">%&gt;%</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(x_cor, <span class="at">by =</span> <span class="fu">join_by</span>(name)) <span class="sc">%&gt;%</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">name =</span> glue<span class="sc">::</span><span class="fu">glue</span>(<span class="st">"{name}, r = {comma(val_corr, .01)}"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> mpg)) <span class="sc">+</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">formula =</span> <span class="st">"y ~ x"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(name), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Response vs Predictor"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>How about <span class="math inline">\((e \sim X_j)\)</span>? <code>disp</code> has a wave pattern; <code>hp</code> is u-shaped; <code>qsec</code> has increase variance in the middle.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>mt_cars <span class="sc">%&gt;%</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>mpg) <span class="sc">%&gt;%</span> </span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">.by =</span> name) <span class="sc">%&gt;%</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">fit =</span> <span class="fu">map</span>(data, <span class="sc">~</span><span class="fu">lm</span>(mpg <span class="sc">~</span> value, <span class="at">data =</span> .)),</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">aug =</span> <span class="fu">map</span>(fit, broom<span class="sc">::</span>augment)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(aug) <span class="sc">%&gt;%</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> .resid)) <span class="sc">+</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">formula =</span> <span class="st">"y ~ x"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(name), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Residuals vs Predictor"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>If the linearity condition fails, change the functional form of the model with non-linear transformations of the explanatory variables. A common way to do this is with Box-Cox transformations.</p>
<p><span class="math display">\[w_t = \begin{cases} \begin{array}{l} log(y_t) \quad \quad \lambda = 0 \\
(y_t^\lambda - 1) / \lambda \quad \text{otherwise} \end{array} \end{cases}\]</span></p>
<p><span class="math inline">\(\lambda\)</span> can take any value, but values near the following yield familiar transformations.</p>
<ul>
<li><span class="math inline">\(\lambda = 1\)</span> yields no substantive transformation.<br>
</li>
<li><span class="math inline">\(\lambda = 0.5\)</span> is a square root plus linear transformation.</li>
<li><span class="math inline">\(\lambda = 0.333\)</span> is a cube root plus linear transformation.</li>
<li><span class="math inline">\(\lambda = 0\)</span> is a natural log transformation.</li>
<li><span class="math inline">\(\lambda = -1\)</span> is an inverse transformation.</li>
</ul>
<p>A common source of non-linearity is skewed response or predictor variables (see discussion <a href="https://blog.minitab.com/blog/applying-statistics-in-quality-projects/how-could-you-benefit-from-a-box-cox-transformation">here</a>). <code>mt_cars</code> has some skewed variables.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>mt_cars <span class="sc">%&gt;%</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">everything</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value)) <span class="sc">+</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(name), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Histogram of numeric vars"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><code>hp</code> has the most skew, <code>cyl</code> the least.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>mt_cars <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span> moments<span class="sc">::</span><span class="fu">skewness</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>       mpg        cyl       disp         hp       drat         wt       qsec 
 0.6404399 -0.1831287  0.4002724  0.7614356  0.2788734  0.4437855  0.3870456 
      gear       carb 
 0.5546495  1.1021304 </code></pre>
</div>
</div>
<p>Compare the residuals vs fitted plots for each variable. Here is <code>hp</code>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(mpg <span class="sc">~</span> hp, <span class="at">data =</span> mt_cars) <span class="sc">%&gt;%</span> <span class="fu">plot</span>(<span class="at">which =</span> <span class="dv">1</span>, <span class="at">main =</span> <span class="st">"hp"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>and here is <code>cyl</code></p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(mpg <span class="sc">~</span> cyl, <span class="at">data =</span> mt_cars) <span class="sc">%&gt;%</span> <span class="fu">plot</span>(<span class="at">which =</span> <span class="dv">1</span>, <span class="at">main =</span> <span class="st">"drat"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Box-Cox transform the numeric variables. The skew is reduced.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>rec <span class="ot">&lt;-</span> <span class="fu">recipe</span>(<span class="sc">~</span>., <span class="at">data =</span> mt_cars)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>mt_cars_boxcox <span class="ot">&lt;-</span> rec <span class="sc">%&gt;%</span> </span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_BoxCox</span>(<span class="fu">all_numeric</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">prep</span>(<span class="at">training =</span> mt_cars) <span class="sc">%&gt;%</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bake</span>(<span class="at">new_data =</span> <span class="cn">NULL</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>mt_cars_boxcox <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span> moments<span class="sc">::</span><span class="fu">skewness</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>         mpg          cyl         disp           hp         drat           wt 
-0.001040985 -0.183128652 -0.056140870 -0.011871603  0.003121934 -0.010164785 
        qsec         gear         carb 
-0.000842791  0.554649467 -0.018867279 </code></pre>
</div>
</div>
<p>The diagnostic plot for <code>hp</code> looks much better (notice the much smaller y-axis).</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(mpg <span class="sc">~</span> hp, <span class="at">data =</span> mt_cars_boxcox) <span class="sc">%&gt;%</span> <span class="fu">plot</span>(<span class="at">which =</span> <span class="dv">1</span>, <span class="at">main =</span> <span class="st">"hp - after Box-Cox"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Did this create a better fitting model? A little: R^2 increased from .87 to .89.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Before</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>fit <span class="sc">%&gt;%</span> <span class="fu">glance</span>()</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 12</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="do">##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 1     0.869         0.807  2.65      13.9 0.000000379    10  -69.9  164.  181.</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="do">## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co"># After</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>fit_boxcox <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mt_cars_boxcox <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span><span class="fu">c</span>(gear, carb)))</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>fit_boxcox <span class="sc">%&gt;%</span> <span class="fu">glance</span>()</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 12</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="do">##   r.squared adj.r.squared sigma statistic       p.value    df logLik   AIC   BIC</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="do">##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 1     0.893         0.855 0.124      23.9 0.00000000215     8   26.8 -33.5 -18.9</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="do">## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Here’s a look at the diagnostic plots for the new model.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit_boxcox, <span class="at">labels.id =</span> <span class="cn">NULL</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>How about those linearity tests? Most look a little better.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> mt_cars_boxcox <span class="sc">%&gt;%</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>mpg)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>x_cor <span class="ot">&lt;-</span> x <span class="sc">%&gt;%</span> </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">.by =</span> name) <span class="sc">%&gt;%</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">val_corr =</span> <span class="fu">map_dbl</span>(data, <span class="sc">~</span><span class="fu">cor</span>(.<span class="sc">$</span>mpg, .<span class="sc">$</span>value)))</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>x <span class="sc">%&gt;%</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(x_cor, <span class="at">by =</span> <span class="fu">join_by</span>(name)) <span class="sc">%&gt;%</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">name =</span> glue<span class="sc">::</span><span class="fu">glue</span>(<span class="st">"{name}, r = {comma(val_corr, .01)}"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> mpg)) <span class="sc">+</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">formula =</span> <span class="st">"y ~ x"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(name), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Response vs Predictor - Box-Cox Transformed"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>How about <span class="math inline">\((e \sim X_j)\)</span>? Much better</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>mt_cars_boxcox <span class="sc">%&gt;%</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>mpg) <span class="sc">%&gt;%</span> </span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">.by =</span> name) <span class="sc">%&gt;%</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">fit =</span> <span class="fu">map</span>(data, <span class="sc">~</span><span class="fu">lm</span>(mpg <span class="sc">~</span> value, <span class="at">data =</span> .)),</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">aug =</span> <span class="fu">map</span>(fit, broom<span class="sc">::</span>augment)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(aug) <span class="sc">%&gt;%</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> .resid)) <span class="sc">+</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">formula =</span> <span class="st">"y ~ x"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(name), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Residuals vs Predictor - Box-Cox Transformed"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="multicollinearity" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="multicollinearity"><span class="header-section-number">1.2.1</span> Multicollinearity</h3>
<p>The multicollinearity condition is violated when two or more of the predictors in a regression model are correlated. Muticollinearity can occur for <em>structural</em> reasons, as when one variable is a transformation of another variable, or for <em>data</em> reasons, as occurs in observational studies. Multicollinearity is a problem because it inflates the variances of the estimated coefficients, resulting in larger confidence intervals.</p>
<p>When predictor variables are correlated, the precision of their estimated regression coefficients decreases. The usual interpretation of a slope coefficient as the change in the mean response for each additional unit increase in the predictor when all the other predictors are held constant breaks down because changing one predictor necessarily changes the others.</p>
<p>The residuals vs fits plot <span class="math inline">\((\epsilon \sim \hat{Y})\)</span> should have correlation <span class="math inline">\(\rho \sim 0\)</span>. A correlation matrix is helpful for picking out the correlation strengths. A good rule of thumb is correlation coefficients should be less than 0.80. However, this test may not work when a variable is correlated with a function of other variables. A model with multicollinearity may have a significant <em>F</em>-test with insignificant individual slope estimator t-tests. Another way to detect multicollinearity is by calculating variance inflation factors (VIF). The predictor variance <span class="math inline">\(Var(\hat{\beta_k})\)</span> increases by a factor</p>
<p><span class="math display">\[\mathrm{VIF}_k = \frac{1}{1 - R_k^2}\]</span></p>
<p>where <span class="math inline">\(R_k^2\)</span> is the <span class="math inline">\(R^2\)</span> of a regression of the <span class="math inline">\(k^{th}\)</span> predictor on the remaining predictors. A <span class="math inline">\(VIF_k\)</span> of <span class="math inline">\(1\)</span> indicates no inflation (no correlation). A <span class="math inline">\(VIF_k &gt;= 4\)</span> warrants investigation. A <span class="math inline">\(VIF_k &gt;= 10\)</span> requires correction.</p>
<p>Does the model <code>mpg ~ .</code> exhibit multicollinearity? Recall that the correlation matrix had several correlated predictors. E.g., <code>disp</code> is strongly correlated with <code>wt</code> (r = 0.89) and <code>hp</code> (r = 0.79). How about the VIFs?</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>(fit_boxcox)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      cyl      disp        hp      drat        wt      qsec        vs        am 
13.793609 18.078964  9.075071  3.401503  9.456176  9.117931  4.866017  4.432487 </code></pre>
</div>
</div>
<p>Three predictors have VIFs greater than 10 (<code>cyl</code>, <code>disp</code>, and <code>wt</code>). One way to address multicollinearity is removing one or more of the violating predictors from the regression model. Try removing <code>disp</code>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">-</span> disp, <span class="at">data =</span> mt_cars_boxcox) <span class="sc">%&gt;%</span> car<span class="sc">::</span><span class="fu">vif</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      cyl        hp      drat        wt      qsec        vs        am      gear 
14.662069  9.587920  4.112070  8.639359  9.590966  5.034117  4.938979  5.684508 
     carb 
 5.046840 </code></pre>
</div>
</div>
<p>Removing <code>disp</code> reduced the VIFs of the other variables, but <code>cyl</code> is still above 10. It may be worth dropping it from the model too. The model summary shows that <code>wt</code> is the only significant predictor.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">-</span> disp, <span class="at">data =</span> mt_cars_boxcox) <span class="sc">%&gt;%</span> <span class="fu">summary</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = mpg ~ . - disp, data = mt_cars_boxcox)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.174183 -0.081250 -0.005585  0.069938  0.245160 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  1.57267    2.97595   0.528  0.60247   
cyl          0.02864    0.04662   0.614  0.54527   
hp          -0.13295    0.08959  -1.484  0.15200   
drat         0.13119    0.32429   0.405  0.68972   
wt          -0.35353    0.12046  -2.935  0.00767 **
qsec         1.00706    1.12656   0.894  0.38104   
vsS         -0.03579    0.09679  -0.370  0.71510   
ammanual    -0.01315    0.09684  -0.136  0.89324   
gear         0.09776    0.07026   1.391  0.17806   
carb        -0.06806    0.07516  -0.906  0.37498   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1211 on 22 degrees of freedom
Multiple R-squared:  0.9015,    Adjusted R-squared:  0.8612 
F-statistic: 22.37 on 9 and 22 DF,  p-value: 4.385e-09</code></pre>
</div>
</div>
</section>
</section>
<section id="prediction" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="prediction"><span class="header-section-number">1.3</span> Prediction</h2>
<p>The standard error in the <strong>expected value</strong> of <span class="math inline">\(\hat{y}\)</span> at some new set of predictors <span class="math inline">\(X_n\)</span> is</p>
<p><span class="math display">\[SE(\mu_\hat{y}) = \sqrt{\hat{\sigma}^2 (X_n (X'X)^{-1} X_n')}.\]</span></p>
<p>The standard error increases the further <span class="math inline">\(X_n\)</span> is from <span class="math inline">\(\bar{X}\)</span>. If <span class="math inline">\(X_n = \bar{X}\)</span>, the equation reduces to <span class="math inline">\(SE(\mu_\hat{y}) = \sigma / \sqrt{n}\)</span>. If <span class="math inline">\(n\)</span> is large, or the predictor values are spread out, <span class="math inline">\(SE(\mu_\hat{y})\)</span> will be relatively small. The <span class="math inline">\((1 - \alpha)\%\)</span> <strong>confidence interval</strong> is <span class="math inline">\(\hat{y} \pm t_{\alpha / 2} SE(\mu_\hat{y})\)</span>.</p>
<p>The standard error in the <strong>predicted value</strong> of <span class="math inline">\(\hat{y}\)</span> at some <span class="math inline">\(X_{new}\)</span> is</p>
<p><span class="math display">\[SE(\hat{y}) = SE(\mu_\hat{y})^2 + \sqrt{\hat{\sigma}^2}.\]</span></p>
<p>Notice the standard error for a predicted value is always greater than the standard error of the expected value. The <span class="math inline">\((1 - \alpha)\%\)</span> <strong>prediction interval</strong> is <span class="math inline">\(\hat{y} \pm t_{\alpha / 2} SE(\hat{y})\)</span>.</p>
<p>Calculate confidence interval and prediction interval for <code>mpg</code> for predictor values at their mean.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>new_data <span class="ot">&lt;-</span> mt_cars <span class="sc">%&gt;%</span> </span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.numeric), mean)) <span class="sc">%&gt;%</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">vs =</span> <span class="st">"S"</span>, <span class="at">am =</span> <span class="st">"manual"</span>)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="fu">list</span>(</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">Confidence =</span> <span class="fu">predict.lm</span>(fit, <span class="at">newdata =</span> new_data, <span class="at">interval =</span> <span class="st">"confidence"</span>),</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">Prediction =</span> <span class="fu">predict.lm</span>(fit, <span class="at">newdata =</span> new_data, <span class="at">interval =</span> <span class="st">"prediction"</span>)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>$Confidence
       fit      lwr      upr
1 21.76575 17.75562 25.77589

$Prediction
       fit      lwr      upr
1 21.76575 14.94985 28.58166</code></pre>
</div>
</div>
<p>Verify this by calculating <span class="math inline">\(SE(\mu_\hat{y}) = \sqrt{\hat{\sigma}^2 (X_{new} (X'X)^{-1} X_{new}')}\)</span> and <span class="math inline">\(SE(\hat{y}) = SE(\mu_\hat{y})^2 + \sqrt{\hat{\sigma}^2}\)</span> with matrix algebra.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="fu">data.frame</span>(<span class="fu">model.matrix</span>(fit)), mean) <span class="sc">%&gt;%</span> <span class="fu">unlist</span>() <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>X2[<span class="dv">8</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>X2[<span class="dv">9</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>y_exp <span class="ot">&lt;-</span> <span class="fu">sum</span>(fit<span class="sc">$</span>coefficients <span class="sc">*</span> <span class="fu">as.numeric</span>(X2))</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard error of expected and predicted values</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>se_y_exp <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">sqrt</span>(rse<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> X2 <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X2)))</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>se_y_hat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(rse<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> se_y_exp<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Margins of error</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>t_crit <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="at">p =</span> .<span class="dv">05</span> <span class="sc">/</span> <span class="dv">2</span>, <span class="at">df =</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>me_exp <span class="ot">&lt;-</span> t_crit <span class="sc">*</span> se_y_exp</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>me_hat <span class="ot">&lt;-</span> t_crit <span class="sc">*</span> se_y_hat</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="fu">list</span>(</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">Confidence =</span> <span class="fu">c</span>(<span class="at">fit =</span> y_exp, <span class="at">lwr =</span> y_exp <span class="sc">-</span> me_exp, <span class="at">upr =</span> y_exp <span class="sc">+</span> me_exp),</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">Prediction =</span> <span class="fu">c</span>(<span class="at">fit =</span> y_exp, <span class="at">lwr =</span> y_exp <span class="sc">-</span> me_hat, <span class="at">upr =</span> y_exp <span class="sc">+</span> me_hat)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>$Confidence
     fit      lwr      upr 
21.76575 17.75562 25.77589 

$Prediction
     fit      lwr      upr 
21.76575 14.94985 28.58166 </code></pre>
</div>
</div>
</section>
<section id="inference" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="inference"><span class="header-section-number">1.4</span> Inference</h2>
<p>Draw conclusions about the significance of the coefficient estimates with the <em>t</em>-test and/or <em>F</em>-test.</p>
<section id="t-test" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="t-test"><span class="header-section-number">1.4.1</span> <em>t</em>-Test</h3>
<p>By assumption, the residuals are normally distributed, so the <em>Z</em>-test statistic could evaluate the parameter estimators,</p>
<p><span class="math display">\[Z = \frac{\hat{\beta} - \beta_0}{\sqrt{\sigma^2 (X'X)^{-1}}}\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is the null-hypothesized value, usually 0. <span class="math inline">\(\sigma\)</span> is unknown, but <span class="math inline">\(\frac{\hat{\sigma}^2 (n - k)}{\sigma^2} \sim \chi^2\)</span>. The ratio of the normal distribution divided by the adjusted chi-square <span class="math inline">\(\sqrt{\chi^2 / (n - k)}\)</span> is t-distributed,</p>
<p><span class="math display">\[t = \frac{\hat{\beta} - \beta_0}{\sqrt{\hat{\sigma}^2 (X'X)^{-1}}} = \frac{\hat{\beta} - \beta_0}{SE(\hat{\beta})}\]</span></p>
<p>The <span class="math inline">\((1 - \alpha)\)</span> confidence intervals are <span class="math inline">\(CI = \hat{\beta} \pm t_{\alpha / 2, df} SE(\hat{\beta})\)</span> with <em>p</em>-value equaling the probability of measuring a <span class="math inline">\(t\)</span> of that extreme, <span class="math inline">\(p = P(t &gt; |t|)\)</span>. For a one-tail test, divide the reported p-value by two. The <span class="math inline">\(SE(\hat{\beta})\)</span> decreases with i) a better fitting regression line (smaller <span class="math inline">\(\hat{\sigma}^2\)</span>), ii) greater variation in the predictor (larger <span class="math inline">\(X'X\)</span>), and iii) larger sample size (larger n).</p>
<p>The <code>summary()</code> output shows the t values and probabilities in the <code>t value</code> and <code>Pr(&gt;|t|)</code> columns. Verify this using matrix algebra to solve <span class="math inline">\(t = \frac{(\hat{\beta} - \beta_1)}{SE(\hat{\beta})}\)</span> with <span class="math inline">\(\beta_1 = 0\)</span>. The <span class="math inline">\((1 - \alpha)\)</span> confidence interval is <span class="math inline">\(CI = \hat{\beta} \pm t_{\alpha / 2, df} SE(\hat{\beta})\)</span>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> beta_hat <span class="sc">/</span> se_beta_hat</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>p_value <span class="ot">&lt;-</span> <span class="fu">pt</span>(<span class="at">q =</span> <span class="fu">abs</span>(t), <span class="at">df =</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>) <span class="sc">*</span> <span class="dv">2</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>t_crit <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="at">p =</span> .<span class="dv">05</span> <span class="sc">/</span> <span class="dv">2</span>, <span class="at">df =</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>lcl <span class="ot">=</span> beta_hat <span class="sc">-</span> t_crit <span class="sc">*</span> se_beta_hat</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>ucl <span class="ot">=</span> beta_hat <span class="sc">+</span> t_crit <span class="sc">*</span> se_beta_hat</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">beta =</span> <span class="fu">round</span>(beta_hat, <span class="dv">4</span>), </span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">se =</span> <span class="fu">round</span>(se_beta_hat, <span class="dv">4</span>), </span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">t =</span> <span class="fu">round</span>(t, <span class="dv">4</span>), </span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">p =</span> <span class="fu">round</span>(p_value, <span class="dv">4</span>),</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>           <span class="at">lcl =</span> <span class="fu">round</span>(lcl,<span class="dv">4</span>), </span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>           <span class="at">ucl =</span> <span class="fu">round</span>(ucl, <span class="dv">4</span>)) <span class="sc">%&gt;%</span> knitr<span class="sc">::</span><span class="fu">kable</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">beta</th>
<th style="text-align: right;">se</th>
<th style="text-align: right;">t</th>
<th style="text-align: right;">p</th>
<th style="text-align: right;">lcl</th>
<th style="text-align: right;">ucl</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">12.3034</td>
<td style="text-align: right;">18.7179</td>
<td style="text-align: right;">0.6573</td>
<td style="text-align: right;">0.5181</td>
<td style="text-align: right;">-26.6226</td>
<td style="text-align: right;">51.2293</td>
</tr>
<tr class="even">
<td style="text-align: left;">cyl</td>
<td style="text-align: right;">-0.1114</td>
<td style="text-align: right;">1.0450</td>
<td style="text-align: right;">-0.1066</td>
<td style="text-align: right;">0.9161</td>
<td style="text-align: right;">-2.2847</td>
<td style="text-align: right;">2.0618</td>
</tr>
<tr class="odd">
<td style="text-align: left;">disp</td>
<td style="text-align: right;">0.0133</td>
<td style="text-align: right;">0.0179</td>
<td style="text-align: right;">0.7468</td>
<td style="text-align: right;">0.4635</td>
<td style="text-align: right;">-0.0238</td>
<td style="text-align: right;">0.0505</td>
</tr>
<tr class="even">
<td style="text-align: left;">hp</td>
<td style="text-align: right;">-0.0215</td>
<td style="text-align: right;">0.0218</td>
<td style="text-align: right;">-0.9868</td>
<td style="text-align: right;">0.3350</td>
<td style="text-align: right;">-0.0668</td>
<td style="text-align: right;">0.0238</td>
</tr>
<tr class="odd">
<td style="text-align: left;">drat</td>
<td style="text-align: right;">0.7871</td>
<td style="text-align: right;">1.6354</td>
<td style="text-align: right;">0.4813</td>
<td style="text-align: right;">0.6353</td>
<td style="text-align: right;">-2.6138</td>
<td style="text-align: right;">4.1881</td>
</tr>
<tr class="even">
<td style="text-align: left;">wt</td>
<td style="text-align: right;">-3.7153</td>
<td style="text-align: right;">1.8944</td>
<td style="text-align: right;">-1.9612</td>
<td style="text-align: right;">0.0633</td>
<td style="text-align: right;">-7.6550</td>
<td style="text-align: right;">0.2243</td>
</tr>
<tr class="odd">
<td style="text-align: left;">qsec</td>
<td style="text-align: right;">0.8210</td>
<td style="text-align: right;">0.7308</td>
<td style="text-align: right;">1.1234</td>
<td style="text-align: right;">0.2739</td>
<td style="text-align: right;">-0.6988</td>
<td style="text-align: right;">2.3409</td>
</tr>
<tr class="even">
<td style="text-align: left;">vsS</td>
<td style="text-align: right;">0.3178</td>
<td style="text-align: right;">2.1045</td>
<td style="text-align: right;">0.1510</td>
<td style="text-align: right;">0.8814</td>
<td style="text-align: right;">-4.0588</td>
<td style="text-align: right;">4.6943</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ammanual</td>
<td style="text-align: right;">2.5202</td>
<td style="text-align: right;">2.0567</td>
<td style="text-align: right;">1.2254</td>
<td style="text-align: right;">0.2340</td>
<td style="text-align: right;">-1.7568</td>
<td style="text-align: right;">6.7973</td>
</tr>
<tr class="even">
<td style="text-align: left;">gear</td>
<td style="text-align: right;">0.6554</td>
<td style="text-align: right;">1.4933</td>
<td style="text-align: right;">0.4389</td>
<td style="text-align: right;">0.6652</td>
<td style="text-align: right;">-2.4500</td>
<td style="text-align: right;">3.7608</td>
</tr>
<tr class="odd">
<td style="text-align: left;">carb</td>
<td style="text-align: right;">-0.1994</td>
<td style="text-align: right;">0.8288</td>
<td style="text-align: right;">-0.2406</td>
<td style="text-align: right;">0.8122</td>
<td style="text-align: right;">-1.9229</td>
<td style="text-align: right;">1.5241</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="f-test" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="f-test"><span class="header-section-number">1.4.2</span> <em>F</em>-Test</h3>
<p>The <em>F</em>-test for the model is a test of the null hypothesis that none of the independent variables linearly predict the dependent variable, that is, the model parameters are jointly zero: <span class="math inline">\(H_0 : \beta_1 = \ldots = \beta_k = 0\)</span>. The regression mean sum of squares <span class="math inline">\(MSR = \frac{(\hat{y} - \bar{y})'(\hat{y} - \bar{y})}{k-1}\)</span> and the error mean sum of squares <span class="math inline">\(MSE = \frac{\hat{\epsilon}'\hat{\epsilon}}{n-k}\)</span> are each chi-square variables. Their ratio has an <em>F</em> distribution with <span class="math inline">\(k - 1\)</span> numerator degrees of freedom and <span class="math inline">\(n - k\)</span> denominator degrees of freedom. The F statistic can also be expressed in terms of the coefficient of correlation <span class="math inline">\(R^2 = \frac{MSR}{MST}\)</span>.</p>
<p><span class="math display">\[F(k - 1, n - k) = \frac{MSR}{MSE} = \frac{R^2}{1 - R^2} \frac{n-k}{k-1}\]</span></p>
<p><em>MSE</em> is <span class="math inline">\(\sigma^2\)</span>. If <span class="math inline">\(H_0\)</span> is true, that is, there is no relationship between the predictors and the response, then <span class="math inline">\(MSR\)</span> is also equal to <span class="math inline">\(\sigma^2\)</span>, so <span class="math inline">\(F = 1\)</span>. As <span class="math inline">\(R^2 \rightarrow 1\)</span>, <span class="math inline">\(F \rightarrow \infty\)</span>, and as <span class="math inline">\(R^2 \rightarrow 0\)</span>, <span class="math inline">\(F \rightarrow 0\)</span>. <em>F</em> increases with <span class="math inline">\(n\)</span> and decreases with <span class="math inline">\(k\)</span>.</p>
<p>What is the probability that all parameters are jointly equal to zero? The <em>F</em>-statistic is presented at the bottom of the <code>summary()</code> function. Verify this manually.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>((fit<span class="sc">$</span>fitted.values <span class="sc">-</span> <span class="fu">mean</span>(mt_cars<span class="sc">$</span>mpg))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>sse <span class="ot">&lt;-</span> <span class="fu">sum</span>(fit<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>sst <span class="ot">&lt;-</span> <span class="fu">sum</span>((fit<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">mean</span>(mt_cars<span class="sc">$</span>mpg))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>msr <span class="ot">&lt;-</span> ssr <span class="sc">/</span> k</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> sse <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>f <span class="ot">=</span> msr <span class="sc">/</span> mse</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>p_value <span class="ot">&lt;-</span> <span class="fu">pf</span>(<span class="at">q =</span> f, <span class="at">df1 =</span> k, <span class="at">df2 =</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"F-statistic: "</span>, <span class="fu">round</span>(f, <span class="dv">4</span>), <span class="st">" on 3 and 65 DF,  p-value: "</span>, p_value)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>F-statistic:  13.9325  on 3 and 65 DF,  p-value:  3.793152e-07</code></pre>
</div>
</div>
<p>There is sufficient evidence (<em>F</em> = 17.35, <em>p</em> &lt; .0001) to reject <span class="math inline">\(H_0\)</span> that the parameter estimators are jointly equal to zero.</p>
<p><code>aov()</code> calculates the sequential sum of squares. The regression sum of squares SSR for <code>mpg ~ cyl</code> is 817.7. Adding <code>disp</code> to the model increases SSR by 37.6. Adding <code>hp</code> to the model increases SSR by 9.4. It would seem that <code>hp</code> does not improve the model.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(fit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            Df Sum Sq Mean Sq F value   Pr(&gt;F)    
cyl          1  817.7   817.7 116.425 5.03e-10 ***
disp         1   37.6    37.6   5.353  0.03091 *  
hp           1    9.4     9.4   1.334  0.26103    
drat         1   16.5    16.5   2.345  0.14064    
wt           1   77.5    77.5  11.031  0.00324 ** 
qsec         1    3.9     3.9   0.562  0.46166    
vs           1    0.1     0.1   0.018  0.89317    
am           1   14.5    14.5   2.061  0.16586    
gear         1    1.0     1.0   0.138  0.71365    
carb         1    0.4     0.4   0.058  0.81218    
Residuals   21  147.5     7.0                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Order matters. Had we started with <code>disp</code>, then added <code>hp</code> we would find both estimators were significant.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> disp <span class="sc">+</span> hp <span class="sc">+</span> ., <span class="at">data =</span> mt_cars)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            Df Sum Sq Mean Sq F value   Pr(&gt;F)    
disp         1  808.9   808.9 115.168 5.55e-10 ***
hp           1   33.7    33.7   4.793  0.04001 *  
cyl          1   22.1    22.1   3.150  0.09043 .  
drat         1   16.5    16.5   2.345  0.14064    
wt           1   77.5    77.5  11.031  0.00324 ** 
qsec         1    3.9     3.9   0.562  0.46166    
vs           1    0.1     0.1   0.018  0.89317    
am           1   14.5    14.5   2.061  0.16586    
gear         1    1.0     1.0   0.138  0.71365    
carb         1    0.4     0.4   0.058  0.81218    
Residuals   21  147.5     7.0                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
</section>
</section>
<section id="interpretation" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="interpretation"><span class="header-section-number">1.5</span> Interpretation</h2>
<p>A plot of the standardized coefficients shows the relative importance of each predictor. The distance the coefficients are from zero shows how much a change in a standard deviation of the predictor changes the mean of the predicted value. The CI shows the precision. The plot shows not only which variables are significant, but also which are important.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>mt_cars_scaled <span class="ot">&lt;-</span> mt_cars <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.numeric), scale))</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>fit_scaled <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., mt_cars_scaled)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>fit_scaled <span class="sc">%&gt;%</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tidy</span>() <span class="sc">%&gt;%</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">lwr =</span> estimate <span class="sc">-</span> <span class="fu">qt</span>(.<span class="dv">05</span><span class="sc">/</span><span class="dv">2</span>, fit_scaled<span class="sc">$</span>df.residual) <span class="sc">*</span> std.error,</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">upr =</span> estimate <span class="sc">+</span> <span class="fu">qt</span>(.<span class="dv">05</span><span class="sc">/</span><span class="dv">2</span>, fit_scaled<span class="sc">$</span>df.residual) <span class="sc">*</span> std.error,</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(term <span class="sc">!=</span> <span class="st">"(Intercept)"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> term, <span class="at">x =</span> estimate)) <span class="sc">+</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">y =</span> term, <span class="at">xmin =</span> lwr, <span class="at">xmax =</span> upr), <span class="at">width =</span> .<span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Model Feature Importance"</span>, <span class="at">x =</span> <span class="st">"Standardized Weight"</span>, <span class="at">y =</span> <span class="cn">NULL</span>)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-31-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The added variable plot shows the bivariate relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_i\)</span> after accounting for the other variables. For example, the partial regression plots of <code>y ~ x1 + x2 + x3</code> would plot the residuals of <code>y ~ x2 + x3</code> vs <code>x1</code>, and so on.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">avPlots</span>(fit, <span class="at">layout =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-32-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-validation" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="model-validation"><span class="header-section-number">1.6</span> Model Validation</h2>
<p>Evaluate predictive accuracy by training the model on a training data set and testing on a test data set.</p>
<section id="accuracy-metrics" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="accuracy-metrics"><span class="header-section-number">1.6.1</span> Accuracy Metrics</h3>
<p>The most common measures of model fit are R-squared, Adjusted R-squared, RMSE, RSE, MAE, AIC, AICc, BIC, and Mallow’s Cp.</p>
</section>
<section id="r-squared" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="r-squared"><span class="header-section-number">1.6.2</span> R-Squared</h3>
<p>The coefficient of determination (<strong>R-squared</strong>) is the percent of total variation in the response variable that is explained by the regression line.</p>
<p><span class="math display">\[R^2 = \frac{RSS}{SST} = 1 - \frac{SSE}{SST}\]</span></p>
<p>where <span class="math inline">\(SSE = \sum_{i=1}^n{(y_i - \hat{y}_i)^2}\)</span> is the sum squared differences between the predicted and observed value, <span class="math inline">\(SST = \sum_{i = 1}^n{(y_i - \bar{y})^2}\)</span> is the sum of squared differences between the observed and overall mean value, and <span class="math inline">\(RSS = \sum_{i=1}^n{(\hat{y}_i - \bar{y})^2}\)</span> is the sum of squared differences between the predicted and overall mean “no-relationship line” value. At the extremes, <span class="math inline">\(R^2 = 1\)</span> means all data points fall perfectly on the regression line - the predictors account for <em>all</em> variation in the response; <span class="math inline">\(R^2 = 0\)</span> means the regression line is horizontal at <span class="math inline">\(\bar{y}\)</span> - the predictors account for <em>none</em> of the variation in the response. In the simple case of a single predictor variable, <span class="math inline">\(R^2\)</span> equals the squared correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, <span class="math inline">\(Cor(x,y)\)</span>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>((fit<span class="sc">$</span>fitted.values <span class="sc">-</span> <span class="fu">mean</span>(mt_cars<span class="sc">$</span>mpg))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>sse <span class="ot">&lt;-</span> <span class="fu">sum</span>(fit<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>sst <span class="ot">&lt;-</span> <span class="fu">sum</span>((mt_cars<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">mean</span>(mt_cars<span class="sc">$</span>mpg))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>(r2 <span class="ot">&lt;-</span> ssr <span class="sc">/</span> sst)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8690158</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>(r2 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> sse <span class="sc">/</span> sst)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8690158</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>(r2 <span class="ot">&lt;-</span> <span class="fu">summary</span>(fit)<span class="sc">$</span>r.squared)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8690158</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The sums of squares are the same thing as the variances multiplied by the degrees of freedom.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>ssr2 <span class="ot">&lt;-</span> <span class="fu">var</span>(<span class="fu">fitted</span>(fit)) <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>sse2 <span class="ot">&lt;-</span> <span class="fu">var</span>(<span class="fu">residuals</span>(fit)) <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>sst2 <span class="ot">&lt;-</span> <span class="fu">var</span>(mt_cars<span class="sc">$</span>mpg) <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>ssr2 <span class="sc">/</span> sst2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.8690158</code></pre>
</div>
</div>
<p><span class="math inline">\(R^2\)</span> is also equal to the correlation between the fitted value and observed values, <span class="math inline">\(R^2 = Cor(Y, \hat{Y})^2\)</span>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(fit<span class="sc">$</span>fitted.values, mt_cars<span class="sc">$</span>mpg)<span class="sc">^</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.8690158</code></pre>
</div>
</div>
<p>R-squared is proportional to the the variance in the response, <em>SST</em>. Given a constant percentage error in predictions, a test set with relatively low variation in the reponse will have a lower R-squared. Conversely, test sets with large variation, e.g., housing data with home sale ranging from $60K to $2M may have a large R-squared despite average prediction errors of &gt;$10K.</p>
<p>A close variant of R-squared is the non-parametric Spearman’s rank correlation. This statistic is the correlation of the <em>ranks</em> of the response and the predicted values. It is used when the model goal is ranking.</p>
<p>The <strong>adjusted R-squared</strong> (<span class="math inline">\(\bar{R}^2\)</span>) penalizes the R-squared metric for increasing number of predictors.</p>
<p><span class="math display">\[\bar{R}^2 = 1 - \frac{SSE}{SST} \cdot \frac{n-1}{n-k-1}\]</span></p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>(adj_r2 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> sse<span class="sc">/</span>sst <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>))</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8066423</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)<span class="sc">$</span>adj.r.squared</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8066423</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="rmse-rse-mae" class="level4" data-number="1.6.2.1">
<h4 data-number="1.6.2.1" class="anchored" data-anchor-id="rmse-rse-mae"><span class="header-section-number">1.6.2.1</span> RMSE, RSE, MAE</h4>
<p>The root mean squared error (<strong>RMSE</strong>) is the average prediction error (square root of mean squared error).</p>
<p><span class="math display">\[RMSE = \sqrt{\frac{\sum_{i=1}^n{(y_i - \hat{y}_i)^2}}{n}}\]</span></p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((mt_cars<span class="sc">$</span>mpg <span class="sc">-</span> fit<span class="sc">$</span>fitted.values)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2.146905</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="fu">augment</span>(fit) <span class="sc">%&gt;%</span> yardstick<span class="sc">::</span><span class="fu">rmse</span>(<span class="at">truth =</span> mpg, <span class="at">estimate =</span> .fitted)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 3</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator .estimate</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard        2.15</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The mean squared error of a model with theoretical residual of mean zero and constant variance <span class="math inline">\(\sigma^2\)</span> can be decomposed into the model’s bias and the model’s variance:</p>
<p><span class="math display">\[E[MSE] = \sigma^2 + Bias^2 + Var.\]</span></p>
<p>A model that predicts the response closely will have low bias, but be relatively sensitive to the training data and thus have high variance. A model that predicts the response conservatively (e.g., a simple mean) will have large bias, but be relatively insensitive to nuances in the training data. Here is an example of a simulated sine wave. A model predicting the mean value at the upper and lower levels has low variance, but high bias, and a model of an actual sine wave has low bias and high variance. This is the variance-bias trade-off.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">2</span>, <span class="dv">10</span>, <span class="fl">0.1</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">e =</span> <span class="fu">runif</span>(<span class="dv">81</span>, <span class="sc">-</span>.<span class="dv">5</span>, .<span class="dv">5</span>),</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="fu">sin</span>(x) <span class="sc">+</span> e,</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">Low var, high bias</span><span class="st">`</span> <span class="ot">=</span> zoo<span class="sc">:::</span><span class="fu">rollmean</span>(y, <span class="at">k =</span> <span class="dv">3</span>, <span class="at">fill =</span> <span class="cn">NA</span>, <span class="at">align =</span> <span class="st">"center"</span>),</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">High var, low bias</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">sin</span>(x)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="st">`</span><span class="at">Low var, high bias</span><span class="st">`</span><span class="sc">:</span><span class="st">`</span><span class="at">High var, low bias</span><span class="st">`</span>) <span class="sc">%&gt;%</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> value, <span class="at">color =</span> name), <span class="at">size =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-38-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The residual standard error (<strong>RSE</strong>, or model sigma <span class="math inline">\(\hat{\sigma}\)</span>) is an estimate of the standard deviation of <span class="math inline">\(\epsilon\)</span>. It is roughly the average amount the response deviates from the true regression line.</p>
<p><span class="math display">\[\sigma = \sqrt{\frac{\sum_{i=1}^n{(y_i - \hat{y}_i)^2}}{n-k-1}}\]</span></p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>((mt_cars<span class="sc">$</span>mpg <span class="sc">-</span> fit<span class="sc">$</span>fitted.values)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>))</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2.650197</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># sd is sqrt(sse / (n-1)), sigma = sqrt(sse / (n - k - 1))</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(fit<span class="sc">$</span>residuals) <span class="sc">*</span> <span class="fu">sqrt</span>((n <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>))  </span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2.650197</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)<span class="sc">$</span>sigma </span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2.650197</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="fu">sigma</span>(fit)</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2.650197</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The mean absolute error (<strong>MAE</strong>) is the average absolute prediction arror. It is less sensitive to outliers.</p>
<p><span class="math display">\[MAE = \frac{\sum_{i=1}^n{|y_i - \hat{y}_i|}}{n}\]</span></p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">abs</span>(mt_cars<span class="sc">$</span>mpg <span class="sc">-</span> fit<span class="sc">$</span>fitted.values)) <span class="sc">/</span> n</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.72274</code></pre>
</div>
</div>
<p>These metrics are good for evaluating a model, but less useful for comparing models. The problem is that they tend to improve with additional variables added to the model, even if the improvement is not significant. The following metrics aid model comparison by penalizing added variables.</p>
</section>
<section id="aic-bic" class="level4" data-number="1.6.2.2">
<h4 data-number="1.6.2.2" class="anchored" data-anchor-id="aic-bic"><span class="header-section-number">1.6.2.2</span> AIC, BIC</h4>
<p>Akaike’s Information Criteria (<strong>AIC</strong>) is a penalization metric. The lower the AIC, the better the model.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(fit)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 163.7098</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="co"># AICc corrects AIC for small sample sizes.</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(fit) <span class="sc">+</span> (<span class="dv">2</span> <span class="sc">*</span> k <span class="sc">*</span> (k <span class="sc">+</span> <span class="dv">1</span>)) <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 174.186</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The Bayesian information criteria (<strong>BIC</strong>) is like AIC, but with a stronger penalty for additional variables.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">BIC</span>(fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 181.2986</code></pre>
</div>
</div>
<p><code>broom::glance()</code> calculates many validation metrics. Compare the full model to a reduced model without <code>cyl</code>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">glance</span>(fit) <span class="sc">%&gt;%</span> <span class="fu">select</span>(adj.r.squared, sigma, AIC, BIC, p.value)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 5
  adj.r.squared sigma   AIC   BIC     p.value
          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;
1         0.807  2.65  164.  181. 0.000000379</code></pre>
</div>
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">glance</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">-</span> cyl, mt_cars)) <span class="sc">%&gt;%</span> <span class="fu">select</span>(adj.r.squared, sigma, AIC, BIC, p.value)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 5
  adj.r.squared sigma   AIC   BIC      p.value
          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;
1         0.815  2.59  162.  178. 0.0000000903</code></pre>
</div>
</div>
<p>The adjusted R2 increased and AIC and BIC decreased, meaning the full model is less efficient at explaining the variability in the response value. The residual standard error <code>sigma</code> is smaller for the reduced model. Finally, the <em>F</em> statistic p-value is smaller for the reduced model, meaning the reduced model is statistically more significant.</p>
<p>Note that these regression metrics are all internal measures, that is they have been computed on the training dataset, not the test dataset.</p>
</section>
</section>
<section id="cross-validation" class="level3" data-number="1.6.3">
<h3 data-number="1.6.3" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">1.6.3</span> Cross-Validation</h3>
<p>Cross-validation is a set of methods for measuring the performance of a predictive model on a test dataset. The main measures of prediction performance are R2, RMSE and MAE.</p>
<section id="validation-set" class="level4" data-number="1.6.3.1">
<h4 data-number="1.6.3.1" class="anchored" data-anchor-id="validation-set"><span class="header-section-number">1.6.3.1</span> Validation Set</h4>
<p>To perform validation set cross validation, randomly split the data into a training data set and a test data set. Fit models to the training data set, then predict values with the validation set. The model that produces the best prediction performance is the preferred model.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>mt_cars_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(mt_cars, <span class="at">prop =</span> <span class="fl">0.7</span>, <span class="at">strata =</span> mpg)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>mt_cars_training <span class="ot">&lt;-</span> mt_cars_split <span class="sc">%&gt;%</span> <span class="fu">training</span>()</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>mt_cars_testing <span class="ot">&lt;-</span> mt_cars_split <span class="sc">%&gt;%</span> <span class="fu">testing</span>()</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>fit_validation <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"lm"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mt_cars_training) <span class="sc">%&gt;%</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(<span class="at">new_data =</span> mt_cars_testing) <span class="sc">%&gt;%</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(mt_cars_testing)</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>fit_validation <span class="sc">%&gt;%</span> <span class="fu">rmse</span>(<span class="at">truth =</span> mpg, <span class="at">estimate =</span> .pred)</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 3</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator .estimate</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rmse    standard        2.28</span></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>fit_validation <span class="sc">%&gt;%</span> <span class="fu">rsq</span>(<span class="at">truth =</span> mpg, <span class="at">estimate =</span> .pred)</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 1 × 3</span></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a><span class="do">##   .metric .estimator .estimate</span></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a><span class="do">##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 rsq     standard       0.802</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Alternatively, <code>last_fit()</code> i) splits train/test, ii) fits, and iii) predicts.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>fit_validation_2 <span class="ot">&lt;-</span> <span class="fu">linear_reg</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"lm"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(mpg <span class="sc">~</span> ., <span class="at">split =</span> mt_cars_split)</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>fit_validation_2 <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>fit_validation_2 <span class="sc">%&gt;%</span> <span class="fu">collect_predictions</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The validation set method is only useful when you have a large data set to partition. A second disadvantage is that building a model on a fraction of the data leaves out information. The test error will vary with which observations are included in the training set.</p>
</section>
<section id="loocv" class="level4" data-number="1.6.3.2">
<h4 data-number="1.6.3.2" class="anchored" data-anchor-id="loocv"><span class="header-section-number">1.6.3.2</span> LOOCV</h4>
<p>Leave one out cross validation (LOOCV) works by successively modeling with training sets leaving out one data point, then averaging the prediction errors.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> ., </span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> d.train[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>],</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">"lm"</span>,</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"LOOCV"</span>))</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(m2)</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(m2, <span class="at">newdata =</span> d.test), </span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">obs =</span> d.test<span class="sc">$</span>mpg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This method isn’t perfect either. It repeats as many times as there are data points, so the execution time may be long. LOOCV is also sensitive to outliers.</p>
</section>
<section id="k-fold-cross-validation" class="level4" data-number="1.6.3.3">
<h4 data-number="1.6.3.3" class="anchored" data-anchor-id="k-fold-cross-validation"><span class="header-section-number">1.6.3.3</span> K-fold Cross-Validation</h4>
<p>K-fold cross-validation splits the dataset into <em>k</em> folds (subsets), then uses <em>k-1</em> of the folds for a training set and the remaining fold for a test set, then repeats for all permutations of k taken k-1 at a time. E.g., 3-fold cross-validation will partition the data into sets A, B, and C, then create train/test splits of [AB, C], [AC, B], and [BC, A].</p>
<p>K-fold cross-validation is less computationally expensive than LOOCV, and often yields more accurate test error rate estimates. What is the right value of k? The lower is <em>k</em> the more biased the estimates; the higher is <em>k</em> the larger the estimate variability. At the extremes <em>k</em> = 2 is the validation set method, and <em>k = n</em> is the LOOCV method. In practice, one typically performs k-fold cross-validation using k = 5 or k = 10 because these values have been empirically shown to balence bias and variance.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> ., </span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> d.train[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>],</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">"lm"</span>,</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"cv"</span>,</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">number =</span> <span class="dv">5</span>))</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(m3)</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(m3, <span class="at">newdata =</span> d.test), </span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">obs =</span> d.test<span class="sc">$</span>mpg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="repeated-k-fold-cv" class="level4" data-number="1.6.3.4">
<h4 data-number="1.6.3.4" class="anchored" data-anchor-id="repeated-k-fold-cv"><span class="header-section-number">1.6.3.4</span> Repeated K-fold CV</h4>
<p>You can also perform k-fold cross-validation multiple times and average the results. Specify <code>method = "repeatedcv"</code> and <code>repeats = 3</code> in the <code>trainControl</code> object for three repeats.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>m4 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> ., </span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> d.train[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>],</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">"lm"</span>,</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"repeatedcv"</span>,</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">number =</span> <span class="dv">5</span>,</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">repeats =</span> <span class="dv">3</span>))</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(m4)</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(m4, <span class="at">newdata =</span> d.test), </span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>             <span class="at">obs =</span> d.test<span class="sc">$</span>mpg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="bootstrapping" class="level4" data-number="1.6.3.5">
<h4 data-number="1.6.3.5" class="anchored" data-anchor-id="bootstrapping"><span class="header-section-number">1.6.3.5</span> Bootstrapping</h4>
<p>Bootstrapping randomly selects a sample of n observations with replacement from the original dataset to evaluate the model. The procedure is repeated many times.</p>
<p>Specify <code>method = "boot"</code> and <code>number = 100</code> to perform 100 bootstrap samples.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>m5 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> ., </span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> d.train[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>],</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">"lm"</span>,</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">"boot"</span>,</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">number =</span> <span class="dv">100</span>))</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(m5)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(m5, <span class="at">newdata =</span> d.test), </span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">obs =</span> d.test<span class="sc">$</span>mpg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="gain-curve" class="level3" data-number="1.6.4">
<h3 data-number="1.6.4" class="anchored" data-anchor-id="gain-curve"><span class="header-section-number">1.6.4</span> Gain Curve</h3>
<p>For supervised learning purposes, a visual way to evaluate a regression model is with the gain curve. This visualization compares a predictive model score to an actual outcome (either binary (0/1) or continuous). The gain curve plot measures how well the model score sorts the data compared to the true outcome value. The x-axis is the fraction of items seen when sorted by score, and the y-axis is the cumulative summed true outcome when sorted by score. For comparison, GainCurvePlot also plots the “wizard curve”: the gain curve when the data is sorted according to its true outcome. A relative Gini score close to 1 means the model sorts responses well.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>fit_validation <span class="sc">%&gt;%</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>  WVPlots<span class="sc">::</span><span class="fu">GainCurvePlot</span>(<span class="at">xvar =</span> <span class="st">".pred"</span>, <span class="at">truthVar =</span> <span class="st">"mpg"</span>, <span class="at">title =</span> <span class="st">"Model Gain Curve"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="01-linear_regression_files/figure-html/unnamed-chunk-50-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Molner2020" class="csl-entry" role="listitem">
Molnar, Christoph. 2020. <em>Interpretable Machine Learning</em>. <a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Penn State University, STAT 501, Lesson 12: Multicollinearity &amp; Other Regression Pitfalls. <a href="https://newonlinecourses.science.psu.edu/stat501/lesson/12">https://newonlinecourses.science.psu.edu/stat501/lesson/12</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>STHDA. Bootstrap Resampling Essentials in R. <a href="http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/">http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Help with matrix algebra in r notes at <a href="https://www.dummies.com/programming/r/how-to-do-matrix-arithmetic-in-r/">R for Dummies</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Intro">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Intro</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./02-generalized_linear_models.html" class="pagination-link" aria-label="Generalized Linear Models (GLM)">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Generalized Linear Models (GLM)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb71" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Ordinary Least Squares</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r include=FALSE}</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="in">library(tidyverse)</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a><span class="in">library(tidymodels)</span></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>These notes rely on PSU^<span class="co">[</span><span class="ot">Penn State University, STAT 501, Lesson 12: Multicollinearity &amp; Other Regression Pitfalls. [https://newonlinecourses.science.psu.edu/stat501/lesson/12](https://newonlinecourses.science.psu.edu/stat501/lesson/12).</span><span class="co">]</span>, STHDA^<span class="co">[</span><span class="ot">STHDA.  Bootstrap Resampling Essentials in R.  [http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/](http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/)</span><span class="co">]</span>, and <span class="co">[</span><span class="ot">@Molner2020</span><span class="co">]</span>,</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>The population regression model $E(Y) = X \beta$ summarizes the trend between the predictors and the mean responses.  The individual responses are assumed to be normally distributed about the population regression, $y_i = X_i \beta + \epsilon_i$ with varying mean, but constant variance, $y_i \sim N(\mu_i, \sigma^2).$  Equivalently, the model presumes a linear relationship between $y$ and $X$ with residuals $\epsilon$ that are independent normal random variables with mean zero and constant variance $\sigma^2$.  Estimate the population regression model coefficients as $\hat{y} = X \hat{\beta}$, and the population variance as $\hat{\sigma}^2$.  The most common method of estimating the $\beta$ coefficients and $\sigma$ is ordinary least squares (OLS).  OLS minimizes the sum of squared residuals from a random sample.  The predicted values vary about the actual value, $e_i = y_i - \hat{y}_i$, where $\hat{y}_i = X_i \hat{\beta}$.</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>The OLS model is the best linear unbiased estimator (BLUE) if the residuals are independent random variables normally distributed with mean zero and constant variance. Recall these conditions with the LINE pneumonic: **L**inear, **I**ndependent, **N**ormal, and **E**qual.</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>**Linearity**. The explanatory variables are each linearly related to the response variable: $E(\epsilon | X_j) = 0$.  </span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>**Independence**. The residuals are unrelated to each other. Independence is violated by repeated measurements and temporal regressors. </span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>**Normality**. The residuals are normally distributed: $\epsilon|X \sim N(0, \sigma^2I)$.  </span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>**Equal Variances**. The variance of the residuals is constant (homoscedasticity): $E(\epsilon \epsilon' | X) = \sigma^2I$</span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>Additionally, there should be little **multicollinearity** among the explanatory variables.</span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a>If the modeling objective is inference, fit the model with the full data set, evaluate the model fit, and interpret the parameter estimates. If the objective is prediction, fit the model with a partitioned train/test data set, cross-validate the fit with the test set, then deploy the model. Both are covered below.</span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## Parameter Estimation</span></span>
<span id="cb71-27"><a href="#cb71-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-28"><a href="#cb71-28" aria-hidden="true" tabindex="-1"></a>There are two model parameters to estimate: $\hat{\beta}$ estimates the coefficient vector $\beta$, and $\hat{\sigma}$ estimates the variance of the residuals along the regression line. Derive $\hat{\beta}$ by minimizing the sum of squared residuals $SSE = (y - X \hat{\beta})' (y - X \hat{\beta})$.  The result is</span>
<span id="cb71-29"><a href="#cb71-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-30"><a href="#cb71-30" aria-hidden="true" tabindex="-1"></a>$$\hat{\beta} = (X'X)^{-1}X'y.$$</span>
<span id="cb71-31"><a href="#cb71-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-32"><a href="#cb71-32" aria-hidden="true" tabindex="-1"></a>The residual standard error (RSE) estimates the sample deviation around the population regression line. *(Think of each value of $X$ along the regression line as a subpopulation with mean $y_i$ and variance $\sigma^2$.  This variance is assumed to be the same for all $X$.)* </span>
<span id="cb71-33"><a href="#cb71-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-34"><a href="#cb71-34" aria-hidden="true" tabindex="-1"></a>$$\hat{\sigma} = \sqrt{(n-k-1)^{-1} e'e}.$$</span>
<span id="cb71-35"><a href="#cb71-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-36"><a href="#cb71-36" aria-hidden="true" tabindex="-1"></a>The standard error for the coefficient estimators is the square root of the error variance divided by $(X'X)$.</span>
<span id="cb71-37"><a href="#cb71-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-38"><a href="#cb71-38" aria-hidden="true" tabindex="-1"></a>$$SE(\hat{\beta}) = \sqrt{\hat{\sigma}^2 (X'X)^{-1}}.$$</span>
<span id="cb71-39"><a href="#cb71-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-40"><a href="#cb71-40" aria-hidden="true" tabindex="-1"></a>Linear regression is related to correlation. The coefficient estimator for $\beta_1$ in simple linear regression is equal to the Pearson correlation coefficient multiplied by the ratio of the standard deviations of $y$ and $x$.</span>
<span id="cb71-41"><a href="#cb71-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-42"><a href="#cb71-42" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb71-43"><a href="#cb71-43" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb71-44"><a href="#cb71-44" aria-hidden="true" tabindex="-1"></a>\hat{\beta}_1 &amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2} <span class="sc">\\</span></span>
<span id="cb71-45"><a href="#cb71-45" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{Cov(X,Y)}{Var(X)} <span class="sc">\\</span></span>
<span id="cb71-46"><a href="#cb71-46" aria-hidden="true" tabindex="-1"></a>&amp;= r \cdot \frac{\sigma_Y}{\sigma_X}</span>
<span id="cb71-47"><a href="#cb71-47" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb71-48"><a href="#cb71-48" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb71-49"><a href="#cb71-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-50"><a href="#cb71-50" aria-hidden="true" tabindex="-1"></a>The Pearson correlation is the ratio of the covariance of $X$ and $Y$ and product of their standard deviations.</span>
<span id="cb71-51"><a href="#cb71-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-52"><a href="#cb71-52" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb71-53"><a href="#cb71-53" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb71-54"><a href="#cb71-54" aria-hidden="true" tabindex="-1"></a>r &amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n(x_i - \bar{x})^2}  \sqrt{\sum_{i=1}^n(y_i - \bar{y})^2}} <span class="sc">\\</span></span>
<span id="cb71-55"><a href="#cb71-55" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}</span>
<span id="cb71-56"><a href="#cb71-56" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb71-57"><a href="#cb71-57" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb71-58"><a href="#cb71-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-59"><a href="#cb71-59" aria-hidden="true" tabindex="-1"></a>The formula for $\hat{\beta}$ is almost identical to that of $r$ with the exception that the denominator is the variance of $X$ rather than the product of the standard deviations. The Pearson correlation is the _standardized_ linear relationship. Pearson correlation measures the strength and direction of the linear relationship between $X$ and $Y$ in a standardized form.</span>
<span id="cb71-60"><a href="#cb71-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-61"><a href="#cb71-61" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $r = 0$, then $\beta_1 = 0$ and there is no linear relationship.</span>
<span id="cb71-62"><a href="#cb71-62" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>if $r = 1$ or $r = -1$, then the relationship is perfectly linear, and $\beta_1$ measures the relative _scale_ of $Y$ and $X$.</span>
<span id="cb71-63"><a href="#cb71-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-64"><a href="#cb71-64" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example {-}</span></span>
<span id="cb71-65"><a href="#cb71-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-66"><a href="#cb71-66" aria-hidden="true" tabindex="-1"></a>Dataset <span class="in">`mtcars`</span> contains response variable fuel consumption <span class="in">`mpg`</span> and 10 aspects of automobile design and performance for 32 automobiles.  What is the relationship between the response and its predictors?</span>
<span id="cb71-67"><a href="#cb71-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-68"><a href="#cb71-68" aria-hidden="true" tabindex="-1"></a><span class="in">```{r message=FALSE, warning=FALSE}</span></span>
<span id="cb71-69"><a href="#cb71-69" aria-hidden="true" tabindex="-1"></a><span class="in">data("mtcars")</span></span>
<span id="cb71-70"><a href="#cb71-70" aria-hidden="true" tabindex="-1"></a><span class="in">mt_cars &lt;- mtcars %&gt;%</span></span>
<span id="cb71-71"><a href="#cb71-71" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(</span></span>
<span id="cb71-72"><a href="#cb71-72" aria-hidden="true" tabindex="-1"></a><span class="in">    vs = factor(vs, levels = c(0, 1), labels = c("V", "S")),</span></span>
<span id="cb71-73"><a href="#cb71-73" aria-hidden="true" tabindex="-1"></a><span class="in">    am = factor(am, levels = c(0, 1), labels = c("automatic", "manual"))</span></span>
<span id="cb71-74"><a href="#cb71-74" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb71-75"><a href="#cb71-75" aria-hidden="true" tabindex="-1"></a><span class="in">glimpse(mt_cars)</span></span>
<span id="cb71-76"><a href="#cb71-76" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-77"><a href="#cb71-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-78"><a href="#cb71-78" aria-hidden="true" tabindex="-1"></a>The correlation matrix of the numeric variables shows <span class="in">`wt`</span> has the strongest association with <span class="in">`mpg`</span> (*r* = -0.87) followed by `disp` (*r* = -0.85) and `hp` (*r* = -0.78). `drat` is moderately correlated (*r* = 0.68), and `qsec` is weakly correlated (*r* = 0.42). Many predictors are strongly correlated with each other.</span>
<span id="cb71-79"><a href="#cb71-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-80"><a href="#cb71-80" aria-hidden="true" tabindex="-1"></a><span class="in">```{r message=FALSE, warning=FALSE}</span></span>
<span id="cb71-81"><a href="#cb71-81" aria-hidden="true" tabindex="-1"></a><span class="in">mt_cars %&gt;%</span></span>
<span id="cb71-82"><a href="#cb71-82" aria-hidden="true" tabindex="-1"></a><span class="in">  select(where(is.numeric)) %&gt;%</span></span>
<span id="cb71-83"><a href="#cb71-83" aria-hidden="true" tabindex="-1"></a><span class="in">  cor() %&gt;%</span></span>
<span id="cb71-84"><a href="#cb71-84" aria-hidden="true" tabindex="-1"></a><span class="in">  corrplot::corrplot(type = "upper", method = "number")</span></span>
<span id="cb71-85"><a href="#cb71-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-86"><a href="#cb71-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-87"><a href="#cb71-87" aria-hidden="true" tabindex="-1"></a>Boxplots of the categorical variables reveal differences in levels.</span>
<span id="cb71-88"><a href="#cb71-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-91"><a href="#cb71-91" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-92"><a href="#cb71-92" aria-hidden="true" tabindex="-1"></a>mt_cars <span class="sc">%&gt;%</span></span>
<span id="cb71-93"><a href="#cb71-93" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(mpg, <span class="fu">where</span>(is.factor)) <span class="sc">%&gt;%</span></span>
<span id="cb71-94"><a href="#cb71-94" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.factor), as.character)) <span class="sc">%&gt;%</span></span>
<span id="cb71-95"><a href="#cb71-95" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="sc">-</span>mpg) <span class="sc">%&gt;%</span></span>
<span id="cb71-96"><a href="#cb71-96" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> mpg)) <span class="sc">+</span></span>
<span id="cb71-97"><a href="#cb71-97" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb71-98"><a href="#cb71-98" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(name), <span class="at">scales =</span> <span class="st">"free_x"</span>)</span>
<span id="cb71-99"><a href="#cb71-99" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-100"><a href="#cb71-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-101"><a href="#cb71-101" aria-hidden="true" tabindex="-1"></a>Fit a population model to the predictors.</span>
<span id="cb71-102"><a href="#cb71-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-105"><a href="#cb71-105" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-106"><a href="#cb71-106" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mt_cars)</span>
<span id="cb71-107"><a href="#cb71-107" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span>
<span id="cb71-108"><a href="#cb71-108" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-109"><a href="#cb71-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-110"><a href="#cb71-110" aria-hidden="true" tabindex="-1"></a><span class="in">`summary()`</span> shows $\hat{\beta}$ as <span class="in">`Estimate`</span>, $SE({\hat{\beta}})$ as <span class="in">`Std. Error`</span>, and $\hat{\sigma}$ as <span class="in">`Residual standard error`</span>.  You can manually perform these calculations using matrix algebra^<span class="co">[</span><span class="ot">Help with matrix algebra in r notes at [R for Dummies](https://www.dummies.com/programming/r/how-to-do-matrix-arithmetic-in-r/)</span><span class="co">]</span>.  Recall, $\hat{\beta} = (X'X)^{-1}X'y$.</span>
<span id="cb71-111"><a href="#cb71-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-114"><a href="#cb71-114" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-115"><a href="#cb71-115" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(fit)</span>
<span id="cb71-116"><a href="#cb71-116" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> mt_cars<span class="sc">$</span>mpg</span>
<span id="cb71-117"><a href="#cb71-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-118"><a href="#cb71-118" aria-hidden="true" tabindex="-1"></a>(beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span>
<span id="cb71-119"><a href="#cb71-119" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-120"><a href="#cb71-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-121"><a href="#cb71-121" aria-hidden="true" tabindex="-1"></a>The residual standard error is $\hat{\sigma} = \sqrt{(n-k-1)^{-1} \hat{e}'\hat{e}}$.</span>
<span id="cb71-122"><a href="#cb71-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-123"><a href="#cb71-123" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb71-124"><a href="#cb71-124" aria-hidden="true" tabindex="-1"></a><span class="in">n &lt;- nrow(X)</span></span>
<span id="cb71-125"><a href="#cb71-125" aria-hidden="true" tabindex="-1"></a><span class="in">k &lt;- ncol(X) - 1  # exclude the intercept term</span></span>
<span id="cb71-126"><a href="#cb71-126" aria-hidden="true" tabindex="-1"></a><span class="in">y_hat &lt;- X %*% beta_hat</span></span>
<span id="cb71-127"><a href="#cb71-127" aria-hidden="true" tabindex="-1"></a><span class="in">sse &lt;- sum((y - y_hat)^2)</span></span>
<span id="cb71-128"><a href="#cb71-128" aria-hidden="true" tabindex="-1"></a><span class="in">(dof &lt;- n - k - 1)</span></span>
<span id="cb71-129"><a href="#cb71-129" aria-hidden="true" tabindex="-1"></a><span class="in">(rse &lt;- sqrt(sse / dof))</span></span>
<span id="cb71-130"><a href="#cb71-130" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-131"><a href="#cb71-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-132"><a href="#cb71-132" aria-hidden="true" tabindex="-1"></a>The standard errors of the coefficients are $SE(\hat{\beta}) = \sqrt{\hat{\sigma}^2 (X'X)^{-1}}$.</span>
<span id="cb71-133"><a href="#cb71-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-136"><a href="#cb71-136" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-137"><a href="#cb71-137" aria-hidden="true" tabindex="-1"></a>(se_beta_hat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(rse<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X))))</span>
<span id="cb71-138"><a href="#cb71-138" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-139"><a href="#cb71-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-140"><a href="#cb71-140" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Assumptions</span></span>
<span id="cb71-141"><a href="#cb71-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-142"><a href="#cb71-142" aria-hidden="true" tabindex="-1"></a>The linear regression model assumes the relationship between the predictors and the response is linear and the residuals are independent random variables normally distributed with mean zero and constant variance. Additionally, you will want to check for multicollinearity in the predictors because it can produce unreliable coefficient estimates and predicted values. The <span class="in">`plot()`</span> function produces a set of diagnostic plots to test the assumptions.</span>
<span id="cb71-143"><a href="#cb71-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-144"><a href="#cb71-144" aria-hidden="true" tabindex="-1"></a>Use the **Residuals vs Fitted** plot, $e \sim \hat{Y}$, to test the linearity and equal error variances assumptions. The plot also identifies outliers. The polynomial trend line should show that the residuals vary around $e = 0$ in a straight horizontal line (linearity). The residuals should have random scatter in a band of constant width around 0, and no fan shape at the low and high ends (equal variances). All tests and intervals are sensitive to the equal variances condition. The plot also reveals multicollinearity. If the residuals and fitted values are correlated, multicollinearity may be a problem.</span>
<span id="cb71-145"><a href="#cb71-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-146"><a href="#cb71-146" aria-hidden="true" tabindex="-1"></a>Use the **Q-Q Residuals** plot (aka, residuals normal probability plot) to test the normality assumption. It plots the theoretical percentiles of the normal distribution versus the observed sample percentiles. It should be approximately linear with no bow-shaped deviations. Sometimes this normality check fails when the linearity check fails, so check for linearity first. Parameter estimation is not sensitive to the normality assumption, but prediction intervals are.</span>
<span id="cb71-147"><a href="#cb71-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-148"><a href="#cb71-148" aria-hidden="true" tabindex="-1"></a>Use the **Scale-Location** plot, $\sqrt{e / sd(e)} \sim \hat{y}$, to test for equal variance (aka, homoscedasticity). The square root of the absolute value of the residuals should be spread equally along a horizontal line.</span>
<span id="cb71-149"><a href="#cb71-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-150"><a href="#cb71-150" aria-hidden="true" tabindex="-1"></a>The **Residuals vs Leverage** plot identifies influential observations. The standardized residuals should fall within the 95% probability band.</span>
<span id="cb71-151"><a href="#cb71-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-152"><a href="#cb71-152" aria-hidden="true" tabindex="-1"></a><span class="in">```{r warning=FALSE, message=FALSE}</span></span>
<span id="cb71-153"><a href="#cb71-153" aria-hidden="true" tabindex="-1"></a><span class="in">par(mfrow = c(2, 2))</span></span>
<span id="cb71-154"><a href="#cb71-154" aria-hidden="true" tabindex="-1"></a><span class="in">plot(fit, labels.id = NULL)</span></span>
<span id="cb71-155"><a href="#cb71-155" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-156"><a href="#cb71-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-157"><a href="#cb71-157" aria-hidden="true" tabindex="-1"></a>These diagnostics are usually sufficient. If any of the diagnostics fail, you can re-specify the model or transform the data.</span>
<span id="cb71-158"><a href="#cb71-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-159"><a href="#cb71-159" aria-hidden="true" tabindex="-1"></a>Consider the linearity assumption. The explanatory variables should _each_ be linearly related to the response variable: $E(\epsilon | X_j) = 0$. The **Residuals vs Fitted** plot tested this overall. A curved pattern in the residuals indicates a curvature in the relationship between the response and the predictor that is not explained by the model. There are other tests of linearity. The full list:</span>
<span id="cb71-160"><a href="#cb71-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-161"><a href="#cb71-161" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Residuals vs fits plot $(e \sim \hat{Y})$ should randomly vary around 0. </span>
<span id="cb71-162"><a href="#cb71-162" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Observed vs fits plot $(Y \sim \hat{Y})$ should be symmetric along the 45-degree line.  </span>
<span id="cb71-163"><a href="#cb71-163" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Each $(Y \sim X_j )$ plot should have correlation $\rho \sim 1$.  </span>
<span id="cb71-164"><a href="#cb71-164" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Each $(e \sim X_j)$ plot should exhibit no pattern.</span>
<span id="cb71-165"><a href="#cb71-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-166"><a href="#cb71-166" aria-hidden="true" tabindex="-1"></a>The diagnostic plots above look pretty good except for some evidence of heteroskedasticity at the upper end of predicted values. You might drill into the linearity condition to start. Start with plots of $(Y \sim X_j )$ - are the correlation coefficients ~1? Yes, all but <span class="in">`qsec`</span> have correlations over .6.</span>
<span id="cb71-167"><a href="#cb71-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-170"><a href="#cb71-170" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-171"><a href="#cb71-171" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> mt_cars <span class="sc">%&gt;%</span></span>
<span id="cb71-172"><a href="#cb71-172" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span></span>
<span id="cb71-173"><a href="#cb71-173" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>mpg)</span>
<span id="cb71-174"><a href="#cb71-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-175"><a href="#cb71-175" aria-hidden="true" tabindex="-1"></a>x_cor <span class="ot">&lt;-</span> x <span class="sc">%&gt;%</span> </span>
<span id="cb71-176"><a href="#cb71-176" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">.by =</span> name) <span class="sc">%&gt;%</span></span>
<span id="cb71-177"><a href="#cb71-177" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">val_corr =</span> <span class="fu">map_dbl</span>(data, <span class="sc">~</span><span class="fu">cor</span>(.<span class="sc">$</span>mpg, .<span class="sc">$</span>value)))</span>
<span id="cb71-178"><a href="#cb71-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-179"><a href="#cb71-179" aria-hidden="true" tabindex="-1"></a>x <span class="sc">%&gt;%</span></span>
<span id="cb71-180"><a href="#cb71-180" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(x_cor, <span class="at">by =</span> <span class="fu">join_by</span>(name)) <span class="sc">%&gt;%</span></span>
<span id="cb71-181"><a href="#cb71-181" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">name =</span> glue<span class="sc">::</span><span class="fu">glue</span>(<span class="st">"{name}, r = {comma(val_corr, .01)}"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb71-182"><a href="#cb71-182" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> mpg)) <span class="sc">+</span></span>
<span id="cb71-183"><a href="#cb71-183" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb71-184"><a href="#cb71-184" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">formula =</span> <span class="st">"y ~ x"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb71-185"><a href="#cb71-185" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(name), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb71-186"><a href="#cb71-186" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Response vs Predictor"</span>)</span>
<span id="cb71-187"><a href="#cb71-187" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-188"><a href="#cb71-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-189"><a href="#cb71-189" aria-hidden="true" tabindex="-1"></a>How about $(e \sim X_j)$? <span class="in">`disp`</span> has a wave pattern; <span class="in">`hp`</span> is u-shaped; <span class="in">`qsec`</span> has increase variance in the middle.</span>
<span id="cb71-190"><a href="#cb71-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-193"><a href="#cb71-193" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-194"><a href="#cb71-194" aria-hidden="true" tabindex="-1"></a>mt_cars <span class="sc">%&gt;%</span></span>
<span id="cb71-195"><a href="#cb71-195" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span></span>
<span id="cb71-196"><a href="#cb71-196" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>mpg) <span class="sc">%&gt;%</span> </span>
<span id="cb71-197"><a href="#cb71-197" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">.by =</span> name) <span class="sc">%&gt;%</span></span>
<span id="cb71-198"><a href="#cb71-198" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb71-199"><a href="#cb71-199" aria-hidden="true" tabindex="-1"></a>    <span class="at">fit =</span> <span class="fu">map</span>(data, <span class="sc">~</span><span class="fu">lm</span>(mpg <span class="sc">~</span> value, <span class="at">data =</span> .)),</span>
<span id="cb71-200"><a href="#cb71-200" aria-hidden="true" tabindex="-1"></a>    <span class="at">aug =</span> <span class="fu">map</span>(fit, broom<span class="sc">::</span>augment)</span>
<span id="cb71-201"><a href="#cb71-201" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb71-202"><a href="#cb71-202" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(aug) <span class="sc">%&gt;%</span></span>
<span id="cb71-203"><a href="#cb71-203" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> .resid)) <span class="sc">+</span></span>
<span id="cb71-204"><a href="#cb71-204" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb71-205"><a href="#cb71-205" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">formula =</span> <span class="st">"y ~ x"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb71-206"><a href="#cb71-206" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(name), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb71-207"><a href="#cb71-207" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Residuals vs Predictor"</span>)</span>
<span id="cb71-208"><a href="#cb71-208" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-209"><a href="#cb71-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-210"><a href="#cb71-210" aria-hidden="true" tabindex="-1"></a>If the linearity condition fails, change the functional form of the model with non-linear transformations of the explanatory variables. A common way to do this is with Box-Cox transformations. </span>
<span id="cb71-211"><a href="#cb71-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-212"><a href="#cb71-212" aria-hidden="true" tabindex="-1"></a>$$w_t = \begin{cases} \begin{array}{l} log(y_t) \quad \quad \lambda = 0 <span class="sc">\\</span></span>
<span id="cb71-213"><a href="#cb71-213" aria-hidden="true" tabindex="-1"></a>(y_t^\lambda - 1) / \lambda \quad \text{otherwise} \end{array} \end{cases}$$</span>
<span id="cb71-214"><a href="#cb71-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-215"><a href="#cb71-215" aria-hidden="true" tabindex="-1"></a>$\lambda$ can take any value, but values near the following yield familiar transformations.</span>
<span id="cb71-216"><a href="#cb71-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-217"><a href="#cb71-217" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\lambda = 1$ yields no substantive transformation.  </span>
<span id="cb71-218"><a href="#cb71-218" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\lambda = 0.5$ is a square root plus linear transformation.</span>
<span id="cb71-219"><a href="#cb71-219" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\lambda = 0.333$ is a cube root plus linear transformation.</span>
<span id="cb71-220"><a href="#cb71-220" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\lambda = 0$ is a natural log transformation.</span>
<span id="cb71-221"><a href="#cb71-221" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\lambda = -1$ is an inverse transformation.</span>
<span id="cb71-222"><a href="#cb71-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-223"><a href="#cb71-223" aria-hidden="true" tabindex="-1"></a>A common source of non-linearity is skewed response or predictor variables (see discussion <span class="co">[</span><span class="ot">here</span><span class="co">](https://blog.minitab.com/blog/applying-statistics-in-quality-projects/how-could-you-benefit-from-a-box-cox-transformation)</span>). <span class="in">`mt_cars`</span> has some skewed variables.</span>
<span id="cb71-224"><a href="#cb71-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-225"><a href="#cb71-225" aria-hidden="true" tabindex="-1"></a><span class="in">```{r message=FALSE}</span></span>
<span id="cb71-226"><a href="#cb71-226" aria-hidden="true" tabindex="-1"></a><span class="in">mt_cars %&gt;%</span></span>
<span id="cb71-227"><a href="#cb71-227" aria-hidden="true" tabindex="-1"></a><span class="in">  select(where(is.numeric)) %&gt;%</span></span>
<span id="cb71-228"><a href="#cb71-228" aria-hidden="true" tabindex="-1"></a><span class="in">  pivot_longer(cols = everything()) %&gt;%</span></span>
<span id="cb71-229"><a href="#cb71-229" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(aes(x = value)) +</span></span>
<span id="cb71-230"><a href="#cb71-230" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_histogram() +</span></span>
<span id="cb71-231"><a href="#cb71-231" aria-hidden="true" tabindex="-1"></a><span class="in">  facet_wrap(facets = vars(name), scales = "free_x") +</span></span>
<span id="cb71-232"><a href="#cb71-232" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(title = "Histogram of numeric vars")</span></span>
<span id="cb71-233"><a href="#cb71-233" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-234"><a href="#cb71-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-235"><a href="#cb71-235" aria-hidden="true" tabindex="-1"></a><span class="in">`hp`</span> has the most skew, <span class="in">`cyl`</span> the least.</span>
<span id="cb71-236"><a href="#cb71-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-239"><a href="#cb71-239" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-240"><a href="#cb71-240" aria-hidden="true" tabindex="-1"></a>mt_cars <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span> moments<span class="sc">::</span><span class="fu">skewness</span>()</span>
<span id="cb71-241"><a href="#cb71-241" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-242"><a href="#cb71-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-243"><a href="#cb71-243" aria-hidden="true" tabindex="-1"></a>Compare the residuals vs fitted plots for each variable. Here is <span class="in">`hp`</span>.</span>
<span id="cb71-244"><a href="#cb71-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-247"><a href="#cb71-247" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-248"><a href="#cb71-248" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(mpg <span class="sc">~</span> hp, <span class="at">data =</span> mt_cars) <span class="sc">%&gt;%</span> <span class="fu">plot</span>(<span class="at">which =</span> <span class="dv">1</span>, <span class="at">main =</span> <span class="st">"hp"</span>)</span>
<span id="cb71-249"><a href="#cb71-249" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-250"><a href="#cb71-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-251"><a href="#cb71-251" aria-hidden="true" tabindex="-1"></a>and here is <span class="in">`cyl`</span></span>
<span id="cb71-252"><a href="#cb71-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-255"><a href="#cb71-255" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-256"><a href="#cb71-256" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(mpg <span class="sc">~</span> cyl, <span class="at">data =</span> mt_cars) <span class="sc">%&gt;%</span> <span class="fu">plot</span>(<span class="at">which =</span> <span class="dv">1</span>, <span class="at">main =</span> <span class="st">"drat"</span>)</span>
<span id="cb71-257"><a href="#cb71-257" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-258"><a href="#cb71-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-259"><a href="#cb71-259" aria-hidden="true" tabindex="-1"></a>Box-Cox transform the numeric variables. The skew is reduced.</span>
<span id="cb71-260"><a href="#cb71-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-263"><a href="#cb71-263" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-264"><a href="#cb71-264" aria-hidden="true" tabindex="-1"></a>rec <span class="ot">&lt;-</span> <span class="fu">recipe</span>(<span class="sc">~</span>., <span class="at">data =</span> mt_cars)</span>
<span id="cb71-265"><a href="#cb71-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-266"><a href="#cb71-266" aria-hidden="true" tabindex="-1"></a>mt_cars_boxcox <span class="ot">&lt;-</span> rec <span class="sc">%&gt;%</span> </span>
<span id="cb71-267"><a href="#cb71-267" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_BoxCox</span>(<span class="fu">all_numeric</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb71-268"><a href="#cb71-268" aria-hidden="true" tabindex="-1"></a>  <span class="fu">prep</span>(<span class="at">training =</span> mt_cars) <span class="sc">%&gt;%</span></span>
<span id="cb71-269"><a href="#cb71-269" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bake</span>(<span class="at">new_data =</span> <span class="cn">NULL</span>)</span>
<span id="cb71-270"><a href="#cb71-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-271"><a href="#cb71-271" aria-hidden="true" tabindex="-1"></a>mt_cars_boxcox <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span> moments<span class="sc">::</span><span class="fu">skewness</span>()</span>
<span id="cb71-272"><a href="#cb71-272" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-273"><a href="#cb71-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-274"><a href="#cb71-274" aria-hidden="true" tabindex="-1"></a>The diagnostic plot for <span class="in">`hp`</span> looks much better (notice the much smaller y-axis).</span>
<span id="cb71-275"><a href="#cb71-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-278"><a href="#cb71-278" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-279"><a href="#cb71-279" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(mpg <span class="sc">~</span> hp, <span class="at">data =</span> mt_cars_boxcox) <span class="sc">%&gt;%</span> <span class="fu">plot</span>(<span class="at">which =</span> <span class="dv">1</span>, <span class="at">main =</span> <span class="st">"hp - after Box-Cox"</span>)</span>
<span id="cb71-280"><a href="#cb71-280" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-281"><a href="#cb71-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-282"><a href="#cb71-282" aria-hidden="true" tabindex="-1"></a>Did this create a better fitting model? A little: R^2 increased from .87 to .89.</span>
<span id="cb71-283"><a href="#cb71-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-284"><a href="#cb71-284" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb71-285"><a href="#cb71-285" aria-hidden="true" tabindex="-1"></a><span class="in"># Before</span></span>
<span id="cb71-286"><a href="#cb71-286" aria-hidden="true" tabindex="-1"></a><span class="in">fit %&gt;% glance()</span></span>
<span id="cb71-287"><a href="#cb71-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-288"><a href="#cb71-288" aria-hidden="true" tabindex="-1"></a><span class="in"># After</span></span>
<span id="cb71-289"><a href="#cb71-289" aria-hidden="true" tabindex="-1"></a><span class="in">fit_boxcox &lt;- lm(mpg ~ ., data = mt_cars_boxcox %&gt;% select(-c(gear, carb)))</span></span>
<span id="cb71-290"><a href="#cb71-290" aria-hidden="true" tabindex="-1"></a><span class="in">fit_boxcox %&gt;% glance()</span></span>
<span id="cb71-291"><a href="#cb71-291" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-292"><a href="#cb71-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-293"><a href="#cb71-293" aria-hidden="true" tabindex="-1"></a>Here's a look at the diagnostic plots for the new model.</span>
<span id="cb71-294"><a href="#cb71-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-297"><a href="#cb71-297" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-298"><a href="#cb71-298" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb71-299"><a href="#cb71-299" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit_boxcox, <span class="at">labels.id =</span> <span class="cn">NULL</span>)</span>
<span id="cb71-300"><a href="#cb71-300" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-301"><a href="#cb71-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-302"><a href="#cb71-302" aria-hidden="true" tabindex="-1"></a>How about those linearity tests? Most look a little better.</span>
<span id="cb71-303"><a href="#cb71-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-306"><a href="#cb71-306" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-307"><a href="#cb71-307" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> mt_cars_boxcox <span class="sc">%&gt;%</span></span>
<span id="cb71-308"><a href="#cb71-308" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span></span>
<span id="cb71-309"><a href="#cb71-309" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>mpg)</span>
<span id="cb71-310"><a href="#cb71-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-311"><a href="#cb71-311" aria-hidden="true" tabindex="-1"></a>x_cor <span class="ot">&lt;-</span> x <span class="sc">%&gt;%</span> </span>
<span id="cb71-312"><a href="#cb71-312" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">.by =</span> name) <span class="sc">%&gt;%</span></span>
<span id="cb71-313"><a href="#cb71-313" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">val_corr =</span> <span class="fu">map_dbl</span>(data, <span class="sc">~</span><span class="fu">cor</span>(.<span class="sc">$</span>mpg, .<span class="sc">$</span>value)))</span>
<span id="cb71-314"><a href="#cb71-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-315"><a href="#cb71-315" aria-hidden="true" tabindex="-1"></a>x <span class="sc">%&gt;%</span></span>
<span id="cb71-316"><a href="#cb71-316" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(x_cor, <span class="at">by =</span> <span class="fu">join_by</span>(name)) <span class="sc">%&gt;%</span></span>
<span id="cb71-317"><a href="#cb71-317" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">name =</span> glue<span class="sc">::</span><span class="fu">glue</span>(<span class="st">"{name}, r = {comma(val_corr, .01)}"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb71-318"><a href="#cb71-318" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> mpg)) <span class="sc">+</span></span>
<span id="cb71-319"><a href="#cb71-319" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb71-320"><a href="#cb71-320" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">formula =</span> <span class="st">"y ~ x"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb71-321"><a href="#cb71-321" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(name), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb71-322"><a href="#cb71-322" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Response vs Predictor - Box-Cox Transformed"</span>)</span>
<span id="cb71-323"><a href="#cb71-323" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-324"><a href="#cb71-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-325"><a href="#cb71-325" aria-hidden="true" tabindex="-1"></a>How about $(e \sim X_j)$? Much better</span>
<span id="cb71-326"><a href="#cb71-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-329"><a href="#cb71-329" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-330"><a href="#cb71-330" aria-hidden="true" tabindex="-1"></a>mt_cars_boxcox <span class="sc">%&gt;%</span></span>
<span id="cb71-331"><a href="#cb71-331" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span></span>
<span id="cb71-332"><a href="#cb71-332" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>mpg) <span class="sc">%&gt;%</span> </span>
<span id="cb71-333"><a href="#cb71-333" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">.by =</span> name) <span class="sc">%&gt;%</span></span>
<span id="cb71-334"><a href="#cb71-334" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb71-335"><a href="#cb71-335" aria-hidden="true" tabindex="-1"></a>    <span class="at">fit =</span> <span class="fu">map</span>(data, <span class="sc">~</span><span class="fu">lm</span>(mpg <span class="sc">~</span> value, <span class="at">data =</span> .)),</span>
<span id="cb71-336"><a href="#cb71-336" aria-hidden="true" tabindex="-1"></a>    <span class="at">aug =</span> <span class="fu">map</span>(fit, broom<span class="sc">::</span>augment)</span>
<span id="cb71-337"><a href="#cb71-337" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb71-338"><a href="#cb71-338" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(aug) <span class="sc">%&gt;%</span></span>
<span id="cb71-339"><a href="#cb71-339" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> .resid)) <span class="sc">+</span></span>
<span id="cb71-340"><a href="#cb71-340" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb71-341"><a href="#cb71-341" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">formula =</span> <span class="st">"y ~ x"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb71-342"><a href="#cb71-342" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(name), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb71-343"><a href="#cb71-343" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Residuals vs Predictor - Box-Cox Transformed"</span>)</span>
<span id="cb71-344"><a href="#cb71-344" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-345"><a href="#cb71-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-346"><a href="#cb71-346" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multicollinearity</span></span>
<span id="cb71-347"><a href="#cb71-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-348"><a href="#cb71-348" aria-hidden="true" tabindex="-1"></a>The multicollinearity condition is violated when two or more of the predictors in a regression model are correlated.  Muticollinearity can occur for *structural* reasons, as when one variable is a transformation of another variable, or for *data* reasons, as occurs in observational studies.  Multicollinearity is a problem because it inflates the variances of the estimated coefficients, resulting in larger confidence intervals.</span>
<span id="cb71-349"><a href="#cb71-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-350"><a href="#cb71-350" aria-hidden="true" tabindex="-1"></a>When predictor variables are correlated, the precision of their estimated regression coefficients decreases.  The usual interpretation of a slope coefficient as the change in the mean response for each additional unit increase in the predictor when all the other predictors are held constant breaks down because changing one predictor necessarily changes the others.</span>
<span id="cb71-351"><a href="#cb71-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-352"><a href="#cb71-352" aria-hidden="true" tabindex="-1"></a>The residuals vs fits plot $(\epsilon \sim \hat{Y})$ should have correlation $\rho \sim 0$. A correlation matrix is helpful for picking out the correlation strengths. A good rule of thumb is correlation coefficients should be less than 0.80. However, this test may not work when a variable is correlated with a function of other variables. A model with multicollinearity may have a significant *F*-test with insignificant individual slope estimator t-tests. Another way to detect multicollinearity is by calculating variance inflation factors (VIF).  The predictor variance $Var(\hat{\beta_k})$ increases by a factor </span>
<span id="cb71-353"><a href="#cb71-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-354"><a href="#cb71-354" aria-hidden="true" tabindex="-1"></a>$$\mathrm{VIF}_k = \frac{1}{1 - R_k^2}$$</span>
<span id="cb71-355"><a href="#cb71-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-356"><a href="#cb71-356" aria-hidden="true" tabindex="-1"></a>where $R_k^2$ is the $R^2$ of a regression of the $k^{th}$ predictor on the remaining predictors.  A $VIF_k$ of $1$ indicates no inflation (no correlation).  A $VIF_k &gt;= 4$ warrants investigation.  A $VIF_k &gt;= 10$ requires correction.</span>
<span id="cb71-357"><a href="#cb71-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-358"><a href="#cb71-358" aria-hidden="true" tabindex="-1"></a>Does the model <span class="in">`mpg ~ .`</span> exhibit multicollinearity? Recall that the correlation matrix had several correlated predictors. E.g., <span class="in">`disp`</span> is strongly correlated with <span class="in">`wt`</span> (r = 0.89) and <span class="in">`hp`</span> (r = 0.79). How about the VIFs?</span>
<span id="cb71-359"><a href="#cb71-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-362"><a href="#cb71-362" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-363"><a href="#cb71-363" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>(fit_boxcox)</span>
<span id="cb71-364"><a href="#cb71-364" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-365"><a href="#cb71-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-366"><a href="#cb71-366" aria-hidden="true" tabindex="-1"></a>Three predictors have VIFs greater than 10 (<span class="in">`cyl`</span>, <span class="in">`disp`</span>, and <span class="in">`wt`</span>). One way to address multicollinearity is removing one or more of the violating predictors from the regression model. Try removing <span class="in">`disp`</span>.</span>
<span id="cb71-367"><a href="#cb71-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-370"><a href="#cb71-370" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-371"><a href="#cb71-371" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">-</span> disp, <span class="at">data =</span> mt_cars_boxcox) <span class="sc">%&gt;%</span> car<span class="sc">::</span><span class="fu">vif</span>()</span>
<span id="cb71-372"><a href="#cb71-372" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-373"><a href="#cb71-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-374"><a href="#cb71-374" aria-hidden="true" tabindex="-1"></a>Removing <span class="in">`disp`</span> reduced the VIFs of the other variables, but <span class="in">`cyl`</span> is still above 10. It may be worth dropping it from the model too. The model summary shows that <span class="in">`wt`</span> is the only significant predictor.</span>
<span id="cb71-375"><a href="#cb71-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-378"><a href="#cb71-378" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-379"><a href="#cb71-379" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">-</span> disp, <span class="at">data =</span> mt_cars_boxcox) <span class="sc">%&gt;%</span> <span class="fu">summary</span>()</span>
<span id="cb71-380"><a href="#cb71-380" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-381"><a href="#cb71-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-382"><a href="#cb71-382" aria-hidden="true" tabindex="-1"></a><span class="fu">## Prediction</span></span>
<span id="cb71-383"><a href="#cb71-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-384"><a href="#cb71-384" aria-hidden="true" tabindex="-1"></a>The standard error in the **expected value** of $\hat{y}$ at some new set of predictors $X_n$ is </span>
<span id="cb71-385"><a href="#cb71-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-386"><a href="#cb71-386" aria-hidden="true" tabindex="-1"></a>$$SE(\mu_\hat{y}) = \sqrt{\hat{\sigma}^2 (X_n (X'X)^{-1} X_n')}.$$  </span>
<span id="cb71-387"><a href="#cb71-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-388"><a href="#cb71-388" aria-hidden="true" tabindex="-1"></a>The standard error increases the further $X_n$ is from $\bar{X}$.  If $X_n = \bar{X}$, the equation reduces to $SE(\mu_\hat{y}) = \sigma / \sqrt{n}$.  If $n$ is large, or the predictor values are spread out, $SE(\mu_\hat{y})$ will be relatively small.  The $(1 - \alpha)\%$ **confidence interval** is $\hat{y} \pm t_{\alpha / 2} SE(\mu_\hat{y})$.</span>
<span id="cb71-389"><a href="#cb71-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-390"><a href="#cb71-390" aria-hidden="true" tabindex="-1"></a>The standard error in the **predicted value** of $\hat{y}$ at some $X_{new}$ is </span>
<span id="cb71-391"><a href="#cb71-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-392"><a href="#cb71-392" aria-hidden="true" tabindex="-1"></a>$$SE(\hat{y}) = SE(\mu_\hat{y})^2 + \sqrt{\hat{\sigma}^2}.$$</span>
<span id="cb71-393"><a href="#cb71-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-394"><a href="#cb71-394" aria-hidden="true" tabindex="-1"></a>Notice the standard error for a predicted value is always greater than the standard error of the expected value.  The $(1 - \alpha)\%$ **prediction interval** is $\hat{y} \pm t_{\alpha / 2} SE(\hat{y})$.</span>
<span id="cb71-395"><a href="#cb71-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-396"><a href="#cb71-396" aria-hidden="true" tabindex="-1"></a>Calculate confidence interval and prediction interval for <span class="in">`mpg`</span> for predictor values at their mean.</span>
<span id="cb71-397"><a href="#cb71-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-398"><a href="#cb71-398" aria-hidden="true" tabindex="-1"></a><span class="in">```{r message=FALSE}</span></span>
<span id="cb71-399"><a href="#cb71-399" aria-hidden="true" tabindex="-1"></a><span class="in">new_data &lt;- mt_cars %&gt;% </span></span>
<span id="cb71-400"><a href="#cb71-400" aria-hidden="true" tabindex="-1"></a><span class="in">  summarize(across(where(is.numeric), mean)) %&gt;%</span></span>
<span id="cb71-401"><a href="#cb71-401" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(vs = "S", am = "manual")</span></span>
<span id="cb71-402"><a href="#cb71-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-403"><a href="#cb71-403" aria-hidden="true" tabindex="-1"></a><span class="in">list(</span></span>
<span id="cb71-404"><a href="#cb71-404" aria-hidden="true" tabindex="-1"></a><span class="in">  Confidence = predict.lm(fit, newdata = new_data, interval = "confidence"),</span></span>
<span id="cb71-405"><a href="#cb71-405" aria-hidden="true" tabindex="-1"></a><span class="in">  Prediction = predict.lm(fit, newdata = new_data, interval = "prediction")</span></span>
<span id="cb71-406"><a href="#cb71-406" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb71-407"><a href="#cb71-407" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-408"><a href="#cb71-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-409"><a href="#cb71-409" aria-hidden="true" tabindex="-1"></a>Verify this by calculating $SE(\mu_\hat{y}) = \sqrt{\hat{\sigma}^2 (X_{new} (X'X)^{-1} X_{new}')}$ and $SE(\hat{y}) = SE(\mu_\hat{y})^2 + \sqrt{\hat{\sigma}^2}$ with matrix algebra.</span>
<span id="cb71-410"><a href="#cb71-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-411"><a href="#cb71-411" aria-hidden="true" tabindex="-1"></a><span class="in">```{r message=FALSE, warning=FALSE}</span></span>
<span id="cb71-412"><a href="#cb71-412" aria-hidden="true" tabindex="-1"></a><span class="in">X2 &lt;- lapply(data.frame(model.matrix(fit)), mean) %&gt;% unlist() %&gt;% t()</span></span>
<span id="cb71-413"><a href="#cb71-413" aria-hidden="true" tabindex="-1"></a><span class="in">X2[8] &lt;- 1</span></span>
<span id="cb71-414"><a href="#cb71-414" aria-hidden="true" tabindex="-1"></a><span class="in">X2[9] &lt;- 1</span></span>
<span id="cb71-415"><a href="#cb71-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-416"><a href="#cb71-416" aria-hidden="true" tabindex="-1"></a><span class="in">y_exp &lt;- sum(fit$coefficients * as.numeric(X2))</span></span>
<span id="cb71-417"><a href="#cb71-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-418"><a href="#cb71-418" aria-hidden="true" tabindex="-1"></a><span class="in"># Standard error of expected and predicted values</span></span>
<span id="cb71-419"><a href="#cb71-419" aria-hidden="true" tabindex="-1"></a><span class="in">se_y_exp &lt;- as.numeric(sqrt(rse^2 * X2 %*% solve(t(X) %*% X) %*% t(X2)))</span></span>
<span id="cb71-420"><a href="#cb71-420" aria-hidden="true" tabindex="-1"></a><span class="in">se_y_hat &lt;- sqrt(rse^2 + se_y_exp^2)</span></span>
<span id="cb71-421"><a href="#cb71-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-422"><a href="#cb71-422" aria-hidden="true" tabindex="-1"></a><span class="in"># Margins of error</span></span>
<span id="cb71-423"><a href="#cb71-423" aria-hidden="true" tabindex="-1"></a><span class="in">t_crit &lt;- qt(p = .05 / 2, df = n - k - 1, lower.tail = FALSE)</span></span>
<span id="cb71-424"><a href="#cb71-424" aria-hidden="true" tabindex="-1"></a><span class="in">me_exp &lt;- t_crit * se_y_exp</span></span>
<span id="cb71-425"><a href="#cb71-425" aria-hidden="true" tabindex="-1"></a><span class="in">me_hat &lt;- t_crit * se_y_hat</span></span>
<span id="cb71-426"><a href="#cb71-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-427"><a href="#cb71-427" aria-hidden="true" tabindex="-1"></a><span class="in">list(</span></span>
<span id="cb71-428"><a href="#cb71-428" aria-hidden="true" tabindex="-1"></a><span class="in">  Confidence = c(fit = y_exp, lwr = y_exp - me_exp, upr = y_exp + me_exp),</span></span>
<span id="cb71-429"><a href="#cb71-429" aria-hidden="true" tabindex="-1"></a><span class="in">  Prediction = c(fit = y_exp, lwr = y_exp - me_hat, upr = y_exp + me_hat)</span></span>
<span id="cb71-430"><a href="#cb71-430" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb71-431"><a href="#cb71-431" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-432"><a href="#cb71-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-433"><a href="#cb71-433" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inference</span></span>
<span id="cb71-434"><a href="#cb71-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-435"><a href="#cb71-435" aria-hidden="true" tabindex="-1"></a>Draw conclusions about the significance of the coefficient estimates with the *t*-test and/or *F*-test.</span>
<span id="cb71-436"><a href="#cb71-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-437"><a href="#cb71-437" aria-hidden="true" tabindex="-1"></a><span class="fu">### *t*-Test</span></span>
<span id="cb71-438"><a href="#cb71-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-439"><a href="#cb71-439" aria-hidden="true" tabindex="-1"></a>By assumption, the residuals are normally distributed, so the *Z*-test statistic could evaluate the parameter estimators, </span>
<span id="cb71-440"><a href="#cb71-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-441"><a href="#cb71-441" aria-hidden="true" tabindex="-1"></a>$$Z = \frac{\hat{\beta} - \beta_0}{\sqrt{\sigma^2 (X'X)^{-1}}}$$ </span>
<span id="cb71-442"><a href="#cb71-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-443"><a href="#cb71-443" aria-hidden="true" tabindex="-1"></a>where $\beta_0$ is the null-hypothesized value, usually 0.  $\sigma$ is unknown, but $\frac{\hat{\sigma}^2 (n - k)}{\sigma^2} \sim \chi^2$.  The ratio of the normal distribution divided by the adjusted chi-square $\sqrt{\chi^2 / (n - k)}$ is t-distributed, </span>
<span id="cb71-444"><a href="#cb71-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-445"><a href="#cb71-445" aria-hidden="true" tabindex="-1"></a>$$t = \frac{\hat{\beta} - \beta_0}{\sqrt{\hat{\sigma}^2 (X'X)^{-1}}} = \frac{\hat{\beta} - \beta_0}{SE(\hat{\beta})}$$</span>
<span id="cb71-446"><a href="#cb71-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-447"><a href="#cb71-447" aria-hidden="true" tabindex="-1"></a>The $(1 - \alpha)$ confidence intervals are $CI = \hat{\beta} \pm t_{\alpha / 2, df} SE(\hat{\beta})$ with *p*-value equaling the probability of measuring a $t$ of that extreme, $p = P(t &gt; |t|)$.  For a one-tail test, divide the reported p-value by two.  The $SE(\hat{\beta})$ decreases with i) a better fitting regression line (smaller $\hat{\sigma}^2$), ii) greater variation in the predictor (larger $X'X$), and iii) larger sample size (larger n).</span>
<span id="cb71-448"><a href="#cb71-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-449"><a href="#cb71-449" aria-hidden="true" tabindex="-1"></a>The <span class="in">`summary()`</span> output shows the t values and probabilities in the <span class="in">`t value`</span> and <span class="in">`Pr(&gt;|t|)`</span> columns. Verify this using matrix algebra to solve $t = \frac{(\hat{\beta} - \beta_1)}{SE(\hat{\beta})}$ with $\beta_1 = 0$. The $(1 - \alpha)$ confidence interval is $CI = \hat{\beta} \pm t_{\alpha / 2, df} SE(\hat{\beta})$.</span>
<span id="cb71-450"><a href="#cb71-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-453"><a href="#cb71-453" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-454"><a href="#cb71-454" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> beta_hat <span class="sc">/</span> se_beta_hat</span>
<span id="cb71-455"><a href="#cb71-455" aria-hidden="true" tabindex="-1"></a>p_value <span class="ot">&lt;-</span> <span class="fu">pt</span>(<span class="at">q =</span> <span class="fu">abs</span>(t), <span class="at">df =</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>) <span class="sc">*</span> <span class="dv">2</span></span>
<span id="cb71-456"><a href="#cb71-456" aria-hidden="true" tabindex="-1"></a>t_crit <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="at">p =</span> .<span class="dv">05</span> <span class="sc">/</span> <span class="dv">2</span>, <span class="at">df =</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb71-457"><a href="#cb71-457" aria-hidden="true" tabindex="-1"></a>lcl <span class="ot">=</span> beta_hat <span class="sc">-</span> t_crit <span class="sc">*</span> se_beta_hat</span>
<span id="cb71-458"><a href="#cb71-458" aria-hidden="true" tabindex="-1"></a>ucl <span class="ot">=</span> beta_hat <span class="sc">+</span> t_crit <span class="sc">*</span> se_beta_hat</span>
<span id="cb71-459"><a href="#cb71-459" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">beta =</span> <span class="fu">round</span>(beta_hat, <span class="dv">4</span>), </span>
<span id="cb71-460"><a href="#cb71-460" aria-hidden="true" tabindex="-1"></a>           <span class="at">se =</span> <span class="fu">round</span>(se_beta_hat, <span class="dv">4</span>), </span>
<span id="cb71-461"><a href="#cb71-461" aria-hidden="true" tabindex="-1"></a>           <span class="at">t =</span> <span class="fu">round</span>(t, <span class="dv">4</span>), </span>
<span id="cb71-462"><a href="#cb71-462" aria-hidden="true" tabindex="-1"></a>           <span class="at">p =</span> <span class="fu">round</span>(p_value, <span class="dv">4</span>),</span>
<span id="cb71-463"><a href="#cb71-463" aria-hidden="true" tabindex="-1"></a>           <span class="at">lcl =</span> <span class="fu">round</span>(lcl,<span class="dv">4</span>), </span>
<span id="cb71-464"><a href="#cb71-464" aria-hidden="true" tabindex="-1"></a>           <span class="at">ucl =</span> <span class="fu">round</span>(ucl, <span class="dv">4</span>)) <span class="sc">%&gt;%</span> knitr<span class="sc">::</span><span class="fu">kable</span>()</span>
<span id="cb71-465"><a href="#cb71-465" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-466"><a href="#cb71-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-467"><a href="#cb71-467" aria-hidden="true" tabindex="-1"></a><span class="fu">### *F*-Test</span></span>
<span id="cb71-468"><a href="#cb71-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-469"><a href="#cb71-469" aria-hidden="true" tabindex="-1"></a>The *F*-test for the model is a test of the null hypothesis that none of the independent variables linearly predict the dependent variable, that is, the model parameters are jointly zero: $H_0 : \beta_1 = \ldots = \beta_k = 0$.  The regression mean sum of squares $MSR = \frac{(\hat{y} - \bar{y})'(\hat{y} - \bar{y})}{k-1}$ and the error mean sum of squares $MSE = \frac{\hat{\epsilon}'\hat{\epsilon}}{n-k}$ are each chi-square variables.  Their ratio has an *F* distribution with $k - 1$ numerator degrees of freedom and $n - k$ denominator degrees of freedom.  The F statistic can also be expressed in terms of the coefficient of correlation $R^2 = \frac{MSR}{MST}$.  </span>
<span id="cb71-470"><a href="#cb71-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-471"><a href="#cb71-471" aria-hidden="true" tabindex="-1"></a>$$F(k - 1, n - k) = \frac{MSR}{MSE} = \frac{R^2}{1 - R^2} \frac{n-k}{k-1}$$</span>
<span id="cb71-472"><a href="#cb71-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-473"><a href="#cb71-473" aria-hidden="true" tabindex="-1"></a>*MSE* is $\sigma^2$.  If $H_0$ is true, that is, there is no relationship between the predictors and the response, then $MSR$ is also equal to $\sigma^2$, so $F = 1$.  As $R^2 \rightarrow 1$, $F \rightarrow \infty$, and as $R^2 \rightarrow 0$, $F \rightarrow 0$.  *F* increases with $n$ and decreases with $k$.</span>
<span id="cb71-474"><a href="#cb71-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-475"><a href="#cb71-475" aria-hidden="true" tabindex="-1"></a>What is the probability that all parameters are jointly equal to zero? The *F*-statistic is presented at the bottom of the <span class="in">`summary()`</span> function. Verify this manually.</span>
<span id="cb71-476"><a href="#cb71-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-479"><a href="#cb71-479" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-480"><a href="#cb71-480" aria-hidden="true" tabindex="-1"></a>ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>((fit<span class="sc">$</span>fitted.values <span class="sc">-</span> <span class="fu">mean</span>(mt_cars<span class="sc">$</span>mpg))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb71-481"><a href="#cb71-481" aria-hidden="true" tabindex="-1"></a>sse <span class="ot">&lt;-</span> <span class="fu">sum</span>(fit<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb71-482"><a href="#cb71-482" aria-hidden="true" tabindex="-1"></a>sst <span class="ot">&lt;-</span> <span class="fu">sum</span>((fit<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">mean</span>(mt_cars<span class="sc">$</span>mpg))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb71-483"><a href="#cb71-483" aria-hidden="true" tabindex="-1"></a>msr <span class="ot">&lt;-</span> ssr <span class="sc">/</span> k</span>
<span id="cb71-484"><a href="#cb71-484" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> sse <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb71-485"><a href="#cb71-485" aria-hidden="true" tabindex="-1"></a>f <span class="ot">=</span> msr <span class="sc">/</span> mse</span>
<span id="cb71-486"><a href="#cb71-486" aria-hidden="true" tabindex="-1"></a>p_value <span class="ot">&lt;-</span> <span class="fu">pf</span>(<span class="at">q =</span> f, <span class="at">df1 =</span> k, <span class="at">df2 =</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb71-487"><a href="#cb71-487" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"F-statistic: "</span>, <span class="fu">round</span>(f, <span class="dv">4</span>), <span class="st">" on 3 and 65 DF,  p-value: "</span>, p_value)</span>
<span id="cb71-488"><a href="#cb71-488" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-489"><a href="#cb71-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-490"><a href="#cb71-490" aria-hidden="true" tabindex="-1"></a>There is sufficient evidence (*F* = 17.35, *p* &lt; .0001) to reject $H_0$ that the parameter estimators are jointly equal to zero.</span>
<span id="cb71-491"><a href="#cb71-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-492"><a href="#cb71-492" aria-hidden="true" tabindex="-1"></a><span class="in">`aov()`</span> calculates the sequential sum of squares. The regression sum of squares SSR for <span class="in">`mpg ~ cyl`</span> is 817.7.  Adding <span class="in">`disp`</span> to the model increases SSR by 37.6. Adding <span class="in">`hp`</span> to the model increases SSR by 9.4. It would seem that <span class="in">`hp`</span> does not improve the model.</span>
<span id="cb71-493"><a href="#cb71-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-496"><a href="#cb71-496" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-497"><a href="#cb71-497" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(fit))</span>
<span id="cb71-498"><a href="#cb71-498" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-499"><a href="#cb71-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-500"><a href="#cb71-500" aria-hidden="true" tabindex="-1"></a>Order matters. Had we started with <span class="in">`disp`</span>, then added <span class="in">`hp`</span> we would find both estimators were significant.</span>
<span id="cb71-501"><a href="#cb71-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-504"><a href="#cb71-504" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-505"><a href="#cb71-505" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> disp <span class="sc">+</span> hp <span class="sc">+</span> ., <span class="at">data =</span> mt_cars)))</span>
<span id="cb71-506"><a href="#cb71-506" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-507"><a href="#cb71-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-508"><a href="#cb71-508" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interpretation</span></span>
<span id="cb71-509"><a href="#cb71-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-510"><a href="#cb71-510" aria-hidden="true" tabindex="-1"></a>A plot of the standardized coefficients shows the relative importance of each predictor. The distance the coefficients are from zero shows how much a change in a standard deviation of the predictor changes the mean of the predicted value. The CI shows the precision. The plot shows not only which variables are significant, but also which are important.</span>
<span id="cb71-511"><a href="#cb71-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-512"><a href="#cb71-512" aria-hidden="true" tabindex="-1"></a><span class="in">```{r warning=FALSE}</span></span>
<span id="cb71-513"><a href="#cb71-513" aria-hidden="true" tabindex="-1"></a><span class="in">mt_cars_scaled &lt;- mt_cars %&gt;% mutate(across(where(is.numeric), scale))</span></span>
<span id="cb71-514"><a href="#cb71-514" aria-hidden="true" tabindex="-1"></a><span class="in">fit_scaled &lt;- lm(mpg ~ ., mt_cars_scaled)</span></span>
<span id="cb71-515"><a href="#cb71-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-516"><a href="#cb71-516" aria-hidden="true" tabindex="-1"></a><span class="in">fit_scaled %&gt;%</span></span>
<span id="cb71-517"><a href="#cb71-517" aria-hidden="true" tabindex="-1"></a><span class="in">  tidy() %&gt;%</span></span>
<span id="cb71-518"><a href="#cb71-518" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(</span></span>
<span id="cb71-519"><a href="#cb71-519" aria-hidden="true" tabindex="-1"></a><span class="in">    lwr = estimate - qt(.05/2, fit_scaled$df.residual) * std.error,</span></span>
<span id="cb71-520"><a href="#cb71-520" aria-hidden="true" tabindex="-1"></a><span class="in">    upr = estimate + qt(.05/2, fit_scaled$df.residual) * std.error,</span></span>
<span id="cb71-521"><a href="#cb71-521" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb71-522"><a href="#cb71-522" aria-hidden="true" tabindex="-1"></a><span class="in">  filter(term != "(Intercept)") %&gt;%</span></span>
<span id="cb71-523"><a href="#cb71-523" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot() +</span></span>
<span id="cb71-524"><a href="#cb71-524" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_point(aes(y = term, x = estimate)) +</span></span>
<span id="cb71-525"><a href="#cb71-525" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_errorbar(aes(y = term, xmin = lwr, xmax = upr), width = .2) +</span></span>
<span id="cb71-526"><a href="#cb71-526" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_vline(xintercept = 0, linetype = 4) +</span></span>
<span id="cb71-527"><a href="#cb71-527" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(title = "Model Feature Importance", x = "Standardized Weight", y = NULL)  </span></span>
<span id="cb71-528"><a href="#cb71-528" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-529"><a href="#cb71-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-530"><a href="#cb71-530" aria-hidden="true" tabindex="-1"></a>The added variable plot shows the bivariate relationship between $Y$ and $X_i$ after accounting for the other variables.  For example, the partial regression plots of <span class="in">`y ~ x1 + x2 + x3`</span> would plot the residuals of <span class="in">`y ~ x2 + x3`</span> vs <span class="in">`x1`</span>, and so on.</span>
<span id="cb71-531"><a href="#cb71-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-532"><a href="#cb71-532" aria-hidden="true" tabindex="-1"></a><span class="in">```{r fig.height=9}</span></span>
<span id="cb71-533"><a href="#cb71-533" aria-hidden="true" tabindex="-1"></a><span class="in">car::avPlots(fit, layout = c(4, 3))</span></span>
<span id="cb71-534"><a href="#cb71-534" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-535"><a href="#cb71-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-536"><a href="#cb71-536" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Validation</span></span>
<span id="cb71-537"><a href="#cb71-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-538"><a href="#cb71-538" aria-hidden="true" tabindex="-1"></a>Evaluate predictive accuracy by training the model on a training data set and testing on a test data set.</span>
<span id="cb71-539"><a href="#cb71-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-540"><a href="#cb71-540" aria-hidden="true" tabindex="-1"></a><span class="fu">### Accuracy Metrics</span></span>
<span id="cb71-541"><a href="#cb71-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-542"><a href="#cb71-542" aria-hidden="true" tabindex="-1"></a>The most common measures of model fit are R-squared, Adjusted R-squared, RMSE, RSE, MAE, AIC, AICc, BIC, and Mallow's Cp.</span>
<span id="cb71-543"><a href="#cb71-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-544"><a href="#cb71-544" aria-hidden="true" tabindex="-1"></a><span class="fu">### R-Squared</span></span>
<span id="cb71-545"><a href="#cb71-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-546"><a href="#cb71-546" aria-hidden="true" tabindex="-1"></a>The coefficient of determination (**R-squared**) is the percent of total variation in the response variable that is explained by the regression line.  </span>
<span id="cb71-547"><a href="#cb71-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-548"><a href="#cb71-548" aria-hidden="true" tabindex="-1"></a>$$R^2 = \frac{RSS}{SST} = 1 - \frac{SSE}{SST}$$</span>
<span id="cb71-549"><a href="#cb71-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-550"><a href="#cb71-550" aria-hidden="true" tabindex="-1"></a>where $SSE = \sum_{i=1}^n{(y_i - \hat{y}_i)^2}$ is the sum squared differences between the predicted and observed value, $SST = \sum_{i = 1}^n{(y_i - \bar{y})^2}$ is the sum of squared differences between the observed and overall mean value, and $RSS = \sum_{i=1}^n{(\hat{y}_i - \bar{y})^2}$ is the sum of squared differences between the predicted and overall mean "no-relationship line" value.  At the extremes, $R^2 = 1$ means all data points fall perfectly on the regression line - the predictors account for *all* variation in the response; $R^2 = 0$ means the regression line is horizontal at $\bar{y}$ - the predictors account for *none* of the variation in the response.  In the simple case of a single predictor variable, $R^2$ equals the squared correlation between $x$ and $y$, $Cor(x,y)$. </span>
<span id="cb71-551"><a href="#cb71-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-552"><a href="#cb71-552" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb71-553"><a href="#cb71-553" aria-hidden="true" tabindex="-1"></a><span class="in">ssr &lt;- sum((fit$fitted.values - mean(mt_cars$mpg))^2)</span></span>
<span id="cb71-554"><a href="#cb71-554" aria-hidden="true" tabindex="-1"></a><span class="in">sse &lt;- sum(fit$residuals^2)</span></span>
<span id="cb71-555"><a href="#cb71-555" aria-hidden="true" tabindex="-1"></a><span class="in">sst &lt;- sum((mt_cars$mpg - mean(mt_cars$mpg))^2)</span></span>
<span id="cb71-556"><a href="#cb71-556" aria-hidden="true" tabindex="-1"></a><span class="in">(r2 &lt;- ssr / sst)</span></span>
<span id="cb71-557"><a href="#cb71-557" aria-hidden="true" tabindex="-1"></a><span class="in">(r2 &lt;- 1 - sse / sst)</span></span>
<span id="cb71-558"><a href="#cb71-558" aria-hidden="true" tabindex="-1"></a><span class="in">(r2 &lt;- summary(fit)$r.squared)</span></span>
<span id="cb71-559"><a href="#cb71-559" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-560"><a href="#cb71-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-561"><a href="#cb71-561" aria-hidden="true" tabindex="-1"></a>The sums of squares are the same thing as the variances multiplied by the degrees of freedom.</span>
<span id="cb71-562"><a href="#cb71-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-565"><a href="#cb71-565" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-566"><a href="#cb71-566" aria-hidden="true" tabindex="-1"></a>ssr2 <span class="ot">&lt;-</span> <span class="fu">var</span>(<span class="fu">fitted</span>(fit)) <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb71-567"><a href="#cb71-567" aria-hidden="true" tabindex="-1"></a>sse2 <span class="ot">&lt;-</span> <span class="fu">var</span>(<span class="fu">residuals</span>(fit)) <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb71-568"><a href="#cb71-568" aria-hidden="true" tabindex="-1"></a>sst2 <span class="ot">&lt;-</span> <span class="fu">var</span>(mt_cars<span class="sc">$</span>mpg) <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb71-569"><a href="#cb71-569" aria-hidden="true" tabindex="-1"></a>ssr2 <span class="sc">/</span> sst2</span>
<span id="cb71-570"><a href="#cb71-570" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-571"><a href="#cb71-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-572"><a href="#cb71-572" aria-hidden="true" tabindex="-1"></a>$R^2$ is also equal to the correlation between the fitted value and observed values, $R^2 = Cor(Y, \hat{Y})^2$.</span>
<span id="cb71-573"><a href="#cb71-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-576"><a href="#cb71-576" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-577"><a href="#cb71-577" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(fit<span class="sc">$</span>fitted.values, mt_cars<span class="sc">$</span>mpg)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb71-578"><a href="#cb71-578" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-579"><a href="#cb71-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-580"><a href="#cb71-580" aria-hidden="true" tabindex="-1"></a>R-squared is proportional to the the variance in the response, *SST*.  Given a constant percentage error in predictions, a test set with relatively low variation in the reponse will have a lower R-squared.  Conversely, test sets with large variation, e.g., housing data with home sale ranging from $60K to $2M may have a large R-squared despite average prediction errors of &gt;$10K.</span>
<span id="cb71-581"><a href="#cb71-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-582"><a href="#cb71-582" aria-hidden="true" tabindex="-1"></a>A close variant of R-squared is the non-parametric Spearman's rank correlation.  This statistic is the correlation of the *ranks* of the response and the predicted values.  It is used when the model goal is ranking.  </span>
<span id="cb71-583"><a href="#cb71-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-584"><a href="#cb71-584" aria-hidden="true" tabindex="-1"></a>The **adjusted R-squared** ($\bar{R}^2$) penalizes the R-squared metric for increasing number of predictors. </span>
<span id="cb71-585"><a href="#cb71-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-586"><a href="#cb71-586" aria-hidden="true" tabindex="-1"></a>$$\bar{R}^2 = 1 - \frac{SSE}{SST} \cdot \frac{n-1}{n-k-1}$$</span>
<span id="cb71-587"><a href="#cb71-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-588"><a href="#cb71-588" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb71-589"><a href="#cb71-589" aria-hidden="true" tabindex="-1"></a><span class="in">(adj_r2 &lt;- 1 - sse/sst * (n - 1) / (n - k - 1))</span></span>
<span id="cb71-590"><a href="#cb71-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-591"><a href="#cb71-591" aria-hidden="true" tabindex="-1"></a><span class="in">summary(fit)$adj.r.squared</span></span>
<span id="cb71-592"><a href="#cb71-592" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-593"><a href="#cb71-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-594"><a href="#cb71-594" aria-hidden="true" tabindex="-1"></a><span class="fu">#### RMSE, RSE, MAE</span></span>
<span id="cb71-595"><a href="#cb71-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-596"><a href="#cb71-596" aria-hidden="true" tabindex="-1"></a>The root mean squared error (**RMSE**) is the average prediction error (square root of mean squared error).</span>
<span id="cb71-597"><a href="#cb71-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-598"><a href="#cb71-598" aria-hidden="true" tabindex="-1"></a>$$RMSE = \sqrt{\frac{\sum_{i=1}^n{(y_i - \hat{y}_i)^2}}{n}}$$</span>
<span id="cb71-599"><a href="#cb71-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-600"><a href="#cb71-600" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb71-601"><a href="#cb71-601" aria-hidden="true" tabindex="-1"></a><span class="in">sqrt(mean((mt_cars$mpg - fit$fitted.values)^2))</span></span>
<span id="cb71-602"><a href="#cb71-602" aria-hidden="true" tabindex="-1"></a><span class="in">augment(fit) %&gt;% yardstick::rmse(truth = mpg, estimate = .fitted)</span></span>
<span id="cb71-603"><a href="#cb71-603" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-604"><a href="#cb71-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-605"><a href="#cb71-605" aria-hidden="true" tabindex="-1"></a>The mean squared error of a model with theoretical residual of mean zero and constant variance $\sigma^2$ can be decomposed into the model's bias and the model's variance:</span>
<span id="cb71-606"><a href="#cb71-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-607"><a href="#cb71-607" aria-hidden="true" tabindex="-1"></a>$$E<span class="co">[</span><span class="ot">MSE</span><span class="co">]</span> = \sigma^2 + Bias^2 + Var.$$</span>
<span id="cb71-608"><a href="#cb71-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-609"><a href="#cb71-609" aria-hidden="true" tabindex="-1"></a>A model that predicts the response closely will have low bias, but be relatively sensitive to the training data and thus have high variance.  A model that predicts the response conservatively (e.g., a simple mean) will have large bias, but be relatively insensitive to nuances in the training data.  Here is an example of a simulated sine wave.  A model predicting the mean value at the upper and lower levels has low variance, but high bias, and a model of an actual sine wave has low bias and high variance.  This is the variance-bias trade-off. </span>
<span id="cb71-610"><a href="#cb71-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-611"><a href="#cb71-611" aria-hidden="true" tabindex="-1"></a><span class="in">```{r warning=FALSE, fig.height=3}</span></span>
<span id="cb71-612"><a href="#cb71-612" aria-hidden="true" tabindex="-1"></a><span class="in">data.frame(x = seq(2, 10, 0.1)) %&gt;%</span></span>
<span id="cb71-613"><a href="#cb71-613" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(</span></span>
<span id="cb71-614"><a href="#cb71-614" aria-hidden="true" tabindex="-1"></a><span class="in">    e = runif(81, -.5, .5),</span></span>
<span id="cb71-615"><a href="#cb71-615" aria-hidden="true" tabindex="-1"></a><span class="in">    y = sin(x) + e,</span></span>
<span id="cb71-616"><a href="#cb71-616" aria-hidden="true" tabindex="-1"></a><span class="in">    `Low var, high bias` = zoo:::rollmean(y, k = 3, fill = NA, align = "center"),</span></span>
<span id="cb71-617"><a href="#cb71-617" aria-hidden="true" tabindex="-1"></a><span class="in">    `High var, low bias` = sin(x)</span></span>
<span id="cb71-618"><a href="#cb71-618" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb71-619"><a href="#cb71-619" aria-hidden="true" tabindex="-1"></a><span class="in">  pivot_longer(cols = `Low var, high bias`:`High var, low bias`) %&gt;%</span></span>
<span id="cb71-620"><a href="#cb71-620" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(aes(x = x)) +</span></span>
<span id="cb71-621"><a href="#cb71-621" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_point(aes(y = y)) +</span></span>
<span id="cb71-622"><a href="#cb71-622" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line(aes(y = value, color = name), size = 1)</span></span>
<span id="cb71-623"><a href="#cb71-623" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-624"><a href="#cb71-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-625"><a href="#cb71-625" aria-hidden="true" tabindex="-1"></a>The residual standard error (**RSE**, or model sigma $\hat{\sigma}$) is an estimate of the standard deviation of $\epsilon$. It is roughly the average amount the response deviates from the true regression line.  </span>
<span id="cb71-626"><a href="#cb71-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-627"><a href="#cb71-627" aria-hidden="true" tabindex="-1"></a>$$\sigma = \sqrt{\frac{\sum_{i=1}^n{(y_i - \hat{y}_i)^2}}{n-k-1}}$$</span>
<span id="cb71-628"><a href="#cb71-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-629"><a href="#cb71-629" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb71-630"><a href="#cb71-630" aria-hidden="true" tabindex="-1"></a><span class="in">sqrt(sum((mt_cars$mpg - fit$fitted.values)^2) / (n - k - 1))</span></span>
<span id="cb71-631"><a href="#cb71-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-632"><a href="#cb71-632" aria-hidden="true" tabindex="-1"></a><span class="in"># sd is sqrt(sse / (n-1)), sigma = sqrt(sse / (n - k - 1))</span></span>
<span id="cb71-633"><a href="#cb71-633" aria-hidden="true" tabindex="-1"></a><span class="in">sd(fit$residuals) * sqrt((n - 1) / (n - k - 1))  </span></span>
<span id="cb71-634"><a href="#cb71-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-635"><a href="#cb71-635" aria-hidden="true" tabindex="-1"></a><span class="in">summary(fit)$sigma </span></span>
<span id="cb71-636"><a href="#cb71-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-637"><a href="#cb71-637" aria-hidden="true" tabindex="-1"></a><span class="in">sigma(fit)</span></span>
<span id="cb71-638"><a href="#cb71-638" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-639"><a href="#cb71-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-640"><a href="#cb71-640" aria-hidden="true" tabindex="-1"></a>The mean absolute error (**MAE**) is the average absolute prediction arror.  It is less sensitive to outliers.</span>
<span id="cb71-641"><a href="#cb71-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-642"><a href="#cb71-642" aria-hidden="true" tabindex="-1"></a>$$MAE = \frac{\sum_{i=1}^n{|y_i - \hat{y}_i|}}{n}$$</span>
<span id="cb71-643"><a href="#cb71-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-646"><a href="#cb71-646" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-647"><a href="#cb71-647" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">abs</span>(mt_cars<span class="sc">$</span>mpg <span class="sc">-</span> fit<span class="sc">$</span>fitted.values)) <span class="sc">/</span> n</span>
<span id="cb71-648"><a href="#cb71-648" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-649"><a href="#cb71-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-650"><a href="#cb71-650" aria-hidden="true" tabindex="-1"></a>These metrics are good for evaluating a model, but less useful for comparing models. The problem is that they tend to improve with additional variables added to the model, even if the improvement is not significant. The following metrics aid model comparison by penalizing added variables. </span>
<span id="cb71-651"><a href="#cb71-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-652"><a href="#cb71-652" aria-hidden="true" tabindex="-1"></a><span class="fu">#### AIC, BIC</span></span>
<span id="cb71-653"><a href="#cb71-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-654"><a href="#cb71-654" aria-hidden="true" tabindex="-1"></a>Akaike's Information Criteria (**AIC**) is a penalization metric.  The lower the AIC, the better the model.</span>
<span id="cb71-655"><a href="#cb71-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-656"><a href="#cb71-656" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb71-657"><a href="#cb71-657" aria-hidden="true" tabindex="-1"></a><span class="in">AIC(fit)</span></span>
<span id="cb71-658"><a href="#cb71-658" aria-hidden="true" tabindex="-1"></a><span class="in"># AICc corrects AIC for small sample sizes.</span></span>
<span id="cb71-659"><a href="#cb71-659" aria-hidden="true" tabindex="-1"></a><span class="in">AIC(fit) + (2 * k * (k + 1)) / (n - k - 1)</span></span>
<span id="cb71-660"><a href="#cb71-660" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-661"><a href="#cb71-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-662"><a href="#cb71-662" aria-hidden="true" tabindex="-1"></a>The Bayesian information criteria (**BIC**) is like AIC, but with a stronger penalty for additional variables.</span>
<span id="cb71-663"><a href="#cb71-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-666"><a href="#cb71-666" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-667"><a href="#cb71-667" aria-hidden="true" tabindex="-1"></a><span class="fu">BIC</span>(fit)</span>
<span id="cb71-668"><a href="#cb71-668" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-669"><a href="#cb71-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-670"><a href="#cb71-670" aria-hidden="true" tabindex="-1"></a><span class="in">`broom::glance()`</span> calculates many validation metrics. Compare the full model to a reduced model without <span class="in">`cyl`</span>.</span>
<span id="cb71-671"><a href="#cb71-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-674"><a href="#cb71-674" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb71-675"><a href="#cb71-675" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">glance</span>(fit) <span class="sc">%&gt;%</span> <span class="fu">select</span>(adj.r.squared, sigma, AIC, BIC, p.value)</span>
<span id="cb71-676"><a href="#cb71-676" aria-hidden="true" tabindex="-1"></a><span class="fu">glance</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">-</span> cyl, mt_cars)) <span class="sc">%&gt;%</span> <span class="fu">select</span>(adj.r.squared, sigma, AIC, BIC, p.value)</span>
<span id="cb71-677"><a href="#cb71-677" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-678"><a href="#cb71-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-679"><a href="#cb71-679" aria-hidden="true" tabindex="-1"></a>The adjusted R2 increased and AIC and BIC decreased, meaning the full model is less efficient at explaining the variability in the response value.  The residual standard error <span class="in">`sigma`</span> is smaller for the reduced model.  Finally, the *F* statistic p-value is smaller for the reduced model, meaning the reduced model is statistically more significant.</span>
<span id="cb71-680"><a href="#cb71-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-681"><a href="#cb71-681" aria-hidden="true" tabindex="-1"></a>Note that these regression metrics are all internal measures, that is they have been computed on the training dataset, not the test dataset.</span>
<span id="cb71-682"><a href="#cb71-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-683"><a href="#cb71-683" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cross-Validation</span></span>
<span id="cb71-684"><a href="#cb71-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-685"><a href="#cb71-685" aria-hidden="true" tabindex="-1"></a>Cross-validation is a set of methods for measuring the performance of a predictive model on a test dataset.  The main measures of prediction performance are R2, RMSE and MAE. </span>
<span id="cb71-686"><a href="#cb71-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-687"><a href="#cb71-687" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Validation Set</span></span>
<span id="cb71-688"><a href="#cb71-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-689"><a href="#cb71-689" aria-hidden="true" tabindex="-1"></a>To perform validation set cross validation, randomly split the data into a training data set and a test data set. Fit models to the training data set, then predict values with the validation set. The model that produces the best prediction performance is the preferred model.</span>
<span id="cb71-690"><a href="#cb71-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-691"><a href="#cb71-691" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb71-692"><a href="#cb71-692" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(123)</span></span>
<span id="cb71-693"><a href="#cb71-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-694"><a href="#cb71-694" aria-hidden="true" tabindex="-1"></a><span class="in">mt_cars_split &lt;- initial_split(mt_cars, prop = 0.7, strata = mpg)</span></span>
<span id="cb71-695"><a href="#cb71-695" aria-hidden="true" tabindex="-1"></a><span class="in">mt_cars_training &lt;- mt_cars_split %&gt;% training()</span></span>
<span id="cb71-696"><a href="#cb71-696" aria-hidden="true" tabindex="-1"></a><span class="in">mt_cars_testing &lt;- mt_cars_split %&gt;% testing()</span></span>
<span id="cb71-697"><a href="#cb71-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-698"><a href="#cb71-698" aria-hidden="true" tabindex="-1"></a><span class="in">fit_validation &lt;- linear_reg() %&gt;% </span></span>
<span id="cb71-699"><a href="#cb71-699" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("lm") %&gt;% </span></span>
<span id="cb71-700"><a href="#cb71-700" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression") %&gt;%</span></span>
<span id="cb71-701"><a href="#cb71-701" aria-hidden="true" tabindex="-1"></a><span class="in">  fit(mpg ~ ., data = mt_cars_training) %&gt;%</span></span>
<span id="cb71-702"><a href="#cb71-702" aria-hidden="true" tabindex="-1"></a><span class="in">  predict(new_data = mt_cars_testing) %&gt;%</span></span>
<span id="cb71-703"><a href="#cb71-703" aria-hidden="true" tabindex="-1"></a><span class="in">  bind_cols(mt_cars_testing)</span></span>
<span id="cb71-704"><a href="#cb71-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-705"><a href="#cb71-705" aria-hidden="true" tabindex="-1"></a><span class="in">fit_validation %&gt;% rmse(truth = mpg, estimate = .pred)</span></span>
<span id="cb71-706"><a href="#cb71-706" aria-hidden="true" tabindex="-1"></a><span class="in">fit_validation %&gt;% rsq(truth = mpg, estimate = .pred)</span></span>
<span id="cb71-707"><a href="#cb71-707" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-708"><a href="#cb71-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-709"><a href="#cb71-709" aria-hidden="true" tabindex="-1"></a>Alternatively, <span class="in">`last_fit()`</span> i) splits train/test, ii) fits, and iii) predicts.</span>
<span id="cb71-710"><a href="#cb71-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-711"><a href="#cb71-711" aria-hidden="true" tabindex="-1"></a><span class="in">```{r eval=FALSE}</span></span>
<span id="cb71-712"><a href="#cb71-712" aria-hidden="true" tabindex="-1"></a><span class="in">fit_validation_2 &lt;- linear_reg() %&gt;% </span></span>
<span id="cb71-713"><a href="#cb71-713" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("lm") %&gt;% </span></span>
<span id="cb71-714"><a href="#cb71-714" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression") %&gt;%</span></span>
<span id="cb71-715"><a href="#cb71-715" aria-hidden="true" tabindex="-1"></a><span class="in">  last_fit(mpg ~ ., split = mt_cars_split)</span></span>
<span id="cb71-716"><a href="#cb71-716" aria-hidden="true" tabindex="-1"></a><span class="in">fit_validation_2 %&gt;% collect_metrics()</span></span>
<span id="cb71-717"><a href="#cb71-717" aria-hidden="true" tabindex="-1"></a><span class="in">fit_validation_2 %&gt;% collect_predictions()</span></span>
<span id="cb71-718"><a href="#cb71-718" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-719"><a href="#cb71-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-720"><a href="#cb71-720" aria-hidden="true" tabindex="-1"></a>The validation set method is only useful when you have a large data set to partition.  A second disadvantage is that building a model on a fraction of the data leaves out information. The test error will vary with which observations are included in the training set.</span>
<span id="cb71-721"><a href="#cb71-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-722"><a href="#cb71-722" aria-hidden="true" tabindex="-1"></a><span class="fu">#### LOOCV</span></span>
<span id="cb71-723"><a href="#cb71-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-724"><a href="#cb71-724" aria-hidden="true" tabindex="-1"></a>Leave one out cross validation (LOOCV) works by successively modeling with training sets leaving out one data point, then averaging the prediction errors.</span>
<span id="cb71-725"><a href="#cb71-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-726"><a href="#cb71-726" aria-hidden="true" tabindex="-1"></a><span class="in">```{r eval=FALSE}</span></span>
<span id="cb71-727"><a href="#cb71-727" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(123)</span></span>
<span id="cb71-728"><a href="#cb71-728" aria-hidden="true" tabindex="-1"></a><span class="in">m2 &lt;- train(mpg ~ ., </span></span>
<span id="cb71-729"><a href="#cb71-729" aria-hidden="true" tabindex="-1"></a><span class="in">            data = d.train[, 1:9],</span></span>
<span id="cb71-730"><a href="#cb71-730" aria-hidden="true" tabindex="-1"></a><span class="in">            method = "lm",</span></span>
<span id="cb71-731"><a href="#cb71-731" aria-hidden="true" tabindex="-1"></a><span class="in">            trControl = trainControl(method = "LOOCV"))</span></span>
<span id="cb71-732"><a href="#cb71-732" aria-hidden="true" tabindex="-1"></a><span class="in">print(m2)</span></span>
<span id="cb71-733"><a href="#cb71-733" aria-hidden="true" tabindex="-1"></a><span class="in">postResample(pred = predict(m2, newdata = d.test), </span></span>
<span id="cb71-734"><a href="#cb71-734" aria-hidden="true" tabindex="-1"></a><span class="in">             obs = d.test$mpg)</span></span>
<span id="cb71-735"><a href="#cb71-735" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-736"><a href="#cb71-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-737"><a href="#cb71-737" aria-hidden="true" tabindex="-1"></a>This method isn't perfect either.  It repeats as many times as there are data points, so the execution time may be long.  LOOCV is also sensitive to outliers.</span>
<span id="cb71-738"><a href="#cb71-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-739"><a href="#cb71-739" aria-hidden="true" tabindex="-1"></a><span class="fu">#### K-fold Cross-Validation</span></span>
<span id="cb71-740"><a href="#cb71-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-741"><a href="#cb71-741" aria-hidden="true" tabindex="-1"></a>K-fold cross-validation splits the dataset into *k* folds (subsets), then uses *k-1* of the folds for a training set and the remaining fold for a test set, then repeats for all permutations of k taken k-1 at a time.  E.g., 3-fold cross-validation will partition the data into sets A, B, and C, then create train/test splits of <span class="co">[</span><span class="ot">AB, C</span><span class="co">]</span>, <span class="co">[</span><span class="ot">AC, B</span><span class="co">]</span>, and <span class="co">[</span><span class="ot">BC, A</span><span class="co">]</span>.  </span>
<span id="cb71-742"><a href="#cb71-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-743"><a href="#cb71-743" aria-hidden="true" tabindex="-1"></a>K-fold cross-validation is less computationally expensive than LOOCV, and often yields more accurate test error rate estimates.  What is the right value of k?  The lower is *k* the more biased the estimates; the higher is *k* the larger the estimate variability. At the extremes *k* = 2 is the validation set method, and *k = n* is the LOOCV method.  In practice, one typically performs k-fold cross-validation using k = 5 or k = 10 because these values have been empirically shown to balence bias and variance. </span>
<span id="cb71-744"><a href="#cb71-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-745"><a href="#cb71-745" aria-hidden="true" tabindex="-1"></a><span class="in">```{r eval=FALSE}</span></span>
<span id="cb71-746"><a href="#cb71-746" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(123)</span></span>
<span id="cb71-747"><a href="#cb71-747" aria-hidden="true" tabindex="-1"></a><span class="in">m3 &lt;- train(mpg ~ ., </span></span>
<span id="cb71-748"><a href="#cb71-748" aria-hidden="true" tabindex="-1"></a><span class="in">            data = d.train[, 1:9],</span></span>
<span id="cb71-749"><a href="#cb71-749" aria-hidden="true" tabindex="-1"></a><span class="in">            method = "lm",</span></span>
<span id="cb71-750"><a href="#cb71-750" aria-hidden="true" tabindex="-1"></a><span class="in">            trControl = trainControl(method = "cv",</span></span>
<span id="cb71-751"><a href="#cb71-751" aria-hidden="true" tabindex="-1"></a><span class="in">                                     number = 5))</span></span>
<span id="cb71-752"><a href="#cb71-752" aria-hidden="true" tabindex="-1"></a><span class="in">print(m3)</span></span>
<span id="cb71-753"><a href="#cb71-753" aria-hidden="true" tabindex="-1"></a><span class="in">postResample(pred = predict(m3, newdata = d.test), </span></span>
<span id="cb71-754"><a href="#cb71-754" aria-hidden="true" tabindex="-1"></a><span class="in">             obs = d.test$mpg)</span></span>
<span id="cb71-755"><a href="#cb71-755" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-756"><a href="#cb71-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-757"><a href="#cb71-757" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Repeated K-fold CV</span></span>
<span id="cb71-758"><a href="#cb71-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-759"><a href="#cb71-759" aria-hidden="true" tabindex="-1"></a>You can also perform k-fold cross-validation multiple times and average the results.  Specify <span class="in">`method = "repeatedcv"`</span> and <span class="in">`repeats = 3`</span> in the <span class="in">`trainControl`</span> object for three repeats.</span>
<span id="cb71-760"><a href="#cb71-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-761"><a href="#cb71-761" aria-hidden="true" tabindex="-1"></a><span class="in">```{r eval=FALSE}</span></span>
<span id="cb71-762"><a href="#cb71-762" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(123)</span></span>
<span id="cb71-763"><a href="#cb71-763" aria-hidden="true" tabindex="-1"></a><span class="in">m4 &lt;- train(mpg ~ ., </span></span>
<span id="cb71-764"><a href="#cb71-764" aria-hidden="true" tabindex="-1"></a><span class="in">            data = d.train[, 1:9],</span></span>
<span id="cb71-765"><a href="#cb71-765" aria-hidden="true" tabindex="-1"></a><span class="in">            method = "lm",</span></span>
<span id="cb71-766"><a href="#cb71-766" aria-hidden="true" tabindex="-1"></a><span class="in">            trControl = trainControl(method = "repeatedcv",</span></span>
<span id="cb71-767"><a href="#cb71-767" aria-hidden="true" tabindex="-1"></a><span class="in">                                     number = 5,</span></span>
<span id="cb71-768"><a href="#cb71-768" aria-hidden="true" tabindex="-1"></a><span class="in">                                     repeats = 3))</span></span>
<span id="cb71-769"><a href="#cb71-769" aria-hidden="true" tabindex="-1"></a><span class="in">print(m4)</span></span>
<span id="cb71-770"><a href="#cb71-770" aria-hidden="true" tabindex="-1"></a><span class="in">postResample(pred = predict(m4, newdata = d.test), </span></span>
<span id="cb71-771"><a href="#cb71-771" aria-hidden="true" tabindex="-1"></a><span class="in">             obs = d.test$mpg)</span></span>
<span id="cb71-772"><a href="#cb71-772" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-773"><a href="#cb71-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-774"><a href="#cb71-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-775"><a href="#cb71-775" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bootstrapping</span></span>
<span id="cb71-776"><a href="#cb71-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-777"><a href="#cb71-777" aria-hidden="true" tabindex="-1"></a>Bootstrapping randomly selects a sample of n observations with replacement from the original dataset to evaluate the model.  The procedure is repeated many times.</span>
<span id="cb71-778"><a href="#cb71-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-779"><a href="#cb71-779" aria-hidden="true" tabindex="-1"></a>Specify <span class="in">`method = "boot"`</span> and <span class="in">`number = 100`</span> to perform 100 bootstrap samples.</span>
<span id="cb71-780"><a href="#cb71-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-781"><a href="#cb71-781" aria-hidden="true" tabindex="-1"></a><span class="in">```{r eval=FALSE}</span></span>
<span id="cb71-782"><a href="#cb71-782" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(123)</span></span>
<span id="cb71-783"><a href="#cb71-783" aria-hidden="true" tabindex="-1"></a><span class="in">m5 &lt;- train(mpg ~ ., </span></span>
<span id="cb71-784"><a href="#cb71-784" aria-hidden="true" tabindex="-1"></a><span class="in">            data = d.train[, 1:9],</span></span>
<span id="cb71-785"><a href="#cb71-785" aria-hidden="true" tabindex="-1"></a><span class="in">            method = "lm",</span></span>
<span id="cb71-786"><a href="#cb71-786" aria-hidden="true" tabindex="-1"></a><span class="in">            trControl = trainControl(method = "boot",</span></span>
<span id="cb71-787"><a href="#cb71-787" aria-hidden="true" tabindex="-1"></a><span class="in">                                     number = 100))</span></span>
<span id="cb71-788"><a href="#cb71-788" aria-hidden="true" tabindex="-1"></a><span class="in">print(m5)</span></span>
<span id="cb71-789"><a href="#cb71-789" aria-hidden="true" tabindex="-1"></a><span class="in">postResample(pred = predict(m5, newdata = d.test), </span></span>
<span id="cb71-790"><a href="#cb71-790" aria-hidden="true" tabindex="-1"></a><span class="in">             obs = d.test$mpg)</span></span>
<span id="cb71-791"><a href="#cb71-791" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb71-792"><a href="#cb71-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-793"><a href="#cb71-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-794"><a href="#cb71-794" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gain Curve</span></span>
<span id="cb71-795"><a href="#cb71-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-796"><a href="#cb71-796" aria-hidden="true" tabindex="-1"></a>For supervised learning purposes, a visual way to evaluate a regression model is with the gain curve.  This visualization compares a predictive model score to an actual outcome (either binary (0/1) or continuous). The gain curve plot measures how well the model score sorts the data compared to the true outcome value.  The x-axis is the fraction of items seen when sorted by score, and the y-axis is the cumulative summed true outcome when sorted by score.  For comparison, GainCurvePlot also plots the "wizard curve": the gain curve when the data is sorted according to its true outcome.  A relative Gini score close to 1 means the model sorts responses well.</span>
<span id="cb71-797"><a href="#cb71-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-798"><a href="#cb71-798" aria-hidden="true" tabindex="-1"></a><span class="in">```{r message=FALSE, warning=FALSE}</span></span>
<span id="cb71-799"><a href="#cb71-799" aria-hidden="true" tabindex="-1"></a><span class="in">fit_validation %&gt;%</span></span>
<span id="cb71-800"><a href="#cb71-800" aria-hidden="true" tabindex="-1"></a><span class="in">  WVPlots::GainCurvePlot(xvar = ".pred", truthVar = "mpg", title = "Model Gain Curve")</span></span>
<span id="cb71-801"><a href="#cb71-801" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>