[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supervised Machine Learning",
    "section": "",
    "text": "Intro\nMachine learning (ML) develops algorithms to identify patterns in data (unsupervised ML) or make predictions and inferences (supervised ML).\nSupervised ML trains the machine to learn from prior examples to predict either a categorical outcome (classification) or a numeric outcome (regression), or to infer the relationships between the outcome and its explanatory variables.\nTwo early forms of supervised ML are linear regression (OLS) and generalized linear models (GLM) (Poisson and logistic regression). These methods have been improved with advanced linear methods, including stepwise selection, regularization (ridge, LASSO, elastic net), principal components regression, and partial least squares. With greater computing capacity, non-linear models are now in use, including polynomial regression, step functions, splines, and generalized additive models (GAM). Decision trees (bagging, random forests, and, boosting) are additional options for regression and classification, and support vector machines is an additional option for classification.",
    "crumbs": [
      "Intro"
    ]
  },
  {
    "objectID": "01-linear_regression.html",
    "href": "01-linear_regression.html",
    "title": "1  Ordinary Least Squares",
    "section": "",
    "text": "1.1 Parameter Estimation\nThere are two model parameters to estimate: \\(\\hat{\\beta}\\) estimates the coefficient vector \\(\\beta\\), and \\(\\hat{\\sigma}\\) estimates the variance of the residuals along the regression line. Derive \\(\\hat{\\beta}\\) by minimizing the sum of squared residuals \\(SSE = (y - X \\hat{\\beta})' (y - X \\hat{\\beta})\\). The result is\n\\[\\hat{\\beta} = (X'X)^{-1}X'y.\\]\nThe residual standard error (RSE) estimates the sample deviation around the population regression line. (Think of each value of \\(X\\) along the regression line as a subpopulation with mean \\(y_i\\) and variance \\(\\sigma^2\\). This variance is assumed to be the same for all \\(X\\).)\n\\[\\hat{\\sigma} = \\sqrt{(n-k-1)^{-1} e'e}.\\]\nThe standard error for the coefficient estimators is the square root of the error variance divided by \\((X'X)\\).\n\\[SE(\\hat{\\beta}) = \\sqrt{\\hat{\\sigma}^2 (X'X)^{-1}}.\\]\nLinear regression is related to correlation. The coefficient estimator for \\(\\beta_1\\) in simple linear regression is equal to the Pearson correlation coefficient multiplied by the ratio of the standard deviations of \\(y\\) and \\(x\\).\n\\[\n\\begin{align}\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\\\\n&= \\frac{Cov(X,Y)}{Var(X)} \\\\\n&= r \\cdot \\frac{\\sigma_Y}{\\sigma_X}\n\\end{align}\n\\]\nThe Pearson correlation is the ratio of the covariance of \\(X\\) and \\(Y\\) and product of their standard deviations.\n\\[\n\\begin{align}\nr &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2}  \\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}} \\\\\n&= \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\n\\end{align}\n\\]\nThe formula for \\(\\hat{\\beta}\\) is almost identical to that of \\(r\\) with the exception that the denominator is the variance of \\(X\\) rather than the product of the standard deviations. The Pearson correlation is the standardized linear relationship. Pearson correlation measures the strength and direction of the linear relationship between \\(X\\) and \\(Y\\) in a standardized form.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "01-linear_regression.html#parameter-estimation",
    "href": "01-linear_regression.html#parameter-estimation",
    "title": "1  Ordinary Least Squares",
    "section": "",
    "text": "If \\(r = 0\\), then \\(\\beta_1 = 0\\) and there is no linear relationship.\nif \\(r = 1\\) or \\(r = -1\\), then the relationship is perfectly linear, and \\(\\beta_1\\) measures the relative scale of \\(Y\\) and \\(X\\).\n\n\nExample\nDataset mtcars contains response variable fuel consumption mpg and 10 aspects of automobile design and performance for 32 automobiles. What is the relationship between the response and its predictors?\n\n\nShow the code\ndata(\"mtcars\")\nmt_cars &lt;- mtcars %&gt;%\n  mutate(\n    vs = factor(vs, levels = c(0, 1), labels = c(\"V\", \"S\")),\n    am = factor(am, levels = c(0, 1), labels = c(\"automatic\", \"manual\"))\n  )\nglimpse(mt_cars)\n\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;fct&gt; V, V, S, S, V, S, V, S, S, S, S, V, V, V, V, V, V, S, S, S, S, V,…\n$ am   &lt;fct&gt; manual, manual, manual, automatic, automatic, automatic, automati…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\nThe correlation matrix of the numeric variables shows wt has the strongest association with mpg (r = -0.87) followed by disp (r = -0.85) and hp (r = -0.78). drat is moderately correlated (r = 0.68), and qsec is weakly correlated (r = 0.42). Many predictors are strongly correlated with each other.\n\n\nShow the code\nmt_cars %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor() %&gt;%\n  corrplot::corrplot(type = \"upper\", method = \"number\")\n\n\n\n\n\n\n\n\n\nBoxplots of the categorical variables reveal differences in levels.\n\n\nShow the code\nmt_cars %&gt;%\n  select(mpg, where(is.factor)) %&gt;%\n  mutate(across(where(is.factor), as.character)) %&gt;%\n  pivot_longer(cols = -mpg) %&gt;%\n  ggplot(aes(x = value, y = mpg)) +\n  geom_boxplot() +\n  facet_wrap(facets = vars(name), scales = \"free_x\")\n\n\n\n\n\n\n\n\n\nFit a population model to the predictors.\n\n\nShow the code\nfit &lt;- lm(mpg ~ ., data = mt_cars)\nsummary(fit)\n\n\n\nCall:\nlm(formula = mpg ~ ., data = mt_cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 12.30337   18.71788   0.657   0.5181  \ncyl         -0.11144    1.04502  -0.107   0.9161  \ndisp         0.01334    0.01786   0.747   0.4635  \nhp          -0.02148    0.02177  -0.987   0.3350  \ndrat         0.78711    1.63537   0.481   0.6353  \nwt          -3.71530    1.89441  -1.961   0.0633 .\nqsec         0.82104    0.73084   1.123   0.2739  \nvsS          0.31776    2.10451   0.151   0.8814  \nammanual     2.52023    2.05665   1.225   0.2340  \ngear         0.65541    1.49326   0.439   0.6652  \ncarb        -0.19942    0.82875  -0.241   0.8122  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.65 on 21 degrees of freedom\nMultiple R-squared:  0.869, Adjusted R-squared:  0.8066 \nF-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n\n\nsummary() shows \\(\\hat{\\beta}\\) as Estimate, \\(SE({\\hat{\\beta}})\\) as Std. Error, and \\(\\hat{\\sigma}\\) as Residual standard error. You can manually perform these calculations using matrix algebra3. Recall, \\(\\hat{\\beta} = (X'X)^{-1}X'y\\).\n\n\nShow the code\nX &lt;- model.matrix(fit)\ny &lt;- mt_cars$mpg\n\n(beta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y)\n\n\n                   [,1]\n(Intercept) 12.30337416\ncyl         -0.11144048\ndisp         0.01333524\nhp          -0.02148212\ndrat         0.78711097\nwt          -3.71530393\nqsec         0.82104075\nvsS          0.31776281\nammanual     2.52022689\ngear         0.65541302\ncarb        -0.19941925\n\n\nThe residual standard error is \\(\\hat{\\sigma} = \\sqrt{(n-k-1)^{-1} \\hat{e}'\\hat{e}}\\).\n\n\nShow the code\nn &lt;- nrow(X)\nk &lt;- ncol(X) - 1  # exclude the intercept term\ny_hat &lt;- X %*% beta_hat\nsse &lt;- sum((y - y_hat)^2)\n(dof &lt;- n - k - 1)\n## [1] 21\n(rse &lt;- sqrt(sse / dof))\n## [1] 2.650197\n\n\nThe standard errors of the coefficients are \\(SE(\\hat{\\beta}) = \\sqrt{\\hat{\\sigma}^2 (X'X)^{-1}}\\).\n\n\nShow the code\n(se_beta_hat &lt;- sqrt(diag(rse^2 * solve(t(X) %*% X))))\n\n\n(Intercept)         cyl        disp          hp        drat          wt \n18.71788443  1.04502336  0.01785750  0.02176858  1.63537307  1.89441430 \n       qsec         vsS    ammanual        gear        carb \n 0.73084480  2.10450861  2.05665055  1.49325996  0.82875250",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "01-linear_regression.html#model-assumptions",
    "href": "01-linear_regression.html#model-assumptions",
    "title": "1  Ordinary Least Squares",
    "section": "1.2 Model Assumptions",
    "text": "1.2 Model Assumptions\nThe linear regression model assumes the relationship between the predictors and the response is linear and the residuals are independent random variables normally distributed with mean zero and constant variance. Additionally, you will want to check for multicollinearity in the predictors because it can produce unreliable coefficient estimates and predicted values. The plot() function produces a set of diagnostic plots to test the assumptions.\nUse the Residuals vs Fitted plot, \\(e \\sim \\hat{Y}\\), to test the linearity and equal error variances assumptions. The plot also identifies outliers. The polynomial trend line should show that the residuals vary around \\(e = 0\\) in a straight horizontal line (linearity). The residuals should have random scatter in a band of constant width around 0, and no fan shape at the low and high ends (equal variances). All tests and intervals are sensitive to the equal variances condition. The plot also reveals multicollinearity. If the residuals and fitted values are correlated, multicollinearity may be a problem.\nUse the Q-Q Residuals plot (aka, residuals normal probability plot) to test the normality assumption. It plots the theoretical percentiles of the normal distribution versus the observed sample percentiles. It should be approximately linear with no bow-shaped deviations. Sometimes this normality check fails when the linearity check fails, so check for linearity first. Parameter estimation is not sensitive to the normality assumption, but prediction intervals are.\nUse the Scale-Location plot, \\(\\sqrt{e / sd(e)} \\sim \\hat{y}\\), to test for equal variance (aka, homoscedasticity). The square root of the absolute value of the residuals should be spread equally along a horizontal line.\nThe Residuals vs Leverage plot identifies influential observations. The standardized residuals should fall within the 95% probability band.\n\n\nShow the code\npar(mfrow = c(2, 2))\nplot(fit, labels.id = NULL)\n\n\n\n\n\n\n\n\n\nThese diagnostics are usually sufficient. If any of the diagnostics fail, you can re-specify the model or transform the data.\nConsider the linearity assumption. The explanatory variables should each be linearly related to the response variable: \\(E(\\epsilon | X_j) = 0\\). The Residuals vs Fitted plot tested this overall. A curved pattern in the residuals indicates a curvature in the relationship between the response and the predictor that is not explained by the model. There are other tests of linearity. The full list:\n\nResiduals vs fits plot \\((e \\sim \\hat{Y})\\) should randomly vary around 0.\nObserved vs fits plot \\((Y \\sim \\hat{Y})\\) should be symmetric along the 45-degree line.\n\nEach \\((Y \\sim X_j )\\) plot should have correlation \\(\\rho \\sim 1\\).\n\nEach \\((e \\sim X_j)\\) plot should exhibit no pattern.\n\nThe diagnostic plots above look pretty good except for some evidence of heteroskedasticity at the upper end of predicted values. You might drill into the linearity condition to start. Start with plots of \\((Y \\sim X_j )\\) - are the correlation coefficients ~1? Yes, all but qsec have correlations over .6.\n\n\nShow the code\nx &lt;- mt_cars %&gt;%\n  select(where(is.numeric)) %&gt;%\n  pivot_longer(-mpg)\n\nx_cor &lt;- x %&gt;% \n  nest(.by = name) %&gt;%\n  mutate(val_corr = map_dbl(data, ~cor(.$mpg, .$value)))\n\nx %&gt;%\n  inner_join(x_cor, by = join_by(name)) %&gt;%\n  mutate(name = glue::glue(\"{name}, r = {comma(val_corr, .01)}\")) %&gt;%\n  ggplot(aes(x = value, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE) +\n  facet_wrap(facets = vars(name), scales = \"free_x\") +\n  labs(title = \"Response vs Predictor\")\n\n\n\n\n\n\n\n\n\nHow about \\((e \\sim X_j)\\)? disp has a wave pattern; hp is u-shaped; qsec has increase variance in the middle.\n\n\nShow the code\nmt_cars %&gt;%\n  select(where(is.numeric)) %&gt;%\n  pivot_longer(-mpg) %&gt;% \n  nest(.by = name) %&gt;%\n  mutate(\n    fit = map(data, ~lm(mpg ~ value, data = .)),\n    aug = map(fit, broom::augment)\n  ) %&gt;%\n  unnest(aug) %&gt;%\n  ggplot(aes(x = value, y = .resid)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE) +\n  facet_wrap(facets = vars(name), scales = \"free_x\") +\n  labs(title = \"Residuals vs Predictor\")\n\n\n\n\n\n\n\n\n\nIf the linearity condition fails, change the functional form of the model with non-linear transformations of the explanatory variables. A common way to do this is with Box-Cox transformations.\n\\[w_t = \\begin{cases} \\begin{array}{l} log(y_t) \\quad \\quad \\lambda = 0 \\\\\n(y_t^\\lambda - 1) / \\lambda \\quad \\text{otherwise} \\end{array} \\end{cases}\\]\n\\(\\lambda\\) can take any value, but values near the following yield familiar transformations.\n\n\\(\\lambda = 1\\) yields no substantive transformation.\n\n\\(\\lambda = 0.5\\) is a square root plus linear transformation.\n\\(\\lambda = 0.333\\) is a cube root plus linear transformation.\n\\(\\lambda = 0\\) is a natural log transformation.\n\\(\\lambda = -1\\) is an inverse transformation.\n\nA common source of non-linearity is skewed response or predictor variables (see discussion here). mt_cars has some skewed variables.\n\n\nShow the code\nmt_cars %&gt;%\n  select(where(is.numeric)) %&gt;%\n  pivot_longer(cols = everything()) %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(facets = vars(name), scales = \"free_x\") +\n  labs(title = \"Histogram of numeric vars\")\n\n\n\n\n\n\n\n\n\nhp has the most skew, cyl the least.\n\n\nShow the code\nmt_cars %&gt;% select(where(is.numeric)) %&gt;% moments::skewness()\n\n\n       mpg        cyl       disp         hp       drat         wt       qsec \n 0.6404399 -0.1831287  0.4002724  0.7614356  0.2788734  0.4437855  0.3870456 \n      gear       carb \n 0.5546495  1.1021304 \n\n\nCompare the residuals vs fitted plots for each variable. Here is hp.\n\n\nShow the code\nlm(mpg ~ hp, data = mt_cars) %&gt;% plot(which = 1, main = \"hp\")\n\n\n\n\n\n\n\n\n\nand here is cyl\n\n\nShow the code\nlm(mpg ~ cyl, data = mt_cars) %&gt;% plot(which = 1, main = \"drat\")\n\n\n\n\n\n\n\n\n\nBox-Cox transform the numeric variables. The skew is reduced.\n\n\nShow the code\nrec &lt;- recipe(~., data = mt_cars)\n\nmt_cars_boxcox &lt;- rec %&gt;% \n  step_BoxCox(all_numeric()) %&gt;%\n  prep(training = mt_cars) %&gt;%\n  bake(new_data = NULL)\n\nmt_cars_boxcox %&gt;% select(where(is.numeric)) %&gt;% moments::skewness()\n\n\n         mpg          cyl         disp           hp         drat           wt \n-0.001040985 -0.183128652 -0.056140870 -0.011871603  0.003121934 -0.010164785 \n        qsec         gear         carb \n-0.000842791  0.554649467 -0.018867279 \n\n\nThe diagnostic plot for hp looks much better (notice the much smaller y-axis).\n\n\nShow the code\nlm(mpg ~ hp, data = mt_cars_boxcox) %&gt;% plot(which = 1, main = \"hp - after Box-Cox\")\n\n\n\n\n\n\n\n\n\nDid this create a better fitting model? A little: R^2 increased from .87 to .89.\n\n\nShow the code\n# Before\nfit %&gt;% glance()\n## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC\n##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     0.869         0.807  2.65      13.9 0.000000379    10  -69.9  164.  181.\n## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# After\nfit_boxcox &lt;- lm(mpg ~ ., data = mt_cars_boxcox %&gt;% select(-c(gear, carb)))\nfit_boxcox %&gt;% glance()\n## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic       p.value    df logLik   AIC   BIC\n##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     0.893         0.855 0.124      23.9 0.00000000215     8   26.8 -33.5 -18.9\n## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nHere’s a look at the diagnostic plots for the new model.\n\n\nShow the code\npar(mfrow = c(2, 2))\nplot(fit_boxcox, labels.id = NULL)\n\n\n\n\n\n\n\n\n\nHow about those linearity tests? Most look a little better.\n\n\nShow the code\nx &lt;- mt_cars_boxcox %&gt;%\n  select(where(is.numeric)) %&gt;%\n  pivot_longer(-mpg)\n\nx_cor &lt;- x %&gt;% \n  nest(.by = name) %&gt;%\n  mutate(val_corr = map_dbl(data, ~cor(.$mpg, .$value)))\n\nx %&gt;%\n  inner_join(x_cor, by = join_by(name)) %&gt;%\n  mutate(name = glue::glue(\"{name}, r = {comma(val_corr, .01)}\")) %&gt;%\n  ggplot(aes(x = value, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE) +\n  facet_wrap(facets = vars(name), scales = \"free_x\") +\n  labs(title = \"Response vs Predictor - Box-Cox Transformed\")\n\n\n\n\n\n\n\n\n\nHow about \\((e \\sim X_j)\\)? Much better\n\n\nShow the code\nmt_cars_boxcox %&gt;%\n  select(where(is.numeric)) %&gt;%\n  pivot_longer(-mpg) %&gt;% \n  nest(.by = name) %&gt;%\n  mutate(\n    fit = map(data, ~lm(mpg ~ value, data = .)),\n    aug = map(fit, broom::augment)\n  ) %&gt;%\n  unnest(aug) %&gt;%\n  ggplot(aes(x = value, y = .resid)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE) +\n  facet_wrap(facets = vars(name), scales = \"free_x\") +\n  labs(title = \"Residuals vs Predictor - Box-Cox Transformed\")\n\n\n\n\n\n\n\n\n\n\n1.2.1 Multicollinearity\nThe multicollinearity condition is violated when two or more of the predictors in a regression model are correlated. Muticollinearity can occur for structural reasons, as when one variable is a transformation of another variable, or for data reasons, as occurs in observational studies. Multicollinearity is a problem because it inflates the variances of the estimated coefficients, resulting in larger confidence intervals.\nWhen predictor variables are correlated, the precision of their estimated regression coefficients decreases. The usual interpretation of a slope coefficient as the change in the mean response for each additional unit increase in the predictor when all the other predictors are held constant breaks down because changing one predictor necessarily changes the others.\nThe residuals vs fits plot \\((\\epsilon \\sim \\hat{Y})\\) should have correlation \\(\\rho \\sim 0\\). A correlation matrix is helpful for picking out the correlation strengths. A good rule of thumb is correlation coefficients should be less than 0.80. However, this test may not work when a variable is correlated with a function of other variables. A model with multicollinearity may have a significant F-test with insignificant individual slope estimator t-tests. Another way to detect multicollinearity is by calculating variance inflation factors (VIF). The predictor variance \\(Var(\\hat{\\beta_k})\\) increases by a factor\n\\[\\mathrm{VIF}_k = \\frac{1}{1 - R_k^2}\\]\nwhere \\(R_k^2\\) is the \\(R^2\\) of a regression of the \\(k^{th}\\) predictor on the remaining predictors. A \\(VIF_k\\) of \\(1\\) indicates no inflation (no correlation). A \\(VIF_k &gt;= 4\\) warrants investigation. A \\(VIF_k &gt;= 10\\) requires correction.\nDoes the model mpg ~ . exhibit multicollinearity? Recall that the correlation matrix had several correlated predictors. E.g., disp is strongly correlated with wt (r = 0.89) and hp (r = 0.79). How about the VIFs?\n\n\nShow the code\ncar::vif(fit_boxcox)\n\n\n      cyl      disp        hp      drat        wt      qsec        vs        am \n13.793609 18.078964  9.075071  3.401503  9.456176  9.117931  4.866017  4.432487 \n\n\nThree predictors have VIFs greater than 10 (cyl, disp, and wt). One way to address multicollinearity is removing one or more of the violating predictors from the regression model. Try removing disp.\n\n\nShow the code\nlm(mpg ~ . - disp, data = mt_cars_boxcox) %&gt;% car::vif()\n\n\n      cyl        hp      drat        wt      qsec        vs        am      gear \n14.662069  9.587920  4.112070  8.639359  9.590966  5.034117  4.938979  5.684508 \n     carb \n 5.046840 \n\n\nRemoving disp reduced the VIFs of the other variables, but cyl is still above 10. It may be worth dropping it from the model too. The model summary shows that wt is the only significant predictor.\n\n\nShow the code\nlm(mpg ~ . - disp, data = mt_cars_boxcox) %&gt;% summary()\n\n\n\nCall:\nlm(formula = mpg ~ . - disp, data = mt_cars_boxcox)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.174183 -0.081250 -0.005585  0.069938  0.245160 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  1.57267    2.97595   0.528  0.60247   \ncyl          0.02864    0.04662   0.614  0.54527   \nhp          -0.13295    0.08959  -1.484  0.15200   \ndrat         0.13119    0.32429   0.405  0.68972   \nwt          -0.35353    0.12046  -2.935  0.00767 **\nqsec         1.00706    1.12656   0.894  0.38104   \nvsS         -0.03579    0.09679  -0.370  0.71510   \nammanual    -0.01315    0.09684  -0.136  0.89324   \ngear         0.09776    0.07026   1.391  0.17806   \ncarb        -0.06806    0.07516  -0.906  0.37498   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1211 on 22 degrees of freedom\nMultiple R-squared:  0.9015,    Adjusted R-squared:  0.8612 \nF-statistic: 22.37 on 9 and 22 DF,  p-value: 4.385e-09",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "01-linear_regression.html#prediction",
    "href": "01-linear_regression.html#prediction",
    "title": "1  Ordinary Least Squares",
    "section": "1.3 Prediction",
    "text": "1.3 Prediction\nThe standard error in the expected value of \\(\\hat{y}\\) at some new set of predictors \\(X_n\\) is\n\\[SE(\\mu_\\hat{y}) = \\sqrt{\\hat{\\sigma}^2 (X_n (X'X)^{-1} X_n')}.\\]\nThe standard error increases the further \\(X_n\\) is from \\(\\bar{X}\\). If \\(X_n = \\bar{X}\\), the equation reduces to \\(SE(\\mu_\\hat{y}) = \\sigma / \\sqrt{n}\\). If \\(n\\) is large, or the predictor values are spread out, \\(SE(\\mu_\\hat{y})\\) will be relatively small. The \\((1 - \\alpha)\\%\\) confidence interval is \\(\\hat{y} \\pm t_{\\alpha / 2} SE(\\mu_\\hat{y})\\).\nThe standard error in the predicted value of \\(\\hat{y}\\) at some \\(X_{new}\\) is\n\\[SE(\\hat{y}) = SE(\\mu_\\hat{y})^2 + \\sqrt{\\hat{\\sigma}^2}.\\]\nNotice the standard error for a predicted value is always greater than the standard error of the expected value. The \\((1 - \\alpha)\\%\\) prediction interval is \\(\\hat{y} \\pm t_{\\alpha / 2} SE(\\hat{y})\\).\nCalculate confidence interval and prediction interval for mpg for predictor values at their mean.\n\n\nShow the code\nnew_data &lt;- mt_cars %&gt;% \n  summarize(across(where(is.numeric), mean)) %&gt;%\n  mutate(vs = \"S\", am = \"manual\")\n\nlist(\n  Confidence = predict.lm(fit, newdata = new_data, interval = \"confidence\"),\n  Prediction = predict.lm(fit, newdata = new_data, interval = \"prediction\")\n)\n\n\n$Confidence\n       fit      lwr      upr\n1 21.76575 17.75562 25.77589\n\n$Prediction\n       fit      lwr      upr\n1 21.76575 14.94985 28.58166\n\n\nVerify this by calculating \\(SE(\\mu_\\hat{y}) = \\sqrt{\\hat{\\sigma}^2 (X_{new} (X'X)^{-1} X_{new}')}\\) and \\(SE(\\hat{y}) = SE(\\mu_\\hat{y})^2 + \\sqrt{\\hat{\\sigma}^2}\\) with matrix algebra.\n\n\nShow the code\nX2 &lt;- lapply(data.frame(model.matrix(fit)), mean) %&gt;% unlist() %&gt;% t()\nX2[8] &lt;- 1\nX2[9] &lt;- 1\n\ny_exp &lt;- sum(fit$coefficients * as.numeric(X2))\n\n# Standard error of expected and predicted values\nse_y_exp &lt;- as.numeric(sqrt(rse^2 * X2 %*% solve(t(X) %*% X) %*% t(X2)))\nse_y_hat &lt;- sqrt(rse^2 + se_y_exp^2)\n\n# Margins of error\nt_crit &lt;- qt(p = .05 / 2, df = n - k - 1, lower.tail = FALSE)\nme_exp &lt;- t_crit * se_y_exp\nme_hat &lt;- t_crit * se_y_hat\n\nlist(\n  Confidence = c(fit = y_exp, lwr = y_exp - me_exp, upr = y_exp + me_exp),\n  Prediction = c(fit = y_exp, lwr = y_exp - me_hat, upr = y_exp + me_hat)\n)\n\n\n$Confidence\n     fit      lwr      upr \n21.76575 17.75562 25.77589 \n\n$Prediction\n     fit      lwr      upr \n21.76575 14.94985 28.58166",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "01-linear_regression.html#inference",
    "href": "01-linear_regression.html#inference",
    "title": "1  Ordinary Least Squares",
    "section": "1.4 Inference",
    "text": "1.4 Inference\nDraw conclusions about the significance of the coefficient estimates with the t-test and/or F-test.\n\n1.4.1 t-Test\nBy assumption, the residuals are normally distributed, so the Z-test statistic could evaluate the parameter estimators,\n\\[Z = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\sigma^2 (X'X)^{-1}}}\\]\nwhere \\(\\beta_0\\) is the null-hypothesized value, usually 0. \\(\\sigma\\) is unknown, but \\(\\frac{\\hat{\\sigma}^2 (n - k)}{\\sigma^2} \\sim \\chi^2\\). The ratio of the normal distribution divided by the adjusted chi-square \\(\\sqrt{\\chi^2 / (n - k)}\\) is t-distributed,\n\\[t = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\hat{\\sigma}^2 (X'X)^{-1}}} = \\frac{\\hat{\\beta} - \\beta_0}{SE(\\hat{\\beta})}\\]\nThe \\((1 - \\alpha)\\) confidence intervals are \\(CI = \\hat{\\beta} \\pm t_{\\alpha / 2, df} SE(\\hat{\\beta})\\) with p-value equaling the probability of measuring a \\(t\\) of that extreme, \\(p = P(t &gt; |t|)\\). For a one-tail test, divide the reported p-value by two. The \\(SE(\\hat{\\beta})\\) decreases with i) a better fitting regression line (smaller \\(\\hat{\\sigma}^2\\)), ii) greater variation in the predictor (larger \\(X'X\\)), and iii) larger sample size (larger n).\nThe summary() output shows the t values and probabilities in the t value and Pr(&gt;|t|) columns. Verify this using matrix algebra to solve \\(t = \\frac{(\\hat{\\beta} - \\beta_1)}{SE(\\hat{\\beta})}\\) with \\(\\beta_1 = 0\\). The \\((1 - \\alpha)\\) confidence interval is \\(CI = \\hat{\\beta} \\pm t_{\\alpha / 2, df} SE(\\hat{\\beta})\\).\n\n\nShow the code\nt &lt;- beta_hat / se_beta_hat\np_value &lt;- pt(q = abs(t), df = n - k - 1, lower.tail = FALSE) * 2\nt_crit &lt;- qt(p = .05 / 2, df = n - k - 1, lower.tail = FALSE)\nlcl = beta_hat - t_crit * se_beta_hat\nucl = beta_hat + t_crit * se_beta_hat\ndata.frame(beta = round(beta_hat, 4), \n           se = round(se_beta_hat, 4), \n           t = round(t, 4), \n           p = round(p_value, 4),\n           lcl = round(lcl,4), \n           ucl = round(ucl, 4)) %&gt;% knitr::kable()\n\n\n\n\n\n\nbeta\nse\nt\np\nlcl\nucl\n\n\n\n\n(Intercept)\n12.3034\n18.7179\n0.6573\n0.5181\n-26.6226\n51.2293\n\n\ncyl\n-0.1114\n1.0450\n-0.1066\n0.9161\n-2.2847\n2.0618\n\n\ndisp\n0.0133\n0.0179\n0.7468\n0.4635\n-0.0238\n0.0505\n\n\nhp\n-0.0215\n0.0218\n-0.9868\n0.3350\n-0.0668\n0.0238\n\n\ndrat\n0.7871\n1.6354\n0.4813\n0.6353\n-2.6138\n4.1881\n\n\nwt\n-3.7153\n1.8944\n-1.9612\n0.0633\n-7.6550\n0.2243\n\n\nqsec\n0.8210\n0.7308\n1.1234\n0.2739\n-0.6988\n2.3409\n\n\nvsS\n0.3178\n2.1045\n0.1510\n0.8814\n-4.0588\n4.6943\n\n\nammanual\n2.5202\n2.0567\n1.2254\n0.2340\n-1.7568\n6.7973\n\n\ngear\n0.6554\n1.4933\n0.4389\n0.6652\n-2.4500\n3.7608\n\n\ncarb\n-0.1994\n0.8288\n-0.2406\n0.8122\n-1.9229\n1.5241\n\n\n\n\n\n\n\n1.4.2 F-Test\nThe F-test for the model is a test of the null hypothesis that none of the independent variables linearly predict the dependent variable, that is, the model parameters are jointly zero: \\(H_0 : \\beta_1 = \\ldots = \\beta_k = 0\\). The regression mean sum of squares \\(MSR = \\frac{(\\hat{y} - \\bar{y})'(\\hat{y} - \\bar{y})}{k-1}\\) and the error mean sum of squares \\(MSE = \\frac{\\hat{\\epsilon}'\\hat{\\epsilon}}{n-k}\\) are each chi-square variables. Their ratio has an F distribution with \\(k - 1\\) numerator degrees of freedom and \\(n - k\\) denominator degrees of freedom. The F statistic can also be expressed in terms of the coefficient of correlation \\(R^2 = \\frac{MSR}{MST}\\).\n\\[F(k - 1, n - k) = \\frac{MSR}{MSE} = \\frac{R^2}{1 - R^2} \\frac{n-k}{k-1}\\]\nMSE is \\(\\sigma^2\\). If \\(H_0\\) is true, that is, there is no relationship between the predictors and the response, then \\(MSR\\) is also equal to \\(\\sigma^2\\), so \\(F = 1\\). As \\(R^2 \\rightarrow 1\\), \\(F \\rightarrow \\infty\\), and as \\(R^2 \\rightarrow 0\\), \\(F \\rightarrow 0\\). F increases with \\(n\\) and decreases with \\(k\\).\nWhat is the probability that all parameters are jointly equal to zero? The F-statistic is presented at the bottom of the summary() function. Verify this manually.\n\n\nShow the code\nssr &lt;- sum((fit$fitted.values - mean(mt_cars$mpg))^2)\nsse &lt;- sum(fit$residuals^2)\nsst &lt;- sum((fit$mpg - mean(mt_cars$mpg))^2)\nmsr &lt;- ssr / k\nmse &lt;- sse / (n - k - 1)\nf = msr / mse\np_value &lt;- pf(q = f, df1 = k, df2 = n - k - 1, lower.tail = FALSE)\ncat(\"F-statistic: \", round(f, 4), \" on 3 and 65 DF,  p-value: \", p_value)\n\n\nF-statistic:  13.9325  on 3 and 65 DF,  p-value:  3.793152e-07\n\n\nThere is sufficient evidence (F = 17.35, p &lt; .0001) to reject \\(H_0\\) that the parameter estimators are jointly equal to zero.\naov() calculates the sequential sum of squares. The regression sum of squares SSR for mpg ~ cyl is 817.7. Adding disp to the model increases SSR by 37.6. Adding hp to the model increases SSR by 9.4. It would seem that hp does not improve the model.\n\n\nShow the code\nsummary(aov(fit))\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncyl          1  817.7   817.7 116.425 5.03e-10 ***\ndisp         1   37.6    37.6   5.353  0.03091 *  \nhp           1    9.4     9.4   1.334  0.26103    \ndrat         1   16.5    16.5   2.345  0.14064    \nwt           1   77.5    77.5  11.031  0.00324 ** \nqsec         1    3.9     3.9   0.562  0.46166    \nvs           1    0.1     0.1   0.018  0.89317    \nam           1   14.5    14.5   2.061  0.16586    \ngear         1    1.0     1.0   0.138  0.71365    \ncarb         1    0.4     0.4   0.058  0.81218    \nResiduals   21  147.5     7.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOrder matters. Had we started with disp, then added hp we would find both estimators were significant.\n\n\nShow the code\nsummary(aov(lm(mpg ~ disp + hp + ., data = mt_cars)))\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndisp         1  808.9   808.9 115.168 5.55e-10 ***\nhp           1   33.7    33.7   4.793  0.04001 *  \ncyl          1   22.1    22.1   3.150  0.09043 .  \ndrat         1   16.5    16.5   2.345  0.14064    \nwt           1   77.5    77.5  11.031  0.00324 ** \nqsec         1    3.9     3.9   0.562  0.46166    \nvs           1    0.1     0.1   0.018  0.89317    \nam           1   14.5    14.5   2.061  0.16586    \ngear         1    1.0     1.0   0.138  0.71365    \ncarb         1    0.4     0.4   0.058  0.81218    \nResiduals   21  147.5     7.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "01-linear_regression.html#interpretation",
    "href": "01-linear_regression.html#interpretation",
    "title": "1  Ordinary Least Squares",
    "section": "1.5 Interpretation",
    "text": "1.5 Interpretation\nA plot of the standardized coefficients shows the relative importance of each predictor. The distance the coefficients are from zero shows how much a change in a standard deviation of the predictor changes the mean of the predicted value. The CI shows the precision. The plot shows not only which variables are significant, but also which are important.\n\n\nShow the code\nmt_cars_scaled &lt;- mt_cars %&gt;% mutate(across(where(is.numeric), scale))\nfit_scaled &lt;- lm(mpg ~ ., mt_cars_scaled)\n\nfit_scaled %&gt;%\n  tidy() %&gt;%\n  mutate(\n    lwr = estimate - qt(.05/2, fit_scaled$df.residual) * std.error,\n    upr = estimate + qt(.05/2, fit_scaled$df.residual) * std.error,\n  ) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  ggplot() +\n  geom_point(aes(y = term, x = estimate)) +\n  geom_errorbar(aes(y = term, xmin = lwr, xmax = upr), width = .2) +\n  geom_vline(xintercept = 0, linetype = 4) +\n  labs(title = \"Model Feature Importance\", x = \"Standardized Weight\", y = NULL)  \n\n\n\n\n\n\n\n\n\nThe added variable plot shows the bivariate relationship between \\(Y\\) and \\(X_i\\) after accounting for the other variables. For example, the partial regression plots of y ~ x1 + x2 + x3 would plot the residuals of y ~ x2 + x3 vs x1, and so on.\n\n\nShow the code\ncar::avPlots(fit, layout = c(4, 3))",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "01-linear_regression.html#model-validation",
    "href": "01-linear_regression.html#model-validation",
    "title": "1  Ordinary Least Squares",
    "section": "1.6 Model Validation",
    "text": "1.6 Model Validation\nEvaluate predictive accuracy by training the model on a training data set and testing on a test data set.\n\n1.6.1 Accuracy Metrics\nThe most common measures of model fit are R-squared, Adjusted R-squared, RMSE, RSE, MAE, AIC, AICc, BIC, and Mallow’s Cp.\n\n\n1.6.2 R-Squared\nThe coefficient of determination (R-squared) is the percent of total variation in the response variable that is explained by the regression line.\n\\[R^2 = \\frac{RSS}{SST} = 1 - \\frac{SSE}{SST}\\]\nwhere \\(SSE = \\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}\\) is the sum squared differences between the predicted and observed value, \\(SST = \\sum_{i = 1}^n{(y_i - \\bar{y})^2}\\) is the sum of squared differences between the observed and overall mean value, and \\(RSS = \\sum_{i=1}^n{(\\hat{y}_i - \\bar{y})^2}\\) is the sum of squared differences between the predicted and overall mean “no-relationship line” value. At the extremes, \\(R^2 = 1\\) means all data points fall perfectly on the regression line - the predictors account for all variation in the response; \\(R^2 = 0\\) means the regression line is horizontal at \\(\\bar{y}\\) - the predictors account for none of the variation in the response. In the simple case of a single predictor variable, \\(R^2\\) equals the squared correlation between \\(x\\) and \\(y\\), \\(Cor(x,y)\\).\n\n\nShow the code\nssr &lt;- sum((fit$fitted.values - mean(mt_cars$mpg))^2)\nsse &lt;- sum(fit$residuals^2)\nsst &lt;- sum((mt_cars$mpg - mean(mt_cars$mpg))^2)\n(r2 &lt;- ssr / sst)\n## [1] 0.8690158\n(r2 &lt;- 1 - sse / sst)\n## [1] 0.8690158\n(r2 &lt;- summary(fit)$r.squared)\n## [1] 0.8690158\n\n\nThe sums of squares are the same thing as the variances multiplied by the degrees of freedom.\n\n\nShow the code\nssr2 &lt;- var(fitted(fit)) * (n - 1)\nsse2 &lt;- var(residuals(fit)) * (n - 1)\nsst2 &lt;- var(mt_cars$mpg) * (n - 1)\nssr2 / sst2\n\n\n[1] 0.8690158\n\n\n\\(R^2\\) is also equal to the correlation between the fitted value and observed values, \\(R^2 = Cor(Y, \\hat{Y})^2\\).\n\n\nShow the code\ncor(fit$fitted.values, mt_cars$mpg)^2\n\n\n[1] 0.8690158\n\n\nR-squared is proportional to the the variance in the response, SST. Given a constant percentage error in predictions, a test set with relatively low variation in the reponse will have a lower R-squared. Conversely, test sets with large variation, e.g., housing data with home sale ranging from $60K to $2M may have a large R-squared despite average prediction errors of &gt;$10K.\nA close variant of R-squared is the non-parametric Spearman’s rank correlation. This statistic is the correlation of the ranks of the response and the predicted values. It is used when the model goal is ranking.\nThe adjusted R-squared (\\(\\bar{R}^2\\)) penalizes the R-squared metric for increasing number of predictors.\n\\[\\bar{R}^2 = 1 - \\frac{SSE}{SST} \\cdot \\frac{n-1}{n-k-1}\\]\n\n\nShow the code\n(adj_r2 &lt;- 1 - sse/sst * (n - 1) / (n - k - 1))\n## [1] 0.8066423\n\nsummary(fit)$adj.r.squared\n## [1] 0.8066423\n\n\n\n1.6.2.1 RMSE, RSE, MAE\nThe root mean squared error (RMSE) is the average prediction error (square root of mean squared error).\n\\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}}{n}}\\]\n\n\nShow the code\nsqrt(mean((mt_cars$mpg - fit$fitted.values)^2))\n## [1] 2.146905\naugment(fit) %&gt;% yardstick::rmse(truth = mpg, estimate = .fitted)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard        2.15\n\n\nThe mean squared error of a model with theoretical residual of mean zero and constant variance \\(\\sigma^2\\) can be decomposed into the model’s bias and the model’s variance:\n\\[E[MSE] = \\sigma^2 + Bias^2 + Var.\\]\nA model that predicts the response closely will have low bias, but be relatively sensitive to the training data and thus have high variance. A model that predicts the response conservatively (e.g., a simple mean) will have large bias, but be relatively insensitive to nuances in the training data. Here is an example of a simulated sine wave. A model predicting the mean value at the upper and lower levels has low variance, but high bias, and a model of an actual sine wave has low bias and high variance. This is the variance-bias trade-off.\n\n\nShow the code\ndata.frame(x = seq(2, 10, 0.1)) %&gt;%\n  mutate(\n    e = runif(81, -.5, .5),\n    y = sin(x) + e,\n    `Low var, high bias` = zoo:::rollmean(y, k = 3, fill = NA, align = \"center\"),\n    `High var, low bias` = sin(x)\n  ) %&gt;%\n  pivot_longer(cols = `Low var, high bias`:`High var, low bias`) %&gt;%\n  ggplot(aes(x = x)) +\n  geom_point(aes(y = y)) +\n  geom_line(aes(y = value, color = name), size = 1)\n\n\n\n\n\n\n\n\n\nThe residual standard error (RSE, or model sigma \\(\\hat{\\sigma}\\)) is an estimate of the standard deviation of \\(\\epsilon\\). It is roughly the average amount the response deviates from the true regression line.\n\\[\\sigma = \\sqrt{\\frac{\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}}{n-k-1}}\\]\n\n\nShow the code\nsqrt(sum((mt_cars$mpg - fit$fitted.values)^2) / (n - k - 1))\n## [1] 2.650197\n\n# sd is sqrt(sse / (n-1)), sigma = sqrt(sse / (n - k - 1))\nsd(fit$residuals) * sqrt((n - 1) / (n - k - 1))  \n## [1] 2.650197\n\nsummary(fit)$sigma \n## [1] 2.650197\n\nsigma(fit)\n## [1] 2.650197\n\n\nThe mean absolute error (MAE) is the average absolute prediction arror. It is less sensitive to outliers.\n\\[MAE = \\frac{\\sum_{i=1}^n{|y_i - \\hat{y}_i|}}{n}\\]\n\n\nShow the code\nsum(abs(mt_cars$mpg - fit$fitted.values)) / n\n\n\n[1] 1.72274\n\n\nThese metrics are good for evaluating a model, but less useful for comparing models. The problem is that they tend to improve with additional variables added to the model, even if the improvement is not significant. The following metrics aid model comparison by penalizing added variables.\n\n\n1.6.2.2 AIC, BIC\nAkaike’s Information Criteria (AIC) is a penalization metric. The lower the AIC, the better the model.\n\n\nShow the code\nAIC(fit)\n## [1] 163.7098\n# AICc corrects AIC for small sample sizes.\nAIC(fit) + (2 * k * (k + 1)) / (n - k - 1)\n## [1] 174.186\n\n\nThe Bayesian information criteria (BIC) is like AIC, but with a stronger penalty for additional variables.\n\n\nShow the code\nBIC(fit)\n\n\n[1] 181.2986\n\n\nbroom::glance() calculates many validation metrics. Compare the full model to a reduced model without cyl.\n\n\nShow the code\nbroom::glance(fit) %&gt;% select(adj.r.squared, sigma, AIC, BIC, p.value)\n\n\n# A tibble: 1 × 5\n  adj.r.squared sigma   AIC   BIC     p.value\n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1         0.807  2.65  164.  181. 0.000000379\n\n\nShow the code\nglance(lm(mpg ~ . - cyl, mt_cars)) %&gt;% select(adj.r.squared, sigma, AIC, BIC, p.value)\n\n\n# A tibble: 1 × 5\n  adj.r.squared sigma   AIC   BIC      p.value\n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1         0.815  2.59  162.  178. 0.0000000903\n\n\nThe adjusted R2 increased and AIC and BIC decreased, meaning the full model is less efficient at explaining the variability in the response value. The residual standard error sigma is smaller for the reduced model. Finally, the F statistic p-value is smaller for the reduced model, meaning the reduced model is statistically more significant.\nNote that these regression metrics are all internal measures, that is they have been computed on the training dataset, not the test dataset.\n\n\n\n1.6.3 Cross-Validation\nCross-validation is a set of methods for measuring the performance of a predictive model on a test dataset. The main measures of prediction performance are R2, RMSE and MAE.\n\n1.6.3.1 Validation Set\nTo perform validation set cross validation, randomly split the data into a training data set and a test data set. Fit models to the training data set, then predict values with the validation set. The model that produces the best prediction performance is the preferred model.\n\n\nShow the code\nset.seed(123)\n\nmt_cars_split &lt;- initial_split(mt_cars, prop = 0.7, strata = mpg)\nmt_cars_training &lt;- mt_cars_split %&gt;% training()\nmt_cars_testing &lt;- mt_cars_split %&gt;% testing()\n\nfit_validation &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\") %&gt;%\n  fit(mpg ~ ., data = mt_cars_training) %&gt;%\n  predict(new_data = mt_cars_testing) %&gt;%\n  bind_cols(mt_cars_testing)\n\nfit_validation %&gt;% rmse(truth = mpg, estimate = .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard        2.28\nfit_validation %&gt;% rsq(truth = mpg, estimate = .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rsq     standard       0.802\n\n\nAlternatively, last_fit() i) splits train/test, ii) fits, and iii) predicts.\n\n\nShow the code\nfit_validation_2 &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\") %&gt;%\n  last_fit(mpg ~ ., split = mt_cars_split)\nfit_validation_2 %&gt;% collect_metrics()\nfit_validation_2 %&gt;% collect_predictions()\n\n\nThe validation set method is only useful when you have a large data set to partition. A second disadvantage is that building a model on a fraction of the data leaves out information. The test error will vary with which observations are included in the training set.\n\n\n1.6.3.2 LOOCV\nLeave one out cross validation (LOOCV) works by successively modeling with training sets leaving out one data point, then averaging the prediction errors.\n\n\nShow the code\nset.seed(123)\nm2 &lt;- train(mpg ~ ., \n            data = d.train[, 1:9],\n            method = \"lm\",\n            trControl = trainControl(method = \"LOOCV\"))\nprint(m2)\npostResample(pred = predict(m2, newdata = d.test), \n             obs = d.test$mpg)\n\n\nThis method isn’t perfect either. It repeats as many times as there are data points, so the execution time may be long. LOOCV is also sensitive to outliers.\n\n\n1.6.3.3 K-fold Cross-Validation\nK-fold cross-validation splits the dataset into k folds (subsets), then uses k-1 of the folds for a training set and the remaining fold for a test set, then repeats for all permutations of k taken k-1 at a time. E.g., 3-fold cross-validation will partition the data into sets A, B, and C, then create train/test splits of [AB, C], [AC, B], and [BC, A].\nK-fold cross-validation is less computationally expensive than LOOCV, and often yields more accurate test error rate estimates. What is the right value of k? The lower is k the more biased the estimates; the higher is k the larger the estimate variability. At the extremes k = 2 is the validation set method, and k = n is the LOOCV method. In practice, one typically performs k-fold cross-validation using k = 5 or k = 10 because these values have been empirically shown to balence bias and variance.\n\n\nShow the code\nset.seed(123)\nm3 &lt;- train(mpg ~ ., \n            data = d.train[, 1:9],\n            method = \"lm\",\n            trControl = trainControl(method = \"cv\",\n                                     number = 5))\nprint(m3)\npostResample(pred = predict(m3, newdata = d.test), \n             obs = d.test$mpg)\n\n\n\n\n1.6.3.4 Repeated K-fold CV\nYou can also perform k-fold cross-validation multiple times and average the results. Specify method = \"repeatedcv\" and repeats = 3 in the trainControl object for three repeats.\n\n\nShow the code\nset.seed(123)\nm4 &lt;- train(mpg ~ ., \n            data = d.train[, 1:9],\n            method = \"lm\",\n            trControl = trainControl(method = \"repeatedcv\",\n                                     number = 5,\n                                     repeats = 3))\nprint(m4)\npostResample(pred = predict(m4, newdata = d.test), \n             obs = d.test$mpg)\n\n\n\n\n1.6.3.5 Bootstrapping\nBootstrapping randomly selects a sample of n observations with replacement from the original dataset to evaluate the model. The procedure is repeated many times.\nSpecify method = \"boot\" and number = 100 to perform 100 bootstrap samples.\n\n\nShow the code\nset.seed(123)\nm5 &lt;- train(mpg ~ ., \n            data = d.train[, 1:9],\n            method = \"lm\",\n            trControl = trainControl(method = \"boot\",\n                                     number = 100))\nprint(m5)\npostResample(pred = predict(m5, newdata = d.test), \n             obs = d.test$mpg)\n\n\n\n\n\n1.6.4 Gain Curve\nFor supervised learning purposes, a visual way to evaluate a regression model is with the gain curve. This visualization compares a predictive model score to an actual outcome (either binary (0/1) or continuous). The gain curve plot measures how well the model score sorts the data compared to the true outcome value. The x-axis is the fraction of items seen when sorted by score, and the y-axis is the cumulative summed true outcome when sorted by score. For comparison, GainCurvePlot also plots the “wizard curve”: the gain curve when the data is sorted according to its true outcome. A relative Gini score close to 1 means the model sorts responses well.\n\n\nShow the code\nfit_validation %&gt;%\n  WVPlots::GainCurvePlot(xvar = \".pred\", truthVar = \"mpg\", title = \"Model Gain Curve\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMolnar, Christoph. 2020. Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "01-linear_regression.html#footnotes",
    "href": "01-linear_regression.html#footnotes",
    "title": "1  Ordinary Least Squares",
    "section": "",
    "text": "Penn State University, STAT 501, Lesson 12: Multicollinearity & Other Regression Pitfalls. https://newonlinecourses.science.psu.edu/stat501/lesson/12.↩︎\nSTHDA. Bootstrap Resampling Essentials in R. http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/↩︎\nHelp with matrix algebra in r notes at R for Dummies↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "02-generalized_linear_models.html",
    "href": "02-generalized_linear_models.html",
    "title": "2  Generalized Linear Models (GLM)",
    "section": "",
    "text": "2.1 Binomial Logistic Regression\nLogistic regression estimates the probability that a categorical dependent variable is a particular level. The dependent variable levels can be binomial, multinomial, or ordinal. The binary logistic regression model is\n\\[y_i = \\mathrm{logit}(\\pi_i) = \\log \\left( \\frac{\\pi_i}{1 - \\pi_i} \\right) = X_i \\beta\\]\nwhere \\(\\pi_i\\) is the response \\(i\\)’s binary level probability. The model predicts the log odds of the level. Why do this? The range of outcomes need to be between 0 and 1, and a sigmoid function, \\(f(y) = 1 / \\left(1 + e^y \\right)\\), does that. If the log odds of the level equals \\(X_i\\beta\\), then the odds of the level equals \\(e^{X\\beta}\\). You can solve for \\(\\pi_i\\) to get \\(\\pi = \\mathrm{odds} / (\\mathrm{odds} + 1)\\). Substituting,\n\\[\\pi_i = \\frac{\\exp(y_i)}{1 + \\exp(y_i)} = \\frac{e^{X_i\\beta}}{1 + e^{X_i\\beta}}\\]\nwhich you can simplify to \\(\\pi_i = 1 / (1 + e^{-X_i\\beta})\\), a sigmoid function. The upshot is \\(X\\beta\\) is the functional relationship between the independent variables and a function of the response, not the response itself.\nThe model parameters are estimated either by iteratively reweighted least squares optimization or by maximum likelihood estimation (MLE). MLE maximizes the probability produced by a set of parameters \\(\\beta\\) given a data set and probability distribution.3 In logistic regression the probability distribution is the binomial and each observation is the outcome of a single Bernoulli trial.\n\\[L(\\beta; y, X) = \\prod_{i=1}^n \\pi_i^{y_i}(1 - \\pi_i)^{(1-y_i)} = \\prod_{i=1}^n\\frac{\\exp(y_i X_i \\beta)}{1 + \\exp(X_i \\beta)}.\\]\nIn practice, multiplying many small probabilities can be unstable, so MLE optimizes the log likelihood instead.\n\\[\\begin{align}\nl(\\beta; y, X) &= \\sum_{i = 1}^n \\left(y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i) \\right) \\\\\n               &= \\sum_{i = 1}^n \\left(y_i X_i \\beta - \\log(1 + e^{X_i\\beta}) \\right)\n\\end{align}\\]\nSometimes you will see the cost function optimized. The cost function is the negative of of the log likelihood function.\nAssumptions\nThe binomial logistic regression model requires a dichotomous dependent variable and independent observations. The sample size should be large, at least 10 observations per dependent variable level and independent variable. There are three conditions related to the data distribution: i) the logit transformation must be linearly related to any continuous independent variables, ii) there must be no multicollinearity, and iii) there must be no influential outliers.\nBe aware of over-dispersion, a common issue with GLM. For a binomial logistic regression, the response variable should be distributed \\(y_i \\sim \\mathrm{Bin}(n_i, \\pi_i)\\) with \\(\\mu_i = n_i \\pi_i\\) and \\(\\sigma^2 = \\pi (1 - \\pi)\\). Over-dispersion means the data shows evidence of variance greater than \\(\\sigma^2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Linear Models (GLM)</span>"
    ]
  },
  {
    "objectID": "02-generalized_linear_models.html#binomiallogistic",
    "href": "02-generalized_linear_models.html#binomiallogistic",
    "title": "2  Generalized Linear Models (GLM)",
    "section": "",
    "text": "Case Study\nThis case study uses the Laerd Statistics article on binomial logistic regression data set. A study investigates the relationship between the incidence of heart disease (Yes|No) and age, weight, gender, and maximal aerobic capacity using data from n = 100 participants.\n\n\nShow the code\ncs1 &lt;- list()\n\ncs1$dat &lt;- foreign::read.spss(\"./input/logistic-regression.sav\", to.data.frame = TRUE)\n\ncs1$dat %&gt;%\n  gtsummary::tbl_summary(\n    by = heart_disease,\n    include = -caseno,\n    percent = \"row\",\n    statistic = list(gtsummary::all_continuous() ~ \"{mean}, {sd}\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nNo N = 651\nYes N = 351\n\n\n\n\nage\n39, 8\n45, 9\n\n\nweight\n77, 14\n85, 15\n\n\ngender\n\n\n\n\n\n\n    Female\n29 (78%)\n8 (22%)\n\n\n    Male\n36 (57%)\n27 (43%)\n\n\nVO2max\n45, 9\n41, 6\n\n\n\n1 Mean, SD; n (%)\n\n\n\n\n\n\n\n\n\nOverall, men are twice as likely to have heart disease. Male odds are .43/.57 = 0.8 and female odds are .22/.78 = 0.3, an male-to-female odds ratio of 2.7.\n\n\nShow the code\ncs1$dat %&gt;% gtsummary::tbl_cross(row = heart_disease, col = gender, percent = \"column\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngender\n\nTotal\n\n\nFemale\nMale\n\n\n\n\nheart_disease\n\n\n\n\n\n\n\n\n    No\n29 (78%)\n36 (57%)\n65 (65%)\n\n\n    Yes\n8 (22%)\n27 (43%)\n35 (35%)\n\n\nTotal\n37 (100%)\n63 (100%)\n100 (100%)\n\n\n\n\n\n\n\nAge, weight, and poor max aerobic capacity are positively associated with heart disease.\n\n\nShow the code\ncs1$dat %&gt;%\n  pivot_longer(cols = c(age, weight, VO2max)) %&gt;%\n  ggplot(aes(x = heart_disease, y = value)) + \n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(aes(color = name)) +\n  facet_wrap(facets = vars(name), scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\nConsider centering the continuous variables around their means to facilitate model interpretation. The intercept term in the fitted model would represent a reasonable condition, not a zero-aged, zero-weighted person with no aerobic capacity. This is the way to go if you want to present your findings in the framework of a baseline probability (or odds) and the incremental effects of the independent variables. You might also standardize the continuous vars to get a more meaningful increment. On the other hand, if you want to use your model for predicting outcomes, you’ll have to back out of the centering when you predict values.\n\nIf your model is predictive rather than inferential, split the data into training/testing data sets.\n\n\nShow the code\n# For reproducibility\nset.seed(123)\n\n(x &lt;- initial_split(cs1$dat, prop = 0.7, strata = heart_disease))\n## &lt;Training/Testing/Total&gt;\n## &lt;69/31/100&gt;\n\ncs1$dat_training &lt;- training(x)\ndim(cs1$dat_training)\n## [1] 69  6\n\ncs1$dat_testing &lt;- testing(x)\ndim(cs1$dat_testing)\n## [1] 31  6\n\n\n\n\nFit the Model\nFit the model using the tidymodels framework. If you want to continue using the classic methodology, the glm object is inside the tidymodels fit. The model fit returns a brief summary with the coefficients and model diagnostics.\n\n\nShow the code\ncs1$model &lt;- \n  logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\") \n\ncs1$fit &lt;-\n  cs1$model %&gt;%\n  fit(heart_disease ~ age + weight + VO2max + gender, data = cs1$dat)\n\n# The fit object returned by glm(). You'll need this for interpretation and \n# checking assumptions.\ncs1$result &lt;-\n  cs1$fit %&gt;%\n  extract_fit_engine()\n\n# If you are fitting a predictive model, use the training set.\ncs1$fit_training &lt;-\n  cs1$model %&gt;%\n  fit(heart_disease ~ age + weight + VO2max + gender, data = cs1$dat_training)\n\ncs1$result %&gt;% summary()\n\n\n\nCall:\nstats::glm(formula = heart_disease ~ age + weight + VO2max + \n    gender, family = stats::binomial, data = data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -1.676469   3.336079  -0.503  0.61530   \nage          0.085098   0.028160   3.022  0.00251 **\nweight       0.005727   0.022442   0.255  0.79858   \nVO2max      -0.099024   0.047944  -2.065  0.03889 * \ngenderMale   1.949639   0.842413   2.314  0.02065 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 129.49  on 99  degrees of freedom\nResidual deviance: 102.09  on 95  degrees of freedom\nAIC: 112.09\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe null deviance, G^2, is the likelihood ratio of the intercept-only model with 69 rows - 1 parameter = 99 degrees of freedom. It is the sum of the squared deviance residuals. The residual deviance is the likelihood ratio of the full model with 100 - 5 parameters = 95 degrees of freedom.\nThe residual deviance is distributed chi-squared and can be used to test whether the model differs from the saturated model (model with as many coefficients as observations, G^2 = 0, df = 0) where \\(H_0\\) = no difference. The deviance test for lack of fit fails to reject the null hypothesis.\n\n\nShow the code\n# G^2 calculations\ncs1$result %&gt;% residuals(type = \"deviance\") %&gt;% .^2 %&gt;% sum()\n## [1] 102.0878\ncs1$result %&gt;% deviance()\n## [1] 102.0878\n\n# df\ndf.residual(cs1$result)\n## [1] 95\n\n# G^2 is distributed chi-squared with df degrees of freedom\npchisq(deviance(cs1$result), df = df.residual(cs1$result), lower.tail = FALSE)\n## [1] 0.2911469\nvcdExtra::LRstats(cs1$result)\n## Likelihood summary table:\n##               AIC    BIC LR Chisq Df Pr(&gt;Chisq)\n## cs1$result 112.09 125.11   102.09 95     0.2911\n\n\nThese two deviances, the null and residual, are shown in the ANOVA summary. An ANOVA table shows the change in deviance from successively adding each variable to the model.\n\n\nShow the code\nanova(cs1$result)\n\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: heart_disease\n\nTerms added sequentially (first to last)\n\n       Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                      99     129.49              \nage     1  11.9074        98     117.58 0.0005591 ***\nweight  1   9.1820        97     108.40 0.0024440 ** \nVO2max  1   0.5045        96     107.89 0.4775433    \ngender  1   5.8076        95     102.09 0.0159568 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDeviance residuals are one of four residuals you can calculate from a binary logistic regression.4 One is the raw residual, \\(\\epsilon_i = y_i - \\hat{p}_i\\), where \\(\\hat{p}_i\\) is the predicted probability. Another is the Pearson residual, \\(r_i = \\frac{\\epsilon_i}{\\sqrt{\\hat{p}_i(1 - \\hat{p}_i)}}\\), the raw residual rescaled by dividing by the estimated standard deviation of a binomial distribution with 1 trial5. A third is the standardized Pearson residual, \\(rs_i = r_i / \\sqrt{1 - \\hat{h}_i}\\), the Pearson residual adjusted for the leverage of the predictors using the hat-values. Hat-values measure the predictor distances from the mean. This residual is especially useful to evaluate model fit because if the model fits well, these residuals have a standard normal distribution. Finally, there are the deviance residuals, \\(d_i = \\mathrm{sign}(\\epsilon_i) \\left[ -2(y_i \\log \\hat{p}_i + (1 - y_i) \\log (1 - \\hat{p}_i)) \\right]^{.5}\\). Deviance Residuals measure how much the estimated probabilities differ from the observed proportions of success. You want deviance residuals to be evenly distributed (in absolute values, 1Q \\(\\approx\\) 3Q, min \\(\\approx\\) max). You also want the min and max to be &lt;3 because deviance residuals are roughly approximated by a standard normal distribution.\n\n\nShow the code\nbind_rows(\n  Raw = cs1$result %&gt;% residuals(type = \"response\") %&gt;% summary(),\n  Pearson = cs1$result %&gt;% residuals(type = \"pearson\") %&gt;% summary(),\n  `Standardized Pearson` = cs1$result %&gt;% rstandard(type = \"pearson\") %&gt;% summary(),\n  Deviance = cs1$result %&gt;% residuals(type = \"deviance\") %&gt;% summary(),\n  .id = \"Residual\"\n)\n\n\n# A tibble: 4 × 7\n  Residual             Min.        `1st Qu.`   Median      Mean  `3rd Qu.` Max. \n  &lt;chr&gt;                &lt;table[1d]&gt; &lt;table[1d]&gt; &lt;table[1d]&gt; &lt;tab&gt; &lt;table[1&gt; &lt;tab&gt;\n1 Raw                  -0.7954587  -0.2500077  -0.1061849  -8.0… 0.3532839 0.91…\n2 Pearson              -1.9720520  -0.5773622  -0.3446596  -1.8… 0.7391867 3.34…\n3 Standardized Pearson -2.1794172  -0.5918874  -0.3521058  -2.1… 0.7592960 3.39…\n4 Deviance             -1.7815642  -0.7585405  -0.4738051  -8.0… 0.9336278 2.23…\n\n\n\n\nInterpretation\nBefore we look at the coefficient estimations, consider what it is they are predicting: the log odds of the binary response. To see what that means, plug in values for the explanatory variables to get predictions. \\(\\hat{y}\\) is the log odds of having heart disease.\n\n\nShow the code\n(mean_person &lt;- \n  cs1$dat %&gt;%\n  select(-caseno) %&gt;%\n  summarize(.by = gender, across(where(is.numeric), mean)))\n##   gender      age   weight   VO2max\n## 1   Male 40.79365 84.83270 46.40095\n## 2 Female 41.62162 70.85324 38.91135\n\npred_log_odds &lt;- cs1$fit %&gt;% predict(new_data = mean_person, type = \"raw\")\nnames(pred_log_odds) &lt;- mean_person$gender\npred_log_odds\n##       Male     Female \n## -0.3643411 -1.5819310\n\n\nExponentiate to get the odds, \\(\\exp (\\hat{y}) = \\frac{\\pi}{1 - \\pi}\\).\n\n\nShow the code\n(pred_odds &lt;- exp(pred_log_odds))\n\n\n     Male    Female \n0.6946542 0.2055777 \n\n\nSolve for \\(\\pi = \\frac{\\exp (\\hat{y})}{1 + \\exp (\\hat{y})}\\) to get the probability. Do the math, or use predict(type = \"prob\").\n\n\nShow the code\n(pred_prob &lt;- pred_odds / (1 + pred_odds))\n##      Male    Female \n## 0.4099091 0.1705222\ncs1$fit %&gt;% predict(new_data = mean_person, type = \"prob\")\n## # A tibble: 2 × 2\n##   .pred_No .pred_Yes\n##      &lt;dbl&gt;     &lt;dbl&gt;\n## 1    0.590     0.410\n## 2    0.829     0.171\n\n\nNow let’s interpret the coefficients.\n\n\nShow the code\ncs1$fit %&gt;% tidy()\n\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -1.68       3.34      -0.503 0.615  \n2 age          0.0851     0.0282     3.02  0.00251\n3 weight       0.00573    0.0224     0.255 0.799  \n4 VO2max      -0.0990     0.0479    -2.07  0.0389 \n5 genderMale   1.95       0.842      2.31  0.0206 \n\n\nThe intercept term is the log-odds of heart disease for the reference case. The reference case in the model is gender = “Female”, age = 0, weight = 0, and VO2max = 0. If the data was centered, the reference case would actually meaningful.\n\n\nShow the code\ncs1$fit %&gt;%\n  predict(new_data = list(age = 0, weight = 0, VO2max = 0, gender = \"Female\"), \n          type = \"raw\")\n\n\n        1 \n-1.676469 \n\n\nColumn “statistic” is the Wald \\(z\\) statistic, \\(z = \\hat{\\beta} / SE(\\hat{\\beta})\\). Its square is the Wald chi-squared statistic. The p-value is the area to the right of \\(z^2\\) in the \\(\\chi_1^2\\) density curve:\n\n\nShow the code\ncs1$fit %&gt;% \n  tidy() %&gt;%\n  mutate(p.chisq = map_dbl(statistic, ~pchisq(.^2, df = 1, lower.tail = FALSE))) %&gt;%\n  pull(p.chisq)\n\n\n[1] 0.615297092 0.002511644 0.798578389 0.038885956 0.020648470\n\n\nInterpret the coefficient estimates as the change in the log odds of \\(y\\) due to a one unit change in \\(x\\). If \\(\\delta = x_a - x_b\\), then a \\(\\delta\\) change in \\(x\\) is associated with a \\(\\delta \\hat{\\beta}\\) change in the log odds of \\(y\\). \\(\\beta\\) is the log odds ratio of \\(x_a\\) vs \\(x_b\\).\n\\[\\log \\left(\\pi / (1 - \\pi) |_{x = x_a} \\right) - \\log \\left(\\pi / (1 - \\pi) |_{x = x_b} \\right) = \\log \\left( \\frac{\\pi / (1 - \\pi) |_{x = x_a}}{\\pi / (1 - \\pi) |_{x = x_b}} \\right) = \\delta \\hat{\\beta}\\]\nThe exponential of the coefficient estimates is the change in the odds of \\(y\\) due to a \\(\\delta\\) change in \\(x\\). \\(\\exp \\beta\\) is the odds ratio of \\(x_a\\) vs \\(x_b\\).\n\\[\\mathrm{odds}(y) = e^{\\delta \\hat{\\beta}}\\]\n\n\nShow the code\ncs1$fit %&gt;% tidy(exponentiate = TRUE)\n\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    0.187    3.34      -0.503 0.615  \n2 age            1.09     0.0282     3.02  0.00251\n3 weight         1.01     0.0224     0.255 0.799  \n4 VO2max         0.906    0.0479    -2.07  0.0389 \n5 genderMale     7.03     0.842      2.31  0.0206 \n\n\nAll covariates held equal, a male’s log odds of heart disease are 1.95 times that of a female’s (log(OR)). A male’s odds are 7.03 times that of a female’s (OR). Of course, all covariate’s are not equal - males are heavier and have higher VO2max. Let’s run the calculations with the mean predictor values for male and female.\n\n\nShow the code\n# Log OR\npred_log_odds[\"Male\"] / pred_log_odds[\"Female\"]\n##      Male \n## 0.2303142\n\n# OR\npred_odds[\"Male\"] / pred_odds[\"Female\"]\n##     Male \n## 3.379034\n\n\nA one-unit increase in any of the continuous independent variables is interpreted similarly. The reference level is unimportant since the change is constant across the range of values. A one year increase in age increases the log-odds of heart disease by a factor of 0.09, and the odds by a factor of 1.09. To calculate the effect of a decade increase in age, multiply \\(\\beta\\) by 10 before exponentiating, or raise the exponentiated coeficient by 10. The effect of a 10-year increase in age is to increase the odds of heart disease by 2.34. The odds double every ten years.\noddsratio::or_glm() is a handy way to calculate odds ratios from arbitrary increments to the predictors. Here are the ORs of a 10-year age change, 10 kg weight change, and VO2max change of 5.\n\n\nShow the code\noddsratio::or_glm(\n  cs1$dat, \n  cs1$result, \n  incr = list(age = 10, weight = 25, VO2max = -12)\n)\n\n\n   predictor oddsratio ci_low (2.5) ci_high (97.5)          increment\n1        age     2.342        1.391          4.270                 10\n2     weight     1.154        0.381          3.572                 25\n3     VO2max     3.281       11.033          1.124                -12\n4 genderMale     7.026        1.428         40.155 Indicator variable\n\n\nNotice that the predicted probabilities have the sigmoidal shape of the binary relationship.\n\n\nShow the code\naugment(cs1$fit, new_data = cs1$dat, type = \"raw\") %&gt;%\n  ggplot(aes(x = age, color = gender)) +\n  geom_point(aes(y = as.numeric(heart_disease == \"Yes\"))) +\n  geom_point(aes(y = .pred_Yes), shape = 4) +\n  geom_smooth(aes(y = .pred_Yes), se = FALSE) +\n  labs(x = \"Age\",\n       y = NULL,\n       title = \"Binary Fitted Line Plot\") +\n  scale_y_continuous(breaks = c(0,1), labels = c(\"Healthy\", \"Heart Disease\")) +\n  theme_light() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\nAssumptions\nFour assumptions relate to the study design: (1) the dependent variable is dichotomous; (2) the observations are independent; (3) the categories of all nominal variables are mutually exclusive and exhaustive; and (4) there are at least 10 observations per dependent variable level and independent variable. These assumptions are all valid here. Three more assumptions related to the data distribution:\n\nThere is a linear relationship between the logit transformation and the continuous independent variables. Test with a plot and with Box-Tidwell.\nThere is no independent variable multicollinearity. Test with correlation coefficients and variance inflation factors (VIF).\nThere are no influential outliers. Test with Cook’s distance.\n\nTest the linearity assumption first. There are two ways to do this (do both). First, fit your model, then plot the fitted values against the continuous predictors. This is the GLM analog to OLS bivariate analysis, except now the dependent variable is the logit transformation. These plotted relationships look pretty linear.\n\n\nShow the code\ncs1$fit %&gt;%\n  augment(new_data = cs1$dat) %&gt;%\n  pivot_longer(c(age, weight, VO2max)) %&gt;%\n  ggplot(aes(x = value, y = .pred_Yes)) +\n  geom_point() +\n  facet_wrap(facets = vars(name), scales = \"free_x\") +\n  geom_smooth(method = \"lm\", formula = \"y~x\") +\n  labs(title = \"Linearity Test: predicted vs continuous predictors\", x = NULL)\n\n\n\n\n\n\n\n\n\nThe second test for linearity is the Box-Tidwell approach. Add transformations of the continuous independent variables to the model, \\(x_{Tx} = x \\log x\\), then test their significance level in the fit.\n\n\nShow the code\n# Using non-centered vars to avoid log(0) errors.\nx &lt;- \n  cs1$dat %&gt;%\n  mutate(\n    age_tx = log(age) * age,\n    weight_tx = log(weight) * weight,\n    VO2max_tx = log(VO2max) * VO2max\n  )\n\ncs1$boxtidwell_fit &lt;- \n  logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  fit(heart_disease ~ age + weight + VO2max + gender + \n        age_tx + weight_tx + VO2max_tx, \n      data = x)\n\ncs1$boxtidwell_fit %&gt;% tidy()\n\n\n# A tibble: 8 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -40.6       21.7      -1.87   0.0615\n2 age           2.73       1.10      2.47   0.0135\n3 weight        0.144      0.783     0.184  0.854 \n4 VO2max        1.32       1.82      0.724  0.469 \n5 genderMale    1.85       0.922     2.01   0.0443\n6 age_tx       -0.543      0.227    -2.40   0.0164\n7 weight_tx    -0.0266     0.146    -0.182  0.855 \n8 VO2max_tx    -0.301      0.382    -0.788  0.431 \n\n\nFocus on the three transformed variables. age_tx is the only one with a p-value nearly &lt;.05, but it is customary to apply a Bonferroni adjustment when testing for linearity. There are eight predictors in the model (including the intercept term), so the Bonferroni adjusted p-value for age_tx is multiplied by 8. Do not reject the null hypothesis of linearity.\nIf the relationship was nonlinear, you could try transforming the variable by raising it to \\(\\lambda = 1 + b / \\gamma\\) where \\(b\\) is the estimated coefficient of the model without the interaction terms, and \\(\\gamma\\) is the estimated coefficient of the interaction term of the model with interactions. For age, \\(b\\) is 0.085 and \\(\\gamma\\) is -0.543, so \\(\\lambda\\) = 0.843. This is approximately 1 (no transformation). It appears to be customary to apply general transformations like .5 (square root), 1/3 (cube root), ln, and the reciprocal.\nNow check for multicollinearity. Variance inflation factors (VIF) estimate how much the variance of a regression coefficient is inflated due to multicollinearity. When independent variables are correlated, it is difficult to say which variable really influences the dependent variable. The VIF for variable \\(i\\) is\n\\[\n\\mathrm{VIF}_i = \\frac{1}{1 - R_i^2}\n\\]\nwhere \\(R_i^2\\) is the coefficient of determination (i.e., the proportion of dependent variance explained by the model) of a regression of \\(X_i\\) against all of the other predictors, \\(X_i = X_{j \\ne i} \\beta + \\epsilon\\). If \\(X_i\\) is totally unrelated to its covariates, then \\(R_i^2\\) will be zero and \\(\\mathrm{VIF}_i\\) will be 1. If \\(R_i^2\\) is .8, \\(\\mathrm{VIF}_i\\) will be 5. The rule of thumb is that \\(R_i^2 \\le 5\\) is tolerable, and \\(R_i^2 &gt; 5\\) is “highly correlated” and you have to do something about it. These are excellent.\n\n\nShow the code\ncar::vif(cs1$result)\n\n\n     age   weight   VO2max   gender \n1.035274 1.900575 2.167067 2.502538 \n\n\nTry calculating the \\(\\mathrm{VIF}\\) for age.\n\nI don’t know why this doesn’t work. It would work if the underlying model was OLS instead of GLM. The answer seems to be related to GVIF vs VIF, but I didn’t figure it out.)\n\n\n\nShow the code\nr2 &lt;- lm(age ~ weight + VO2max + gender, data = cs1$dat_training) %&gt;%\n  summary() %&gt;% pluck(\"r.squared\")\n\n(vif &lt;- 1 / (1 - r2))\n## [1] 1.049814\n\n\nNow check for influential outliers. Predict the response probabilities and filter for the predictions more than two standard deviations from the actual value and a Cook’s Distance greater than 4/N = 0.04.6 Cook’s distance measures how much predicted values change when observation i is removed from the data. Only two fitted values were both an outlier and influential, row ids 59 and 70. An index plot of Cook’s Distance shows the two points at the far left. You might examine the observations for validity. Otherwise, proceed and explain that there were two standardized residuals with value of 2.01 and 2.27 standard deviations which were kept in the analysis.\n\n\nShow the code\naugment(cs1$result) %&gt;%\n  mutate(\n    id = row_number(),\n    outlier = if_else(abs(.std.resid) &gt;= 2, \"Outlier\", \"Other\"),\n    influential = if_else(.cooksd &gt; 4 / nrow(cs1$dat), \"Influential\", \"Other\"),\n    status = case_when(\n      outlier == \"Outlier\" & influential == \"Influential\" ~ \"Influential Outlier\",\n      outlier == \"Outlier\" ~ \"Outlier\",\n      influential == \"Influential\" ~ \"Influential\",\n      TRUE ~ \"Other\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = .fitted, y = .cooksd)) +\n  geom_point(aes(color = status)) +\n  geom_text(aes(label = if_else(influential == \"Influential\", id, NA_integer_)), \n            check_overlap = TRUE, size = 3, nudge_x = .025) +\n  geom_hline(yintercept = 4 / nrow(cs1$dat), linetype = 2, color = \"goldenrod\") + \n  scale_color_manual(values = c(\"Influential Outlier\" = \"firebrick\", \n                                \"Influential\" = \"goldenrod\",\n                                \"Outlier\" = \"slategray\",\n                                \"Other\" = \"black\")) +\n  theme(legend.position = \"right\") +\n  labs(title = \"Index Plot of Cook's Distance.\",\n       subtitle = \"Row id labeled for values &gt; 4 / N.\")\n\n\n\n\n\n\n\n\n\n\n\nEvaluate the Fit\nThere are several ways to evaluate the model fit.\n\nThe likelihood ratio test\nPseudo R-squared7.\nAccuracy measures\nGain and ROC curves\n\nThe likelihood ratio test compares the log likelihood of the fitted model to an intercept-only model.\n\n\nShow the code\nintercept_only &lt;-\n  logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  fit(heart_disease ~ 1, data = cs1$dat)\n\n(cs1$lrtest &lt;- lmtest::lrtest(cs1$result, intercept_only$fit))\n\n\nLikelihood ratio test\n\nModel 1: heart_disease ~ age + weight + VO2max + gender\nModel 2: heart_disease ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   5 -51.044                         \n2   1 -64.745 -4 27.402  1.649e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe fitted model is significant, \\(\\chi^2\\)(4) = 27.4, p &lt; .001. Calculate the pseuedo-R2 with DescTools::PseudoR2().\n\nI Can’t get this to work in the tidymodels framework. Using glm() for now.\n\n\n\nShow the code\nx &lt;- glm(heart_disease ~ age + weight + VO2max + gender, \n         data = cs1$dat,\n         family = \"binomial\")\ncs1$pseudo_r2 &lt;- DescTools::PseudoR2(x, which = c(\"CoxSnell\", \"Nagelkerke\", \"McFadden\"))\n\ncs1$pseudo_r2\n\n\n  CoxSnell Nagelkerke   McFadden \n 0.2396799  0.3301044  0.2116126 \n\n\nLaerd interprets this as the model explained 33.0% (Nagelkerke R2) of the variance in heart disease. This is how your would interpret R2 in an OLS model. UCLA points out that the various pseudo R-squareds measure other aspects of the model and are unique to the measured quantity. A pseudo R-squared is not very informative on its own; it is useful for comparing models. Accuracy measures formed by the cross-tabulation of observed and predicted classes is the better way to go.\n\n\nShow the code\ncs1$conf_mat &lt;-\n  cs1$fit %&gt;% \n  augment(new_data = cs1$dat) %&gt;%\n  # conf_mat requires truth to be first level of the factor variable.\n  mutate(across(c(heart_disease, .pred_class), ~fct_relevel(., \"Yes\"))) %&gt;%\n  conf_mat(truth = heart_disease, estimate = .pred_class)\n\ncs1$conf_mat\n##           Truth\n## Prediction Yes No\n##        Yes  16 10\n##        No   19 55\n\ncs1$conf_mat %&gt;% summary()\n## # A tibble: 13 × 3\n##    .metric              .estimator .estimate\n##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n##  1 accuracy             binary         0.71 \n##  2 kap                  binary         0.322\n##  3 sens                 binary         0.457\n##  4 spec                 binary         0.846\n##  5 ppv                  binary         0.615\n##  6 npv                  binary         0.743\n##  7 mcc                  binary         0.330\n##  8 j_index              binary         0.303\n##  9 bal_accuracy         binary         0.652\n## 10 detection_prevalence binary         0.26 \n## 11 precision            binary         0.615\n## 12 recall               binary         0.457\n## 13 f_meas               binary         0.525\n\ncs1$conf_mat %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\nThe model accuracy, 71.0%, is the percent of observations correctly classified. The sensitivity, 45.7%, is the accuracy with regard to predicting positive cases. The specificity, 84.6%, is the accuracy with regard to predicting negative cases. If you are fitting a predictive model, use the testing data set for this.\n\n\nShow the code\ncs1$fit_training %&gt;% \n  augment(new_data = cs1$dat_testing) %&gt;%\n  conf_mat(truth = heart_disease, estimate = .pred_class)\n\n\n          Truth\nPrediction No Yes\n       No  16   5\n       Yes  4   6\n\n\nFinally, plot the gain curve or ROC curve. In the gain curve, the x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulatively summed true outcome. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicts the response well. The gray area is the “gain” over a random prediction.\n\n\nShow the code\ncs1$dat_testing %&gt;%\n  bind_cols(predict(cs1$fit, new_data = cs1$dat_testing, type = \"prob\")) %&gt;%\n  # event_level = \"second\" sets the second level as success\n  yardstick::gain_curve(.pred_Yes, truth = heart_disease, event_level = \"second\") %&gt;%\n  autoplot() +\n  labs(title = \"Gain Curve\")\n\n\n\n\n\n\n\n\n\n11 of the 31 participants had heart disease in the test data set.\n\nThe gain curve encountered 6 heart disease cases (50%) within the first 8 observations (55%). It encountered all 11 heart disease cases on the 18th observation.\nThe bottom of the grey area is the outcome of a random model. Only half the heart disease cases would be observed within 50% of the observations.\n\nThe top of the grey area is the outcome of the perfect model, the “wizard curve”. Half the heart disease cases would be observed in 6/30=20% of the observations.\n\nThe ROC (Receiver Operating Characteristics) curve plots sensitivity vs specificity at varying cut-off values for the probability ranging from 0 to 1. Ideally, you want very little trade-off between sensitivity and specificity, with a curve hugging the left and top axes.\n\n\nShow the code\ncs1$dat_testing %&gt;%\n  bind_cols(predict(cs1$fit, new_data = cs1$dat_testing, type = \"prob\")) %&gt;%\n  # event_level = \"second\" sets the second level as success\n  yardstick::roc_curve(.pred_Yes, truth = heart_disease, event_level = \"second\") %&gt;%\n  autoplot() +\n  labs(title = \"ROC Curve\")\n\n\n\n\n\n\n\n\n\n\n\nReporting\n\nA binomial logistic regression was performed to ascertain the effects of age, weight, gender and VO2max on the likelihood that participants have heart disease. Linearity of the continuous variables with respect to the logit of the dependent variable was assessed via the Box-Tidwell (1962) procedure. A Bonferroni correction was applied using all eight terms in the model resulting in statistical significance being accepted when p &lt; 0.00625 (Tabachnick & Fidell, 2014). Based on this assessment, all continuous independent variables were found to be linearly related to the logit of the dependent variable. There were two standardized residuals with value of 2.01 and 2.27 standard deviations, which were kept in the analysis. The logistic regression model was statistically significant, χ2(4) = 27.40, p &lt; .001. The model explained 33.0% (Nagelkerke R2) of the variance in heart disease and correctly classified 71.0% of cases. Sensitivity was 45.7%, specificity was 84.6%, positive predictive value was and negative predictive value was . Of the five predictor variables only three were statistically significant: age, gender and VO2max (as shown in Table 1). Females had 0.14 times lower odds to exhibit heart disease than males. Increasing age was associated with an increased likelihood of exhibiting heart disease, but increasing VO2max was associated with a reduction in the likelihood of exhibiting heart disease.\n\n\n\nShow the code\ngtsummary::tbl_regression(\n  cs1$fit,\n  exponentiate = TRUE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nage\n1.09\n1.03, 1.16\n0.003\n\n\nweight\n1.01\n0.96, 1.05\n0.8\n\n\nVO2max\n0.91\n0.82, 0.99\n0.039\n\n\ngender\n\n\n\n\n\n\n\n\n    Female\n—\n—\n\n\n\n\n    Male\n7.03\n1.43, 40.2\n0.021\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Linear Models (GLM)</span>"
    ]
  },
  {
    "objectID": "02-generalized_linear_models.html#multinomiallogistic",
    "href": "02-generalized_linear_models.html#multinomiallogistic",
    "title": "2  Generalized Linear Models (GLM)",
    "section": "2.2 Multinomial Logistic Regression",
    "text": "2.2 Multinomial Logistic Regression\nThe multinomial logistic regression model is \\(J - 1\\) baseline logits,\n\\[y_i = \\log \\left( \\frac{\\pi_{ij}}{\\pi_{ij^*}} \\right) = X_i \\beta_j, \\hspace{5mm} j \\ne j^*\\]\nwhere \\(j\\) is a level of the multinomial response variable. Whereas binomial logistic regression models the log odds of the response level, multinomial logistic regression models the log relative risk, the probability relative to the baseline \\(j^*\\) level.8\nInterpret the \\(k^{th}\\) element of \\(\\beta_j\\) as the increase in log relative risk of \\(Y_i = j\\) relative to \\(Y_i = j^*\\) given a one-unit increase in the \\(k^{th}\\) element of \\(X\\), holding the other terms constant. The individual probabilities, \\(\\pi_{ij}\\), are\n\\[\\pi_{ij} = \\frac{\\exp(y_{ij})}{1 + \\sum_{j \\ne j^*} \\exp(y_{ij})} = \\frac{e^{X_i\\beta_j}}{1 + \\sum_{j \\ne j^*} e^{X_i\\beta_j}}\\]\nand for the baseline category,\n\\[\\pi_{ij^*} = \\frac{1}{1 + \\sum_{j \\ne j^*} \\exp(y_{ij})} = \\frac{1}{1 + \\sum_{j \\ne j^*} e^{X_i\\beta_j}}\\]\nAssumptions\nMultinomial logistic regression applies when the dependent variable is categorical. It presumes a linear relationship between the log relative risk of the dependent variable and \\(X\\) with residuals \\(\\epsilon\\) that are independent. It also assumes there is no severe multicollinearity in the predictors, and there is independence of irrelevant alternatives (IIA). IIA means the relative likelihood of being in one category compared to the base category would not change if you added other categories.\n\nCase Study\nThis case study uses the data set from this UCLA tutorial. A study measures the association between students’ academic program (academic, general, and vocational) and their socioeconomic status (SES) (low, middle, high) and writing score.\n\n\nShow the code\ndownload.file(\n  \"https://stats.idre.ucla.edu/stat/data/hsbdemo.dta\",\n  \"./input/hsbdemo.dta\",\n  mode = \"wb\"\n)\n\n\n\n\nShow the code\ncs2 &lt;- list()\n\ncs2$dat &lt;- foreign::read.dta(\"./input/hsbdemo.dta\") %&gt;%\n  # Just keep cols relevant to study\n  select(id, prog, ses, write) %&gt;%\n  mutate(prog = fct_relevel(prog, \"academic\", after = 0))\n\ncs2$dat %&gt;%\n  gtsummary::tbl_summary(\n    by = prog,\n    include = c(prog, ses, write),\n    statistic = list(gtsummary::all_continuous() ~ \"{mean}, {sd}\")\n  ) %&gt;%\n  gtsummary::add_overall()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall N = 2001\nacademic N = 1051\ngeneral N = 451\nvocation N = 501\n\n\n\n\nses\n\n\n\n\n\n\n\n\n\n\n    low\n47 (24%)\n19 (18%)\n16 (36%)\n12 (24%)\n\n\n    middle\n95 (48%)\n44 (42%)\n20 (44%)\n31 (62%)\n\n\n    high\n58 (29%)\n42 (40%)\n9 (20%)\n7 (14%)\n\n\nwrite\n53, 9\n56, 8\n51, 9\n47, 9\n\n\n\n1 n (%); Mean, SD\n\n\n\n\n\n\n\n\n\nConduct a brief exploratory analysis to establish your expectations. Academic programs are associated with higher writing scores and SES. General and vocational programs are the opposite, although SES has opposing effects for general (increased probability) and vocational (decreased probability).\n\n\nShow the code\ncs2$dat %&gt;%\n  mutate(write_bin = cut(write, breaks = 5, dig.lab = 1, right = FALSE)) %&gt;%\n  count(prog, ses, write_bin) %&gt;%\n  mutate(.by = c(ses, write_bin), prob = n / sum(n)) %&gt;%\n  ggplot(aes(x = write_bin, y = prob, color = ses)) +\n  geom_point() +\n  geom_line(aes(group = ses)) +\n  facet_wrap(facets = vars(prog)) +\n  theme(legend.position = \"top\") +\n  labs(title = \"Program Proportions by SES\")\n\n\n\n\n\n\n\n\n\nIf your model is predictive rather than inferential, split the data into training/testing data sets.\n\n\nShow the code\n# For reproducibility\nset.seed(123)\n\n(x &lt;- initial_split(cs2$dat, prop = 0.7, strata = prog))\n## &lt;Training/Testing/Total&gt;\n## &lt;139/61/200&gt;\n\ncs2$dat_training &lt;- training(x)\ndim(cs2$dat_training)\n## [1] 139   4\n\ncs2$dat_testing &lt;- testing(x)\ndim(cs2$dat_testing)\n## [1] 61  4\n\n\n\n\nFit the Model\nThe multinomial logitistic regression model engines all seem to be related to neural networks and advise that predictors be set on a common scale by normalizing. I have only one predictor other than SES in this model, but I’ll normalize it anyway. Normalizing write will also facilitate model interpretation. The intercept will represent a reasonable condition (average writing score) and a one-unit increase in write will represent a 1 SD increase in writing.\n\nThis case study will use both the parsnip package to fit a model directly to a data set, and the recipes package to preprocess the data. parsnip is fine for most applications and it seems to be better supported by other functions, like tidy(), so stick with it for inferential applications. recipes has the advantage of being able to process data in a workflow, so you don’t have to transform new data to make predictions. See more uses of recipes on the tidy models documentation.\n\nLet’s first use parsnip to fit the full data set for an inferential application.\n\n\nShow the code\n# Create the model. This much is the same for parsnip or recipes.\ncs2$model &lt;-\n  multinom_reg() %&gt;%\n  set_engine(\"nnet\")\n\n# For parsnip, you need to normalize the data explicitly. nnet requires dummy\n# vars for factor predictors, but thankfully parsnip does that implicitly. \n# However, I want the outcome variable base level to be \"academic\" and while\n# recipes can do that in pre-processing, I have to do it manually for parsnip.\ncs2$dat_normalized &lt;-\n  cs2$dat %&gt;%\n  # Cast to numeric, otherwise scale() returns an nmatrix. :(\n  mutate(write = as.numeric(scale(write, center = TRUE, scale = TRUE)))\n\n# Fit the whole data set for an explanatory model.\ncs2$fit &lt;-\n  cs2$model %&gt;%\n  fit(prog ~ ses + write, data = cs2$dat_normalized)\n\n# Extract the fit object returned by the engine. Use for interpretation and \n# checking assumptions.\ncs2$result &lt;-\n  cs2$fit %&gt;%\n  extract_fit_engine()\n\n\nIn parallel, let’s use recipes to fit a predictive model to the training data. The model object is already created, so we just need to pair a recipe object with a workflow.\n\n\nShow the code\ncs2$rec &lt;-\n  # The `data` argument can be the base data, or training, or even testing. \n  # recipe() only uses it to catalog variable names and data types.\n  recipe(prog ~ ses + write, data = cs2$dat) %&gt;%\n  # You could have specified the formula as prog ~ ., then assigned roles. E.g.,\n  # Keep \"id\" in data set, but don't use it in the model like this:\n  # update_role(id, new_role = \"ID\") %&gt;%\n  # Unlike parsnip, recipe does not automatically create dummy vars.\n  step_dummy(all_nominal_predictors()) %&gt;%\n  # Not relevant here, but good practice: if a factor level has few values, it may\n  # not appear in the training set. If so, its dummy will contain a single value\n  # (0). You can prevent that by dropping zero-value cols.\n  step_zv(all_predictors()) %&gt;%\n  # Set the reference level of the outcome here if you want.\n  step_relevel(prog, ref_level = \"academic\") %&gt;%\n  # Normalize write.\n  step_normalize(write)\n\n# The workflow pairs the model and recipe.\ncs2$wflow &lt;-\n  workflow() %&gt;%\n  add_model(cs2$model) %&gt;%\n  add_recipe(cs2$rec)\n\n# Fit the training data set for a predictive model.\ncs2$fit_training &lt;-\n  cs2$wflow %&gt;%\n  fit(data = cs2$dat_training)\n\n# You can't extract the engine fit and pipe into summary. Seems like a bug\n# cs2$result_training &lt;- cs2$fit_training %&gt;% extract_fit_engine()\n\n\nLet’s look at the explanatory model summary object. The model produces a set of coefficient estimates for each non-reference level of the dependent variable. The nnet engine presents the coefficient estimates, then their standard errors as a second section, but does not present the z-statistic or p-values!\n\n\nShow the code\ncs2$result %&gt;% summary()\n\n\nCall:\nnnet::multinom(formula = prog ~ ses + write, data = data, trace = FALSE)\n\nCoefficients:\n         (Intercept)  sesmiddle    seshigh      write\ngeneral   -0.2049851 -0.5332857 -1.1628363 -0.5490814\nvocation  -0.7771705  0.2913906 -0.9826773 -1.0767939\n\nStd. Errors:\n         (Intercept) sesmiddle   seshigh     write\ngeneral    0.3491296 0.4437324 0.5142201 0.2029457\nvocation   0.4111072 0.4763734 0.5955665 0.2106132\n\nResidual Deviance: 359.9635 \nAIC: 375.9635 \n\n\nThe residual deviance is \\(G^2 = 2 \\sum_{i,j}y_{ij} \\log \\frac{y_{ij}}{\\hat{\\pi}_{ij}}\\). Another model diagnostic is the log-likelihood, \\(-G^2 / 2\\) (not shown) and AIC. More on these in the Model Fit section.\n\n\nShow the code\ndeviance(cs2$result)\n## [1] 359.9635\nlogLik(cs2$result, sum = TRUE)\n## 'log Lik.' -179.9817 (df=8)\n\n\nThe Wald z-statistic is \\(z = \\hat{\\beta} / SE(\\hat{\\beta})\\). Its square is the Wald chi-squared statistic. The p-value is the area to the right of \\(z^2\\) in the \\(\\chi_1^2\\) density curve. Get these from tidy().\n\n\nShow the code\ncs2$result %&gt;% tidy()\n\n\n# A tibble: 8 × 6\n  y.level  term        estimate std.error statistic     p.value\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 general  (Intercept)   -0.205     0.349    -0.587 0.557      \n2 general  sesmiddle     -0.533     0.444    -1.20  0.229      \n3 general  seshigh       -1.16      0.514    -2.26  0.0237     \n4 general  write         -0.549     0.203    -2.71  0.00682    \n5 vocation (Intercept)   -0.777     0.411    -1.89  0.0587     \n6 vocation sesmiddle      0.291     0.476     0.612 0.541      \n7 vocation seshigh       -0.983     0.596    -1.65  0.0989     \n8 vocation write         -1.08      0.211    -5.11  0.000000318\n\n\n\n\nInterpretation\nStart with interpreting the dependent variable. The model fits the log relative risk of belonging to program \\(j \\in\\) [vocation, general] vs. \\(j^*\\) = academic. However, predict() returns either the risk (type = “probs”) or outcome (type = “class), not the log relative risk. Plug in values for the predictor variables to get predictions. The relative risk is \\(RR = \\exp (\\hat{y}_j) = \\pi_j / \\pi_{j^*}\\). We see here that a student of low SES and mean writing score is less likely to be in a general or vocation program than an academic program.\n\n\nShow the code\n(risk &lt;- predict(cs2$result, newdata = list(ses = \"low\", write = 0), type = \"probs\"))\n##  academic   general  vocation \n## 0.4396833 0.3581922 0.2021246\n\n# Relative risk.\n(rr &lt;- risk[-1] / risk[1])\n##   general  vocation \n## 0.8146595 0.4597049\n\n# Log-relative risk (the modeled outcome)\n(log_rr &lt;- log(rr))\n##    general   vocation \n## -0.2049851 -0.7771705\n\n\nMove on to the coefficients. Interpret \\(\\hat{\\beta}\\) as the change in the log relative risk of \\(y_j\\) relative to \\(y_{j^*}\\) due to a \\(\\delta\\) = one unit change in \\(x\\). A \\(\\delta = x_a - x_b\\) change in \\(x\\) is associated with a \\(\\delta \\hat{\\beta}\\) change. \\(\\delta\\beta\\) is the log relative risk ratio.\n\\[\\log \\left(\\pi_j / \\pi_{j^*} |_{x = x_a} \\right) - \\log \\left(\\pi_j / \\pi_{j^*} |_{x = x_b} \\right) = \\log \\left( \\frac{\\pi_j / \\pi_{j^*} |_{x = x_a}}{\\pi_j / \\pi_{j^*} |_{x = x_b}} \\right) = \\delta \\hat{\\beta}\\]\nThe exponential of \\(\\hat{\\beta}\\) is the change in the relative risk of \\(y_j\\) relative to \\(y_{j^*}\\) due to a \\(\\delta\\) = one unit change in \\(x\\). \\(\\exp \\delta \\beta\\) is the relative risk ratio.\n\\[\\pi_j / \\pi_{j^*} |_{x = x_a} = \\exp{\\delta \\hat{\\beta}}\\]\nThe intercept term is the log-relative risk of \\(y_j\\) relative to \\(y_{j^*}\\) for the reference case. The reference case in the model is ses = “low” and write centered at 52.8. Notice how the intercept matches the predicted values above.\n\n\nShow the code\n(ref_log_rr &lt;- coef(cs2$result)[,\"(Intercept)\"])\n##    general   vocation \n## -0.2049851 -0.7771705\n\n# From log-relative risk to relative risk.\n(ref_rr &lt;- exp(ref_log_rr))\n##   general  vocation \n## 0.8146595 0.4597049\n\n\nThe log relative risks of a low SES student with a 52.8 writing score being in program general vs academic is \\(\\hat{y} = \\mathrm{Intercept}_1\\) = -0.205, and \\(\\hat{y} = \\mathrm{Intercept}_2\\) = -0.777 for vocation vs academic. The corresponding relative risks are \\(\\exp(\\hat{y}_j)\\) = 0.815 and 0.460. The expected probabilities are 44.0%, 35.8%, and 20.2% for academic, general, and vocation respectively.\nIf SES was high instead of low, the expected probabilities of being in program general vs academic would be 70.1%, 17.8%, and 12.1% for academic, general, and vocation respectively.\nWhat if the writing score increases by 1 SD (one unit)? The log RR of being in program general vs academic change by a factor of coef(cs2$result)[\"general\", \"write\"] = -0.549, RR = 0.577. For program vocation vs academic, it would change by a factor of -1.077, RR = 0.341. To get a 2 SD increase, multiply the coefficient by 2, then exponentiate. The two RRs would then be 0.333 and 0.116.\nVisualize the parameter estimates by plotting the predicted values.\n\n\nShow the code\nnew_data &lt;- expand.grid(\n  ses = levels(cs2$dat$ses),\n  write = seq(from = round(min(cs2$dat_normalized$write)), \n              to = round(max(cs2$dat_normalized$write)), \n              by = 1)\n)\n\nbind_cols(\n  new_data,\n  predict(cs2$fit, new_data = new_data, type = \"prob\")\n) %&gt;%\n  pivot_longer(cols = -c(ses, write), names_to = \"prog\", values_to = \"probability\") %&gt;%\n  ggplot(aes(x = write, y = probability, color = ses)) +\n  geom_line() + \n  facet_wrap(facets = vars(prog))\n\n\n\n\n\n\n\n\n\n\n\nAssumptions\nFour assumptions relate to the study design: (1) the dependent variable is multinomial; (2) the observations are independent; (3) the categories of all nominal variables are mutually exclusive and exhaustive; and (4) there are at least 10 observations per dependent variable level and independent variable. These assumptions are all valid. Three more assumptions related to the data distribution:\n\nThere is a linear relationship between the logit transformation and the continuous independent variables. Test with a plot and with Box-Tidwell.\nThere is no independent variable multicollinearity. Test with correlation coefficients and variance inflation factors (VIF).\nThere are no influential outliers. Test with Cook’s distance.\n\nThere are two ways to test for linearity (do both). First, plot the fitted values against the continuous predictors. This is the GLM analog to OLS bivariate analysis, except now the dependent variable is the logit transformation. These plotted relationships look good, except that in the prog = general level, writing score appears to interact with SES.\n\n\nShow the code\ncs2$fit %&gt;%\n  augment(new_data = cs2$dat_normalized) %&gt;%\n  pivot_longer(cols = c(.pred_academic:.pred_vocation), values_to = \".fitted\") %&gt;%\n  filter(str_detect(name, as.character(prog))) %&gt;%\n  ggplot(aes(x = write, y = .fitted, color = ses)) +\n  geom_point() +\n  facet_wrap(facets = vars(prog), scales = \"free_x\") +\n  geom_smooth(method = \"lm\", formula = \"y~x\") +\n  labs(title = \"Linearity Test: predicted vs continuous predictors\", x = NULL)\n\n\n\n\n\n\n\n\n\nThe second test for linearity is the Box-Tidwell approach. Add transformations of the continuous independent variables to the model, \\(x_{Tx} = x \\log x\\), then test their significance level in the fit. Focus on the transformed variable. write_tx has a p-value &lt;.05 for general. It is customary to apply a Bonferroni adjustment when testing for linearity. There are ten predictors in the model (including the intercept terms), so the Bonferroni adjusted p-values for write_tx are multiplied by 10. We should reject the null hypothesis of linearity because the adjusted p.value is still below .05.\n\n\nShow the code\n# Using non-centered vars to avoid log(0) errors.\ncs2$boxtidwell &lt;- cs2$dat %&gt;%\n  mutate(write_tx = log(write) * write) %&gt;%\n  fit(cs2$model, prog ~ ses + write + write_tx, data = .)\n\ntidy(cs2$boxtidwell) %&gt;% filter(term == \"write_tx\") %&gt;% mutate(adj_p = p.value * 10)\n\n\n# A tibble: 2 × 7\n  y.level  term     estimate std.error statistic p.value  adj_p\n  &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 general  write_tx   0.0713    0.0231     3.09  0.00198 0.0198\n2 vocation write_tx   0.0125    0.0239     0.522 0.601   6.01  \n\n\nIf the relationship is nonlinear, you can try transforming the variable by raising it to \\(\\lambda = 1 + b / \\gamma\\) where \\(b\\) is the estimated coefficient of the model without the interaction terms, and \\(\\gamma\\) is the estimated coefficient of the interaction term of the model with interactions. For write, \\(b\\) is for general and \\(\\gamma\\) is , so \\(\\lambda\\) = 1 + / = . It seems customary to apply general transformations like .5 (square root), 1/3 (cube root), ln, and the reciprocal, so maybe try raising write_c to . It seems in this case that the better solution is to add an interaction between write_c and ses to the model. I’m not going to pursue this further here.\nCheck for multicollinearity using variance inflation factors (VIF). VIFs estimate how much the variance of a regression coefficient is inflated due to multicollinearity. When independent variables are correlated, it is difficult to say which variable really influences the dependent variable. The VIF for variable \\(i\\) is\n\\[\n\\mathrm{VIF}_i = \\frac{1}{1 - R_i^2}\n\\]\nwhere \\(R_i^2\\) is the coefficient of determination (i.e., the proportion of dependent variance explained by the model) of a regression of \\(X_i\\) against all of the other predictors, \\(X_i = X_{j \\ne i} \\beta + \\epsilon\\). If \\(X_i\\) is totally unrelated to its covariates, then \\(R_i^2\\) will be zero and \\(\\mathrm{VIF}_i\\) will be 1. If \\(R_i^2\\) is .8, \\(\\mathrm{VIF}_i\\) will be 5. The rule of thumb is that \\(R_i^2 \\le 5\\) is tolerable, and \\(R_i^2 &gt; 5\\) is “highly correlated” and you have to do something about it. car::vif() doesn’t work for multinomial logistic regression. The model type is not actually important here - we’re concerned about the covariate relationships. Below, I successively collapse the dependent variable into two-levels, then fit a binomial logistic regression and pipe that into car::vif().\n\n\nShow the code\ntmp_fit_general &lt;-\n  logistic_reg() %&gt;%\n  fit(\n    prog ~ ses + write, \n    data = cs2$dat_normalized %&gt;% mutate(prog = fct_collapse(prog, vocation = \"academic\"))\n  ) %&gt;%\n  extract_fit_engine() \n\ntmp_fit_general %&gt;% car::vif()\n##           GVIF Df GVIF^(1/(2*Df))\n## ses   1.037207  2        1.009175\n## write 1.037207  1        1.018433\n\ntmp_fit_vocation &lt;- \n  logistic_reg() %&gt;%\n  fit(\n    prog ~ ses + write, \n    data = cs2$dat_normalized %&gt;% mutate(prog = fct_collapse(prog, general = \"academic\"))\n  ) %&gt;%\n  extract_fit_engine()\n\ntmp_fit_vocation %&gt;% car::vif()\n##           GVIF Df GVIF^(1/(2*Df))\n## ses   1.020017  2        1.004967\n## write 1.020017  1        1.009959\n\n\nCheck for influential outliers. Outliers are predicted values greater than two standard deviations from the actual value. Influential points have a Cook’s Distance greater than 4/N (4 / 200 = 0.02.9 Influential outliers are both. There is no simple way to do this for the multinomial regression because neither VGAM nor nnet support the augment() generic. Instead, I will use the two binomial logistic regressions from the VIF diagnostic.\n\n\nShow the code\noutlier_dat &lt;-\n  bind_rows(\n    general = augment(tmp_fit_general, type.predict = \"response\"),\n    vocation = augment(tmp_fit_vocation, type.predict = \"response\"),\n    .id = \"logit\"\n  ) %&gt;%\n  mutate(\n    id = row_number(),\n    outlier = if_else(abs(.std.resid) &gt;= 2, \"Outlier\", \"Other\"),\n    influential = if_else(.cooksd &gt; 4 / nrow(cs1$dat), \"Influential\", \"Other\"),\n    status = case_when(\n      outlier == \"Outlier\" & influential == \"Influential\" ~ \"Influential Outlier\",\n      outlier == \"Outlier\" ~ \"Outlier\",\n      influential == \"Influential\" ~ \"Influential\",\n      TRUE ~ \"Other\"\n    ),\n    status = factor(status, levels = c(\"Influential\", \"Outlier\", \"Influential Outlier\", \"Other\"))\n  )\n\noutlier_dat %&gt;%\n  ggplot(aes(x = .fitted, y = .cooksd)) +\n  geom_point(aes(color = status)) +\n  geom_text(aes(label = if_else(influential == \"Influential\", id, NA_integer_)), \n            check_overlap = TRUE, size = 3, nudge_x = .025) +\n  geom_hline(yintercept = 4 / nrow(cs1$dat), linetype = 2, color = \"goldenrod\") + \n  scale_color_manual(values = c(\"Influential Outlier\" = \"firebrick\", \n                                \"Influential\" = \"goldenrod\",\n                                \"Outlier\" = \"salmon\",\n                                \"Other\" = \"black\")) +\n  theme(legend.position = \"right\") +\n  labs(title = \"Index Plot of Cook's Distance.\",\n       subtitle = \"Row id labeled for influential points.\") +\n  facet_wrap(facets = vars(logit), ncol = 1)\n\n\n\n\n\n\n\n\n\nNo fitted values were influential outliers in the first fit, and only two were influential outliers in the second fit.\n\n\nShow the code\n(cs2$outliers &lt;-\n  outlier_dat %&gt;% filter(status == \"Influential Outlier\") %&gt;% pull(.std.resid))\n\n\n[1] 2.202098 2.593458\n\n\nAn index plot of Cook’s Distance shows the two points at the far left. You might examine the observations for validity. Otherwise, proceed and explain that there were two standardized residuals with value of 2.20 and 2.59 standard deviations which were kept in the analysis.\n\n\nEvaluate the Fit\nThere are several ways to evaluate the model fit.\n\nDeviance and chi-squared tests for lack of fit\nThe likelihood ratio test\nPseudo R-squared10.\nAccuracy measures\nGain and ROC curves\n\nThe deviance test for lack of fit and the likelihood ratio test are residuals tests. The deviance residual is defined as \\(d_i = \\mathrm{sign}(\\epsilon_i) \\left[ -2(y_i \\log \\hat{\\pi}_i + (1 - y_i) \\log (1 - \\hat{\\pi}_i)) \\right]^{.5}\\). The model deviance, \\(G^2\\), is the sum of the squared deviance residuals. It also equals \\(G^2 = 2 \\sum_{i,j}y_{ij} \\log \\frac{y_{ij}}{\\hat{\\pi}_{ij}}\\). You can calculate them by hand.\n\n\nShow the code\n# Actual values (1s and 0s for three response levels)\ny &lt;- cs2$dat %&gt;% \n  mutate(val = 1) %&gt;% \n  pivot_wider(names_from = prog, values_from = val, values_fill = 0) %&gt;%\n  select(levels(cs2$dat$prog)) %&gt;%\n  as.matrix()\n\n# Predicted values (probabilities for three response levels)\npi &lt;- predict(cs2$result, type = \"prob\") * 1\n\n# Raw residuals, by hand or by formula\n# e &lt;- y - pi\ne &lt;- residuals(cs2$result, type = \"response\")\n\n# Deviance residuals\nd &lt;- sign(e) * sqrt(-2 * y * log(pi) + (1 - y) * log(1 - pi))\n\n(g2 &lt;- sum(d^2, na.rm = TRUE))\n## [1] 359.9635\n(g2 &lt;- 2 * sum(y * log(y / pi), na.rm = TRUE))\n## [1] 359.9635\n(g2 &lt;- deviance(cs2$result))\n## [1] 359.9635\n\n\nThe related Pearson statistic, \\(X\\), is the sum of the squared Pearson residuals, \\(pr_i = \\epsilon_i / \\sqrt{\\hat{\\pi}_i}\\), the raw residual scaled by dividing by the estimated standard deviation of a binomial distribution with 1 trial. I don’t see this calculated in the residual() functions. You can do it yourself.\n\n\nShow the code\n(x2 &lt;- sum((e / sqrt(pi))^2))\n## [1] 406.0656\n\n\nThe deviance and Pearson statistic are distributed chi-squared with \\((N - p)(r - 1)\\) degrees of freedom where \\(p\\) = 4 predictor variables (3 SES levels + intercept), and \\(r\\) = 3 levels of the dependent variable for 392 degrees of freedom. The deviance and Pearson tests for lack of fit calculate the probability of the test statistic. The null hypothesis is that the model is correct. Neither test rejects the null hypothesis.\n\n\nShow the code\n# Deviance test for lack of fit\n(N &lt;- nrow(cs2$dat))\n## [1] 200\n(r &lt;- length(levels(cs2$dat$prog)))\n## [1] 3\n(p &lt;- length(coef(cs2$result)) / (r - 1)) # coefficients for each level, so divide by # levels\n## [1] 4\n(df &lt;- (N - p) * (r - 1))\n## [1] 392\n\npchisq(g2, df, lower.tail = FALSE)\n## [1] 0.8755302\n\npchisq(x2, df, lower.tail = FALSE)\n## [1] 0.3014625\n\n\nYou can do the same calculations for the intercept-only model.\n\n\nShow the code\nio &lt;- multinom_reg() %&gt;% fit(prog ~ 1, data = cs2$dat) %&gt;% extract_fit_engine()\n\ndeviance(io)\n## [1] 408.1933\n\n# degrees of freedom\n((nrow(cs2$dat) - length(coef(io)) / (r - 1)) * (r - 1))\n## [1] 398\n\n\nThe log-likelihood measures the unexplained variability in the model. The likelihood ratio test compares the log likelihood of the fitted model to the intercept-only model. You can use lmtest::lrtest() to test. anova() does the same thing using the residual deviance, \\(G2 = -2 \\times \\mathrm{log likelihood}\\), although it does not seem to work with the nnet engine.\n\n\nShow the code\n(cs2$lrtest &lt;- lmtest::lrtest(io, cs2$result))\n\n\nLikelihood ratio test\n\nModel 1: prog ~ 1\nModel 2: prog ~ ses + write\n  #Df  LogLik Df Chisq Pr(&gt;Chisq)    \n1   2 -204.10                        \n2   8 -179.98  6 48.23  1.063e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nShow the code\n# (cs2$lrtest &lt;- anova(io, cs2$result, type = \"I\", test = \"LR\"))\n\n\nThe difference in deviances is \\(LR\\) = 48.23 with 6 degrees of freedom. This is distributed chi-squared, with p-value = 1.063e-08. The deviance test for lack of fit concludes that the model fits significantly better than an empty (intercept-only) model, \\(\\chi^2\\)(6) = 48.23, p &lt; .001.\nYou can use lmtest::lrtest() to perform likelihood ratio tests on the significance of the predictors too. The likelihood ratio test compares the log likelihood with and without the predictor. Unfortunately, this does not seem to work within the parsnip framework.\n\n\nShow the code\n# (cs2$lrtest_ses &lt;- lmtest::lrtest(cs2$result, \"ses\"))\nno_ses &lt;- multinom_reg() %&gt;% fit(prog ~ write, data = cs2$dat) %&gt;% extract_fit_engine()\n(cs2$lrtest_ses &lt;- lmtest::lrtest(cs2$result, no_ses))\n## Likelihood ratio test\n## \n## Model 1: prog ~ ses + write\n## Model 2: prog ~ write\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)  \n## 1   8 -179.98                       \n## 2   4 -185.51 -4 11.058    0.02592 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# (cs2$lrtest_write &lt;- lmtest::lrtest(cs2$fit_nnet, \"write\"))\nno_write &lt;- multinom_reg() %&gt;% fit(prog ~ ses, data = cs2$dat) %&gt;% extract_fit_engine()\n(cs2$lrtest_write &lt;- lmtest::lrtest(cs2$result, no_write))\n## Likelihood ratio test\n## \n## Model 1: prog ~ ses + write\n## Model 2: prog ~ ses\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n## 1   8 -179.98                         \n## 2   6 -195.71 -2 31.447  1.484e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth SES, \\(X^2\\) = 11.058, p = 0.026, and writing score \\(X^2\\) = 31.447, p = 1.484e-07, had significant effects on the program.\nLogistic regression does not have a direct R-squared statistic like OLS does (the proportion of variance explained by the model). However, there are some analogs, called pseudo R-squared. You’ll encounter three pseudo R-squared measures: Cox and Snell, Nagelkerke, and McFadden. This one does not work for the nnet engine.\n\n\nShow the code\n# DescTools::PseudoR2(cs2$result, which = c(\"CoxSnell\", \"Nagelkerke\", \"McFadden\"))\n\n\nAccuracy measures formed by the cross-tabulation of observed and predicted classes is the better model fit diagnostic the the pseudo r-squares.\n\n\nShow the code\ncs2$conf_mat &lt;-\n  cs2$fit %&gt;% \n  augment(new_data = cs2$dat_normalized) %&gt;%\n  # conf_mat requires truth to be first level of the factor variable.\n  # mutate(across(c(prog, .pred_class), ~fct_relevel(., \"academic\"))) %&gt;%\n  conf_mat(truth = prog, estimate = .pred_class)\n\ncs2$conf_mat\n\n\n          Truth\nPrediction academic general vocation\n  academic       92      27       23\n  general         4       7        4\n  vocation        9      11       23\n\n\nShow the code\ncs2$conf_mat %&gt;% summary()\n\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             multiclass     0.61 \n 2 kap                  multiclass     0.299\n 3 sens                 macro          0.497\n 4 spec                 macro          0.763\n 5 ppv                  macro          0.550\n 6 npv                  macro          0.799\n 7 mcc                  multiclass     0.320\n 8 j_index              macro          0.260\n 9 bal_accuracy         macro          0.630\n10 detection_prevalence macro          0.333\n11 precision            macro          0.550\n12 recall               macro          0.497\n13 f_meas               macro          0.491\n\n\nShow the code\ncs1$conf_mat %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\nThe model accuracy, 61.0%, is the percent of observations correctly classified. The sensitivities are the accuracy with regard to predicting positive cases in each level of the dependent variable. The specificities are the accuracy with regard to predicting negative cases. The prevalences are the proportion of cases that were positive.\nFinally, plot the gain curve or ROC curve. In the gain curve, the x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulatively summed true outcome. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicts the response well. The gray area is the “gain” over a random prediction.\n\n\nShow the code\ncs2$dat_normalized %&gt;%\n  bind_cols(predict(cs2$fit, new_data = cs2$dat_normalized, type = \"prob\")) %&gt;%\n  # event_level = \"second\" sets the second level as success\n  yardstick::gain_curve(.pred_academic, .pred_general, .pred_vocation, \n                        truth = prog, event_level = \"second\") %&gt;%\n  autoplot() +\n  labs(title = \"Gain Curve\")\n\n\n\n\n\n\n\n\n\n105 of the 200 participants were in the academic program.\n\nThe gain curve encountered 52 academic programs (50%) within the first 72 observations (36%). It encountered all 105 cases on the 189th observation.\nThe bottom of the grey area is the outcome of a random model. Only half the academic program cases would be observed within 50% of the observations.\n\nThe top of the grey area is the outcome of the perfect model, the “wizard curve”. Half the academic program cases would be observed in 52.5/200=26.25% of the observations.\n\nThe ROC (Receiver Operating Characteristics) curve plots sensitivity vs specificity at varying cut-off values for the probability ranging from 0 to 1. Ideally, you want very little trade-off between sensitivity and specificity, with a curve hugging the left and top axes.\n\n\nShow the code\ncs2$dat_normalized %&gt;%\n  bind_cols(predict(cs2$fit, new_data = cs2$dat_normalized, type = \"prob\")) %&gt;%\n  # event_level = \"second\" sets the second level as success\n  yardstick::roc_curve(.pred_academic, .pred_general, .pred_vocation, \n                        truth = prog, event_level = \"second\") %&gt;%\n  autoplot() +\n  labs(title = \"Gain Curve\")\n\n\n\n\n\n\n\n\n\n\n\nReporting\n\nA multinomial logistic regression was performed to ascertain the effects of socioeconomic status (ses) and writing score on the likelihood that participants are enrolled in an academic, general, or vocation program. Linearity of the continuous variables with respect to the logit of the dependent variable was assessed via the Box-Tidwell (1962) procedure. A Bonferroni correction was applied using all eight terms in the model resulting in statistical significance being accepted when p &lt; 0.00625 (Tabachnick & Fidell, 2014). Based on this assessment, the continuous write independent variable was found to be linearly related to the logit of the dependent variable levels. There were two standardized residuals with value of 2.20 and 2.59 standard deviations, which were kept in the analysis. The multinomial logistic regression model was statistically significant, \\(\\chi^2\\)(6) = 48.23, p &lt; .001. The model correctly classified 61% of cases. Sensitivity was 50%, specificity was 76%, positive predictive value was 55% and negative predictive value was 80%. The write predictor variable was statistically significant for both outcome levels and high SES was statistically significant for the general program (as shown in Table 1). A 1SD increase in the writing score was associated with a (1 - 0.58) decrease in the odds of choosing a general program instead of academic and a (1 - 0.34) decrease in the odds of choosing a vocation program over academic. A high SEC was associated with a (1 - 0.31) decrease in the odds of chooseing a general program over academic.\n\n\n\nShow the code\ngtsummary::tbl_regression(\n  cs2$result,\n  exponentiate = TRUE\n) %&gt;%\n  gtsummary::as_flex_table() %&gt;%\n  flextable::theme_apa()\n\n\nOutcomeCharacteristicOR195% CI1p-valuegeneralseslow——middle0.590.25, 1.400.2high0.310.11, 0.860.024write0.580.39, 0.860.007vocationseslow——middle1.340.53, 3.400.5high0.370.12, 1.200.10write0.340.23, 0.51&lt;0.0011OR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Linear Models (GLM)</span>"
    ]
  },
  {
    "objectID": "02-generalized_linear_models.html#ordinallogistic",
    "href": "02-generalized_linear_models.html#ordinallogistic",
    "title": "2  Generalized Linear Models (GLM)",
    "section": "2.3 Ordinal Logistic Regression",
    "text": "2.3 Ordinal Logistic Regression\nOrdinal logistic regression, also call cumulative link model (CLM), is a generalized linear model (GZLM), an extension of the general linear model (GLM) to non-continuous outcome variables. There are many approaches to ordinal logistic regression, including cumulative, adjacent, and continuation categories, but the most popular is the cumulative odds ordinal logistic regression with proportional odds.11. The model for ordinal response random variable \\(Y_i\\) with \\(J\\) levels is\n\\[\\gamma_{ij} = F(\\eta_{ij}), \\hspace{5 mm} \\eta_{ij} = \\theta_j - x_i^\\mathrm{T}\\beta, \\hspace{5 mm} i = 1, \\ldots, n, \\hspace{5 mm} j = 1, \\ldots, J-1\\]\nwhere \\(\\gamma_{ij} = P(Y_i \\le j) = \\pi_{i1} + \\cdots + \\pi_{ij}\\). \\(\\eta_{ij}\\) is a linear predictor with \\(J-1\\) intercepts. \\(F\\) is the inverse link function. The regression models the logit link function of \\(\\gamma_{ij}\\).\n\\[\\mathrm{logit}(\\gamma_{ij}) = \\log \\left[\\frac{P(Y_i \\le j)}{P(Y_i \\gt j)} \\right] = \\theta_j - x_i^\\mathrm{T}\\beta\\] The cumulative logit is the log-odds of the cumulative probabilities that the response is in category \\(\\le j\\) versus \\(\\gt j\\). \\(\\theta_j\\) is the log-odds when \\(x_i^\\mathrm{T}=0\\) and \\(\\beta\\) is the increase in the log odds attributed to a one unit increase in \\(x_i^\\mathrm{T}\\). Notice \\(\\beta\\) is the same for all \\(j\\). The exponential of the predicted value is the odds. Solve this for the probability,\n\\[P(Y_i \\gt j) = \\frac{\\mathrm{exp}(\\hat{y}_i)}{1 + \\mathrm{exp}(\\hat{y}_i)}.\\]\nThe exponential of \\(\\beta\\) is the odds ratio of \\(x_1^\\mathrm{T} - x_0^\\mathrm{T}\\). Solve this for the odds ratio\n\\[\\mathrm{OR} = \\frac{\\mathrm{exp}(\\theta_j - x_1^\\mathrm{T}\\beta)}{\\mathrm{exp}(\\theta_j - x_2^\\mathrm{T}\\beta)} = \\mathrm{exp}(\\beta(x_1^\\mathrm{T} - x_0^\\mathrm{T}))\\]\nIf \\(x\\) is a binary factor factor, then \\(\\exp(\\beta)\\) is the odds ratio of \\(x=1\\) vs \\(x=0\\). Thus the odds-ratio is proportional to the difference between values of \\(x\\) and \\(\\beta\\) is the constant of proportionality.\nThe model is estimated with a regularized Newton-Raphson algorithm with step-halving (line search) using analytic expressions for the gradient and Hessian of the negative log-likelihood function. This is beyond me, but the upshot is that the estimation is an iterative maximization exercise, not a formulaic matrix algebra process. It is possible for the model estimation to fail to converge on a maximum.\nYou will sometimes encounter discussion about the latent variable. That is just the underlying quality you are trying to measure. If you rate something a 4 on a 5-level Likert scale, 4 is the expression of your valuation, the latent variable. Your precise valuation is somewhere between 3 and 5 on a continuous scale. The link function defines the distribution of the latent variable.\nThere are variations on the ordinal model. Structured thresholds impose restrictions on \\(\\theta_j\\), for example requiring equal distances between levels. Partial proportional odds allow \\(\\theta_j\\) to vary with nominal predictors. You can also use link functions other than logit.\nThere are two assumptions underling ordinal logistic regression: (a) no multicollinearity, and (b) proportional odds.\n\nCase Study\nThis case study uses the Laerd Statistics article on ordinal logistic regression data set. A study investigates the relationship between attitude toward tax levels and participant values and background. 192 participants in a study responded to the statement “Taxes are too high” on a 4-level Likert scale (tax_too_high, Strongly Disagree, Disagree, Agree, Strongly Agree). Participant attributes include business owner (Y|N), age, and political affiliation (Liberal, Conservative, Labor).\n\n\nShow the code\ncs3 &lt;- list()\n\ncs3$dat &lt;- foreign::read.spss(\"./input/ordinal-logistic-regression.sav\", to.data.frame = TRUE) %&gt;%\n  mutate(tax_too_high = factor(tax_too_high, ordered = TRUE),\n         biz_owner = fct_relevel(biz_owner, \"No\", \"Yes\"),\n         politics = fct_relevel(politics, \"Lab\")) %&gt;%\n  select(-c(biz_friends, uni_educated, income))\n\ncs3$dat %&gt;% \n  gtsummary::tbl_summary(\n  by = politics, \n  statistic = list(gtsummary::all_continuous() ~ \"{mean} ({sd})\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nLab N = 621\nLib N = 541\nCon N = 761\n\n\n\n\nbiz_owner\n36 (58%)\n26 (48%)\n47 (62%)\n\n\nage\n37.4 (5.6)\n35.4 (5.8)\n37.2 (4.8)\n\n\ntax_too_high\n\n\n\n\n\n\n\n\n    Strongly Disagree\n10 (16%)\n10 (19%)\n4 (5.3%)\n\n\n    Disagree\n13 (21%)\n16 (30%)\n9 (12%)\n\n\n    Agree\n28 (45%)\n21 (39%)\n42 (55%)\n\n\n    Strongly Agree\n11 (18%)\n7 (13%)\n21 (28%)\n\n\n\n1 n (%); Mean (SD)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ncs3$dat %&gt;%\n  mutate(age = case_when(age &lt; quantile(age, .33) ~ \"young\",\n                         age &lt; quantile(age, .67) ~ \"middle\",\n                         TRUE ~ \"old\"),\n         age = factor(age, levels = c(\"young\", \"middle\", \"old\"))) %&gt;%\n  count(tax_too_high, biz_owner, age, politics) %&gt;%\n  ggplot(aes(x = tax_too_high, y = n, fill = biz_owner)) +\n  geom_col(position = position_dodge2(preserve = \"single\")) +\n  facet_grid(rows = vars(age), cols = vars(politics), space = \"free\") +\n  scale_x_discrete(labels = function (x) str_wrap(x, width = 10)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Taxes too high?\",\n       subtitle = \"Reponse count by business owner, age, and politics.\")\n\n\n\n\n\n\n\n\n\n\n\nFit the Model\nFit a cumulative link model for the cumulative probability of the \\(i\\)th response falling in \\(j\\)th category or below where \\(i\\) indexes the (\\(n = 192\\)) responses, \\(j = 1, \\ldots, J\\) indexes the (\\(J = 4\\)) response categories, and \\(\\theta_j\\) is the threshold for the \\(j\\)th cumulative logit.\n\\[\\mathrm{logit}(P(Y_i \\le j)) = \\theta_j - \\beta_1(\\mathrm{politics}_i) - \\beta_2(\\mathrm{biz\\_owner}_i) - \\beta_3(\\mathrm{age}_i)\\]\n\n\nShow the code\ncs3$fmla &lt;- formula(tax_too_high ~ biz_owner + age + politics)\n\ncs3$result &lt;- ordinal::clm(cs3$fmla, data = cs3$dat)\n\ncs3$result %&gt;% summary()\n\n\nformula: tax_too_high ~ biz_owner + age + politics\ndata:    cs3$dat\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit flexible  192  -197.62 409.23 6(0)  3.14e-12 3.2e+05\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \nbiz_ownerYes  0.66462    0.28894   2.300 0.021435 *  \nage           0.24189    0.03260   7.421 1.17e-13 ***\npoliticsLib   0.03695    0.36366   0.102 0.919072    \npoliticsCon   1.16142    0.34554   3.361 0.000776 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n                           Estimate Std. Error z value\nStrongly Disagree|Disagree    7.026      1.166   6.024\nDisagree|Agree                8.766      1.231   7.119\nAgree|Strongly Agree         11.653      1.357   8.590\n\n\nThe summary object shows several fit statistics. More about these in the fit evaluation section below. The Coefficients table is the familiar parameter estimates. The coefficient estimate for biz_ownerYes is 0.665 with standard error 0.289, \\(z = \\hat{\\beta} / se =\\) 2.300, and \\(p = 2 \\cdot P(Z&gt;z) =\\) 0.021. Some programs (e.g., SPSS) also show the Wald chi-squared statistic, \\(z^2 =\\) 5.291. The square of a normal variable has a \\(\\chi^2\\) distribution, so the p value for the Wald chi-squared statistic is pchisq(z^2, df = 1) = 0.021.\nThe Threshold coefficients table are the intercepts, or cut-points. The first cut-point is the log-odds of response level Strongly Disagree (or less) vs greater than Strongly Disagree when all factor variables are at their reference level and the continuous vars are at 0.\nThere may be interaction effects between biz_owner and politics. You can check this by comparing the log likelihood to the saturated model with a likelihood ratio test.\n\n\nShow the code\nsaturated &lt;- ordinal::clm(tax_too_high ~ biz_owner*politics + age, data = cs3$dat)\n\n(cs3$sat_anova &lt;- anova(cs3$result, saturated))\n\n\nLikelihood ratio tests of cumulative link models:\n \n           formula:                                  link: threshold:\ncs3$result cs3$fmla                                  logit flexible  \nsaturated  tax_too_high ~ biz_owner * politics + age logit flexible  \n\n           no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)\ncs3$result      7 409.23 -197.62                      \nsaturated       9 411.75 -196.87  1.4805  2      0.477\n\n\nThe likelihood ratio test indicates the main-effects model fits about the same as the saturated model, \\(\\chi^2\\)(2) = 1.48, p = 0.477)\n\n\nAssumptions\nCumulative odds ordinal logistic regression with proportional odds models require a) no multicollinearity, and b) proportional odds.\nMulticollinearity occurs when two or more independent variables are highly correlated so that they do not provide unique or independent information in the regression model. Multicollinearity inflates the variances of the estimated coefficients, resulting in larger confidence intervals. The usual interpretation of a slope coefficient as the change in the mean response per unit increase in the predictor when all the other predictors are held constant breaks down because changing one predictor necessarily changes other predictors.\nTest for multicollinearity with variance inflation factors (VIF). The VIF is the inflation percentage of the parameter variance due to multicollinearity. E.g., a VIF of 1.9 means the parameter variance is 90% larger than what it would be if it was not correlated with other predictors.\nPredictor k’s variance, \\(Var(\\hat{\\beta_k})\\), is inflated by a factor of\n\\[\\mathrm{VIF}_k = \\frac{1}{1 - R_k^2}\\]\ndue to collinearity with other predictors, where \\(R_k^2\\) is the \\(R^2\\) of a regression of the \\(k^{th}\\) predictor on the remaining predictors. If predictor \\(k\\) is unrelated to the other variables, \\(R_k^2 = 0\\) and \\(VIF = 1\\) (no variance inflation). If 100% of the variance in predictor \\(k\\) is explained by the other predictors, then \\(R_k^2 = 1\\) and \\(VIF = \\infty\\). The rule of thumb is that \\(VIF \\le 5\\) is acceptable.\n\n\nShow the code\n# Cannot use CLM model with vif(). Re-express as a linear model.\nlm(as.numeric(tax_too_high) ~ politics + biz_owner + age, dat = cs3$dat) %&gt;%\n  car::vif()\n\n\n              GVIF Df GVIF^(1/(2*Df))\npolitics  1.035831  2        1.008840\nbiz_owner 1.023642  1        1.011752\nage       1.036491  1        1.018082\n\n\nThe VIFs in column GVIF are all below 5, so this model is not compromised by multicollinearity.\nThe proportional odds assumption means the independent variable effects are constant across each cumulative split of the ordinal dependent variable. Test for proportional odds using a full likelihood ratio test comparing the proportional odds model with a multinomial logit model, also called an unconstrained baseline logit model. This is also called the test of parallel lines. The multinomial logit model fits a slope to each of the \\(J – 1\\) levels. The proportional odds model is nested within the multinomial model, so you can use a likelihood ratio test to see if the models are statistically different. Fit the proportional odds model and a multinomial model using VGAM::vglm() and capture the log likelihoods and degrees of freedom. Perform a likelihood ratio test on the differences in log likelihoods, \\(D = -2 \\mathrm{loglik}(\\beta)\\).\n\n\nShow the code\ncs3$vglm_ordinal     &lt;- VGAM::vglm(cs3$fmla, VGAM::propodds,   data = cs3$dat)\n\ncs3$vglm_multinomial &lt;- VGAM::vglm(cs3$fmla, VGAM::cumulative, data = cs3$dat)\n\n(cs3$po_lrt &lt;- VGAM::lrtest(cs3$vglm_multinomial, cs3$vglm_ordinal))\n\n\nLikelihood ratio test\n\nModel 1: tax_too_high ~ biz_owner + age + politics\nModel 2: tax_too_high ~ biz_owner + age + politics\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n1 561 -193.31                     \n2 569 -197.62  8 8.6197     0.3754\n\n\n\nThe assumption of proportional odds was met, as assessed by a full likelihood ratio test comparing the fit of the proportional odds model to a model with varying location parameters, \\(\\chi^2\\)(8) = 8.620, p = 0.375.\n\nAnother option is the partial proportional odds test. This test locates specific variables causing the rejection of proportional odds.\n\n\nShow the code\n(cs3$po_lrt2 &lt;- ordinal::clm(cs3$fmla, data = cs3$dat) %&gt;% ordinal::nominal_test())\n\n\nTests of nominal effects\n\nformula: tax_too_high ~ biz_owner + age + politics\n          Df  logLik    AIC     LRT Pr(&gt;Chi)\n&lt;none&gt;       -197.62 409.23                 \nbiz_owner  2 -197.34 412.67 0.55974   0.7559\nage                                         \npolitics   4 -196.20 414.40 2.83415   0.5860\n\n\n\nThe assumption of proportional odds was met, as assessed by a full likelihood ratio test comparing the fit of the proportional odds model to a model with varying location parameters for business owner, \\(\\chi^2\\)(2) = 0.560, p = 0.756 and politics, \\(\\chi^2\\)(4) = 2.834, p = 0.586.\n\n\n\nEvaluate the Fit\nThere are three ways to assess overall model fit: The Deviance and Pearson goodness-of-fit tests of the overall model fit; the Cox and Snell, Nagelkerke, and McFadden pseudo R measures of explained variance; and the likelihood ratio test comparing the model fit to the intercept-only model. However, these tests rely on large frequencies in each cell, that is, each possible combination of predictor values. Overall goodness-of-fit statistics should be treated with suspicion when a continuous independent variable is present and/or there are a large number of cells with zero frequency.\nThe Pearson goodness-of-fit statistic is \\(X^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\) where \\(i\\) is the observation number and \\(j\\) is the response variable level. It is a summary of the Pearson residuals, the difference between the observed and expected cell counts, \\(O_{ij} - E_{ij}\\). The deviance goodness-of-fit statistic is the difference in fit between the model and a full model; a full model being a model that fits the data perfectly, \\(G^2 = 2 \\sum_{ij} O_{ij} \\log \\left( \\frac{O_{ij}}{E_{ij}} \\right)\\). Neither of these tests are reliable if there are many cells with zero frequencies and/or small expected frequencies and are generally not recommended. Generally, the chi-squared test requires a frequency count of at least 5 per cell.\n\n\nShow the code\n# Observed combinations of model vars\ncs3$cell_patterns &lt;- \n  cs3$dat %&gt;% count(biz_owner, age, politics, tax_too_high) %&gt;% nrow()\n\n# Observed combinations of predictor vars * levels of response var\ncs3$covariate_patterns &lt;- \n  cs3$dat %&gt;% count(biz_owner, age, politics) %&gt;% nrow()\ncs3$possible_cells &lt;- \n  cs3$covariate_patterns * length(levels(cs3$dat$tax_too_high))\n\n# 1 - ratio of observed to possible\ncs3$pct_freq_zero &lt;- 1 - cs3$cell_patterns / cs3$possible_cells\n\n\nThere are 137 observed combinations of model variables (predictors), and 372 possible combinations (predictors * outcome levels), so 63.2% of cells have zero frequencies. Ideally, zero frequencies should be less than 20%, so if you were to use the deviance or Pearson tests, you would need to report this. The results below are contradictory and bogus. I think you’d only use this test if you didn’t have continuous predictor variables.\n\n\nShow the code\nobserved &lt;- cs3$dat %&gt;% \n  count(biz_owner, age, politics, tax_too_high) %&gt;%\n  pivot_wider(names_from = tax_too_high, values_from = n, values_fill = 0) %&gt;%\n  pivot_longer(\n    cols = `Strongly Disagree`:`Strongly Agree`, \n    names_to = \"outcome\", \n    values_to = \"observed\"\n  )\n\nexpected &lt;- bind_cols(\n  cs3$dat, \n  cs3$result %&gt;% predict(newdata = cs3$dat %&gt;% select(-\"tax_too_high\")) %&gt;% data.frame()\n) %&gt;%\n  rename_with(~str_remove(., \"fit\\\\.\"), starts_with(\"fit\")) %&gt;%\n  rename_with(~str_replace(., \"\\\\.\", \" \")) %&gt;%\n  summarize(.by = c(biz_owner, age, politics), \n            across(`Strongly Disagree`:`Strongly Agree`, sum)) %&gt;%\n  pivot_longer(cols = `Strongly Disagree`:`Strongly Agree`, names_to = \"outcome\", values_to = \"expected\")\n\nobs_exp &lt;- observed %&gt;%\n  inner_join(expected, by = c(\"politics\", \"biz_owner\", \"age\", \"outcome\")) %&gt;%\n  mutate(epsilon_sq = (observed - expected)^2,\n         chi_sq = epsilon_sq / expected,\n         g_sq = 2 * observed * log((observed+.0001) / expected)\n  )\n\ncs3$chisq &lt;- list()\ncs3$chisq$X2 = sum(obs_exp$chi_sq)\ncs3$chisq$G2 = sum(obs_exp$g_sq)\ncs3$chisq$df = cs3$covariate_patterns * (length(levels(cs3$dat$tax_too_high)) - 1) - 7\ncs3$chisq$X2_p.value = pchisq(cs3$chisq$X2, df = cs3$chisq$df, lower.tail = FALSE)\ncs3$chisq$G2_p.value = pchisq(cs3$chisq$G2, df = cs3$chisq$df, lower.tail = FALSE)\n\n\n\nThe Pearson goodness-of-fit test indicated that the model was not a good fit to the observed data, \\(\\chi^2\\)(272) = 745.4, p &lt; .001$. The deviance goodness-of-fit test indicated that the model was a good fit to the observed data, \\(G^2\\)(272) = 232.6, p = 0.960.\n\nThere are a number of measures in ordinal regression that attempt to provide a similar “variance explained” measure as that provided in ordinary least-squares linear regression. However, these measures do not have the direct interpretation that they do in ordinary linear regression and are often, therefore, referred to as “pseudo” R2 measures. The three most common measures (Cox and Snell, Nagelkerke, and McFadden) are not particularly good and not universally used. It is presented in the SPSS output, so you might encounter it in published work.\n\n\nShow the code\ncs3$nagelkerke &lt;- rcompanion::nagelkerke(cs3$result)\ncs3$nagelkerke$Pseudo.R.squared.for.model.vs.null\n\n\n                             Pseudo.R.squared\nMcFadden                             0.181957\nCox and Snell (ML)                   0.367369\nNagelkerke (Cragg and Uhler)         0.399641\n\n\nThe best way to assess model fit is the likelihood ratio test comparing the model to an intercept-only model. The difference in the -2 log likelihood between the models has a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in the number of parameters.\n\n\nShow the code\nintercept_only &lt;- ordinal::clm(tax_too_high ~ 1, data = cs3$dat)\ncs3$lrt &lt;- anova(cs3$result, intercept_only)\ncs3$lrt\n\n\nLikelihood ratio tests of cumulative link models:\n \n               formula:         link: threshold:\nintercept_only tax_too_high ~ 1 logit flexible  \ncs3$result     cs3$fmla         logit flexible  \n\n               no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)    \nintercept_only      3 489.14 -241.57                          \ncs3$result          7 409.23 -197.62  87.911  4  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe table shows the log likelihoods of the two models. LR.stat is the difference between 2 * the logLik values.\n\nThe final model statistically significantly predicted the dependent variable over and above the intercept-only model, \\(\\chi^2(4)\\) = 87.9, p = 0.000.\n\n\n\nInterpret Results\nReturn to the model summary.\n\n\nShow the code\ntidy(cs3$clm)\n\n\n# A tibble: 0 × 0\n\n\nThe coefficients for biz_owner, age, and politics are positive. Positive parameters increase the likelihood of stronger agreement with the statement. In this case, discontent with taxes are higher for business owners, increase with age, and are higher for Liberal Democrats and Conservatives relative to the Labor Party. The expected cumulative log-odds of declaring \\(\\le j\\) level of agreement with the statement for the baseline group (biz_ownerNo, age = 0, politicsLib) is for \\(j = 1\\) (Strongly Disagree), for \\(j = 2\\) (Disagree), and for \\(j = 3\\) (Agree).\nYou could solve the logit equation for\n\\[\\pi_j = \\frac{\\mathrm{exp}(Y_i)} {1 + \\mathrm{exp}(Y_i)}\\] to get the cumulative probabilities for each level. That’s what predict(type = \"cum.prob\") does. But it might be more intuitive to work with individual probabilities, the lagged differences to get the individual probabilities for each \\(j\\). That’s what predict(type = \"prob\") does. I like to play with predicted values to get a sense of the outcome distributions. In this case, I’ll take the median age, and each combination of biz_owner and politics.\n\n\nShow the code\nnew_data &lt;- cs3$dat %&gt;% \n  mutate(age = median(cs3$dat$age)) %&gt;% \n  expand(age, politics, biz_owner)\n\npreds &lt;- predict(cs3$result, newdata = new_data, type = \"prob\")[[\"fit\"]] %&gt;% as.data.frame()\n\nbind_cols(new_data, preds) %&gt;%\n  pivot_longer(cols = `Strongly Disagree`:`Strongly Agree`) %&gt;% \n  mutate(name = factor(name, levels = levels(cs3$dat$tax_too_high))) %&gt;%\n  ggplot(aes(y = politics, x = value, fill = fct_rev(name))) +\n  geom_col() +\n  geom_text(aes(label = scales::percent(value, accuracy = 1)), \n            size = 3, position = position_stack(vjust=0.5)) +\n  facet_grid(~paste(\"Bus Owner = \", biz_owner)) +\n  scale_fill_grey(start = 0.5, end = 0.8) +\n  theme_bw() + \n  theme(legend.position = \"top\",\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(title = \"Taxes too High for Conservative Business Owners?\", \n       x = NULL, fill = NULL)\n\n\n\n\n\n\n\n\n\nYou will want to establish whether politics is statistically significant overall before exploring any specific contrasts. The ANOVA procedure with type I test reports an overall test of significance for each variable entered into the model.\n\n\nShow the code\n(cs3$anovaI &lt;- anova(cs3$result, type = \"I\"))\n\n\nType I Analysis of Deviance Table with Wald chi-square tests\n\n          Df  Chisq Pr(&gt;Chisq)    \nbiz_owner  1 13.201  0.0002798 ***\nage        1 57.413  3.533e-14 ***\npolitics   2 14.636  0.0006635 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe political party last voted for has a statistically significant effect on the prediction of whether tax is thought to be too high, Wald \\(\\chi^2\\)(2) = 14.6, p = 0.001.\n\nThe best way to work with the data is with the tidy(exponentiate = TRUE) version.\n\n\nShow the code\ncs3$tidy &lt;- cs3$result %&gt;% tidy(conf.int = TRUE, exponentiate = TRUE)\ncs3$tidy\n\n\n# A tibble: 7 × 8\n  term        estimate std.error statistic  p.value conf.low conf.high coef.type\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    \n1 Strongly D…   1.13e3    1.17       6.02  1.70e- 9   NA         NA    intercept\n2 Disagree|A…   6.41e3    1.23       7.12  1.08e-12   NA         NA    intercept\n3 Agree|Stro…   1.15e5    1.36       8.59  8.72e-18   NA         NA    intercept\n4 biz_ownerY…   1.94e0    0.289      2.30  2.14e- 2    1.11       3.44 location \n5 age           1.27e0    0.0326     7.42  1.17e-13    1.20       1.36 location \n6 politicsLib   1.04e0    0.364      0.102 9.19e- 1    0.508      2.12 location \n7 politicsCon   3.19e0    0.346      3.36  7.76e- 4    1.63       6.35 location \n\n\nThen you can summarize the table in words.\n\nThe odds of business owners considering tax to be too high was 1.944 (95% CI, 1.107 to 3.443) times that of non-business owners, a statistically significant effect, z = 2.300, p = 0.021.\n\n\nThe odds of Conservative voters considering tax to be too high was 3.194 (95% CI, 1.635 to 6.351) times that of Labour voters, a statistically significant effect, z = 3.361, p = 0.001. The odds of Liberal Democrat voters considering tax to be too high was similar to that of Labour voters (odds ratio of 1.038 (95% CI, 0.508 to 2.121), p = 0.919.\n\n\nAn increase in age (expressed in years) was associated with an increase in the odds of considering tax too high, with an odds ratio of 1.274 (95% CI, 1.197 to 1.360), z = 7.421, p = 0.000.\n\n\n\nReporting\nHere is the complete write-up.\n\nA cumulative odds ordinal logistic regression with proportional odds was run to determine the effect of business ownership, political party voted for, and age, on the belief that taxes are too high. There were proportional odds, as assessed by a full likelihood ratio test comparing the fitted model to a model with varying location parameters, \\(\\chi^2\\)(8) = 8.620, p = 0.375. The final model statistically significantly predicted the dependent variable over and above the intercept-only model, \\(\\chi^2(4)\\) = 87.9, p = 0.000. The odds of business owners considering tax to be too high was 1.944 (95% CI, 1.107 to 3.443) times that of non-business owners, a statistically significant effect, z = 2.300, p = 0.021. The political party last voted for has a statistically significant effect on the prediction of whether tax is thought to be too high, Wald \\(\\chi^2\\)(2) = 14.6, p = 0.001. The odds of Conservative voters considering tax to be too high was 3.194 (95% CI, 1.635 to 6.351) times that of Labour voters, a statistically significant effect, z = 3.361, p = 0.001. The odds of Liberal Democrat voters considering tax to be too high was similar to that of Labour voters (odds ratio of 1.038 (95% CI, 0.508 to 2.121), p = 0.919. An increase in age (expressed in years) was associated with an increase in the odds of considering tax too high, with an odds ratio of 1.274 (95% CI, 1.197 to 1.360), z = 7.421, p = 0.000.\n\nPackage gtsummary shows a nice summary table.\n\n\nShow the code\ngtsummary::tbl_regression(cs3$result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\nbiz_owner\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n0.66\n0.10, 1.2\n0.021\n\n\nage\n0.24\n0.18, 0.31\n&lt;0.001\n\n\npolitics\n\n\n\n\n\n\n\n\n    Lab\n—\n—\n\n\n\n\n    Lib\n0.04\n-0.68, 0.75\n&gt;0.9\n\n\n    Con\n1.2\n0.49, 1.8\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Linear Models (GLM)</span>"
    ]
  },
  {
    "objectID": "02-generalized_linear_models.html#poissonregression",
    "href": "02-generalized_linear_models.html#poissonregression",
    "title": "2  Generalized Linear Models (GLM)",
    "section": "2.4 Poisson Regression",
    "text": "2.4 Poisson Regression\nPoisson models count data, like “traffic tickets per day”, or “website hits per day”. The response is an expected rate or intensity. For count data, specify the generalized model, this time with family = poisson or family = quasipoisson.\nRecall that the probability of achieving a count \\(y\\) when the expected rate is \\(\\lambda\\) is distributed\n\\[P(Y = y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^y}{y!}.\\]\nThe poisson regression model is\n\\[\\lambda = \\exp(X \\beta).\\]\nYou can solve this for \\(y\\) to get\n\\[y = X\\beta = \\ln(\\lambda).\\]\nThat is, the model predicts the log of the response rate. For a sample of size n, the likelihood function is\n\\[L(\\beta; y, X) = \\prod_{i=1}^n \\frac{e^{-\\exp({X_i\\beta})}\\exp({X_i\\beta})^{y_i}}{y_i!}.\\]\nThe log-likelihood is\n\\[l(\\beta) = \\sum_{i=1}^n (y_i X_i \\beta - \\sum_{i=1}^n\\exp(X_i\\beta) - \\sum_{i=1}^n\\log(y_i!).\\]\nMaximizing the log-likelihood has no closed-form solution, so the coefficient estimates are found through interatively reweighted least squares.\nPoisson processes assume the variance of the response variable equals its mean. “Equals” means the mean and variance are of a similar order of magnitude. If that assumption does not hold, use the quasi-poisson. Use Poisson regression for large datasets. If the predicted counts are much greater than zero (&gt;30), the linear regression will work fine. Whereas RMSE is not useful for logistic models, it is a good metric in Poisson.\n\nCase Study 4\nDataset fire contains response variable injuries counting the number of injuries during the month and one explanatory variable, the month mo.\n\n\nShow the code\nfire &lt;- read_csv(file = \"C:/Users/mpfol/OneDrive/Documents/Data Science/Data/CivilInjury_0.csv\")\nfire &lt;- fire %&gt;% \n  mutate(mo = as.POSIXlt(`Injury Date`)$mon + 1) %&gt;%\n  rename(dt = `Injury Date`,\n         injuries = `Total Injuries`)\nstr(fire)\n\n\nIn a situation like this where there the relationship is bivariate, start with a visualization.\n\n\nShow the code\nggplot(fire, aes(x = mo, y = injuries)) +\n  geom_jitter() +\n  geom_smooth(method = \"glm\", method.args = list(family = \"poisson\")) +\n  labs(title = \"Injuries by Month\")\n\n\nFit a poisson regression in R using glm(formula, data, family = poisson). But first, check whether the mean and variance of injuries are the same magnitude? If not, then use family = quasipoisson.\n\n\nShow the code\nmean(fire$injuries)\nvar(fire$injuries)\n\n\nThey are of the same magnitude, so fit the regression with family = poisson.\n\n\nShow the code\nm2 &lt;- glm(injuries ~ mo, family = poisson, data = fire)\nsummary(m2)\n\n\nThe predicted value \\(\\hat{y}\\) is the estimated log of the response variable,\n\\[\\hat{y} = X \\hat{\\beta} = \\ln (\\lambda).\\]\nSuppose mo is January (mo = ), then the log ofinjuries` is \\(\\hat{y} = 0.323787\\). Or, more intuitively, the expected count of injuries is \\(\\exp(0.323787) = 1.38\\)\n\n\nShow the code\npredict(m2, newdata = data.frame(mo=1))\npredict(m2, newdata = data.frame(mo=1), type = \"response\")\n\n\nHere is a plot of the predicted counts in red.\n\n\nShow the code\naugment(m2, type.predict = \"response\") %&gt;%\n  ggplot(aes(x = mo, y = injuries)) +\n  geom_point() +\n  geom_point(aes(y = .fitted), color = \"red\") + \n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Month\",\n       y = \"Injuries\",\n       title = \"Poisson Fitted Line Plot\")\n\n\nEvaluate a logistic model fit with an analysis of deviance.\n\n\nShow the code\n(perf &lt;- glance(m2))\n(pseudoR2 &lt;- 1 - perf$deviance / perf$null.deviance)\n\n\nThe deviance of the null model (no regressors) is 139.9. The deviance of the full model is 132.2. The psuedo-R2 is very low at .05. How about the RMSE?\n\n\nShow the code\nRMSE(pred = predict(m2, type = \"response\"), obs = fire$injuries)\n\n\nThe average prediction error is about 0.99. That’s almost as much as the variance of injuries - i.e., just predicting the mean of injuries would be almost as good! Use the GainCurvePlot() function to plot the gain curve.\n\n\nShow the code\naugment(m2, type.predict = \"response\") %&gt;%\n  ggplot(aes(x = injuries, y = .fitted)) +\n  geom_point() +\n  geom_smooth(method =\"lm\") +\n  labs(x = \"Actual\",\n       y = \"Predicted\",\n       title = \"Poisson Fitted vs Actual\")\n\n\n\n\nShow the code\naugment(m2) %&gt;% data.frame() %&gt;% \n  GainCurvePlot(xvar = \".fitted\", truthVar = \"injuries\", title = \"Poisson Model\")\n\n\nIt seems that mo was a poor predictor of injuries.\n\n\n\n\n\n\nAgresti, Alan. 2013. Categorical Data Analysis. 3rd ed. Wiley. https://www.amazon.com/Categorical-Analysis-Wiley-Probability-Statistics-ebook-dp-B00CAYUFM2/dp/B00CAYUFM2/ref=mt_kindle?_encoding=UTF8&me=&qid=.\n\n\nMolnar, Christoph. 2020. Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Linear Models (GLM)</span>"
    ]
  },
  {
    "objectID": "02-generalized_linear_models.html#footnotes",
    "href": "02-generalized_linear_models.html#footnotes",
    "title": "2  Generalized Linear Models (GLM)",
    "section": "",
    "text": "These notes are primarily from PSU’s Analysis of Discrete Data which uses Alan Agresti’s Categorical Data Analysis (Agresti 2013). I also reviewed PSU’s Regression Methods, DataCamp’s Generalized Linear Models in R, DataCamp’s Multiple and Logistic Regression, and Interpretable machine learning (Molnar 2020).↩︎\nThe related probit regression link function is \\(f(E(Y|X)) = \\Phi^{-1}(E(Y|X)) = \\Phi^{-1}(\\pi)\\). The difference between the logistic and probit link function is theoretical, and the practical significance is slight. You can safely ignore probit.↩︎\nNotes from Machine Learning Mastery.↩︎\nUVA discusses the four types of residuals you can calculate from a binary logistic regression.↩︎\nSee probability notes on the binomial distribution here↩︎\nThis article suggests using three standard deviations for an outliers threshold.↩︎\nNice explanation here. Sounds like you should use pseudo R2 for model comparison, not for reporting goodness of fit↩︎\nThese notes rely on the course notes from PSU STAT 504, Analysis of Discrete Data and UCLA.↩︎\nCook’s distance measures how much predicted values change when observation i is removed from the data. This article suggests using three standard deviations for an outliers threshold.↩︎\nNice write-up here. Sounds like you should use pseudo R2 for model comparison, not for reporting goodness of fit↩︎\nThese notes rely on UVA, PSU, Laerd, and the CLM package article vignette↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Linear Models (GLM)</span>"
    ]
  },
  {
    "objectID": "03-linear_mixed_effects.html",
    "href": "03-linear_mixed_effects.html",
    "title": "3  Mixed Effects Models",
    "section": "",
    "text": "3.1 The LMM Model\nMixed-effects models are called “mixed” because they simultaneously model fixed and random effects. Fixed effects are population-level effects that should persist across experiments. Fixed effects are the traditional predictors in a regression model. modality is a fixed effect. Random effects are level effects that account for variability at different levels of the data hierarchy. For example, in a study with students from different schools, the variation between schools would be random effects. PID and stim are random effects because they are randomly sampled and you expect variability within the populations.\nThe LMM represents the \\(i\\)-th observation for the \\(j\\)-th random-effects group as\n\\[\ny_{ij} = \\beta_0 + \\beta_1x_{ij} + u_j + \\epsilon_{ij}\n\\]\nWhere \\(\\beta_0\\) and \\(\\beta_1\\) are the fixed effect intercept and slope, and \\(u_j\\) is the random effect for the \\(j\\)-th group. The \\(u_j\\) terms are the group deviations from the population mean and are assumed to be normally distributed with mean 0 and variance \\(\\sigma_u^2\\). The residual term is also assumed to be normally distributed with mean 0 and variance \\(\\sigma_\\epsilon^2\\)\nIn our case, \\(\\beta_0\\) and \\(\\beta_1\\) are the fixed effects of modality. \\(u_j\\) would be the \\(j\\) = 53 levels of PID. There might also be an \\(\\eta_k\\) term representing \\(k\\) = 543 word stimuli.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mixed Effects Models</span>"
    ]
  },
  {
    "objectID": "03-linear_mixed_effects.html#the-lmer-engine",
    "href": "03-linear_mixed_effects.html#the-lmer-engine",
    "title": "3  Mixed Effects Models",
    "section": "3.2 The lmer Engine",
    "text": "3.2 The lmer Engine\nFit an LMM with the lmer package. The pipes (|) in the formula indicate the value to the left varies by the variable to the right. 1 is the intercept (no interactions).\n\nfit_1 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lmer\") |&gt;\n  fit(\n    RT ~ modality + (1 | PID) + (1 | stim), \n    data = mdl_dat\n  )\n\nsummary(extract_fit_engine(fit_1))\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ modality + (1 | PID) + (1 | stim)\n   Data: data\n\nREML criterion at convergence: 302861.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3572 -0.6949 -0.0205  0.5814  4.9120 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n stim     (Intercept)   360.2   18.98  \n PID      (Intercept) 28065.7  167.53  \n Residual             67215.3  259.26  \nNumber of obs: 21679, groups:  stim, 543; PID, 53\n\nFixed effects:\n                    Estimate Std. Error t value\n(Intercept)         1044.449     23.164   45.09\nmodalityAudiovisual   82.525      3.529   23.38\n\nCorrelation of Fixed Effects:\n            (Intr)\nmodltyAdvsl -0.078\n\n\nThe Random effects section shows the variability in RT due to stim and PID. The variability among participants (PID sd = 168 ms) is much greater than the variability among words (stim sd = 19 ms).\nThe Fixed effects section shows audio-visual modality slows RT by 83 ms over audio-only.\nThe last section, Correlation of Fixed Effects, indicates there is a weak negative correlation between the intercept and the audio-visual modality effect, meaning when RT was higher, the effect of audiovisual was slightly less.\nModel interactions between the fixed effect variable(s) and random effects variable(s) by changing the random intercept to the fixed effect variable so both the intercept and the slope can vary.\n\nfit_2 &lt;- \n  linear_reg() |&gt; \n  set_engine(\"lmer\") |&gt;\n  fit(\n    RT ~ modality + (modality | PID) + (modality | stim), \n    data = mdl_dat\n  )\n\nsummary(extract_fit_engine(fit_2))\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ modality + (modality | PID) + (modality | stim)\n   Data: data\n\nREML criterion at convergence: 302385.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3646 -0.6964 -0.0141  0.5886  5.0003 \n\nRandom effects:\n Groups   Name                Variance Std.Dev. Corr \n stim     (Intercept)           304.0   17.44        \n          modalityAudiovisual   216.9   14.73   0.16 \n PID      (Intercept)         28559.0  168.99        \n          modalityAudiovisual  7709.0   87.80   -0.17\n Residual                     65258.8  255.46        \nNumber of obs: 21679, groups:  stim, 543; PID, 53\n\nFixed effects:\n                    Estimate Std. Error t value\n(Intercept)          1044.14      23.36  44.700\nmodalityAudiovisual    83.18      12.57   6.615\n\nCorrelation of Fixed Effects:\n            (Intr)\nmodltyAdvsl -0.178\n\n\nThis formulation includes random slopes for modality within PID and stim, meaning you’re accounting for differences in how each participant and stimulus responds to modality. Response times varied around 1,044 ms by sd = 17 ms for stim and sd = 169 ms for PID. The modality effect on response times varied around 83 ms by sd = 15 ms for stim and sd = 88 ms for PID. That last satistic means a participant whose slope is 1 SD below the mean would have a slope near zero - a modality effect of zero.\nThe added correlations in the Random effects section show how the intercept and slopes for modality vary together within each group. The 0.16 correlation between the stimuli and fixed-effect intercepts indicates that stimuli with longer response times in the audio modality tended to have more positive slopes (stronger positive audiovisual effect). The -0.17 correlation between the participant and fixed-effect intercepts indicates that participants with longer response times in the audio modality tended to have a less positive slope (weaker positive, or possibly even a negative, audiovisual effect).\nThe audio-visual modality slows RT by 83 ms over audio-only - same as in the first model. The REML criterion fell, so adding the random slopes for modality improved the fit.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mixed Effects Models</span>"
    ]
  },
  {
    "objectID": "03-linear_mixed_effects.html#model-assumptions",
    "href": "03-linear_mixed_effects.html#model-assumptions",
    "title": "3  Mixed Effects Models",
    "section": "3.3 Model Assumptions",
    "text": "3.3 Model Assumptions\nThe linear regression model assumptions apply here: the relationship between the predictors and the response is linear and the residuals are independent random variables normally distributed with mean zero and constant variance. Additionally, the the random effects intercepts and slopes are assumed to follow a normal distribution. Use residual plots to assess linearity, homoscedasticity, and independence. Use Q-Q plots to check the normality of residuals and random effects. Variance Inflation Factor (VIF) can help detect multicollinearity among predictors.\n\nresid_panel(extract_fit_engine(fit_2), smoother = TRUE, qqbands = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mixed Effects Models</span>"
    ]
  },
  {
    "objectID": "03-linear_mixed_effects.html#evaluate-the-fit",
    "href": "03-linear_mixed_effects.html#evaluate-the-fit",
    "title": "3  Mixed Effects Models",
    "section": "3.4 Evaluate the Fit",
    "text": "3.4 Evaluate the Fit\nUse likelihood ratio test to determine whether predictors affect the response variable. Compare the fit to a model without the predictor of interest. The likelihood ratio test is usually performed with the anova function, but there is a better way. afex::mixed(method = \"LRT\") performs the test for all fixed effects variables in the model. In this case study we have only one.\n\nlrt &lt;- afex::mixed(\n  RT ~ modality + (modality | PID) + (modality | stim), \n  data = mdl_dat, \n  method = \"LRT\"\n)\n\nlrt\n\nMixed Model Anova Table (Type 3 tests, LRT-method)\n\nModel: RT ~ modality + (modality | PID) + (modality | stim)\nData: mdl_dat\nDf full model: 9\n    Effect df     Chisq p.value\n1 modality  1 32.39 ***   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nThe likelihood-ratio test indicated that the model including modality provided a better fit for the data than a model without it, \\(\\chi^2\\)(1) = 32.4, p &lt; .001.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mixed Effects Models</span>"
    ]
  },
  {
    "objectID": "03-linear_mixed_effects.html#reporting",
    "href": "03-linear_mixed_effects.html#reporting",
    "title": "3  Mixed Effects Models",
    "section": "3.5 Reporting",
    "text": "3.5 Reporting\n\nav_terms &lt;- broom.mixed::tidy(fit_2) |&gt; filter(term == \"modalityAudiovisual\")\n\n\nA likelihood-ratio test indicated that the model including modality provided a better fit for the data than a model without it, \\(\\chi^2\\)(1) = 32.4, p &lt; .001. Examination of the summary output for the full model indicated that response times were on average an estimated 83.2 ms slower in the audiovisual relative to the audio-only condition (\\(\\hat{\\beta}\\) = 83.2, SE = 12.6, t = 83.2, SE = 6.6).\n\n\ngtsummary::tbl_regression(fit_2)\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\n\n\n\n\nmodality\n\n\n\n\n\n\n    Audio-only\n—\n—\n\n\n    Audiovisual\n83\n59, 108\n\n\n\n1 CI = Confidence Interval",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mixed Effects Models</span>"
    ]
  },
  {
    "objectID": "03-linear_mixed_effects.html#appendix-comparison-to-ols",
    "href": "03-linear_mixed_effects.html#appendix-comparison-to-ols",
    "title": "3  Mixed Effects Models",
    "section": "3.6 Appendix: Comparison to OLS",
    "text": "3.6 Appendix: Comparison to OLS\nYou might consider just throwing PID and stim into a simple linear regression as covariates.\n\n\nShow the code\nlinear_reg() |&gt;\n  fit(RT ~ modality + PID + stim, data = mdl_dat) |&gt;\n  tidy() |&gt; \n  filter(\n    !str_detect(term, \"^PID\"), \n    !str_detect(term, \"^stim\")\n  )\n\n\n# A tibble: 2 × 5\n  term                estimate std.error statistic   p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)            940.      44.0       21.4 2.84e-100\n2 modalityAudiovisual     82.7      3.55      23.3 1.23e-118\n\n\nOr maybe pre-summarize the data by calculating the mean response time across all words by PID and modality, then run the regression controlling for just PID.\n\n\nShow the code\nx &lt;- summarize(mdl_dat, .by = c(modality, PID), MRT = mean(RT))\n\nlinear_reg() |&gt;\n  fit(MRT ~ modality + PID, data = x) |&gt;\n  tidy() |&gt; \n  filter(!str_detect(term, \"^PID\"))\n\n\n# A tibble: 2 × 5\n  term                estimate std.error statistic  p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)            973.       46.2     21.1  4.07e-27\n2 modalityAudiovisual     83.2      12.6      6.61 2.05e- 8\n\n\nOr what the heck, don’t even control for pID or stim.\n\n\nShow the code\nlinear_reg() |&gt;\n  fit(RT ~ modality, data = mdl_dat) |&gt;\n  tidy()\n\n\n# A tibble: 2 × 5\n  term                estimate std.error statistic  p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)           1041.       3.00     347.  0       \n2 modalityAudiovisual     83.1      4.21      19.7 5.50e-86\n\n\n\n\n\n\n\n\nBrown, Violet A. 2021. “An Introduction to Linear Mixed-Effects Modeling in r.” Advances in Methods and Practices in Psychological Science 4 (1): 2515245920960351. https://doi.org/10.1177/2515245920960351.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Mixed Effects Models</span>"
    ]
  },
  {
    "objectID": "04-nonlinear_regression.html",
    "href": "04-nonlinear_regression.html",
    "title": "4  Non-linear Models",
    "section": "",
    "text": "4.1 Splines\nA regression spline fits a piecewise polynomial to the range of X partitioned by knots (K knots produce K + 1 piecewise polynomials) James et al (James et al. 2013). The polynomials can be of any degree d, but are usually in the range [0, 3], most commonly 3 (a cubic spline). To avoid discontinuities in the fit, a degree-d spline is constrained to have continuity in derivatives up to degree d−1 at each knot.\nA cubic spline fit to a data set with K knots, performs least squares regression with an intercept and 3 + K predictors, of the form\n\\[y_i = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\beta_4h(X, \\xi_1) + \\beta_5h(X, \\xi_2) + \\dots + \\beta_{K+3}h(X, \\xi_K)\\]\nwhere \\(\\xi_1, \\dots, \\xi_K\\) are the knots are truncated power basis functions \\(h(X, \\xi) = (X - \\xi)^3\\) if \\(X &gt; \\xi\\), else 0.\nSplines can have high variance at the outer range of the predictors. A natural spline is a regression spline additionally constrained to be linear at the boundaries.\nHow many knots should there be, and Where should the knots be placed? It is common to place knots in a uniform fashion, with equal numbers of points between each knot. The number of knots is typically chosen by trial and error using cross-validation to minimize the RSS. The number of knots is usually expressed in terms of degrees of freedom. A cubic spline will have K + 3 + 1 degrees of freedom. A natural spline has K + 3 + 1 - 5 degrees of freedom due to the constraints at the endpoints.\nA further constraint can be added to reduce overfitting by enforcing smoothness in the spline. Instead of minimizing the loss function \\(\\sum{(y - g(x))^2}\\) where \\(g(x)\\) is a natural spline, minimize a loss function with an additional penalty for variability:\n\\[L = \\sum{(y_i - g(x_i))^2 + \\lambda \\int g''(t)^2dt}.\\]\nThe function \\(g(x)\\) that minimizes the loss function is a natural cubic spline with knots at each \\(x_1, \\dots, x_n\\). This is called a smoothing spline. The larger g is, the greater the penalty on variation in the spline. In a smoothing spline, you do not optimize the number or location of the knots – there is a knot at each training observation. Instead, you optimize \\(\\lambda\\). One way to optimze \\(\\lambda\\) is cross-validation to minimize RSS. Leave-one-out cross-validation (LOOCV) can be computed efficiently for smoothing splines.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Non-linear Models</span>"
    ]
  },
  {
    "objectID": "04-nonlinear_regression.html#mars",
    "href": "04-nonlinear_regression.html#mars",
    "title": "4  Non-linear Models",
    "section": "4.2 MARS",
    "text": "4.2 MARS\nMultivariate adaptive regression splines (MARS) is a non-parametric algorithm that creates a piecewise linear model to capture nonlinearities and interactions effects. The resulting model is a weighted sum of basis functions \\(B_i(X)\\):\n\\[\\hat{y} = \\sum_{i=1}^{k}{w_iB_i(x)}\\]\nThe basis functions are either a constant (for the intercept), a hinge function of the form \\(\\max(0, x - x_0)\\) or \\(\\max(0, x_0 - x)\\) (a more concise representation is \\([\\pm(x - x_0)]_+\\)), or products of two or more hinge functions (for interactions). MARS automatically selects which predictors to use and what predictor values to serve as the knots of the hinge functions.\nMARS builds a model in two phases: the forward pass and the backward pass, similar to growing and pruning of tree models. MARS starts with a model consisting of just the intercept term equaling the mean of the response values. It then asseses every predictor to find a basis function pair consisting of opposing sides of a mirrored hinge function which produces the maximum improvement in the model error. MARS repeats the process until either it reaches a predefined limit of terms or the error improvement reaches a predefined limit. MARS generalizes the model by removing terms according to the generalized cross validation (GCV) criterion. GCV is a form of regularization: it trades off goodness-of-fit against model complexity.\nThe earth::earth() function (documentation) performs the MARS algorithm (the term “MARS” is trademarked, so open-source implementations use “Earth” instead). The caret implementation tunes two parameters: nprune and degree. nprune is the maximum number of terms in the pruned model. degree is the maximum degree of interaction (default is 1 (no interactions)). However, there are other hyperparameters in the model that may improve performance, including minspan which regulates the number of knots in the predictors.\nHere is an example using the Ames housing data set (following this tutorial.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(earth)\nlibrary(caret)\n\n# set up\names &lt;- AmesHousing::make_ames()\nset.seed(12345)\nidx &lt;- createDataPartition(ames$Sale_Price, p = 0.80, list = FALSE)\names_train &lt;- ames[idx, ] %&gt;% as.data.frame()\names_test  &lt;- ames[-idx, ]\n\nm &lt;- train(\n  x = subset(ames_train, select = -Sale_Price),\n  y = ames_train$Sale_Price,\n  method = \"earth\",\n  metric = \"RMSE\",\n  minspan = -15,\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneGrid = expand.grid(\n    degree = 1:3, \n    nprune = seq(2, 100, length.out = 10) %&gt;% floor()\n  )\n)\n\n\nThe model plot shows the best tuning parameter combination.\n\n\nShow the code\nplot(m, main = \"MARS Parameter Tuning\")\nm$bestTune\n\n\nHow does this model perform against the holdout data?\n\n\nShow the code\ncaret::postResample(\n  pred = log(predict(m, newdata = ames_test)),\n  obs = log(ames_test$Sale_Price)\n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Non-linear Models</span>"
    ]
  },
  {
    "objectID": "04-nonlinear_regression.html#gam",
    "href": "04-nonlinear_regression.html#gam",
    "title": "4  Non-linear Models",
    "section": "4.3 GAM",
    "text": "4.3 GAM\nGeneralized additive models (GAM) allow for non-linear relationships between each feature and the response by replacing each linear component \\(\\beta_j x_{ij}\\) with a nonlinear function \\(f_j(x_{ij})\\). The GAM model is of the form\n\\[y_i = \\beta_0 + \\sum{f_j(x_{ij})} + \\epsilon_i.\\]\nIt is called an additive model because we calculate a separate \\(f_j\\) for each \\(X_j\\), and then add together all of their contributions.\nThe advantage of GAMs is that they automatically model non-linear relationships so you do not need to manually try out many diﬀerent transformations on each variable individually. And because the model is additive, you can still examine the eﬀect of each \\(X_j\\) on \\(Y\\) individually while holding all of the other variables ﬁxed. The main limitation of GAMs is that the model is restricted to be additive, so important interactions can be missed unless you explicitly add them.\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning: With Applications in r. 1st ed. New York, NY: Springer. http://faculty.marshall.usc.edu/gareth-james/ISL/book.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Non-linear Models</span>"
    ]
  },
  {
    "objectID": "05-regularization.html",
    "href": "05-regularization.html",
    "title": "5  Regularization",
    "section": "",
    "text": "5.1 Bias-Variance Trade-off\nThe linear regression model is \\(Y = X \\beta + \\epsilon\\), where \\(\\epsilon \\sim N(0, \\sigma^2)\\). OLS estimates the coefficients by minimizing the loss function\n\\[L = \\sum_{i = 1}^n \\left(y_i - x_i^{'} \\hat\\beta \\right)^2\\]\nwhere \\(\\hat{f} = x_i^{'} \\hat\\beta\\) is the estimator of the true regression function, \\(f\\).\nThis results estimate for the coefficients of\n\\[\\hat{\\beta} = \\left(X'X\\right)^{-1}\\left(X'Y\\right).\\]\nThere are two important characteristics of any estimator: its bias and its variance. For OLS, these are\n\\[Bias(\\hat{\\beta}) = E(\\hat{\\beta}) - \\beta = 0\\]\nand\n\\[Var(\\hat{\\beta}) = \\sigma^2(X'X)^{-1}\\]\nwhere the unknown population variance \\(\\sigma^2\\) is estimated from the residuals\n\\[\\hat\\sigma^2 = \\frac{\\epsilon' \\epsilon}{n - k}.\\]\nThe OLS estimator is unbiased, but can have a large variance when the predictor variables are highly correlated with each other, or when there are many predictors (notice how \\(\\hat{\\sigma}^2\\) increases as \\(k \\rightarrow n\\)). Stepwise selection balances the trade-off by eliminating variables, but this throws away information. Regularization keeps all the predictors, but reduces coefficient magnitudes to reduce variance at the expense of some bias.\nWe can decompose the expected prediction error (EPE) into a reducible error related to the type of model, and irreducible error related to the variance \\(y\\) (noise).\n\\[\n\\text{EPE} = \\mathbb{E}\\left[\\left(f(x) - \\hat{f}(x)\\right)^2\\right] +\n  \\mathbb{V}[y|x]\n\\]\nThe reducible error, which is the mean squared error (MSE) of the estimate, can be further decomposed into the estimator bias and variance.\n\\[\n\\text{EPE} = \\left(f(x) - \\mathbb{E}\\left[\\hat{f}(x)\\right]\\right)^2 +\n  \\mathbb{E}\\left[\\left(\\hat{f}(x) - \\mathbb{E}\\left[\\hat{f}(x)\\right]\\right)^2\\right] +\n  \\mathbb{V}[y|x]\n\\]\nIllustration. Let’s set up a bias-variance tradeoff by fitting several models of increasing complexity to a random data-generating process, \\(f(x) = x^3\\). We’ll create 250 random samples by choosing \\(x\\) between 1 and 5 and \\(y = f(x) + e\\) where \\(e\\) is noise.\nf &lt;- function(x) {x^3}\n\nmake_sample = function() {\n  x = runif(n = 100, 1, 5)\n  e = rnorm(n = 100, 0, 5)\n  y = f(x) + e\n  data.frame(x, y)\n}\n\nsim_dat &lt;-\n  tibble(sim = 1:250) |&gt;\n  mutate(dat = map(sim, ~ make_sample()))\nWe’ll fit five models. The first is a linear model, \\(y \\sim x\\). Then we’ll fit polynomial model of degree 2. These first two models underfit the data. Then one of degree 3 - matching the underlying data-generating process. Then we’ll overfit with one of degree 9. Finally, we’ll use regularization to improve on the degree 9 model.\nsim_fit &lt;-\n  sim_dat |&gt;\n  mutate(\n    dat = map(sim, ~ make_sample()),\n    fit_1 = map(dat, ~ linear_reg() |&gt; fit(y ~ poly(x, degree = 1), data = .)),\n    fit_2 = map(dat, ~ linear_reg() |&gt; fit(y ~ poly(x, degree = 2), data = .)),\n    fit_3 = map(dat, ~ linear_reg() |&gt; fit(y ~ poly(x, degree = 3), data = .)),\n    fit_9 = map(dat, ~ linear_reg() |&gt; fit(y ~ poly(x, degree = 9), data = .)),\n    fit_9a = map(dat, \n                  ~ linear_reg(engine = \"glmnet\", penalty = 0.2, mixture = 1) |&gt;\n                    fit(y ~ poly(x, degree = 9), data = .))\n  )\nThis gives us 5 models fit to 250 random samples, each sample containing 100 points from an \\(f(x) = x^3 + \\epsilon\\) data-generating process. Let’s predict a single value, \\(f(x = 4)\\) from each model.\ntest_dat &lt;- data.frame(x = 4)\n\nsim_preds &lt;-\n  sim_fit |&gt;\n  mutate(\n    `1` = map_dbl(fit_1, ~ predict(., new_data = test_dat) |&gt; pull(.pred)),\n    `2` = map_dbl(fit_2, ~ predict(., new_data = test_dat) |&gt; pull(.pred)),\n    `3` = map_dbl(fit_3, ~ predict(., new_data = test_dat) |&gt; pull(.pred)),\n    `9` = map_dbl(fit_9, ~ predict(., new_data = test_dat) |&gt; pull(.pred)),\n    `9 (LASSO)` = map_dbl(fit_9a, ~ predict(., new_data = test_dat) |&gt; pull(.pred))\n  )\nCheck out the distribution of fitted values. The degree 3 polynomial is the gold standard, and you see both low bias and variance. The degree-1 polynomial was just bad. The degree 2 had a high bias, but at least the variance was small. The degree 9 model overfit the data. It had low bias overall, but very high variance. The last model used regularization to improve its reliability, and you see the variance has shrunk with only a small reduction in bias.\nShow the code\nsim_preds |&gt;\n  select(`1`:`9 (LASSO)`) |&gt;\n  pivot_longer(c(everything())) |&gt;\n  ggplot(aes(x = name, y = value)) +\n  geom_hline(yintercept = f(4), linewidth = 1, color = \"goldenrod\") +\n  geom_boxplot(fill = NA, outlier.shape = NA) +\n  geom_jitter(height = 0, width = .2, alpha = .2) +\n  theme(panel.grid = element_blank()) +\n  labs(\n    x = \"Polynomial Degree\", y = \"Preds\",\n    title = \"Increasing complexity improves bias at expense of higher variance.\",\n    subtitle = \"Simulated Predictions from Four Linear Models\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "05-regularization.html#case-study",
    "href": "05-regularization.html#case-study",
    "title": "5  Regularization",
    "section": "5.2 Case Study",
    "text": "5.2 Case Study\nThe sections below use the mtcars dataset to predict mpg from the other variables with regularization function glmnet::glmnet(). Set up a train-test split, training on 5-fold CV.\n\ndata(\"mtcars\")\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n(mt_split &lt;- initial_split(mtcars))\n\n&lt;Training/Testing/Total&gt;\n&lt;24/8/32&gt;\n\n(mt_folds &lt;- vfold_cv(training(mt_split), v = 5))\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits         id   \n  &lt;list&gt;         &lt;chr&gt;\n1 &lt;split [19/5]&gt; Fold1\n2 &lt;split [19/5]&gt; Fold2\n3 &lt;split [19/5]&gt; Fold3\n4 &lt;split [19/5]&gt; Fold4\n5 &lt;split [20/4]&gt; Fold5",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "05-regularization.html#ridge",
    "href": "05-regularization.html#ridge",
    "title": "5  Regularization",
    "section": "5.3 Ridge",
    "text": "5.3 Ridge\nRidge regression estimates the linear model coefficients by minimizing an augmented loss function which includes a term, \\(\\lambda\\), that penalizes the magnitude of the coefficient estimates,\n\\[L = \\sum_{i = 1}^n \\left(y_i - x_i^{'} \\hat\\beta \\right)^2 + \\lambda \\sum_{j=1}^k \\hat{\\beta}_j^2.\\]\nThe resulting estimate for the coefficients is\n\\[\\hat{\\beta} = \\left(X'X + \\lambda I\\right)^{-1}\\left(X'Y \\right).\\]\nAs \\(\\lambda \\rightarrow 0\\), ridge regression approaches OLS. The bias and variance for the ridge estimator are\n\\[\\mathrm{Bias}(\\hat{\\beta}) = -\\lambda \\left(X'X + \\lambda I \\right)^{-1} \\beta\\] \\[\\mathbb{V}(\\hat{\\beta}) = \\sigma^2 \\left(X'X + \\lambda I \\right)^{-1}X'X \\left(X'X + \\lambda I \\right)^{-1}\\]\nNotice how the estimator bias increases with \\(\\lambda\\) while the variance decreases with \\(\\lambda\\). The optimal level for \\(\\lambda\\) is the one that minimizes the root mean squared error (RMSE) or the Akaike or Bayesian Information Criterion (AIC or BIC), or R-squared.\nTo fit a linear model with ridge regression, specify linear_reg(mixture = 0). Standardize the predictors in the recipe so that each contributes to the model on a similar scale. Otherwise, predictors with larger scales would naturally have larger coefficients.\n\nridge_mdl &lt;- linear_reg(engine = \"glmnet\", penalty = tune(), mixture = 0)\n\nridge_rec &lt;-\n  recipe(mpg ~ ., data = mtcars) |&gt;\n  step_normalize(all_numeric_predictors())\n\nridge_wflow &lt;-\n  workflow() |&gt;\n  add_model(ridge_mdl) |&gt;\n  add_recipe(ridge_rec)\n\n# Set up the hyperparameter space for penalty. Set range through trial and error.\nridge_grid &lt;-\n  grid_latin_hypercube(\n    penalty(range = c(0, 10), trans = NULL), \n    size = 30\n  )\n\n# tune_grid() returns a resamples object, essentially a tibble with turning\n# results for each hyperparameter combination and fold.\nridge_resamples &lt;-\n  ridge_wflow |&gt;\n  tune_grid(\n    resamples = mt_folds,\n    grid = ridge_grid\n  )\n\nridge_resamples |&gt; show_best(metric = \"rmse\") |&gt; knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\npenalty\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n4.035107\nrmse\nstandard\n2.748411\n5\n0.4425804\nPreprocessor1_Model13\n\n\n3.913497\nrmse\nstandard\n2.748456\n5\n0.4431044\nPreprocessor1_Model12\n\n\n4.462717\nrmse\nstandard\n2.750088\n5\n0.4406450\nPreprocessor1_Model14\n\n\n3.558491\nrmse\nstandard\n2.750365\n5\n0.4443715\nPreprocessor1_Model11\n\n\n4.899727\nrmse\nstandard\n2.754607\n5\n0.4383952\nPreprocessor1_Model15\n\n\n\n\n\n\n\nShow the code\nridge_resamples |&gt;\n  collect_metrics() |&gt;\n  ggplot(aes(x = penalty, y = mean)) +\n  geom_point() +\n  facet_wrap(facets = vars(.metric), scales = \"free_y\") +\n  labs(title = \"Ridge Regression Hyperparameter Tuning\")\n\n\n\n\n\n\n\n\n\nFinalize the workflow with the optimal hyperparameter setting.\n\n\nShow the code\n# Update the workflow with the optimal hyperparameter values.\nridge_final &lt;-\n  ridge_wflow |&gt;\n  finalize_workflow(parameters = select_best(ridge_resamples, metric = \"rmse\"))\n\n# Refit the model with the optimal hyperparameters using the _full_ training \n# dataset (not the folds).\nridge_fit &lt;- ridge_final |&gt; last_fit(mt_split)\n\n# Metrics are evaluated on the testing dataset.\nridge_fit |&gt; collect_metrics() |&gt; knitr::kable()\n\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nrmse\nstandard\n2.4889825\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.8844566\nPreprocessor1_Model1\n\n\n\n\n\n The most important variables here were am and wt.\n\n\nShow the code\nridge_final |&gt;\n  fit(training(mt_split)) |&gt;\n  pull_workflow_fit() |&gt;\n  vi() |&gt;\n  ggplot(aes(x = Importance, y = fct_rev(fct_inorder(Variable)), color = Sign)) +\n  geom_point(size = 2) +\n  geom_segment(aes(x = 0, xend = Importance))  +\n  theme(legend.position = \"top\") +\n  labs(\n    color = NULL, y = NULL,\n    title = \"Ridge Variable Importance\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "05-regularization.html#lasso",
    "href": "05-regularization.html#lasso",
    "title": "5  Regularization",
    "section": "5.4 Lasso",
    "text": "5.4 Lasso\nLasso stands for “least absolute shrinkage and selection operator”. Like ridge, lasso adds a penalty for coefficients, but instead of penalizing the sum of squared coefficients (L2 penalty), lasso penalizes the sum of absolute values (L1 penalty). As a result, coefficients can be zeroed under lasso for high values of \\(\\lambda\\).\nThe loss function for lasso is\n\\[L = \\sum_{i = 1}^n \\left(y_i - x_i^{'} \\hat\\beta \\right)^2 + \\lambda \\sum_{j=1}^k \\left| \\hat{\\beta}_j \\right|.\\]\nTo fit a linear model with lasso regression, specify linear_reg(mixture = 1).\n\nlasso_mdl &lt;- linear_reg(engine = \"glmnet\", penalty = tune(), mixture = 1)\n\nlasso_rec &lt;-\n  recipe(mpg ~ ., data = mtcars) |&gt;\n  step_normalize(all_numeric_predictors())\n\nlasso_wflow &lt;-\n  workflow() |&gt;\n  add_model(lasso_mdl) |&gt;\n  add_recipe(lasso_rec)\n\n# Set up the hyperparameter space for penalty. Set range through trial and error.\nlasso_grid &lt;-\n  grid_latin_hypercube(\n    penalty(range = c(0, 4), trans = NULL), \n    size = 30\n  )\n\n# tune_grid() returns a resamples object, essentially a tibble with turning\n# results for each hyperparameter combination and fold.\nlasso_resamples &lt;-\n  lasso_wflow |&gt;\n  tune_grid(\n    resamples = mt_folds,\n    grid = lasso_grid\n  )\n\nlasso_resamples |&gt; show_best(metric = \"rmse\") |&gt; knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\npenalty\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n1.0515463\nrmse\nstandard\n3.135101\n5\n0.5020066\nPreprocessor1_Model08\n\n\n0.9219008\nrmse\nstandard\n3.141623\n5\n0.5186401\nPreprocessor1_Model07\n\n\n1.1547527\nrmse\nstandard\n3.144581\n5\n0.4892161\nPreprocessor1_Model09\n\n\n1.2441210\nrmse\nstandard\n3.161189\n5\n0.4772235\nPreprocessor1_Model10\n\n\n0.7771826\nrmse\nstandard\n3.173642\n5\n0.5315058\nPreprocessor1_Model06\n\n\n\n\n\n\n\nShow the code\nlasso_resamples |&gt;\n  collect_metrics() |&gt;\n  ggplot(aes(x = penalty, y = mean)) +\n  geom_point() +\n  facet_wrap(facets = vars(.metric), scales = \"free_y\") +\n  labs(title = \"Lasso Regression Hyperparameter Tuning\")\n\n\n\n\n\n\n\n\n\nFinalize the workflow with the optimal hyperparameter setting.\n\n\nShow the code\n# Update the workflow with the optimal hyperparameter values.\nlasso_final &lt;-\n  lasso_wflow |&gt;\n  finalize_workflow(parameters = select_best(lasso_resamples, metric = \"rmse\"))\n\n# Refit the model with the optimal hyperparameters using the _full_ training \n# dataset (not the folds).\nlasso_fit &lt;- lasso_final |&gt; last_fit(mt_split)\n\n# Metrics are evaluated on the testing dataset.\nlasso_fit |&gt; collect_metrics() |&gt; knitr::kable()\n\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nrmse\nstandard\n2.1770670\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.9076329\nPreprocessor1_Model1\n\n\n\n\n\n The most important variables here were am and wt.\n\n\nShow the code\nlasso_final |&gt;\n  fit(training(mt_split)) |&gt;\n  pull_workflow_fit() |&gt;\n  vi() |&gt;\n  ggplot(aes(x = Importance, y = fct_rev(fct_inorder(Variable)), color = Sign)) +\n  geom_point(size = 2) +\n  geom_segment(aes(x = 0, xend = Importance))  +\n  theme(legend.position = \"top\") +\n  labs(\n    color = NULL, y = NULL,\n    title = \"Lasso Variable Importance\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "05-regularization.html#elastic-net",
    "href": "05-regularization.html#elastic-net",
    "title": "5  Regularization",
    "section": "5.5 Elastic Net",
    "text": "5.5 Elastic Net\nElastic Net combines the penalties of ridge and lasso to get the best of both worlds. The loss function for elastic net is\n\\[L = \\frac{\\sum_{i = 1}^n \\left(y_i - x_i^{'} \\hat\\beta \\right)^2}{2n} + \\lambda \\frac{1 - \\alpha}{2}\\sum_{j=1}^k \\hat{\\beta}_j^2 + \\lambda \\alpha\\left| \\hat{\\beta}_j \\right|.\\]\nIn this loss function, new parameter \\(\\alpha\\) is a “mixing” parameter that balances the two approaches. If \\(\\alpha\\) is zero, you are back to ridge regression, and if \\(\\alpha\\) is one, you are back to lasso. To fit a linear model with elastic net regression, specify linear_reg(mixture = tune()).\n\nelnet_mdl &lt;- linear_reg(engine = \"glmnet\", penalty = tune(), mixture = tune())\n\nelnet_rec &lt;-\n  recipe(mpg ~ ., data = mtcars) |&gt;\n  step_normalize(all_numeric_predictors())\n\nelnet_wflow &lt;-\n  workflow() |&gt;\n  add_model(elnet_mdl) |&gt;\n  add_recipe(elnet_rec)\n\n# Set up the hyperparameter space for penalty and mixture. Set penalty range \n# through trial and error.\nelnet_grid &lt;-\n  grid_latin_hypercube(\n    penalty(range = c(2, 7), trans = NULL), \n    mixture(range = c(.1, .8)),\n    size = 30\n  )\n\n# tune_grid() returns a resamples object, essentially a tibble with turning\n# results for each hyperparameter combination and fold.\nelnet_resamples &lt;-\n  elnet_wflow |&gt;\n  tune_grid(\n    resamples = mt_folds,\n    grid = elnet_grid\n  )\n\nelnet_resamples |&gt; show_best(metric = \"rmse\") |&gt; knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenalty\nmixture\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n3.451338\n0.1427038\nrmse\nstandard\n2.935395\n5\n0.4578454\nPreprocessor1_Model02\n\n\n2.523840\n0.2380821\nrmse\nstandard\n2.993297\n5\n0.4722182\nPreprocessor1_Model06\n\n\n5.434235\n0.1124067\nrmse\nstandard\n3.041341\n5\n0.4382533\nPreprocessor1_Model01\n\n\n3.928826\n0.2142507\nrmse\nstandard\n3.107241\n5\n0.4555658\nPreprocessor1_Model05\n\n\n2.403868\n0.3727664\nrmse\nstandard\n3.119993\n5\n0.4754769\nPreprocessor1_Model12\n\n\n\n\n\n\n\nShow the code\nelnet_resamples |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(x = penalty, y = mixture, color = mean)) +\n  geom_point(size = 2) +\n  scale_color_gradient(low = \"red\", high = \"blue\") +\n  # facet_wrap(facets = vars(.metric), scales = \"free_y\") +\n  labs(title = \"Elastic Net Regression Hyperparameter Tuning\", color = \"rmse\")\n\n\n\n\n\n\n\n\n\nFinalize the workflow with the optimal hyperparameter setting.\n\n# Update the workflow with the optimal hyperparameter values.\nelnet_final &lt;-\n  elnet_wflow |&gt;\n  finalize_workflow(parameters = select_best(elnet_resamples, metric = \"rmse\"))\n\n# Refit the model with the optimal hyperparameters using the _full_ training \n# dataset (not the folds).\nelnet_fit &lt;- elnet_final |&gt; last_fit(mt_split)\n\n# Metrics are evaluated on the testing dataset.\nelnet_fit |&gt; collect_metrics() |&gt; knitr::kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nrmse\nstandard\n2.4026534\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.8865235\nPreprocessor1_Model1\n\n\n\n\n\n The most important variables here were wt, qsec and disp.\n\n\nShow the code\nelnet_final |&gt;\n  fit(training(mt_split)) |&gt;\n  pull_workflow_fit() |&gt;\n  vi() |&gt;\n  ggplot(aes(x = Importance, y = fct_rev(fct_inorder(Variable)), color = Sign)) +\n  geom_point(size = 2) +\n  geom_segment(aes(x = 0, xend = Importance))  +\n  theme(legend.position = \"top\") +\n  labs(\n    color = NULL, y = NULL,\n    title = \"Elastic Net Variable Importance\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "05-regularization.html#model-summary",
    "href": "05-regularization.html#model-summary",
    "title": "5  Regularization",
    "section": "Model Summary",
    "text": "Model Summary\nLasso performed best on RMSE and R-squared.\n\n\nShow the code\nbind_rows(\n  Ridge = collect_metrics(ridge_fit),\n  Lasso = collect_metrics(lasso_fit),\n  ElNet = collect_metrics(elnet_fit),\n  .id = \"Model\"\n) |&gt;\n  select(Model, .metric, .estimate) |&gt;\n  pivot_wider(names_from = .metric, values_from = .estimate) |&gt;\n  knitr::kable()\n\n\n\n\n\nModel\nrmse\nrsq\n\n\n\n\nRidge\n2.488983\n0.8844566\n\n\nLasso\n2.177067\n0.9076329\n\n\nElNet\n2.402653\n0.8865235\n\n\n\n\n\nFinal thoughts on the models.\n\nLasso can set some coefficients to zero, thus performing variable selection.\nLasso and Ridge address multicollinearity differently: in ridge regression, the coefficients of correlated predictors are similar; In lasso, one of the correlated predictors has a larger coefficient, while the rest are (nearly) zeroed.\nLasso tends to do well if there are a small number of significant parameters and the others are close to zero. Ridge tends to work well if there are many large parameters of about the same value.\nIn practice, you don’t know which will be best, so run cross-validation pick the best.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "06-decision_trees.html",
    "href": "06-decision_trees.html",
    "title": "6  Decision Trees",
    "section": "",
    "text": "6.1 Case Studies\nThe sections in this chapter work through two case studies. The first fits classification trees to the ISLR::OJ dataset to predict which of two brands of orange juice customers Purchase. The second fits regression trees to the ISLR::Carseats dataset to predict Sales.\noj_dat &lt;- ISLR::OJ\n\nglimpse(oj_dat)\n\nRows: 1,070\nColumns: 18\n$ Purchase       &lt;fct&gt; CH, CH, CH, MM, CH, CH, CH, CH, CH, CH, CH, CH, CH, CH,…\n$ WeekofPurchase &lt;dbl&gt; 237, 239, 245, 227, 228, 230, 232, 234, 235, 238, 240, …\n$ StoreID        &lt;dbl&gt; 1, 1, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 2, 2…\n$ PriceCH        &lt;dbl&gt; 1.75, 1.75, 1.86, 1.69, 1.69, 1.69, 1.69, 1.75, 1.75, 1…\n$ PriceMM        &lt;dbl&gt; 1.99, 1.99, 2.09, 1.69, 1.69, 1.99, 1.99, 1.99, 1.99, 1…\n$ DiscCH         &lt;dbl&gt; 0.00, 0.00, 0.17, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ DiscMM         &lt;dbl&gt; 0.00, 0.30, 0.00, 0.00, 0.00, 0.00, 0.40, 0.40, 0.40, 0…\n$ SpecialCH      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ SpecialMM      &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0…\n$ LoyalCH        &lt;dbl&gt; 0.500000, 0.600000, 0.680000, 0.400000, 0.956535, 0.965…\n$ SalePriceMM    &lt;dbl&gt; 1.99, 1.69, 2.09, 1.69, 1.69, 1.99, 1.59, 1.59, 1.59, 1…\n$ SalePriceCH    &lt;dbl&gt; 1.75, 1.75, 1.69, 1.69, 1.69, 1.69, 1.69, 1.75, 1.75, 1…\n$ PriceDiff      &lt;dbl&gt; 0.24, -0.06, 0.40, 0.00, 0.00, 0.30, -0.10, -0.16, -0.1…\n$ Store7         &lt;fct&gt; No, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes,…\n$ PctDiscMM      &lt;dbl&gt; 0.000000, 0.150754, 0.000000, 0.000000, 0.000000, 0.000…\n$ PctDiscCH      &lt;dbl&gt; 0.000000, 0.000000, 0.091398, 0.000000, 0.000000, 0.000…\n$ ListPriceDiff  &lt;dbl&gt; 0.24, 0.24, 0.23, 0.00, 0.00, 0.30, 0.30, 0.24, 0.24, 0…\n$ STORE          &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2…\n\ncs_dat &lt;- ISLR::Carseats\n\nglimpse(cs_dat)\n\nRows: 400\nColumns: 11\n$ Sales       &lt;dbl&gt; 9.50, 11.22, 10.06, 7.40, 4.15, 10.81, 6.63, 11.85, 6.54, …\n$ CompPrice   &lt;dbl&gt; 138, 111, 113, 117, 141, 124, 115, 136, 132, 132, 121, 117…\n$ Income      &lt;dbl&gt; 73, 48, 35, 100, 64, 113, 105, 81, 110, 113, 78, 94, 35, 2…\n$ Advertising &lt;dbl&gt; 11, 16, 10, 4, 3, 13, 0, 15, 0, 0, 9, 4, 2, 11, 11, 5, 0, …\n$ Population  &lt;dbl&gt; 276, 260, 269, 466, 340, 501, 45, 425, 108, 131, 150, 503,…\n$ Price       &lt;dbl&gt; 120, 83, 80, 97, 128, 72, 108, 120, 124, 124, 100, 94, 136…\n$ ShelveLoc   &lt;fct&gt; Bad, Good, Medium, Medium, Bad, Bad, Medium, Good, Medium,…\n$ Age         &lt;dbl&gt; 42, 65, 59, 55, 38, 78, 71, 67, 76, 76, 26, 50, 62, 53, 52…\n$ Education   &lt;dbl&gt; 17, 10, 12, 14, 13, 16, 15, 10, 10, 17, 10, 13, 18, 18, 18…\n$ Urban       &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, No, No, No, Yes, Ye…\n$ US          &lt;fct&gt; Yes, Yes, Yes, Yes, No, Yes, No, Yes, No, Yes, Yes, Yes, N…\nPartition the data into training and testing datasets. We’ll fit models to the training data and compare their performance using testing data.\nset.seed(12345)\n\n(oj_split &lt;- initial_split(oj_dat, prop = 0.8, strata = Purchase))\n## &lt;Training/Testing/Total&gt;\n## &lt;855/215/1070&gt;\noj_train &lt;- training(oj_split)\noj_test &lt;- testing(oj_split)\n\n(cs_split &lt;- initial_split(cs_dat, prop = 0.8, strata = Sales))\n## &lt;Training/Testing/Total&gt;\n## &lt;319/81/400&gt;\ncs_train &lt;- training(cs_split)\ncs_test &lt;- testing(cs_split)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "06-decision_trees.html#classification-tree",
    "href": "06-decision_trees.html#classification-tree",
    "title": "6  Decision Trees",
    "section": "6.2 Classification Tree",
    "text": "6.2 Classification Tree\nFunction parsnip::decision_tree() defines a decision tree model. Its default engine is rpart and has three hyperparameters.\n\ntree_depth: maximum number of layers. Larger depths result in more complex models, but they are also more prone to overfitting.\nmin_n: minimal node size required for splitting.\ncost_complexity: a penalty for adding more nodes to the tree. This is another control regulating the tree complexity.\n\nThe hyperparameters can either take assigned values, or you can tune them with tune(). We’ll predict Purchase [CH, MM] as a function of all 17 predictors.\n\noj_cart_mdl &lt;-\n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune(),\n    min_n = tune()\n  ) %&gt;%\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\noj_cart_wf &lt;-\n  workflow() |&gt;\n  add_model(oj_cart_mdl) |&gt;\n  add_formula(Purchase ~ .)\n\n\nFit the Model\nTune the hyperparameters with 10-fold CV using a regular grid. With 3 hyperparameters and 5 levels, the grid has 5^3=125 combinations. That means the tuning exercise will fit 125 models for each of the 10 folds - 1,250 fits! We’ll evaluate the fits based on accuracy and AUC. The dataset is somewhat imbalanced (61% “CH”), so AUC will be the more important metric. It takes about a minute and a half to fit the 855 x 18 dataset.\n\ntic()\n\noj_cart_resamples &lt;-\n  oj_cart_wf |&gt;\n  tune_grid(\n    resamples = vfold_cv(oj_train, v = 10), \n    grid = grid_regular(\n      cost_complexity(), \n      tree_depth(), \n      min_n(), \n      levels = 5\n    ),\n    metrics = metric_set(roc_auc, accuracy)\n  )\n\ntoc()\n\n50.01 sec elapsed\n\n\ntune_grid() returns a resamples object with one row per fold.\n\noj_cart_resamples\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics           .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [769/86]&gt; Fold01 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [769/86]&gt; Fold02 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [769/86]&gt; Fold03 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [769/86]&gt; Fold04 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [769/86]&gt; Fold05 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [770/85]&gt; Fold06 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [770/85]&gt; Fold07 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [770/85]&gt; Fold08 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [770/85]&gt; Fold09 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [770/85]&gt; Fold10 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;\n\n\nEach row contains a metrics list with values for each hyperparameter combination and performance metric. There were 125 parameter combinations and two metrics, so 250 rows. Here’s what the metrics object from the first fold look like.\n\noj_cart_resamples[1, ]$.metrics[[1]] |&gt; glimpse()\n\nRows: 250\nColumns: 7\n$ cost_complexity &lt;dbl&gt; 1.000000e-10, 1.000000e-10, 1.778279e-08, 1.778279e-08…\n$ tree_depth      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ min_n           &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ .metric         &lt;chr&gt; \"accuracy\", \"roc_auc\", \"accuracy\", \"roc_auc\", \"accurac…\n$ .estimator      &lt;chr&gt; \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"bin…\n$ .estimate       &lt;dbl&gt; 0.7558140, 0.7310950, 0.7558140, 0.7310950, 0.7558140,…\n$ .config         &lt;chr&gt; \"Preprocessor1_Model001\", \"Preprocessor1_Model001\", \"P…\n\n\ncollect_metrics() unnests the metrics column and averages the values over the folds. show_best() shows the best model hyperparameter combination, but the mean AUC is the same for several combinations. The best performing tree is 8 layers deep.\n\noj_cart_resamples |&gt; show_best(metric = \"roc_auc\") |&gt; knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncost_complexity\ntree_depth\nmin_n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n0.0000000\n8\n30\nroc_auc\nbinary\n0.8671621\n10\n0.0142507\nPreprocessor1_Model086\n\n\n0.0000000\n8\n30\nroc_auc\nbinary\n0.8671621\n10\n0.0142507\nPreprocessor1_Model087\n\n\n0.0000032\n8\n30\nroc_auc\nbinary\n0.8671621\n10\n0.0142507\nPreprocessor1_Model088\n\n\n0.0005623\n8\n30\nroc_auc\nbinary\n0.8671621\n10\n0.0142507\nPreprocessor1_Model089\n\n\n0.0000000\n11\n30\nroc_auc\nbinary\n0.8671621\n10\n0.0142507\nPreprocessor1_Model091\n\n\n\n\n\nPlot the sensitivity to the hyperparameter combinations to make sure you’ve explored a range that gives you a peek.\n\n\nShow the code\ndf &lt;- collect_metrics(oj_cart_resamples)\n\nbind_rows(\n  cost_complextity = df |&gt; rename(x = cost_complexity) |&gt;\n    summarize(.by = c(x, .metric), mean_auc = mean(mean)),\n  tree_depth = df |&gt; rename(x = tree_depth) |&gt;\n    summarize(.by = c(x, .metric), mean_auc = mean(mean)),\n  min_n = df |&gt; rename(x = min_n) |&gt;\n    summarize(.by = c(x, .metric), mean_auc = mean(mean)),\n  .id = \"hyperparameter\"\n) |&gt;\n  ggplot(aes(x = x, y = mean_auc, color = .metric)) +\n  geom_line(linewidth = 1.5, alpha = .6) +\n  facet_wrap(vars(hyperparameter), scales = \"free_x\") +\n  labs(x = NULL, y = \"Mean AUC\", color = NULL, title = \"Hyperparameter Tuning\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nSelect the hyperparameter combination with the highest AUC and finalize the workflow. last_fit() fits the model with the full training set and evaluates it on the testing data.\n\n\nShow the code\noj_cart_hyperparameters &lt;- select_best(oj_cart_resamples, metric = \"roc_auc\")\n\noj_cart_fit &lt;- \n  finalize_workflow(oj_cart_wf, oj_cart_hyperparameters) |&gt;\n  last_fit(oj_split)\n\n\nHere is the tree. The output starts with the root node. Terminal nodes are labeled with an asterisk (*), so this is just a summary node of everything beneath. Node 1 Contains all 855 observations with 333 misclassifications. The majority class is CH, with a proportion of 0.6105 for CH and 0.3895 for MM. Node 2 Splits on LoyalCH &gt;= 0.48285, containing 534 observations with 92 misclassifications. The majority class is CH, with a higher proportion of CH. The child nodes of node “x” having labeling pattern 2x) and 2x+1), so for node 1) it’s 2) and 3), and for 2) it’s 4) and 5).\n\nextract_workflow(oj_cart_fit)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nPurchase ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 855 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 855 333 CH (0.61052632 0.38947368)  \n   2) LoyalCH&gt;=0.48285 534  92 CH (0.82771536 0.17228464)  \n     4) LoyalCH&gt;=0.7535455 273  13 CH (0.95238095 0.04761905) *\n     5) LoyalCH&lt; 0.7535455 261  79 CH (0.69731801 0.30268199)  \n      10) PriceDiff&gt;=-0.165 221  48 CH (0.78280543 0.21719457)  \n        20) ListPriceDiff&gt;=0.135 185  30 CH (0.83783784 0.16216216) *\n        21) ListPriceDiff&lt; 0.135 36  18 CH (0.50000000 0.50000000)  \n          42) PriceDiff&gt;=0.015 17   6 CH (0.64705882 0.35294118) *\n          43) PriceDiff&lt; 0.015 19   7 MM (0.36842105 0.63157895) *\n      11) PriceDiff&lt; -0.165 40   9 MM (0.22500000 0.77500000) *\n   3) LoyalCH&lt; 0.48285 321  80 MM (0.24922118 0.75077882)  \n     6) LoyalCH&gt;=0.2761415 148  58 MM (0.39189189 0.60810811)  \n      12) SalePriceMM&gt;=2.04 71  31 CH (0.56338028 0.43661972)  \n        24) PriceMM&lt; 2.205 58  22 CH (0.62068966 0.37931034)  \n          48) SpecialMM&lt; 0.5 48  15 CH (0.68750000 0.31250000) *\n          49) SpecialMM&gt;=0.5 10   3 MM (0.30000000 0.70000000) *\n        25) PriceMM&gt;=2.205 13   4 MM (0.30769231 0.69230769) *\n      13) SalePriceMM&lt; 2.04 77  18 MM (0.23376623 0.76623377)  \n        26) SpecialCH&gt;=0.5 14   6 CH (0.57142857 0.42857143) *\n        27) SpecialCH&lt; 0.5 63  10 MM (0.15873016 0.84126984) *\n     7) LoyalCH&lt; 0.2761415 173  22 MM (0.12716763 0.87283237) *\n\n\nA diagram of the tree can sometimes help if its not too large. The node label indicates the predicted value, error rate, and proportion of observations included. Below the nodes are the splitting criteria.\n\n\nShow the code\noj_cart_fit |&gt;\n  extract_workflow() |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot(yesno = TRUE, roundint = FALSE)\n\n\n\n\n\n\n\n\n\nA variable’s importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. From the rpart vignette (page 12),\n\n“An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate.”\n\nLoyalCH was the most important variable, followed by PriceDiff.\n\n\nShow the code\noj_cart_fit |&gt;\n  extract_workflow() |&gt;\n  extract_fit_parsnip() |&gt;\n  vip::vip()\n\n\n\n\n\n\n\n\n\n\n\nPerformance\ncollect_metrics() shows the performance metrics on the 215 testing dataset observations.\n\n\nShow the code\ncollect_metrics(oj_cart_fit)\n\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.856 Preprocessor1_Model1\n2 roc_auc     binary         0.915 Preprocessor1_Model1\n3 brier_class binary         0.109 Preprocessor1_Model1\n\n\nUse the full confusion matrix and ROC curve to explore performance.\n\nConfusion Matrix\nThe confusion matrix calculates the model performance on the testing dataset.\n\noj_cart_cm &lt;-\n  oj_cart_fit |&gt;\n  collect_predictions() |&gt;\n  conf_mat(truth = Purchase, estimate = .pred_class)\n\nplot(oj_cart_cm$table, main = \"Classification Tree Confusion Matrix\")\n\n\n\n\n\n\n\n\nSeveral measures come directly from the confusion matrix. The first value (CH) is considered a “positive” (P) result and the second (MM) “negative” (N). The metrics are usually expressed in terms of true and false positives and negatives (TP, FP, TN, FN).\n\noj_cart_cm\n\n          Truth\nPrediction  CH  MM\n        CH 117  17\n        MM  14  67\n\nsummary(oj_cart_cm)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.856\n 2 kap                  binary         0.695\n 3 sens                 binary         0.893\n 4 spec                 binary         0.798\n 5 ppv                  binary         0.873\n 6 npv                  binary         0.827\n 7 mcc                  binary         0.696\n 8 j_index              binary         0.691\n 9 bal_accuracy         binary         0.845\n10 detection_prevalence binary         0.623\n11 precision            binary         0.873\n12 recall               binary         0.893\n13 f_meas               binary         0.883\n\n\n\n\n\n\n\n\n\n\n\nMetric\nDesc\nFormula\nCalculation\n\n\n\n\nAccuracy\nCorrect predictions percent.\n\\(\\frac{TP + TN}{total}\\)\n\\(\\frac{117+67}{215}=.856\\)\n\n\nCohen’s Kappa\nAccuracy vs a “random system”\n\\(\\kappa = \\frac{\\text{Acc} - RA}{1-RA}\\) where \\(RA = \\frac{AN \\cdot PN + AP \\cdot PP}{total^2}\\)\n\\(\\frac{.856 - .527}{1-.527} = .695\\)\n\n\nSensitivity, Recall\nProportion of positives identified.\n\\(\\frac{TP}{TP + FN}\\)\n\\(\\frac{117}{117+14}=.893\\)\n\n\nSpecificity\nProportion of negatives identified.\n\\(\\frac{TN}{TN + FP}\\)\n\\(\\frac{67}{67 + 17}=.798\\)\n\n\nPositive Predictive Value, Precision\nProportion of positive predictions that are correct.\n\\(\\frac{TP}{TP + FP}\\)\n\\(\\frac{117}{117 + 17}=.873\\)\n\n\nNegative Predictive Value\nProportion of negative predictions that are correct.\n\\(\\frac{TN}{TN + FN}\\)\n\\(\\frac{67}{67 + 14}=.827\\)\n\n\nF1-Score\nHarmonic mean of precision and recall.\n\\(2 \\times \\frac{\\text{prec} \\times \\text{recall}} {\\text{prec} + \\text{recall}}\\)\n\\(2 \\times \\frac{0.873 \\times 0.893} {0.873 + 0.893} = .883\\)\n\n\n\n Which metric is appropriate?\n\nAccuracy is a natural starting point in cases like this where you aren’t more concerned about identifying one outcome more than the other. However, be cautious when the dataset is imbalanced. The model accuracy was 0.856, but you could have achieved 0.623 just by predicting “CH” every time.\nWhen the prevalence is not ~50%, use Cohen’s Kappa. It compares the model accuracy to the accuracy of a random system. \\(\\kappa = \\frac{\\text{Acc} - RA}{1-RA}\\) where \\(RA = \\frac{\\text{AN} \\cdot \\text{PN} + \\text{AP} \\cdot \\text{PP}}{\\text{Tot}^2}\\) is the hypothetical probability of a chance agreement.\nIf you want to minimize false alarms, e.g., in a spam detector, maximize precision.\nMedical testing should balance catching positives (maximize sensitivity/recall) and false alarms (maximize precision). The F1 score gives you that balance.\nIf there is a presumption of a negative outcome, like in criminal trial, where false positives are more costly than false negatives, maximize specificity.\n\nCohen’s kappa was rather involved. Here is its calculation.\n\n\nShow the code\nAN &lt;- sum(oj_cart_cm$table[, 2])\nPN &lt;- sum(oj_cart_cm$table[2, ])\nAP &lt;- sum(oj_cart_cm$table[, 1])\nPP &lt;- sum(oj_cart_cm$table[1, ])\n\ntrue_preds &lt;- sum(diag(oj_cart_cm$table))\ntotal_preds &lt;- sum(oj_cart_cm$table)\nacc &lt;- true_preds / total_preds\n\n(ra &lt;- (AN*PN + AP*PP) / (total_preds)^2)\n## [1] 0.5269443\n\n(kap &lt;- (acc - ra) / (1 - ra))\n## [1] 0.6952028\n\n\nIf you use a metric that is a simple proportion, you can bound it with a 95% CI using the binomial test. For example, the accuracy was 184/215=.856. You can use these values in binom.test(). You might add a benchmark probability for comparison. The NIR (no information rate) statistic is the class rate for the larger class. It is the accuracy you would achieve by just guessing the more common outcome every time. Here, CH is the larger class, so NIR = 131/215 = 0.609. Comparing the accuracy to the NIR, this model is certainly superior to a naive guess.\n\n\nShow the code\nnir &lt;- sum(oj_cart_cm$table[, 1]) / sum(oj_cart_cm$table)\nbinom.test(x = true_preds, n = total_preds, p = nir, alternative = \"greater\")\n\n\n\n    Exact binomial test\n\ndata:  true_preds and total_preds\nnumber of successes = 184, number of trials = 215, p-value = 1.947e-15\nalternative hypothesis: true probability of success is greater than 0.6093023\n95 percent confidence interval:\n 0.8104028 1.0000000\nsample estimates:\nprobability of success \n              0.855814 \n\n\n\n\nROC Curve\nThe ROC (receiver operating characteristics) curve (Fawcett 2005) is another way to measure model performance. The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold. yardstick::roc_curve calculates the ROC curve and roc_auc() calculates the area under the curve. Generally, you are looking for a curve the at hugs the upper left corner of the grid. If you are looking for a balance between true- and false positives, you might use the plot to identify a probability threshold different from .5.\n\n\nShow the code\np &lt;-\n  oj_cart_fit |&gt;\n  collect_predictions() |&gt;\n  yardstick::roc_curve(Purchase, .pred_CH) |&gt;\n  mutate(tt = glue(\"Threshold: {comma(.threshold, .001)}\\n\",\n                   \"Specificity: {comma(specificity, .001)}\\n\",\n                   \"Sensitivity: {comma(sensitivity, .001)}\\n\")) |&gt;\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line() +\n  geom_point_interactive(aes(tooltip = tt)) +\n  labs(title = \"Classification Tree ROC Curve\")\n\ngirafe(ggobj = p)\n\n\n\n\n\n\nA few points on the ROC space are helpful for understanding how to use it.\n\nThe upper right point (1, 1) is the result of always predicting “positive” (CH). You identify all the positives (sensitivity = 1), but miss all the negatives (specificity = 0).\nThe lower left point (0, 0) is the opposite, the result of always predicting “negative” (MM).\nThe upper left point (0, 1) is perfect accuracy.\nThe lower right point (1, 0) is perfect imbecility, the wrong prediction every time.\nThe 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time.\n\nThe goal is for all nodes to bunch up in the upper left. Points to the left of the diagonal with a low TPR can be thought of as “conservative” predictors - they only make positive (CH) predictions with strong evidence. Points to the left of the diagonal with a high TPR can be thought of as “liberal” predictors - they make positive (CH) predictions with weak evidence. A single value to characterize the ROC curve is the area under the curve. It can range from 0 - 1.\n\n\nShow the code\noj_cart_fit |&gt; \n  collect_predictions() |&gt;\n  yardstick::roc_auc(Purchase, .pred_CH) |&gt;\n  pull(.estimate)\n\n\n[1] 0.91494\n\n\n\n\nGain Curve\nThe gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The gray area is the “gain” over a random prediction.\n\n\nShow the code\noj_cart_fit %&gt;%\n  collect_predictions() %&gt;%\n  yardstick::gain_curve(Purchase, .pred_CH) %&gt;%\n  autoplot() +\n  labs(title = \"OJ CART Gain Curve\")\n\n\n\n\n\n\n\n\n\n131 of the 215 consumers in the holdout testing set purchased CH.\n\nThe gain curve encountered 79 CH purchasers (60.3%) within the first 82 observations (38.1%).\nIt encountered all 131 CH purchasers on the 140th observation (100%).\nThe bottom of the gray area is the outcome of a random model. Only half the CH purchasers would be observed within 50% of the observations. The top of the gray area is the outcome of the perfect model, the “wizard curve”. Half the CH purchasers would be observed in ~30% of the observations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "06-decision_trees.html#regression-tree",
    "href": "06-decision_trees.html#regression-tree",
    "title": "6  Decision Trees",
    "section": "6.3 Regression Tree",
    "text": "6.3 Regression Tree\nA simple regression tree is built in a manner similar to a simple classification tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic.\nThe first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp). The only difference here is the set_Mode(\"regression\") call to produce a regression tree.\n\n\nShow the code\ncs_cart &lt;- list()\n\n# `decision_tree` has 3 hyperparameters (`cost_complexity`, `tree_depth`, and\n# `min_n`). Set their value to `tune()` if you want to optimize any one. Let's\n# optimize just `cost_complexity` and `tree_depth`.\ncs_cart$model &lt;-\n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\n# Tune a model using the workflow framework.\ncs_cart$workflow &lt;-\n  workflow() %&gt;%\n  add_model(cs_cart$model) %&gt;%\n  add_formula(Sales ~ .)\n\n# Tune the model with 10-fold CV using a regular grid of cost complexity values.\n# With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That\n# means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.\ncs_cart$tune_grid &lt;-\n  cs_cart$workflow %&gt;%\n  tune_grid(\n    resamples = vfold_cv(cs_train, v = 10), \n    grid = grid_regular(cost_complexity(), tree_depth(), levels = 5)\n  )\n\n# `collect_metrics()` returns two metrics: rmse and rsq.\ncs_cart$tune_grid %&gt;% \n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(x = cost_complexity, y = mean, color = tree_depth)) +\n  geom_line(linewidth = 1.5, alpha = .6) +\n  facet_wrap(facets = vars(.metric), scales = \"free\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nThe best models in terms of RMSE was the tree depth of 8 and any cp &lt; 5.6E-04.\n\n\nShow the code\ncs_cart$tune %&gt;% show_best(metric = \"rmse\")\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1    0.0000000001          8 rmse    standard    2.02    10  0.0731 Preprocesso…\n2    0.0000000178          8 rmse    standard    2.02    10  0.0731 Preprocesso…\n3    0.00000316            8 rmse    standard    2.02    10  0.0731 Preprocesso…\n4    0.000562              8 rmse    standard    2.02    10  0.0731 Preprocesso…\n5    0.0000000001         11 rmse    standard    2.02    10  0.0731 Preprocesso…\n\n\nSelect the best model in terms of rmse and finalize the model.\n\n\nShow the code\ncs_cart$best_tune &lt;- select_best(cs_cart$tune_grid, metric = \"rmse\")\n\n# `finalize_workflow()` applies the tuning parameters to the workflow.\ncs_cart$final_workflow &lt;- finalize_workflow(cs_cart$workflow, cs_cart$best_tune)\n\n# last_fit() fits the model with the full training set and evaluates it on the \n# testing data.\ncs_cart$fit &lt;-\n  cs_cart$final_workflow %&gt;%\n  last_fit(cs_split)\n\n\nHere is the tree. The output starts with the root node. The predicted sales at the root is the mean sales in the testing data set is 7.47 (values are $000s). The deviance at the root is the SSE, 2,412. The first split is at ShelveLoc = [Bad, Medium] vs Good.\n\n\nShow the code\ncs_cart$fit %&gt;% extract_workflow()\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nSales ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 319 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n  1) root 319 2411.784000  7.473072  \n    2) ShelveLoc=Bad,Medium 250 1564.689000  6.807960  \n      4) Price&gt;=105.5 159  791.704800  6.040252  \n        8) ShelveLoc=Bad 51  219.627100  4.827059  \n         16) Population&lt; 196.5 18   72.973230  3.686111 *\n         17) Population&gt;=196.5 33  110.441200  5.449394  \n           34) Age&gt;=60 7    9.771743  3.797143 *\n           35) Age&lt; 60 26   76.415030  5.894231  \n             70) CompPrice&lt; 137.5 18   29.894430  5.313889 *\n             71) CompPrice&gt;=137.5 8   26.818000  7.200000 *\n        9) ShelveLoc=Medium 108  461.567300  6.613148  \n         18) Income&lt; 60.5 41  193.069500  5.589268  \n           36) Price&gt;=135.5 12   49.608090  4.014167 *\n           37) Price&lt; 135.5 29  101.370900  6.241034  \n             74) CompPrice&lt; 124.5 15   24.591290  5.199333 *\n             75) CompPrice&gt;=124.5 14   43.062690  7.357143 *\n         19) Income&gt;=60.5 67  199.214200  7.239701  \n           38) Advertising&lt; 13.5 50  131.275800  6.799600  \n             76) Age&gt;=68 12   11.557370  5.348333 *\n             77) Age&lt; 68 38   86.463030  7.257895  \n              154) Price&gt;=135 11   11.881470  6.124545 *\n              155) Price&lt; 135 27   54.695900  7.719630  \n                310) CompPrice&lt; 129.5 15   21.461290  6.987333 *\n                311) CompPrice&gt;=129.5 12   15.135900  8.635000 *\n           39) Advertising&gt;=13.5 17   29.770210  8.534118 *\n      5) Price&lt; 105.5 91  515.537400  8.149341  \n       10) CompPrice&lt; 123.5 65  339.278900  7.467077  \n         20) Price&gt;=92.5 33  110.968900  6.316970  \n           40) Income&lt; 96 26   69.841400  5.850000  \n             80) Population&gt;=246 14   21.230090  4.899286 *\n             81) Population&lt; 246 12   21.194290  6.959167 *\n           41) Income&gt;=96 7   14.399490  8.051429 *\n         21) Price&lt; 92.5 32  139.644700  8.653125  \n           42) ShelveLoc=Bad 12   40.694300  7.025000 *\n           43) ShelveLoc=Medium 20   48.055200  9.630000  \n             86) Education&gt;=15.5 7    3.955971  8.394286 *\n             87) Education&lt; 15.5 13   27.654720 10.295380 *\n       11) CompPrice&gt;=123.5 26   70.360850  9.855000  \n         22) Advertising&lt; 9 12   26.948700  8.865000 *\n         23) Advertising&gt;=9 14   21.569920 10.703570 *\n    3) ShelveLoc=Good 69  335.799800  9.882899  \n      6) Price&gt;=108.5 49  195.188000  9.127347  \n       12) Advertising&lt; 14 42  127.875600  8.696190  \n         24) Income&lt; 43 12    9.912025  7.442500 *\n         25) Income&gt;=43 30   91.558340  9.197667  \n\n...\nand 6 more lines.\n\n\nHere is a diagram of the tree. The node label indicates the predicted value (mean) and the proportion of observations that are in the node (or child nodes). Below the nodes are the splitting criteria.\n\n\nShow the code\ncs_cart$fit %&gt;%\n  extract_workflow() %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot(yesno = TRUE, roundint = FALSE)\n\n\n\n\n\n\n\n\n\nPrice and ShelveLoc were the most important variables.\n\n\nShow the code\ncs_cart$fit %&gt;%\n  extract_workflow() %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip::vip()\n\n\n\n\n\n\n\n\n\n\nMeasuring Performance\ncollect_metrics() returns the RMSE, \\(RMSE = \\sqrt{(1/2) \\sum{(actual - pred)^2}})\\) and the model \\(R^2\\). The RMSE of 2.14 in the test data set is pretty good considering the standard deviation of Sales is 3.10.\n\n\nShow the code\ncs_cart$fit %&gt;% collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2.14  Preprocessor1_Model1\n2 rsq     standard       0.548 Preprocessor1_Model1\n\n\nHere is a predicted vs actual plot.\n\n\nShow the code\ncs_cart$fit %&gt;% \n  collect_predictions() %&gt;%\n   ggplot(aes(x = Sales, y = .pred)) +\n   geom_point(alpha = 0.6, color = \"cadetblue\") +\n   geom_smooth(method = \"loess\", formula = \"y~x\") +\n   geom_abline(intercept = 0, slope = 1, linetype = 2) +\n   labs(title = \"Carseats CART, Predicted vs Actual\")\n\n\n\n\n\n\n\n\n\nThe tree nodes do a decent job of binning the observations. The predictions vs actuals plot suggests the model over-estimates at the low end and underestimates at the high end. Calculate the test data set RMSE.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "06-decision_trees.html#bagged-trees",
    "href": "06-decision_trees.html#bagged-trees",
    "title": "6  Decision Trees",
    "section": "6.4 Bagged Trees",
    "text": "6.4 Bagged Trees\nOne drawback of decision trees is that they are high-variance estimators. A small number of additional training observations can dramatically alter the prediction performance of a learned tree.\nBootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method. The algorithm constructs B regression trees using B bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging the B trees reduces the variance. The predicted value for an observation is the mode (classification) or mean (regression) of the trees. B usually equals ~25.\nTo test the model accuracy, the out-of-bag observations are predicted from the models. For a training set of size n, each tree is composed of \\(\\sim (1 - e^{-1})n = .632n\\) unique observations in-bag and \\(.368n\\) out-of-bag. For each tree in the ensemble, bagging makes predictions on the tree’s out-of-bag observations. I think (see page 197 of (Kuhn and Johnson 2016)) bagging measures the performance (RMSE, Accuracy, ROC, etc.) of each tree in the ensemble and averages them to produce an overall performance estimate. (This makes no sense to me. If each tree has poor performance, then the average performance of many trees will still be poor. An ensemble of B trees will produce \\(\\sim .368 B\\) predictions per unique observation. Seems like you should take the mean/mode of each observation’s prediction as the final prediction. Then you have n predictions to compare to n actuals, and you assess performance on that.)\nThe downside to bagging is that there is no single tree with a set of rules to interpret. It becomes unclear which variables are more important than others.\nThe next section explains how bagged trees are a special case of random forests.\n\n6.4.1 Bagged Classification Tree\nLeaning by example, I’ll predict Purchase from the OJ data set again, this time using the bagging with parsnip::bag_tree().\n\n\nShow the code\noj_bag &lt;- list()\n\n# `bag_tree` has 4 hyperparameters (`cost_complexity`, `tree_depth`, and\n# `min_n`). Set their value to `tune()` if you want to optimize any one. Let's\n# optimize just `cost_complexity` and `tree_depth`.\noj_bag$model &lt;-\n  bag_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n# Tune a model using the workflow framework.\noj_bag$workflow &lt;-\n  workflow() %&gt;%\n  add_model(oj_bag$model) %&gt;%\n  add_formula(Purchase ~ .)\n\n# Tune the model with 10-fold CV using a regular grid of cost complexity values.\n# With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That\n# means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.\noj_bag$tune_grid &lt;-\n  oj_bag$workflow %&gt;%\n  tune_grid(\n    resamples = vfold_cv(oj_train, v = 10), \n    grid = grid_regular(cost_complexity(), tree_depth(), levels = 5)\n  )\n\n# `collect_metrics()` returns two metrics: accuracy and ROC-AUC.\noj_bag$tune_grid %&gt;% \n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(x = cost_complexity, y = mean, color = tree_depth)) +\n  geom_line(linewidth = 1.5, alpha = .6) +\n  facet_wrap(facets = vars(.metric), scales = \"free\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nThe best models in terms of accuracy and ROC was the tree depth of 4 and any cp &lt;= 3.16e-06.\n\n\nShow the code\noj_bag$tune %&gt;% show_best(metric = \"accuracy\")\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n1    0.000562              4 accuracy binary     0.825    10  0.0107 Preprocess…\n2    0.0000000001          4 accuracy binary     0.823    10  0.0109 Preprocess…\n3    0.0000000178          4 accuracy binary     0.822    10  0.0108 Preprocess…\n4    0.00000316            4 accuracy binary     0.821    10  0.0126 Preprocess…\n5    0.000562              8 accuracy binary     0.813    10  0.0156 Preprocess…\n\n\nSelect the best model in terms of accuracy and finalize the model.\n\n\nShow the code\noj_bag$best_tune &lt;- select_best(oj_bag$tune_grid, metric = \"accuracy\")\n\n# `finalize_workflow()` applies the tuning parameters to the workflow.\noj_bag$final_workflow &lt;- finalize_workflow(oj_bag$workflow, oj_bag$best_tune)\n\n# last_fit() fits the model with the full training set and evaluates it on the \n# testing data.\noj_bag$fit &lt;-\n  oj_bag$final_workflow %&gt;%\n  last_fit(oj_split)\n\n\nI think tidymodels started by splitting the training set into 10 folds, then using 9 of the folds to run the bagging algorithm and collect performance measures on the hold-out fold. After repeating the process for all 10 folds, it averaged the performance measures to produce the resampling results shown below. With hyperparameters, the process is repeated for all combinations and the resampling results above are from the best performing combination.\nThere is no single tree to visualize, and you can’t even produce a VIP. Let’s look at the performance on the holdout data set. collect_metrics() shows the accuracy and ROC AUC metrics. The accuracy is slightly lower than the single tree, but ROC AUC is higher.\n\n\nShow the code\noj_bag$fit %&gt;% collect_metrics()\n\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.842 Preprocessor1_Model1\n2 roc_auc     binary         0.925 Preprocessor1_Model1\n3 brier_class binary         0.105 Preprocessor1_Model1\n\n\nYou can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.\n\n\nShow the code\noj_bag$confmat &lt;-\n  oj_bag$fit %&gt;% \n  collect_predictions() %&gt;% \n  conf_mat(truth = Purchase, estimate = .pred_class)\n\noj_bag$confmat\n##           Truth\n## Prediction  CH  MM\n##         CH 115  18\n##         MM  16  66\n\nsummary(oj_bag$confmat)\n## # A tibble: 13 × 3\n##    .metric              .estimator .estimate\n##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n##  1 accuracy             binary         0.842\n##  2 kap                  binary         0.666\n##  3 sens                 binary         0.878\n##  4 spec                 binary         0.786\n##  5 ppv                  binary         0.865\n##  6 npv                  binary         0.805\n##  7 mcc                  binary         0.667\n##  8 j_index              binary         0.664\n##  9 bal_accuracy         binary         0.832\n## 10 detection_prevalence binary         0.619\n## 11 precision            binary         0.865\n## 12 recall               binary         0.878\n## 13 f_meas               binary         0.871\n\noj_bag$fit %&gt;% \n  collect_predictions() %&gt;% \n  select(Purchase, .pred_class) %&gt;%\n  plot(\n    main = \"Bagged Trees: Predicted vs. Actual\",\n    xlab = \"Actual\",\n    ylab = \"Predicted\"\n  )\n\n\n\n\n\n\n\n\n\nThe ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is 0.925.\n\n\nShow the code\noj_bag$fit %&gt;%\n  collect_predictions() %&gt;%\n  yardstick::roc_curve(Purchase, .pred_CH) %&gt;%\n  autoplot() +\n  labs(title = \"OJ Bagged Trees ROC Curve\")\n\n\n\n\n\n\n\n\n\nThe gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.\n\n\nShow the code\noj_bag$fit %&gt;%\n  collect_predictions() %&gt;%\n  yardstick::gain_curve(Purchase, .pred_CH) %&gt;%\n  autoplot() +\n  labs(title = \"OJ Bagged Trees Gain Curve\")\n\n\n\n\n\n\n\n\n\n\n\n6.4.2 Bagging Regression Tree\nI’ll predict Sales from the Carseats data set again, this time with parsnip::bag_tree().\n\n\nShow the code\nlibrary(baguette)\n\ncs_bag &lt;- list()\n\n# `bag_tree` has 4 hyperparameters (`cost_complexity`, `tree_depth`, and\n# `min_n`). Set their value to `tune()` if you want to optimize any one. Let's\n# optimize just `cost_complexity` and `tree_depth`.\ncs_bag$model &lt;-\n  bag_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\n# Tune a model using the workflow framework.\ncs_bag$workflow &lt;-\n  workflow() %&gt;%\n  add_model(cs_bag$model) %&gt;%\n  add_formula(Sales ~ .)\n\n# Tune the model with 10-fold CV using a regular grid of cost complexity values.\n# With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That\n# means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.\ncs_bag$tune_grid &lt;-\n  cs_bag$workflow %&gt;%\n  tune_grid(\n    resamples = vfold_cv(cs_train, v = 10), \n    grid = grid_regular(cost_complexity(), tree_depth(), levels = 5)\n  )\n\n# `collect_metrics()` returns two metrics: accuracy and ROC-AUC.\ncs_bag$tune_grid %&gt;% \n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(x = cost_complexity, y = mean, color = tree_depth)) +\n  geom_line(linewidth = 1.5, alpha = .6) +\n  facet_wrap(facets = vars(.metric), scales = \"free\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nThe best models in terms of RMSE was the tree depth of 8 and any cp &lt; 5.6E-04.\n\n\nShow the code\ncs_bag$tune %&gt;% show_best(metric = \"rmse\")\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1    0.000562             11 rmse    standard    1.58    10  0.0416 Preprocesso…\n2    0.0000000178         15 rmse    standard    1.58    10  0.0585 Preprocesso…\n3    0.000562             15 rmse    standard    1.59    10  0.0639 Preprocesso…\n4    0.0000000001         11 rmse    standard    1.60    10  0.0652 Preprocesso…\n5    0.00000316            8 rmse    standard    1.63    10  0.0618 Preprocesso…\n\n\nSelect the best model in terms of rmse and finalize the model.\n\n\nShow the code\ncs_bag$best_tune &lt;- select_best(cs_bag$tune_grid, metric = \"rmse\")\n\n# `finalize_workflow()` applies the tuning parameters to the workflow.\ncs_bag$final_workflow &lt;- finalize_workflow(cs_bag$workflow, cs_bag$best_tune)\n\n# last_fit() fits the model with the full training set and evaluates it on the \n# testing data.\ncs_bag$fit &lt;-\n  cs_bag$final_workflow %&gt;%\n  last_fit(cs_split)\n\n\ncollect_metrics() returns the RMSE, \\(RMSE = \\sqrt{(1/2) \\sum{(actual - pred)^2}})\\) and the model \\(R^2\\). The RMSE of cs_bag$fit %&gt;% collect_metrics() %&gt;% filter(.metric == \"rmse\") %&gt;% pull(.estimate) %&gt;% comma(.01) in the test data set is pretty good considering the standard deviation of Sales is comma(sd(cs_test$Sales), .01).\n\n\nShow the code\ncs_bag$fit %&gt;% collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       1.65  Preprocessor1_Model1\n2 rsq     standard       0.738 Preprocessor1_Model1\n\n\nHere is a predicted vs actual plot.\n\n\nShow the code\ncs_bag$fit %&gt;% \n  collect_predictions() %&gt;%\n   ggplot(aes(x = Sales, y = .pred)) +\n   geom_point(alpha = 0.6, color = \"cadetblue\") +\n   geom_smooth(method = \"loess\", formula = \"y~x\") +\n   geom_abline(intercept = 0, slope = 1, linetype = 2) +\n   labs(title = \"Carseats Bagged Trees, Predicted vs Actual\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "06-decision_trees.html#random-forests",
    "href": "06-decision_trees.html#random-forests",
    "title": "6  Decision Trees",
    "section": "6.5 Random Forests",
    "text": "6.5 Random Forests\nRandom forests improve bagged trees by way of a small tweak that de-correlates the trees. As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of mtry predictors is chosen as split candidates from the full set of p predictors. A fresh sample of mtry predictors is taken at each split. Typically \\(mtry \\sim \\sqrt{p}\\). Bagged trees are thus a special case of random forests where mtry = p.\n\n6.5.0.1 Random Forest Classification Tree\nLeaning by example, I’ll predict Purchase from the OJ data set again, this time using the bagging with parsnip::rand_forest().\n\n\nShow the code\noj_rf &lt;- list()\n\n# `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and\n# `min_n`). Set their value to `tune()` if you want to optimize any one. Let's\n# optimize just `trees` and `min_n`.\noj_rf$model &lt;-\n  rand_forest(\n    trees = tune(),\n    min_n = tune()\n  ) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# Tune a model using the workflow framework.\noj_rf$workflow &lt;-\n  workflow() %&gt;%\n  add_model(oj_rf$model) %&gt;%\n  add_formula(Purchase ~ .)\n\n# Tune the model with 10-fold CV using a regular grid of cost complexity values.\n# With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That\n# means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.\noj_rf$tune_grid &lt;-\n  oj_rf$workflow %&gt;%\n  tune_grid(\n    resamples = vfold_cv(oj_train, v = 10), \n    grid = grid_regular(trees(), min_n(), levels = 5)\n  )\n\n# `collect_metrics()` returns two metrics: accuracy and ROC-AUC.\noj_rf$tune_grid %&gt;% \n  collect_metrics() %&gt;%\n  mutate(trees = factor(trees)) %&gt;%\n  ggplot(aes(x = min_n, y = mean, color = trees)) +\n  geom_line(linewidth = 1.5, alpha = .6) +\n  facet_wrap(facets = vars(.metric), scales = \"free\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nThe best models in terms of accuracy and ROC was the trees of 1000 and any minimum node size of 40.\n\n\nShow the code\noj_rf$tune %&gt;% show_best(metric = \"accuracy\")\n\n\n# A tibble: 5 × 8\n  trees min_n .metric  .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1  1500    30 accuracy binary     0.813    10  0.0122 Preprocessor1_Model19\n2  1000    40 accuracy binary     0.812    10  0.0111 Preprocessor1_Model23\n3   500    40 accuracy binary     0.812    10  0.0125 Preprocessor1_Model22\n4  1000    30 accuracy binary     0.812    10  0.0130 Preprocessor1_Model18\n5   500    30 accuracy binary     0.812    10  0.0127 Preprocessor1_Model17\n\n\nSelect the best model in terms of accuracy and finalize the model.\n\n\nShow the code\noj_rf$best_tune &lt;- select_best(oj_rf$tune_grid, metric = \"accuracy\")\n\n# `finalize_workflow()` applies the tuning parameters to the workflow.\noj_rf$final_workflow &lt;- finalize_workflow(oj_rf$workflow, oj_rf$best_tune)\n\n# last_fit() fits the model with the full training set and evaluates it on the \n# testing data.\noj_rf$fit &lt;-\n  oj_rf$final_workflow %&gt;%\n  last_fit(oj_split)\n\n\nThere is no single tree to visualize, and you can’t even produce a VIP. Let’s look at the performance on the holdout data set. collect_metrics() shows the accuracy and ROC AUC metrics. The accuracy is slightly lower than the single tree, but ROC AUC is higher than both the single tree and bagged trees.\n\n\nShow the code\noj_rf$fit %&gt;% collect_metrics()\n\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.847 Preprocessor1_Model1\n2 roc_auc     binary         0.935 Preprocessor1_Model1\n3 brier_class binary         0.107 Preprocessor1_Model1\n\n\nYou can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.\n\n\nShow the code\noj_rf$confmat &lt;-\n  oj_rf$fit %&gt;% \n  collect_predictions() %&gt;% \n  conf_mat(truth = Purchase, estimate = .pred_class)\n\noj_rf$confmat\n##           Truth\n## Prediction  CH  MM\n##         CH 116  18\n##         MM  15  66\n\nsummary(oj_rf$confmat)\n## # A tibble: 13 × 3\n##    .metric              .estimator .estimate\n##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n##  1 accuracy             binary         0.847\n##  2 kap                  binary         0.676\n##  3 sens                 binary         0.885\n##  4 spec                 binary         0.786\n##  5 ppv                  binary         0.866\n##  6 npv                  binary         0.815\n##  7 mcc                  binary         0.676\n##  8 j_index              binary         0.671\n##  9 bal_accuracy         binary         0.836\n## 10 detection_prevalence binary         0.623\n## 11 precision            binary         0.866\n## 12 recall               binary         0.885\n## 13 f_meas               binary         0.875\n\noj_rf$fit %&gt;% \n  collect_predictions() %&gt;% \n  select(Purchase, .pred_class) %&gt;%\n  plot(\n    main = \"Random Forest: Predicted vs. Actual\",\n    xlab = \"Actual\",\n    ylab = \"Predicted\"\n  )\n\n\n\n\n\n\n\n\n\nThe ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is 0.935.\n\n\nShow the code\noj_rf$fit %&gt;%\n  collect_predictions() %&gt;%\n  yardstick::roc_curve(Purchase, .pred_CH) %&gt;%\n  autoplot() +\n  labs(title = \"OJ Random Forest ROC Curve\")\n\n\n\n\n\n\n\n\n\nThe gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.\n\n\nShow the code\noj_rf$fit %&gt;%\n  collect_predictions() %&gt;%\n  yardstick::gain_curve(Purchase, .pred_CH) %&gt;%\n  autoplot() +\n  labs(title = \"OJ Random Forest Gain Curve\")\n\n\n\n\n\n\n\n\n\n\n\n6.5.0.2 Random Forest Regression Tree\nI’ll predict Sales from the Carseats data set again, this time with parsnip::rand_forest().\n\n\nShow the code\ncs_rf &lt;- list()\n\n# `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and\n# `min_n`). Set their value to `tune()` if you want to optimize any one. Let's\n# optimize just `trees` and `min_n`.\ncs_rf$model &lt;-\n  rand_forest(\n    trees = tune(),\n    min_n = tune()\n  ) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# Tune a model using the workflow framework.\ncs_rf$workflow &lt;-\n  workflow() %&gt;%\n  add_model(cs_rf$model) %&gt;%\n  add_formula(Sales ~ .)\n\n# Tune the model with 10-fold CV using a regular grid of cost complexity values.\n# With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That\n# means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.\ncs_rf$tune_grid &lt;-\n  cs_rf$workflow %&gt;%\n  tune_grid(\n    resamples = vfold_cv(cs_train, v = 10), \n    grid = grid_regular(trees(), min_n(), levels = 5)\n  )\n\n# `collect_metrics()` returns two metrics: accuracy and ROC-AUC.\ncs_rf$tune_grid %&gt;% \n  collect_metrics() %&gt;%\n  mutate(trees = factor(trees)) %&gt;%\n  ggplot(aes(x = min_n, y = mean, color = trees)) +\n  geom_line(linewidth = 1.5, alpha = .6) +\n  facet_wrap(facets = vars(.metric), scales = \"free\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nThe best models in terms of RMSE was 2000 tree with at least 2 data points per node.\n\n\nShow the code\ncs_rf$tune %&gt;% show_best(metric = \"rmse\")\n\n\n# A tibble: 5 × 8\n  trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1  1000     2 rmse    standard    1.72    10  0.0707 Preprocessor1_Model03\n2   500     2 rmse    standard    1.73    10  0.0724 Preprocessor1_Model02\n3  1500     2 rmse    standard    1.73    10  0.0696 Preprocessor1_Model04\n4  2000     2 rmse    standard    1.73    10  0.0687 Preprocessor1_Model05\n5  1500    11 rmse    standard    1.77    10  0.0681 Preprocessor1_Model09\n\n\nSelect the best model in terms of rmse and finalize the model.\n\n\nShow the code\ncs_rf$best_tune &lt;- select_best(cs_rf$tune_grid, metric = \"rmse\")\n\n# `finalize_workflow()` applies the tuning parameters to the workflow.\ncs_rf$final_workflow &lt;- finalize_workflow(cs_rf$workflow, cs_rf$best_tune)\n\n# last_fit() fits the model with the full training set and evaluates it on the \n# testing data.\ncs_rf$fit &lt;-\n  cs_rf$final_workflow %&gt;%\n  last_fit(cs_split)\n\n\ncollect_metrics() returns the RMSE, \\(RMSE = \\sqrt{(1/2) \\sum{(actual - pred)^2}})\\) and the model \\(R^2\\). The RMSE of 1.90 in the test data set is pretty good considering the standard deviation of Sales is 3.10.\n\n\nShow the code\ncs_rf$fit %&gt;% collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       1.90  Preprocessor1_Model1\n2 rsq     standard       0.732 Preprocessor1_Model1\n\n\nHere is a predicted vs actual plot.\n\n\nShow the code\ncs_rf$fit %&gt;% \n  collect_predictions() %&gt;%\n   ggplot(aes(x = Sales, y = .pred)) +\n   geom_point(alpha = 0.6, color = \"cadetblue\") +\n   geom_smooth(method = \"loess\", formula = \"y~x\") +\n   geom_abline(intercept = 0, slope = 1, linetype = 2) +\n   labs(title = \"Carseats Random Forest, Predicted vs Actual\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "06-decision_trees.html#gradient-boosting",
    "href": "06-decision_trees.html#gradient-boosting",
    "title": "6  Decision Trees",
    "section": "6.6 Gradient Boosting",
    "text": "6.6 Gradient Boosting\nNote: I learned gradient boosting from explained.ai.\nGradient boosting machine (GBM) is an additive modeling algorithm that gradually builds a composite model by iteratively adding M weak sub-models based on the performance of the prior iteration’s composite,\n\\[F_M(x) = \\sum_m^M f_m(x).\\]\nThe idea is to fit a weak model, then replace the response values with the residuals from that model, and fit another model. Adding the residual prediction model to the original response prediction model produces a more accurate model. GBM repeats this process over and over, running new models to predict the residuals of the previous composite models, and adding the results to produce new composites. With each iteration, the model becomes stronger and stronger. The successive trees are usually weighted to slow down the learning rate. “Shrinkage” reduces the influence of each individual tree and leaves space for future trees to improve the model.\n\\[F_M(x) = f_0 + \\eta\\sum_{m = 1}^M f_m(x).\\]\nThe smaller the learning rate, \\(\\eta\\), the larger the number of trees, \\(M\\). \\(\\eta\\) and \\(M\\) are hyperparameters. Other constraints to the trees are usually applied as additional hyperparameters, including, tree depth, number of nodes, minimum observations per split, and minimum improvement to loss.\nThe name “gradient boosting” refers to the boosting of a model with a gradient. Each round of training builds a weak learner and uses the residuals to calculate a gradient, the partial derivative of the loss function. Gradient boosting “descends the gradient” to adjust the model parameters to reduce the error in the next round of training.\nIn the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error. GBM continues until it reaches maximum number of trees or an acceptable error level.\n\n6.6.0.1 Gradient Boosting Classification Tree\nLeaning by example, I’ll predict Purchase from the OJ data set again, this time using the bagging with parsnip::boost_tree()\n\n\nShow the code\noj_xgb &lt;- list()\n\n# `boost_tree` has 8 hyperparameters. Let's optimize just `trees` and `min_n`.\noj_xgb$model &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\n# Tune a model using the workflow framework.\noj_xgb$workflow &lt;-\n  workflow() %&gt;%\n  add_model(oj_xgb$model) %&gt;%\n  add_formula(Purchase ~ .)\n\n# Tune the model with 10-fold CV using a regular grid of cost complexity values.\n# With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That\n# means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.\noj_xgb$tune_grid &lt;-\n  oj_xgb$workflow %&gt;%\n  tune_grid(\n    resamples = vfold_cv(oj_train, v = 10), \n    grid = grid_regular(trees(), min_n(), levels = 5)\n  )\n\n# `collect_metrics()` returns two metrics: accuracy and ROC-AUC.\noj_xgb$tune_grid %&gt;% \n  collect_metrics() %&gt;%\n  mutate(trees = factor(trees)) %&gt;%\n  ggplot(aes(x = min_n, y = mean, color = trees)) +\n  geom_line(linewidth = 1.5, alpha = .6) +\n  facet_wrap(facets = vars(.metric), scales = \"free\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nThe best models in terms of accuracy and ROC was the trees of 1000 and any minimum node size of 40.\n\n\nShow the code\noj_xgb$tune %&gt;% show_best(metric = \"accuracy\")\n\n\n# A tibble: 5 × 8\n  trees min_n .metric  .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1   500    30 accuracy binary     0.814    10 0.0146  Preprocessor1_Model17\n2  1500    30 accuracy binary     0.814    10 0.0121  Preprocessor1_Model19\n3  2000    30 accuracy binary     0.814    10 0.0121  Preprocessor1_Model20\n4     1     2 accuracy binary     0.814    10 0.00999 Preprocessor1_Model01\n5  1000    30 accuracy binary     0.811    10 0.0149  Preprocessor1_Model18\n\n\nSelect the best model in terms of accuracy and finalize the model.\n\n\nShow the code\noj_xgb$best_tune &lt;- select_best(oj_xgb$tune_grid, metric = \"accuracy\")\n\n# `finalize_workflow()` applies the tuning parameters to the workflow.\noj_xgb$final_workflow &lt;- finalize_workflow(oj_xgb$workflow, oj_xgb$best_tune)\n\n# last_fit() fits the model with the full training set and evaluates it on the \n# testing data.\noj_xgb$fit &lt;-\n  oj_xgb$final_workflow %&gt;%\n  last_fit(oj_split)\n\n\nThere is no single tree to visualize, and you can’t even produce a VIP. Let’s look at the performance on the holdout data set. collect_metrics() shows the accuracy and ROC AUC metrics.\n\n\nShow the code\noj_xgb$fit %&gt;% collect_metrics()\n\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.842 Preprocessor1_Model1\n2 roc_auc     binary         0.932 Preprocessor1_Model1\n3 brier_class binary         0.103 Preprocessor1_Model1\n\n\nYou can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.\n\n\nShow the code\noj_xgb$confmat &lt;-\n  oj_xgb$fit %&gt;% \n  collect_predictions() %&gt;% \n  conf_mat(truth = Purchase, estimate = .pred_class)\n\noj_xgb$confmat\n##           Truth\n## Prediction  CH  MM\n##         CH 114  17\n##         MM  17  67\n\nsummary(oj_xgb$confmat)\n## # A tibble: 13 × 3\n##    .metric              .estimator .estimate\n##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n##  1 accuracy             binary         0.842\n##  2 kap                  binary         0.668\n##  3 sens                 binary         0.870\n##  4 spec                 binary         0.798\n##  5 ppv                  binary         0.870\n##  6 npv                  binary         0.798\n##  7 mcc                  binary         0.668\n##  8 j_index              binary         0.668\n##  9 bal_accuracy         binary         0.834\n## 10 detection_prevalence binary         0.609\n## 11 precision            binary         0.870\n## 12 recall               binary         0.870\n## 13 f_meas               binary         0.870\n\noj_xgb$fit %&gt;% \n  collect_predictions() %&gt;% \n  select(Purchase, .pred_class) %&gt;%\n  plot(\n    main = \"XGBoost: Predicted vs. Actual\",\n    xlab = \"Actual\",\n    ylab = \"Predicted\"\n  )\n\n\n\n\n\n\n\n\n\nThe ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is 0.932.\n\n\nShow the code\noj_xgb$fit %&gt;%\n  collect_predictions() %&gt;%\n  yardstick::roc_curve(Purchase, .pred_CH) %&gt;%\n  autoplot() +\n  labs(title = \"OJ XGBoost ROC Curve\")\n\n\n\n\n\n\n\n\n\nThe gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.\n\n\nShow the code\noj_xgb$fit %&gt;%\n  collect_predictions() %&gt;%\n  yardstick::gain_curve(Purchase, .pred_CH) %&gt;%\n  autoplot() +\n  labs(title = \"OJ XGBoost Gain Curve\")\n\n\n\n\n\n\n\n\n\n\n\n6.6.0.2 Gradient Boosting Regression Tree\nI’ll predict Sales from the Carseats data set again, this time with parsnip::rand_forest().\n\n\nShow the code\ncs_xgb &lt;- list()\n\n# `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and\n# `min_n`). Set their value to `tune()` if you want to optimize any one. Let's\n# optimize just `trees` and `min_n`.\ncs_xgb$model &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# Tune a model using the workflow framework.\ncs_xgb$workflow &lt;-\n  workflow() %&gt;%\n  add_model(cs_xgb$model) %&gt;%\n  add_formula(Sales ~ .)\n\n# Tune the model with 10-fold CV using a regular grid of cost complexity values.\n# With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That\n# means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.\ncs_xgb$tune_grid &lt;-\n  cs_xgb$workflow %&gt;%\n  tune_grid(\n    resamples = vfold_cv(cs_train, v = 10), \n    grid = grid_regular(trees(), min_n(), levels = 5)\n  )\n\n# `collect_metrics()` returns two metrics: accuracy and ROC-AUC.\ncs_xgb$tune_grid %&gt;% \n  collect_metrics() %&gt;%\n  mutate(trees = factor(trees)) %&gt;%\n  ggplot(aes(x = min_n, y = mean, color = trees)) +\n  geom_line(linewidth = 1.5, alpha = .6) +\n  facet_wrap(facets = vars(.metric), scales = \"free\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nThe best models in terms of RMSE was 500 trees with at least 21 data points per node.\n\n\nShow the code\ncs_xgb$tune %&gt;% show_best(metric = \"rmse\")\n\n\n# A tibble: 5 × 8\n  trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1   500    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model07\n2  1000    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model08\n3  1500    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model09\n4  2000    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model10\n5   500    40 rmse    standard    1.38    10  0.0759 Preprocessor1_Model22\n\n\nSelect the best model in terms of rmse and finalize the model.\n\n\nShow the code\ncs_xgb$best_tune &lt;- select_best(cs_xgb$tune_grid, metric = \"rmse\")\n\n# `finalize_workflow()` applies the tuning parameters to the workflow.\ncs_xgb$final_workflow &lt;- finalize_workflow(cs_xgb$workflow, cs_xgb$best_tune)\n\n# last_fit() fits the model with the full training set and evaluates it on the \n# testing data.\ncs_xgb$fit &lt;-\n  cs_rf$final_workflow %&gt;%\n  last_fit(cs_split)\n\n\ncollect_metrics() returns the RMSE, \\(RMSE = \\sqrt{(1/2) \\sum{(actual - pred)^2}})\\) and the model \\(R^2\\). The RMSE of 1.89 in the test data set is pretty good considering the standard deviation of Sales is 3.10.\n\n\nShow the code\ncs_xgb$fit %&gt;% collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       1.89  Preprocessor1_Model1\n2 rsq     standard       0.740 Preprocessor1_Model1\n\n\nHere is a predicted vs actual plot.\n\n\nShow the code\ncs_xgb$fit %&gt;% \n  collect_predictions() %&gt;%\n   ggplot(aes(x = Sales, y = .pred)) +\n   geom_point(alpha = 0.6, color = \"cadetblue\") +\n   geom_smooth(method = \"loess\", formula = \"y~x\") +\n   geom_abline(intercept = 0, slope = 1, linetype = 2) +\n   labs(title = \"Carseats Random Forest, Predicted vs Actual\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFawcett, Tom. 2005. An Introduction to ROC Analysis. ELSEVIER. https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning: With Applications in r. 1st ed. New York, NY: Springer. http://faculty.marshall.usc.edu/gareth-james/ISL/book.html.\n\n\nKuhn, Max, and Kjell Johnson. 2016. Applied Predictive Modeling. 1st ed. New York, NY: Springer. http://appliedpredictivemodeling.com/.\n\n\nTherneau, Terry, and Elizabeth Atkinson. 2019. An Introduction to Recursive Partitioning Using the RPART Routines. Boca Raton, Florida: Chapman; Hall/CRC. https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "06-decision_trees.html#footnotes",
    "href": "06-decision_trees.html#footnotes",
    "title": "6  Decision Trees",
    "section": "",
    "text": "You might come across a variation of the Gini index called entropy, the information statistic, \\(D = - \\sum_{c=1}^C{\\hat{p}_{kc} \\log \\hat{p}_{kc}}\\). Whereas Gini can range from 0 to .25, entropy ranges from 0 to \\(-(.5 \\log(.5)) \\cdot 2 = 0.69\\).↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "07-support_vector_machines.html",
    "href": "07-support_vector_machines.html",
    "title": "7  Support Vector Machines",
    "section": "",
    "text": "7.1 Maximal Margin Classifier\nThe maximal margin classifier is the optimal hyperplane defined in the (rare) case where two classes are linearly separable. Given an \\(X_{n \\times p}\\) predictor matrix with a binary response variable \\(y \\in \\{-1, 1\\}\\) it might be possible to define a p-dimensional hyperplane \\(h(x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 \\dots + \\beta_px_p = X_i^{'} \\beta + \\beta_0 = 0\\) such that all of the \\(y_i = -1\\) observations fall on the negative side of the hyperplane and the \\(y_i = +1\\) observations fall on the positive side:\n\\[y_i \\left(x_i^{'} \\beta + \\beta_0 \\right) &gt; 0\\]\nThis separating hyperplane is a simple classifier, and the magnitude of \\(\\left(x_i^{'} \\beta + \\beta_0 \\right)\\) is an indicator of confidence in the predicted classification.\nIf you constrain \\(\\beta\\) to be a unit vector, \\(||\\beta|| = \\sum\\beta^2 = 1\\), then the products of the hyperplane and response variables, \\(\\left(x_i^{'} \\beta + \\beta_0 \\right)\\), are the positive perpendicular distances from the hyperplane. If a separating hyperplane exists, there are an infinite number of possible hyperplanes. Evaluate a hyperplane by its margin, \\(M\\), the perpendicular distance to the closest observation.\n\\[M = \\min \\left\\{y_i (x_i^{'} \\beta + \\beta_0) \\right\\}.\\]\nThe maximal margin classifier is the hyperplane that maximizes \\(M.\\) The figure below (adapted from figure 9.3 from (James et al. 2013)) shows a maximal marginal classifier. The three vectors shown in the figure anchor the hyperplane and are called the support vectors. Interestingly, it is only these three observations that factor into the determination of the maximal marginal classifier.\nSo, to put it all together, if a separating hyperplane exists, one could calculate it by maximizing \\(M\\) subject to \\(||\\beta|| = 1\\) and \\(y_i (x_i^{'} \\beta + \\beta_0) \\ge M\\) for all \\(i\\). However, a separating hyperplane rarely exists. In fact, even if a separating hyperplane does exist, its maximal margin classifier is probably undesirably narrow. A maximal margin classifier is sensitive to outliers so it tends to overfit data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "07-support_vector_machines.html#support-vector-classifier",
    "href": "07-support_vector_machines.html#support-vector-classifier",
    "title": "7  Support Vector Machines",
    "section": "7.2 Support Vector Classifier",
    "text": "7.2 Support Vector Classifier\nThe maximal margin classifier can be generalized to non-separable cases using a so-called soft margin. The generalization is called the support vector classifier. The soft margin allows some misclassification in the interest of greater robustness to individual observations.\nThe support vector classifier maximizes \\(M\\) subject to \\(||\\beta|| = 1\\) and \\(y_i (x_i^{'} \\beta + \\beta_0) \\ge M(1 - \\xi_i)\\) and \\(\\sum \\xi_i \\le \\Xi\\) for all \\(i\\). The \\(\\xi_i\\) are slack variables whose sum is bounded by some constant tuning parameter \\(\\Xi\\). The slack variable values indicate where the observation lies: \\(\\xi_i = 0\\) observations lie on the correct side of the margin; \\(\\xi_i &gt; 0\\) observation lie on the wrong side of the margin; \\(\\xi_i &gt; 1\\) observations lie on the wrong side of the hyperplane. \\(\\Xi\\) sets the tolerance for margin violation. If \\(\\Xi = 0\\), then all observations must reside on the correct side of the margin, as in the maximal margin classifier. \\(\\Xi\\) controls the bias-variance trade-off: as \\(\\Xi\\) increases, the margin widens and allows more violations, increasing bias and decreasing variance. Similar to the maximal margin classifier, only the observations that are on the margin or that violate the margin factor into the determination of the support vector classifier. These observations are the support vectors.\nThe figure below (adaptation of figure 9.7 from (James et al. 2013)) shows two support vector classifiers. The one on the left uses a large \\(\\Xi\\) and as a result includes many support vectors. The one on the right uses a smaller \\(\\Xi.\\)\n\n\n\n\n\n\n\n\n\nAs \\(\\Xi\\) increases, the number of violating observations increase, and thus the number of support vectors increases. This property makes the algorithm robust to the extreme observations far away from the hyperplane. The only shortcoming with the algorithm is that it presumes a linear decision boundary.\nLet’s build a support vector classifier model to predict credit default in the ISLM:Default data set. I’ll build the model in caret with the svmLinear method using 10-fold cross-validation (CV-10) to optimize the hyperparameters. There is only one hyperparameter for this model, the cost parameter:\n\n\nShow the code\ncaret::modelLookup(\"svmLinear\")\n\n\nI’m not sure if \\(C = Xi\\), but it seems to function the same way. The documentation notes in e1071::svm() say C is the “cost of constraints violation (default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.” CV-10 will fit 10 models for each candidate value of C and keep the model with the best performance on resamples according to our evaluation metric. I can evaluate with “Accuracy”, “Kappa”, or “ROC”. I’ll use ROC, so I need to also set summaryFunction = twoClassSummary and classProbs = TRUE.\nsvmLinear expects the response variable to be a factor with labels that can double as R variable names. This data set is fine because default is a factor with labels “No” and “Yes”. The predictor variables should be of comparable scale, but that is not the case here: student is binary, balance has a range of $0 - $2654, and income has a range of $772 - $73,554. I’ll one-hot encode student and standardize balance and income inside a recipe object.\n\n\nShow the code\nmdl_ctrl &lt;- trainControl(\n  method = \"cv\", number = 10,\n  summaryFunction = twoClassSummary, classProbs = TRUE\n)\n\nrcpe &lt;- recipe(default ~ ., data = dat_train) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_center(balance, income) %&gt;%\n  step_scale(balance, income)\n\ntic()\nset.seed(1234)\ncapture.output(\n  mdl_svm_linear &lt;- train(\n    rcpe,\n    data = dat_train,\n    method = \"svmLinear\",\n    metric = \"ROC\",\n    trControl = mdl_ctrl,\n    tuneGrid = expand.grid(C = c(1e-1, 1e0, 1e1, 1e2, 1e3, 1e4))\n  )\n)\ntoc()\n\n\nI experimented with the tuneGrid and found that smaller values for C (C &lt;= 10) produced poorer performance on resamples and on the holdout set. Unfortunately, the time to fit the models increased with C so that expand.grid(C = c(1e-1, 1e0, 1e1, 1e2, 1e3, 1e4) ran ~6 minutes.\nThe cross-validation maximized ROC with C = 10,000. The first three values (.1, 1, and 10) resulted in models that predicted no default every time.\n\n\nShow the code\nmdl_svm_linear\n\n\nAs C increases, the model variance decreases at the expense of more bias. The plot of the optimization results makes you wonder if C = 100 is basically just as good as C = 10,000 at a fraction of the fitting time.\n\n\nShow the code\nggplot(mdl_svm_linear)\n\n\nPredictions on the holdout set yield 96.8% accuracy. It found 15 of the 66 defaulters (sensitivity = 0.227), and misclassified 13 of the 1933 non-defaulters (specificity = 0.993).\n\n\nShow the code\npreds_svm_linear &lt;- bind_cols(\n  dat_test,\n  predict(mdl_svm_linear, newdata = dat_test, type = \"prob\"),\n  Predicted = predict(mdl_svm_linear, newdata = dat_test, type = \"raw\")\n)\nconfusionMatrix(preds_svm_linear$Predicted, reference = preds_svm_linear$default, positive = \"Yes\")\n\n\nMetrics::auc() will calculate the confusion matrix values from the model using the holdout data set. The AUC on the holdout set is 0.9541.\n\n\nShow the code\nmdl_svm_linear_auc &lt;- Metrics::auc(actual = preds_svm_linear$default == \"Yes\", preds_svm_linear$Yes)\nyardstick::roc_curve(preds_svm_linear, default, Yes) %&gt;%\n  autoplot() +\n  labs(\n    title = \"SVM Linear Model ROC Curve, Test Data\",\n    subtitle = paste0(\"AUC = \", round(mdl_svm_linear_auc, 4))\n  )\n\n\nThere are just a few predictors in this model, so there is a chance I can visualize the model.\n\n\nShow the code\nfits_svm_linear &lt;- bind_cols(\n  dat_train,\n  predict(mdl_svm_linear, newdata = dat_train, type = \"prob\"),\n  Predicted = predict(mdl_svm_linear, newdata = dat_train, type = \"raw\"),\n)\n\nbind_rows(\n  fits_svm_linear %&gt;% mutate(set = \"Actual\", outcome = default),\n  fits_svm_linear %&gt;% mutate(set = \"Fitted\", outcome = Predicted)\n) %&gt;%\n  ggplot(aes(x = balance, y = income, color = outcome)) +\n  geom_point(aes(shape = student), alpha = 0.6) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_y_continuous(labels=scales::dollar_format()) +\n  scale_x_continuous(labels=scales::dollar_format()) +\n  scale_color_manual(values = list(No = \"#B6E2D3\", Yes = \"#EF7C8E\")) +\n  labs(title = \"Training Set\") +\n  facet_wrap(vars(set))\n\n\nLooks like the hyperplane slopes slightly right, so high credit card balances are a little less likely to fall into default if income is high. Distinguishing students is difficult, but they are generally at the low end of the income scale, and they seem to exert a positive association with default. Let’s look at the same figure with the training set.\n\n\nShow the code\nbind_rows(\n  preds_svm_linear %&gt;% mutate(set = \"Actual\", outcome = default),\n  preds_svm_linear %&gt;% mutate(set = \"Fitted\", outcome = Predicted)\n) %&gt;%\n  ggplot(aes(x = balance, y = income, color = outcome)) +\n  geom_point(aes(shape = student), alpha = 0.6) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_y_continuous(labels=scales::dollar_format()) +\n  scale_x_continuous(labels=scales::dollar_format()) +\n  scale_color_manual(values = list(No = \"#B6E2D3\", Yes = \"#EF7C8E\")) +\n  labs(title = \"Test Set\") +\n  facet_wrap(vars(set))\n\n\nVisually, the model performed consistently between the training and test sets.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "07-support_vector_machines.html#support-vector-machines",
    "href": "07-support_vector_machines.html#support-vector-machines",
    "title": "7  Support Vector Machines",
    "section": "7.3 Support Vector Machines",
    "text": "7.3 Support Vector Machines\nEnlarging the feature space of the support vector classifier accommodates nonlinear relationships. Support vector machines do this in a specific way, using kernels. Before diving into kernels, you need to understand (somewhat) the solution to the support vector classifier optimization problem.\nThe linear support vector classifier can be represented as\n\\[f(x) = \\beta_0 + \\sum_i^n \\alpha_i \\langle x, x_i \\rangle.\\]\nThat is, the classification of test observation \\(x\\) is the sum of the dot products of \\(x\\) with all the \\(n\\) observations in the training set, multiplied by the vector \\(\\alpha\\) (plus the constant \\(\\beta_0\\)). The \\(\\alpha\\) vector is calculated from the \\(n \\choose 2\\) dot products of the training data set. Actually, the classification is simpler than that because \\(\\alpha_i = 0\\) for all observation that are not support vectors, so you can actually represent the solution as\n\\[f(x) = \\beta_0 + \\sum_{i \\in S} \\alpha_i \\langle x, x_i \\rangle\\] where \\(S\\) is the set of support vector indices.\nNow, you can generalize the inner dot product with a wrapper function, called a kernel, \\(K(x_i, x_{i^{'}})\\).\n\\[f(x) = \\beta_0 + \\sum_{i \\in S} \\alpha_i K(x, x_i)\\]\nTo get the the support vector classifier, you’d defined \\(K\\) to be a linear kernel:\n\\[K(x_i, x_i^{'}) = \\langle x, x_i \\rangle\\]\nBut you could also use other kernels, like the polynomial of degree \\(d\\),\n\\[K(x, x') = (1 + \\langle x, x' \\rangle)^d\\]\nor radial\n\\[K(x, x') = \\exp\\{-\\gamma ||x - x'||^2\\}.\\]\nThe figure below (figure 9.9 from (James et al. 2013)) shows two support vector classifiers. The one on the left uses a polynomial kernel and the one on the right uses a radial kernel.\n\n\n\nFIGURE 9.9 from An Introduction to Statistical Learning\n\n\nThe SVM model can be expressed in the familiar “loss + penalty” minimization structure, \\(\\min_{\\beta} \\left \\{ L(X,y,\\beta) + \\lambda P(\\beta) \\right \\}\\) as\n\\[\\min_\\beta \\left \\{ \\sum_{i=1}^n \\max [0, 1-y_i f(x_i)] + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right \\}\\]\nIncreasing \\(\\lambda\\), shrinks \\(\\beta\\) and more violations to the margin are tolerated, resulting in a lower-variance/higher-bias model. The loss function above is known as a hinge loss.\nLet’s build a support vector machine model to predict credit default in the ISLM:Default data set again. I’ll try a polynomial kernel with the svmPoly method and a radial kernal with svmRadial. svmPoly has three hyperparameters. What is the Scale parameter?\n\n\nShow the code\ncaret::modelLookup(\"svmPoly\")\n\n\nsvmRadial has two hyperparameters:\n\n\nShow the code\ncaret::modelLookup(\"svmRadial\")\n\n\nI’ll use the same trControl object as with the support vector classifier, and I’ll use the same one-hot encoded binaries and scaled and centered data. I fixed scale at its default value and tried polynomials of degree 1-3. I used the same candidate cost values.\n\n\nShow the code\ntic()\nset.seed(1234)\ncapture.output(\n  mdl_svm_poly &lt;- train(\n    rcpe,\n    data = dat_train,\n    method = \"svmPoly\",\n    metric = \"ROC\",\n    trControl = mdl_ctrl,\n    tuneGrid = expand.grid(\n      C = c(1e-1, 1e0, 1e1, 1e2, 1e3, 1e4),\n      degree = c(1, 2, 3),\n      scale = 0.001)\n  )\n)\ntoc()\n\n\nThe model ran ~3 minutes. The cross-validation maximized ROC with degree = 2 and C = 0.1.\n\n\nShow the code\nmdl_svm_poly\n\n\nHere is svmRadial. At this point, I do not know how the sigma tuning parameter works, so I expanded around the default value used from tuneLength = 1.\n\n\nShow the code\ntic()\nset.seed(1234)\ncapture.output(\n  mdl_svm_radial &lt;- train(\n    rcpe,\n    data = dat_train,\n    method = \"svmRadial\",\n    metric = \"ROC\",\n    trControl = mdl_ctrl,\n    tuneGrid = expand.grid(\n      C = c(1e-1, 1e0, 1e1, 1e2, 1e3),\n      sigma = c(.01, .1, 1.0)\n    )\n  )\n)\ntoc()\n\n\nThis model ran in ~11 minutes and optimized at C = 0.1 and sigma = 0.01.\n\n\nShow the code\nmdl_svm_radial\n\n\nHere are the optimization plots I used to help tune the models.\n\n\nShow the code\np1 &lt;- ggplot(mdl_svm_poly) + labs(title = \"SVMPoly Tuning\") + \n  theme_minimal() + theme(legend.position = \"bottom\")\np2 &lt;- ggplot(mdl_svm_radial) + labs(title = \"SVMRadial Tuning\") + \n  theme_minimal() + theme(legend.position = \"bottom\")\ngridExtra::grid.arrange(p1, p2, nrow = 1)\n\n\nThe polynomial model predictions on the holdout set yielded 96.85% accuracy. It found 20 of the 66 defaulters (sensitivity = 0.303), and misclassified 17 of the 1933 non-defaulters (specificity = 0.991). So the polynomial model found a few more defaulters at the expense of a few more mistakes.\nThe radial model predictions on the holdout set yielded 97% accuracy. It found 17 defaulters (sensitivity = 0.258), and misclassified 11 non-defaulters (specificity = 0.994). So the radial was somewhere between the linear and polynomial models.\n\n\nShow the code\npreds_svm_poly &lt;- bind_cols(\n  dat_test,\n  predict(mdl_svm_poly, newdata = dat_test, type = \"prob\"),\n  Predicted = predict(mdl_svm_poly, newdata = dat_test, type = \"raw\")\n)\nconfusionMatrix(preds_svm_poly$Predicted, reference = preds_svm_poly$default, positive = \"Yes\")\n\npreds_svm_radial &lt;- bind_cols(\n  dat_test,\n  predict(mdl_svm_radial, newdata = dat_test, type = \"prob\"),\n  Predicted = predict(mdl_svm_radial, newdata = dat_test, type = \"raw\")\n)\nconfusionMatrix(preds_svm_radial$Predicted, reference = preds_svm_radial$default, positive = \"Yes\")\n\n\nThe AUCs on the holdout set is where 0.9536 for the polynmomial and 0.8836 for the radial.\n\n\nShow the code\nMetrics::auc(actual = preds_svm_poly$default == \"Yes\", preds_svm_poly$Yes)\nMetrics::auc(actual = preds_svm_radial$default == \"Yes\", preds_svm_radial$Yes)\n\n\nLet’s see what the two models look like on the training data.\n\n\nShow the code\nfits_svm_poly &lt;- bind_cols(\n  dat_train,\n  predict(mdl_svm_poly, newdata = dat_train, type = \"prob\"),\n  Predicted = predict(mdl_svm_poly, newdata = dat_train, type = \"raw\"),\n)\n\nbind_rows(\n  fits_svm_poly %&gt;% mutate(set = \"Actual\", outcome = default),\n  fits_svm_poly %&gt;% mutate(set = \"Fitted\", outcome = Predicted)\n) %&gt;%\n  ggplot(aes(x = balance, y = income, color = outcome)) +\n  geom_point(aes(shape = student), alpha = 0.6) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_y_continuous(labels=scales::dollar_format()) +\n  scale_x_continuous(labels=scales::dollar_format()) +\n  scale_color_manual(values = list(No = \"#B6E2D3\", Yes = \"#EF7C8E\")) +\n  labs(title = \"Training Set, Polynomial Kernel\") +\n  facet_wrap(vars(set))\n\n\n\n\nShow the code\nfits_svm_radial &lt;- bind_cols(\n  dat_train,\n  predict(mdl_svm_radial, newdata = dat_train, type = \"prob\"),\n  Predicted = predict(mdl_svm_radial, newdata = dat_train, type = \"raw\"),\n)\n\nbind_rows(\n  fits_svm_radial %&gt;% mutate(set = \"Actual\", outcome = default),\n  fits_svm_radial %&gt;% mutate(set = \"Fitted\", outcome = Predicted)\n) %&gt;%\n  ggplot(aes(x = balance, y = income, color = outcome)) +\n  geom_point(aes(shape = student), alpha = 0.6) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_y_continuous(labels=scales::dollar_format()) +\n  scale_x_continuous(labels=scales::dollar_format()) +\n  scale_color_manual(values = list(No = \"#B6E2D3\", Yes = \"#EF7C8E\")) +\n  labs(title = \"Training Set, Radial Kernel\") +\n  facet_wrap(vars(set))\n\n\nYou can see the slight curvature in the hyperplane now. The polynomial and the radial models look pretty much identical to me.\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. The Elements of Statistical Learning. 2nd ed. New York, NY: Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning: With Applications in r. 1st ed. New York, NY: Springer. http://faculty.marshall.usc.edu/gareth-james/ISL/book.html.\n\n\nKuhn, Max, and Kjell Johnson. 2016. Applied Predictive Modeling. 1st ed. New York, NY: Springer. http://appliedpredictivemodeling.com/.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "08-bayesian_regression.html",
    "href": "08-bayesian_regression.html",
    "title": "8  Bayesian Regression",
    "section": "",
    "text": "8.1 Compared to Frequentist Regression\nThis section is my notes from DataCamp course Bayesian Regression Modeling with rstanarm. The course refers to Andrew Gelman’s book “Bayesian Data Analysis” (Gelman2013?). It is an intuitive approach to Bayesian inference.\nFrequentist regression estimates fixed population parameters from a random sample of data whose test statistics are random variables. The Bayesian approach to regression works the other direction: Bayesian regression characterizes the distribution of the random variable population parameters from the evidence of a fixed sample of data.\nThe frequentist p-value is the probability of observing a test statistic of equal or greater magnitude if the null hypothesis is true. The 95% confidence interval has a 95% probability of capturing the population value (repeated sampling would produce similar CIs, 95% of which would capture the population value). The 95% credible interval captures the confidence the value falls within the range. There is an important difference here. In the Bayesian framework, you can state the probability the parameter value falls with a specified range, but there is no equivalent in frequentist regression.\nBayesian regression uses maximum likelihood to fit the model because the integral in the denominator (the marginal distribution) cannot (or is difficult) to calculate analytically. Instead, the algorithm samples from the posterior distribution in groups (chains). Each chain begins at a random location. Each sample (iteration) moves toward the area where the combination of likelihood and prior indicates a high probability of the true parameter value residing. The more iterations per chain, the larger the total sample size, and more robust the outcome. The chains need to converge to have a stable estimate. The iterations prior to convergence are sometimes referred to as the “warm up” and are not included in the posterior distribution estimate. By default, the rstanarm package estimates 4 chains, each with 2,000 iterations, and the first 1,000 set aside as warm-up. The final posterior combines the chains, so is composed of 4,000 iterations.\nA Bayesian model estimates each coefficient parameter and model error with a prior*likelihood/marginal dist = posterior framework.\nShow the code\nlibrary(rstanarm)\nconflicted::conflicts_prefer(rstanarm::exponential)\n\nsink(type = \"message\")\nstan_mdl &lt;- stan_glm(kid_score ~ mom_iq, data = kidiq)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 9.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.98 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.044 seconds (Warm-up)\nChain 1:                0.059 seconds (Sampling)\nChain 1:                0.103 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.047 seconds (Warm-up)\nChain 2:                0.056 seconds (Sampling)\nChain 2:                0.103 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.048 seconds (Warm-up)\nChain 3:                0.06 seconds (Sampling)\nChain 3:                0.108 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.059 seconds (Warm-up)\nChain 4:                0.073 seconds (Sampling)\nChain 4:                0.132 seconds (Total)\nChain 4: \n\n\nShow the code\nsink(type = \"message\")\n\nsummary(stan_mdl)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_iq\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 434\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 25.9    6.0 18.2  25.9  33.6 \nmom_iq       0.6    0.1  0.5   0.6   0.7 \nsigma       18.3    0.6 17.5  18.3  19.1 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 86.8    1.2 85.2  86.8  88.3 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.1  1.0  4098 \nmom_iq        0.0  1.0  4093 \nsigma         0.0  1.0  3972 \nmean_PPD      0.0  1.0  3659 \nlog-posterior 0.0  1.0  1966 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nShow the code\nbroom.mixed::tidy(stan_mdl)\n\n\n# A tibble: 2 × 3\n  term        estimate std.error\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   25.9      6.00  \n2 mom_iq         0.609    0.0596\n\n\nShow the code\nposterior_interval(stan_mdl)\n\n\n                    5%        95%\n(Intercept) 15.9076405 35.8096606\nmom_iq       0.5123667  0.7077018\nsigma       17.3416781 19.3581018\nBy default, rstanarm priors are normal distributions with mean 0 and variance equal to a scaler multiple of the data variance.\nShow the code\nprior_summary(stan_mdl)\n\n\nPriors for model 'stan_mdl' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 87, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 87, scale = 51)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 3.4)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.049)\n------\nSee help('prior_summary.stanreg') for more details\n\n\nShow the code\n# Calculate the adjusted scale for the intercept\n2.5 * sd(kidiq$kid_score)\n\n\n[1] 51.02672\n\n\nShow the code\n# Calculate the adjusted scale for `mom_iq`\n(2.5 / sd(kidiq$mom_iq)) * sd(kidiq$kid_score)\n\n\n[1] 3.401781\nYou can override the priors to set your own:\nShow the code\nstan_mdl &lt;- stan_glm(kid_score ~ mom_iq, data = kidiq,\n                     prior_intercept = normal(location = 0, scale = 10, autoscale = FALSE),\n                     prior = normal(location = 0, scale = 2.5, autoscale = FALSE),\n                     prior_aux = exponential(rate = 1, autoscale = FALSE))\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.046 seconds (Warm-up)\nChain 1:                0.067 seconds (Sampling)\nChain 1:                0.113 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.052 seconds (Warm-up)\nChain 2:                0.068 seconds (Sampling)\nChain 2:                0.12 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.3e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.051 seconds (Warm-up)\nChain 3:                0.071 seconds (Sampling)\nChain 3:                0.122 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.041 seconds (Warm-up)\nChain 4:                0.069 seconds (Sampling)\nChain 4:                0.11 seconds (Total)\nChain 4:\nBayesian estimation samples from the posterior distribution, so there is no point estimate and test statistic.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Regression</span>"
    ]
  },
  {
    "objectID": "08-bayesian_regression.html#model-evaluation",
    "href": "08-bayesian_regression.html#model-evaluation",
    "title": "8  Bayesian Regression",
    "section": "8.2 Model Evaluation",
    "text": "8.2 Model Evaluation\nThe R-squared statistic is not available from the summary object, but you can still calculate it manually from the model data, or use the bayes_R2 function.\n\n\nShow the code\nsse &lt;- var(stan_mdl$residuals)\nssr &lt;- var(stan_mdl$fitted.values)\nsst &lt;- ssr + sse\nr2 &lt;- 1 - sse/sst\n# bayes_R2 returns a vector of length equal to the posterior sample size.\nbayes_R2(stan_mdl) %&gt;% summary()\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.09451 0.18493 0.20682 0.20694 0.22914 0.31866 \n\n\nShow the code\nbayes_R2(stan_mdl) %&gt;% quantile(c(.025, .975))\n\n\n     2.5%     97.5% \n0.1448767 0.2693601 \n\n\nBayesian regression has other model fit statistics.\n\n\nShow the code\n# Calculate posterior predictive scores\npredictions &lt;- posterior_linpred(stan_mdl)\n# Print a summary of the 1st replication\nsummary(predictions[1,])\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  67.44   78.81   84.78   86.12   92.75  111.21 \n\n\nOr produce a posterior predictive model check.\n\n\nShow the code\n# peaks of observed data and model are similar places, but there is a second \n# mode of popularity scores around 10 that is not captured by the model.\npp_check(stan_mdl, \"dens_overlay\")\n\n\n\n\n\n\n\n\n\nShow the code\npp_check(stan_mdl, \"stat\")\n\n\n\n\n\n\n\n\n\nShow the code\n# mean and sd of the observed data in middle of expected distribution - these \n# two characteristics are recovered well.\npp_check(stan_mdl, \"stat_2d\")\n\n\n\n\n\n\n\n\n\n\n8.2.1 Model Comparison\nUse the loo (leave on out) package to compare models. loo approximates cross-validation. In the initial summary, 4000 is the number of iterations in the posterior, and 434 is the number of observations in the data set.\nekpd_loo is the LOO estimate; p_loo is the effective number of parameters in the model; and looic is the LOO estimate converted to the deviance scale, -2 * LOO.\n\n\nShow the code\nstan_loo &lt;- loo(stan_mdl)\nstan_loo\n\n\n\nComputed from 4000 by 434 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -1878.9 14.9\np_loo         2.9  0.3\nlooic      3757.8 29.8\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.7, 1.1]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nThe statistics are not useful in isolation - they should be used in comparison to competing models.\n\n\nShow the code\nstan_mdl_2 &lt;- stan_glm(kid_score ~ mom_iq * mom_hs, data = kidiq)\n\n\nWhich model has more predictive power? As a rule of thumb, if the absolute value of the difference is greater than the standard error, then the result is significant. In this case, the new model is significantly better.\n\n\nShow the code\nstan_loo_2 &lt;- loo(stan_mdl_2) \n\nloo_compare(stan_loo, stan_loo_2)\n\n\n           elpd_diff se_diff\nstan_mdl_2  0.0       0.0   \nstan_mdl   -6.4       4.2   \n\n\n\n\n8.2.2 Visualization\nUse tidy() to pull out the intercept and slope. Use the posterior distributions to create a predicted regression line from each draw in the posterior samples. These lines will show the uncertainty around the overall line.\n\n\nShow the code\nlibrary(broom)\n\nstan_tdy_2 &lt;- tidy(stan_mdl_2)\n\ndraws &lt;- tidybayes::spread_draws(stan_mdl_2, `(Intercept)`, mom_iq)\n\nkidiq %&gt;%\n  ggplot(aes(x = mom_iq, y = kid_score)) +\n  geom_point() +\n  geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = mom_iq), \n              size = 0.1, alpha = 0.2, color = \"skyblue\") +\n  geom_abline(intercept = stan_tdy_2$estimate[1], slope = stan_tdy_2$estimate[2]) \n\n\n\n\n\n\n\n\n\nUse the model to get posterior predictions with posterior_predict().\n\n\nShow the code\nposteriors &lt;- posterior_predict(stan_mdl_2)\n\nnew_data &lt;- data.frame(mom_iq = 100, mom_hs = c(0, 1))\nposteriors_2 &lt;- posterior_predict(stan_mdl_2, newdata = new_data) %&gt;%\n  as.data.frame()\n\ncolnames(posteriors_2) &lt;- c(\"No HS\", \"Completed HS\")\nposteriors_2 %&gt;%\n  pivot_longer(everything(), names_to = \"HS\", values_to = \"predict\") %&gt;%\n  ggplot(aes(x = predict)) +\n  geom_density() +\n  facet_wrap(~HS, ncol = 1)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Regression</span>"
    ]
  },
  {
    "objectID": "09-emmeans.html",
    "href": "09-emmeans.html",
    "title": "9  Estimated Marginal Means",
    "section": "",
    "text": "9.1 Ordinary and Estimated Marginal Means\nThe “ordinary” marginal means (OMM) are just the conditional means of the data. The OMM monotonically increases until percent = 18.\npigs |&gt; summarize(.by = percent, OMM = mean(conc))\n\n  percent      OMM\n1       9 32.70000\n2      12 38.01111\n3      15 40.12857\n4      18 39.94000\nThe estimated marginal means are the means of a model’s predicted values over some reference grid. Fit the inverse concentration (that’s the best linear relationship).\nmdl &lt;- \n  linear_reg() |&gt;\n  fit(inverse(conc) ~ ., data = pigs) |&gt;\n  extract_fit_engine()\nThe reference grid is a grid of the model predictor variables. For factor vars, there’s a row for each level, and for continuous vars, there is a row for the mean. The manual way to calculate this is\nRG &lt;- expand.grid(source = levels(pigs$source), percent = levels(pigs$percent))\n\naugment(mdl, newdata = RG) |&gt;\n  summarize(.by = percent, EMM = 1 / mean(.fitted)) \n\n# A tibble: 4 × 2\n  percent   EMM\n  &lt;fct&gt;   &lt;dbl&gt;\n1 9        31.0\n2 12       37.0\n3 15       38.1\n4 18       41.5\nThat’s what emmeans() does by default. This is like a weighted average so that groups with many observations are underweighted.\nemmeans(mdl, \"percent\", type = \"response\")\n\n percent response    SE df lower.CL upper.CL\n 9           31.0 0.993 23     29.1     33.2\n 12          37.0 1.330 23     34.5     40.0\n 15          38.1 1.600 23     35.0     41.7\n 18          41.5 2.310 23     37.3     46.9\n\nResults are averaged over the levels of: source \nConfidence level used: 0.95 \nIntervals are back-transformed from the inverse scale",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimated Marginal Means</span>"
    ]
  },
  {
    "objectID": "09-emmeans.html#ggeffects",
    "href": "09-emmeans.html#ggeffects",
    "title": "9  Estimated Marginal Means",
    "section": "9.2 ggeffects",
    "text": "9.2 ggeffects\nThe ggeffects package does a nice job working with EMMs.\n\nmdl_2 &lt;- \n  linear_reg() |&gt; \n  fit(1/conc ~ ., data = pigs) |&gt;\n  extract_fit_engine() \n\npredict_response(mdl_2, terms = \"percent\", margin = \"marginalmeans\")\n\n# Predicted values of conc\n\npercent | Predicted |       95% CI\n----------------------------------\n9       |     31.01 | 33.21, 29.08\n12      |     37.03 | 40.00, 34.47\n15      |     38.06 | 41.68, 35.01\n18      |     41.53 | 46.92, 37.25\n\n\nYou can pipe the responses right into ggplot.\n\n\nShow the code\npredict_response(mdl_2, terms = c(\"percent\", \"source\")) |&gt;\n  ggplot(aes(x = x, y = predicted, color = group)) +\n  geom_point(size = 1.5) +\n  geom_line(aes(group = group), size = 1) +\n  labs(\n    title = \"Marginal predicted values of leucine from regression model.\",\n    y = \"Marginal predicted concentration\", x = \"protein percent\"\n  ) +\n  scale_color_manual(values = emm_pal) +\n  theme(legend.position = \"top\", legend.justification = \"left\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimated Marginal Means</span>"
    ]
  },
  {
    "objectID": "09-emmeans.html#nusisance-variables",
    "href": "09-emmeans.html#nusisance-variables",
    "title": "9  Estimated Marginal Means",
    "section": "9.3 Nusisance Variables",
    "text": "9.3 Nusisance Variables\nSuppose you have a model with several predictors and want to estimate the marginal means for a factor. Let’s create a dataset to estimate the effect of sex on salary. By construction, salary is a function only of career level, but, 2/3 of entry-level employees are female, and only 2/5 of senior-level employees are female. Our EMM analysis should convey this. We’ll throw in age just to have a continuous variable in model.\n\nset.seed(123)\n\nsal_dat &lt;- tibble(\n  salary = c(rnorm(30, 30e3, 2e3), rnorm(15, 50e3, 3e3), rnorm(5, 100e3, 5e3)),\n  car_level = c(rep(\"Entry\", 30), rep(\"Mid\", 15), rep(\"Senior\", 5)),\n  sex = c(rep(\"F\",20), rep(\"M\",10), rep(\"F\",8), rep(\"M\",7), rep(\"F\",2), rep(\"M\",3)),\n  age = c(rnorm(30, 25, 2), rnorm(15, 40, 2), rnorm(5, 55, 2))\n)\n\nsal_fit &lt;-\n  linear_reg() |&gt;\n  fit(salary ~ car_level + sex + age, data = sal_dat) |&gt;\n  extract_fit_engine()\n\nsal_fit |&gt; tidy()\n\n# A tibble: 5 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)       33769.     4860.     6.95  1.22e- 8\n2 car_levelMid      23363.     3132.     7.46  2.13e- 9\n3 car_levelSenior   73451.     6044.    12.2   8.23e-16\n4 sexM               -709.      699.    -1.01  3.16e- 1\n5 age                -145.      192.    -0.756 4.53e- 1\n\n\nAs designed, sex and age are insignificant while car_level is significant and a large. Let’s calculate the EMMs for sex.\n\n\nShow the code\n(sal_emm_1 &lt;- emmeans(sal_fit, specs = \"sex\"))\n\n\n sex emmean   SE df lower.CL upper.CL\n F    61281 1630 45    58007    64555\n M    60572 1520 45    57509    63634\n\nResults are averaged over the levels of: car_level \nConfidence level used: 0.95 \n\n\nemmeans() averaged the predicted values from the default reference grid. The grid is the combination of factor levels and means of continuous variables.\n\n\nShow the code\nref_grid(sal_fit) |&gt; tidy()\n\n\n# A tibble: 6 × 8\n  car_level sex     age estimate std.error    df statistic  p.value\n  &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 Entry     F      32.8   29009.     1535.    45      18.9 5.02e-23\n2 Mid       F      32.8   52372.     1726.    45      30.3 1.33e-31\n3 Senior    F      32.8  102460.     4617.    45      22.2 7.15e-26\n4 Entry     M      32.8   28300.     1699.    45      16.7 7.31e-21\n5 Mid       M      32.8   51663.     1624.    45      31.8 1.73e-32\n6 Senior    M      32.8  101752.     4487.    45      22.7 2.94e-26\n\n\nThe EMM for sex=F ($61,281) is the average of the three sex=F salaries in the reference grid. From here, you can use contrast() to say females get paid a premium of $354 and men get paid $354 less (although neither result is significant).\n\n\nShow the code\nemmeans::contrast(sal_emm_1)\n\n\n contrast estimate  SE df t.ratio p.value\n F effect      354 349 45   1.015  0.3157\n M effect     -354 349 45  -1.015  0.3157\n\nResults are averaged over the levels of: car_level \nP value adjustment: fdr method for 2 tests \n\n\n$354 more (less) than what though? Well, it’s $354 less than whatever predicted value you would have at any career level (at the average age). A nice reference point is the “mediod” employee - the employee who is a proportional mixture of all career levels and all sexes. Not the proportional mixture of the combinations (women are entry-level, men senior-level), but just the proportional mixture of all variables taken independently. The way to do that is by reducing “nuisance” factor levels into a single set. Instead of car_levelEntry = [0,1], we let it, and each other level, have a continuous range, with the sum across levels adding to 1. The values could be equal, but far better for them to be prortional - .6 for entry-level, .3 for mid-level, and .1 for senior.\n\n\nShow the code\n(nuis_grid &lt;- ref_grid(sal_fit, nuisance = \"car_level\", wt.nuis = \"proportional\"))\n\n\n'emmGrid' object with variables:\n    sex = F, M\n    age = 32.793\nNuisance factors that have been collapsed by averaging:\n    car_level(3)\n\n\nNow let’s see the EMMs and contrast.\n\n\nShow the code\n(sal_emm_2 &lt;- emmeans(nuis_grid, specs = \"sex\"))\n\n\n sex emmean  SE df lower.CL upper.CL\n F    43363 433 45    42490    44236\n M    42654 534 45    41578    43730\n\nResults are averaged over the levels of: 1 nuisance factors \nConfidence level used: 0.95 \n\n\nThose are more realistic marginal means. The contrast is still the same.\n\n\nShow the code\ncontrast(sal_emm_2)\n\n\n contrast estimate  SE df t.ratio p.value\n F effect      354 349 45   1.015  0.3157\n M effect     -354 349 45  -1.015  0.3157\n\nResults are averaged over the levels of: 1 nuisance factors \nP value adjustment: fdr method for 2 tests \n\n\nBut now the reference point is the “mediod” employee.\n\n\nShow the code\nemmeans(nuis_grid, specs = ~1)\n\n\n 1       emmean  SE df lower.CL upper.CL\n overall  43009 338 45    42327    43691\n\nResults are averaged over the levels of: 1 nuisance factors, sex \nConfidence level used: 0.95 \n\n\nFinally, how about this slick way to show EMMs? We’ll show the reference point, then all factor levels. (Code below pretends .5 is a significant level, just so I can show the color contrast.)\n\n\nShow the code\ncontrast_color &lt;- function(bg) {\n  if_else(colorspace::contrast_ratio(bg, \"black\") &gt; \n            colorspace::contrast_ratio(bg, \"white\"),\n          \"black\", \"white\")\n}\n\nsmry &lt;- \n  bind_rows(\n    emmeans(sal_emm_2, specs = ~1) |&gt; tidy() |&gt; rename(contrast = `1`),\n    contrast(sal_emm_2) |&gt; tidy()\n  ) |&gt;\n  select(contrast, estimate, adj.p.value) |&gt;\n  mutate(contrast = fct_relevel(contrast, \"overall\", after = 0)) |&gt;\n  arrange(contrast) |&gt;\n  mutate(\n    pct = estimate / first(estimate),\n    bg = case_when(\n      adj.p.value &lt; .50 & estimate &lt; 0 ~ \"firebrick\",\n      adj.p.value &lt; .50 & estimate &gt; 0 ~ \"dodgerblue\",\n      TRUE ~ \"white\"\n    ),\n    color = contrast_color(bg)\n  )\n\nsmry |&gt;\n  select(contrast, estimate) |&gt;\n  mutate(contrast = str_remove(contrast, \" effect\")) |&gt;\n  pivot_wider(names_from = contrast, values_from = estimate) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_currency(decimals = 0) |&gt;\n  gt::tab_spanner(\"Effect\", columns = 2:3) |&gt;\n  gt::tab_header(title = \"Effect of gender on estimated marginal means.\") |&gt;\n  gt::tab_options(heading.align = \"left\") |&gt;\n  gt::tab_style(\n    style = gt::cell_fill(\"firebrick\"),\n    locations = gt::cells_body(columns = which(smry$bg == \"firebrick\"))\n  ) |&gt;\n  gt::tab_style(\n    style = gt::cell_fill(\"dodgerblue\"),\n    locations = gt::cells_body(columns = which(smry$bg == \"dodgerblue\"))\n  ) |&gt;\n  gt::tab_style(\n    style = gt::cell_text(\"white\"),\n    locations = gt::cells_body(columns = which(smry$color == \"white\"))\n  ) |&gt;\n  gt::tab_style(\n    style = gt::cell_text(\"black\"),\n    locations = gt::cells_body(columns = which(smry$color == \"black\"))\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect of gender on estimated marginal means.\n\n\noverall\n\nEffect\n\n\n\nF\nM\n\n\n\n\n$43,009\n$354\n−$354",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimated Marginal Means</span>"
    ]
  },
  {
    "objectID": "09-emmeans.html#learn-more",
    "href": "09-emmeans.html#learn-more",
    "title": "9  Estimated Marginal Means",
    "section": "9.4 Learn More",
    "text": "9.4 Learn More\nThis Very statisticious blog post is helpful. I also worked through the emmeans vignettes on CRAN and ggeffects on GitHub.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimated Marginal Means</span>"
    ]
  },
  {
    "objectID": "10-references.html",
    "href": "10-references.html",
    "title": "References",
    "section": "",
    "text": "Agresti, Alan. 2013. Categorical Data Analysis. 3rd ed. Wiley.\nhttps://www.amazon.com/Categorical-Analysis-Wiley-Probability-Statistics-ebook-dp-B00CAYUFM2/dp/B00CAYUFM2/ref=mt_kindle?_encoding=UTF8&me=&qid=.\n\n\nBrown, Violet A. 2021. “An Introduction to Linear Mixed-Effects\nModeling in r.” Advances in Methods and Practices in\nPsychological Science 4 (1): 2515245920960351. https://doi.org/10.1177/2515245920960351.\n\n\nFawcett, Tom. 2005. An Introduction to ROC Analysis. ELSEVIER.\nhttps://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. The\nElements of Statistical Learning. 2nd ed. New York, NY: Springer.\nhttps://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2013. An Introduction to Statistical Learning: With Applications in\nr. 1st ed. New York, NY: Springer. http://faculty.marshall.usc.edu/gareth-james/ISL/book.html.\n\n\nKuhn, Max, and Kjell Johnson. 2016. Applied Predictive\nModeling. 1st ed. New York, NY: Springer. http://appliedpredictivemodeling.com/.\n\n\nMolnar, Christoph. 2020. Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/.\n\n\nTherneau, Terry, and Elizabeth Atkinson. 2019. An Introduction to\nRecursive Partitioning Using the RPART Routines. Boca Raton,\nFlorida: Chapman; Hall/CRC. https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf.",
    "crumbs": [
      "References"
    ]
  }
]