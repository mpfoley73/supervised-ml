<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Bayesian Regression | Supervised Machine Learning</title>
  <meta name="description" content="These are my personal notes related to supervised machine learning techniques." />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Bayesian Regression | Supervised Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These are my personal notes related to supervised machine learning techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Bayesian Regression | Supervised Machine Learning" />
  
  <meta name="twitter:description" content="These are my personal notes related to supervised machine learning techniques." />
  

<meta name="author" content="Michael Foley" />


<meta name="date" content="2023-07-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="EMMs.html"/>
<link rel="next" href="references-1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet" />
<script src="libs/tabwid-1.1.3/tabwid.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Supervised Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>1</b> Ordinary Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#parameter-estimation"><i class="fa fa-check"></i><b>1.1</b> Parameter Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#example"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-assumptions"><i class="fa fa-check"></i><b>1.2</b> Model Assumptions</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#multicollinearity"><i class="fa fa-check"></i><b>1.2.1</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#prediction"><i class="fa fa-check"></i><b>1.3</b> Prediction</a></li>
<li class="chapter" data-level="1.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#inference"><i class="fa fa-check"></i><b>1.4</b> Inference</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#t-test"><i class="fa fa-check"></i><b>1.4.1</b> <em>t</em>-Test</a></li>
<li class="chapter" data-level="1.4.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#f-test"><i class="fa fa-check"></i><b>1.4.2</b> <em>F</em>-Test</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#interpretation"><i class="fa fa-check"></i><b>1.5</b> Interpretation</a></li>
<li class="chapter" data-level="1.6" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-validation"><i class="fa fa-check"></i><b>1.6</b> Model Validation</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#accuracy-metrics"><i class="fa fa-check"></i><b>1.6.1</b> Accuracy Metrics</a></li>
<li class="chapter" data-level="1.6.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#r-squared"><i class="fa fa-check"></i><b>1.6.2</b> R-Squared</a></li>
<li class="chapter" data-level="1.6.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#cross-validation"><i class="fa fa-check"></i><b>1.6.3</b> Cross-Validation</a></li>
<li class="chapter" data-level="1.6.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#gain-curve"><i class="fa fa-check"></i><b>1.6.4</b> Gain Curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html"><i class="fa fa-check"></i><b>2</b> Generalized Linear Models (GLM)</a>
<ul>
<li class="chapter" data-level="2.1" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#binomiallogistic"><i class="fa fa-check"></i><b>2.1</b> Binomial Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs1"><i class="fa fa-check"></i>Case Study 1</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#the-model"><i class="fa fa-check"></i>The Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpretation-1"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#model-fit"><i class="fa fa-check"></i>Model Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#multinomiallogistic"><i class="fa fa-check"></i><b>2.2</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs2"><i class="fa fa-check"></i>Case Study 2</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#the-model-1"><i class="fa fa-check"></i>The Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpretation-2"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions-1"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#model-fit-1"><i class="fa fa-check"></i>Model Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting-1"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#ordinallogistic"><i class="fa fa-check"></i><b>2.3</b> Ordinal Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs3"><i class="fa fa-check"></i>Case Study 3</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#fit-the-model"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#verify-assumptions"><i class="fa fa-check"></i>Verify Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assess-the-model-fit"><i class="fa fa-check"></i>Assess the Model Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpret-results"><i class="fa fa-check"></i>Interpret Results</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting-2"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#poissonregression"><i class="fa fa-check"></i><b>2.4</b> Poisson Regression</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs4"><i class="fa fa-check"></i>Case Study 4</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>3</b> Regularization</a>
<ul>
<li class="chapter" data-level="3.1" data-path="regularization.html"><a href="regularization.html#ridge"><i class="fa fa-check"></i><b>3.1</b> Ridge</a></li>
<li class="chapter" data-level="3.2" data-path="regularization.html"><a href="regularization.html#lasso"><i class="fa fa-check"></i><b>3.2</b> Lasso</a></li>
<li class="chapter" data-level="3.3" data-path="regularization.html"><a href="regularization.html#elastic-net"><i class="fa fa-check"></i><b>3.3</b> Elastic Net</a></li>
<li class="chapter" data-level="" data-path="regularization.html"><a href="regularization.html#model-summary"><i class="fa fa-check"></i>Model Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>4</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="4.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-tree"><i class="fa fa-check"></i><b>4.1</b> Classification Tree</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="decision-trees.html"><a href="decision-trees.html#measuring-performance"><i class="fa fa-check"></i><b>4.1.1</b> Measuring Performance</a></li>
<li class="chapter" data-level="4.1.2" data-path="decision-trees.html"><a href="decision-trees.html#training-with-caret"><i class="fa fa-check"></i><b>4.1.2</b> Training with Caret</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-tree"><i class="fa fa-check"></i><b>4.2</b> Regression Tree</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="decision-trees.html"><a href="decision-trees.html#training-with-caret-1"><i class="fa fa-check"></i><b>4.2.1</b> Training with Caret</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>4.3</b> Bagged Trees</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="decision-trees.html"><a href="decision-trees.html#bagged-classification-tree"><i class="fa fa-check"></i><b>4.3.1</b> Bagged Classification Tree</a></li>
<li class="chapter" data-level="4.3.2" data-path="decision-trees.html"><a href="decision-trees.html#bagging-regression-tree"><i class="fa fa-check"></i><b>4.3.2</b> Bagging Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="decision-trees.html"><a href="decision-trees.html#random-forests"><i class="fa fa-check"></i><b>4.4</b> Random Forests</a></li>
<li class="chapter" data-level="4.5" data-path="decision-trees.html"><a href="decision-trees.html#gradient-boosting"><i class="fa fa-check"></i><b>4.5</b> Gradient Boosting</a></li>
<li class="chapter" data-level="4.6" data-path="decision-trees.html"><a href="decision-trees.html#summary"><i class="fa fa-check"></i><b>4.6</b> Summary</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-trees"><i class="fa fa-check"></i><b>4.6.1</b> Classification Trees</a></li>
<li class="chapter" data-level="4.6.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>4.6.2</b> Regression Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="non-linear-models.html"><a href="non-linear-models.html"><i class="fa fa-check"></i><b>5</b> Non-linear Models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="non-linear-models.html"><a href="non-linear-models.html#splines"><i class="fa fa-check"></i><b>5.1</b> Splines</a></li>
<li class="chapter" data-level="5.2" data-path="non-linear-models.html"><a href="non-linear-models.html#mars"><i class="fa fa-check"></i><b>5.2</b> MARS</a></li>
<li class="chapter" data-level="5.3" data-path="non-linear-models.html"><a href="non-linear-models.html#gam"><i class="fa fa-check"></i><b>5.3</b> GAM</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>6</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="6.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximal-margin-classifier"><i class="fa fa-check"></i><b>6.1</b> Maximal Margin Classifier</a></li>
<li class="chapter" data-level="6.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-classifier"><i class="fa fa-check"></i><b>6.2</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="6.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-machines-1"><i class="fa fa-check"></i><b>6.3</b> Support Vector Machines</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="EMMs.html"><a href="EMMs.html"><i class="fa fa-check"></i><b>7</b> Topics: EMMs</a>
<ul>
<li class="chapter" data-level="7.1" data-path="EMMs.html"><a href="EMMs.html#references"><i class="fa fa-check"></i><b>7.1</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="BayesRegression.html"><a href="BayesRegression.html"><i class="fa fa-check"></i><b>8</b> Bayesian Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="BayesRegression.html"><a href="BayesRegression.html#compared-to-frequentist-regression"><i class="fa fa-check"></i><b>8.1</b> Compared to Frequentist Regression</a></li>
<li class="chapter" data-level="8.2" data-path="BayesRegression.html"><a href="BayesRegression.html#model-evaluation"><i class="fa fa-check"></i><b>8.2</b> Model Evaluation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="BayesRegression.html"><a href="BayesRegression.html#model-comparison"><i class="fa fa-check"></i><b>8.2.1</b> Model Comparison</a></li>
<li class="chapter" data-level="8.2.2" data-path="BayesRegression.html"><a href="BayesRegression.html#visualization"><i class="fa fa-check"></i><b>8.2.2</b> Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supervised Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="BayesRegression" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Bayesian Regression<a href="BayesRegression.html#BayesRegression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="compared-to-frequentist-regression" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Compared to Frequentist Regression<a href="BayesRegression.html#compared-to-frequentist-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section is my notes from DataCamp course <a href="https://campus.datacamp.com/courses/bayesian-regression-modeling-with-rstanarm/">Bayesian Regression Modeling with rstanarm</a>. The course refers to Andrew Gelman’s book “Bayesian Data Analysis” <span class="citation">(<a href="#ref-Gelman2013"><strong>Gelman2013?</strong></a>)</span>. It is an intuitive approach to Bayesian inference.</p>
<p>Frequentist regression estimates <em>fixed</em> population parameters from a <em>random</em> sample of data whose test statistics are random variables. The Bayesian approach to regression works the other direction: Bayesian regression characterizes the distribution of the <em>random</em> variable population parameters from the evidence of a <em>fixed</em> sample of data.</p>
<p>The frequentist p-value is the probability of observing a test statistic of equal or greater magnitude if the null hypothesis is true. The 95% confidence interval has a 95% probability of capturing the population value (repeated sampling would produce similar CIs, 95% of which would capture the population value). The 95% <em>credible interval</em> captures the confidence the value falls within the range. There is an important difference here. In the Bayesian framework, you can state the probability the parameter value falls with a specified range, but there is no equivalent in frequentist regression.</p>
<p>Bayesian regression uses maximum likelihood to fit the model because the integral in the denominator (the marginal distribution) cannot (or is difficult) to calculate analytically. Instead, the algorithm samples from the posterior distribution in groups (chains). Each chain begins at a random location. Each sample (iteration) moves toward the area where the combination of likelihood and prior indicates a high probability of the true parameter value residing. The more iterations per chain, the larger the total sample size, and more robust the outcome. The chains need to converge to have a stable estimate. The iterations prior to convergence are sometimes referred to as the “warm up” and are not included in the posterior distribution estimate. By default, the <strong>rstanarm</strong> package estimates 4 chains, each with 2,000 iterations, and the first 1,000 set aside as warm-up. The final posterior combines the chains, so is composed of 4,000 iterations.</p>
<p>A Bayesian model estimates each coefficient parameter and model error with a prior*likelihood/marginal dist = posterior framework.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="BayesRegression.html#cb426-1" tabindex="-1"></a><span class="fu">library</span>(rstanarm)</span>
<span id="cb426-2"><a href="BayesRegression.html#cb426-2" tabindex="-1"></a></span>
<span id="cb426-3"><a href="BayesRegression.html#cb426-3" tabindex="-1"></a><span class="fu">sink</span>(<span class="at">type =</span> <span class="st">&quot;message&quot;</span>)</span>
<span id="cb426-4"><a href="BayesRegression.html#cb426-4" tabindex="-1"></a>stan_mdl <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(kid_score <span class="sc">~</span> mom_iq, <span class="at">data =</span> kidiq)</span></code></pre></div>
<pre><code>## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000109 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.09 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.036 seconds (Warm-up)
## Chain 1:                0.058 seconds (Sampling)
## Chain 1:                0.094 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.4e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.037 seconds (Warm-up)
## Chain 2:                0.061 seconds (Sampling)
## Chain 2:                0.098 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 1.2e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.035 seconds (Warm-up)
## Chain 3:                0.07 seconds (Sampling)
## Chain 3:                0.105 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 6e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.051 seconds (Warm-up)
## Chain 4:                0.058 seconds (Sampling)
## Chain 4:                0.109 seconds (Total)
## Chain 4:</code></pre>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="BayesRegression.html#cb428-1" tabindex="-1"></a><span class="fu">sink</span>(<span class="at">type =</span> <span class="st">&quot;message&quot;</span>)</span>
<span id="cb428-2"><a href="BayesRegression.html#cb428-2" tabindex="-1"></a></span>
<span id="cb428-3"><a href="BayesRegression.html#cb428-3" tabindex="-1"></a><span class="fu">summary</span>(stan_mdl)</span></code></pre></div>
<pre><code>## 
## Model Info:
##  function:     stan_glm
##  family:       gaussian [identity]
##  formula:      kid_score ~ mom_iq
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help(&#39;prior_summary&#39;)
##  observations: 434
##  predictors:   2
## 
## Estimates:
##               mean   sd   10%   50%   90%
## (Intercept) 25.8    5.9 18.4  25.8  33.4 
## mom_iq       0.6    0.1  0.5   0.6   0.7 
## sigma       18.3    0.6 17.5  18.3  19.1 
## 
## Fit Diagnostics:
##            mean   sd   10%   50%   90%
## mean_PPD 86.8    1.2 85.2  86.8  88.4 
## 
## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)).
## 
## MCMC diagnostics
##               mcse Rhat n_eff
## (Intercept)   0.1  1.0  4119 
## mom_iq        0.0  1.0  4066 
## sigma         0.0  1.0  3892 
## mean_PPD      0.0  1.0  4059 
## log-posterior 0.0  1.0  1864 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).</code></pre>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="BayesRegression.html#cb430-1" tabindex="-1"></a>broom.mixed<span class="sc">::</span><span class="fu">tidy</span>(stan_mdl)</span></code></pre></div>
<pre><code>## # A tibble: 2 × 3
##   term        estimate std.error
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   25.8      5.94  
## 2 mom_iq         0.610    0.0593</code></pre>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="BayesRegression.html#cb432-1" tabindex="-1"></a><span class="fu">posterior_interval</span>(stan_mdl)</span></code></pre></div>
<pre><code>##                     5%       95%
## (Intercept) 16.1471464 35.391103
## mom_iq       0.5149525  0.706094
## sigma       17.3167852 19.328867</code></pre>
<p>By default, <strong>rstanarm</strong> priors are normal distributions with mean 0 and variance equal to a scaler multiple of the data variance.</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="BayesRegression.html#cb434-1" tabindex="-1"></a><span class="fu">prior_summary</span>(stan_mdl)</span></code></pre></div>
<pre><code>## Priors for model &#39;stan_mdl&#39; 
## ------
## Intercept (after predictors centered)
##   Specified prior:
##     ~ normal(location = 87, scale = 2.5)
##   Adjusted prior:
##     ~ normal(location = 87, scale = 51)
## 
## Coefficients
##   Specified prior:
##     ~ normal(location = 0, scale = 2.5)
##   Adjusted prior:
##     ~ normal(location = 0, scale = 3.4)
## 
## Auxiliary (sigma)
##   Specified prior:
##     ~ exponential(rate = 1)
##   Adjusted prior:
##     ~ exponential(rate = 0.049)
## ------
## See help(&#39;prior_summary.stanreg&#39;) for more details</code></pre>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="BayesRegression.html#cb436-1" tabindex="-1"></a><span class="co"># Calculate the adjusted scale for the intercept</span></span>
<span id="cb436-2"><a href="BayesRegression.html#cb436-2" tabindex="-1"></a><span class="fl">2.5</span> <span class="sc">*</span> <span class="fu">sd</span>(kidiq<span class="sc">$</span>kid_score)</span></code></pre></div>
<pre><code>## [1] 51.02672</code></pre>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="BayesRegression.html#cb438-1" tabindex="-1"></a><span class="co"># Calculate the adjusted scale for `mom_iq`</span></span>
<span id="cb438-2"><a href="BayesRegression.html#cb438-2" tabindex="-1"></a>(<span class="fl">2.5</span> <span class="sc">/</span> <span class="fu">sd</span>(kidiq<span class="sc">$</span>mom_iq)) <span class="sc">*</span> <span class="fu">sd</span>(kidiq<span class="sc">$</span>kid_score)</span></code></pre></div>
<pre><code>## [1] 3.401781</code></pre>
<p>You can override the priors to set your own:</p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="BayesRegression.html#cb440-1" tabindex="-1"></a>stan_mdl <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(kid_score <span class="sc">~</span> mom_iq, <span class="at">data =</span> kidiq,</span>
<span id="cb440-2"><a href="BayesRegression.html#cb440-2" tabindex="-1"></a>                     <span class="at">prior_intercept =</span> <span class="fu">normal</span>(<span class="at">location =</span> <span class="dv">0</span>, <span class="at">scale =</span> <span class="dv">10</span>, <span class="at">autoscale =</span> <span class="cn">FALSE</span>),</span>
<span id="cb440-3"><a href="BayesRegression.html#cb440-3" tabindex="-1"></a>                     <span class="at">prior =</span> <span class="fu">normal</span>(<span class="at">location =</span> <span class="dv">0</span>, <span class="at">scale =</span> <span class="fl">2.5</span>, <span class="at">autoscale =</span> <span class="cn">FALSE</span>),</span>
<span id="cb440-4"><a href="BayesRegression.html#cb440-4" tabindex="-1"></a>                     <span class="at">prior_aux =</span> <span class="fu">exponential</span>(<span class="at">rate =</span> <span class="dv">1</span>, <span class="at">autoscale =</span> <span class="cn">FALSE</span>))</span></code></pre></div>
<pre><code>## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.4e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.037 seconds (Warm-up)
## Chain 1:                0.051 seconds (Sampling)
## Chain 1:                0.088 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 6e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.034 seconds (Warm-up)
## Chain 2:                0.048 seconds (Sampling)
## Chain 2:                0.082 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 7e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.032 seconds (Warm-up)
## Chain 3:                0.047 seconds (Sampling)
## Chain 3:                0.079 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 6e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.031 seconds (Warm-up)
## Chain 4:                0.047 seconds (Sampling)
## Chain 4:                0.078 seconds (Total)
## Chain 4:</code></pre>
<p>Bayesian estimation samples from the posterior distribution, so there is no point estimate and test statistic.</p>
</div>
<div id="model-evaluation" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Model Evaluation<a href="BayesRegression.html#model-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The R-squared statistic is not available from the summary object, but you can still calculate it manually from the model data, or use the <code>bayes_R2</code> function.</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a href="BayesRegression.html#cb442-1" tabindex="-1"></a>sse <span class="ot">&lt;-</span> <span class="fu">var</span>(stan_mdl<span class="sc">$</span>residuals)</span>
<span id="cb442-2"><a href="BayesRegression.html#cb442-2" tabindex="-1"></a>ssr <span class="ot">&lt;-</span> <span class="fu">var</span>(stan_mdl<span class="sc">$</span>fitted.values)</span>
<span id="cb442-3"><a href="BayesRegression.html#cb442-3" tabindex="-1"></a>sst <span class="ot">&lt;-</span> ssr <span class="sc">+</span> sse</span>
<span id="cb442-4"><a href="BayesRegression.html#cb442-4" tabindex="-1"></a>r2 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> sse<span class="sc">/</span>sst</span>
<span id="cb442-5"><a href="BayesRegression.html#cb442-5" tabindex="-1"></a><span class="co"># bayes_R2 returns a vector of length equal to the posterior sample size.</span></span>
<span id="cb442-6"><a href="BayesRegression.html#cb442-6" tabindex="-1"></a><span class="fu">bayes_R2</span>(stan_mdl) <span class="sc">%&gt;%</span> <span class="fu">summary</span>()</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.07899 0.18431 0.20692 0.20718 0.22924 0.31917</code></pre>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb444-1"><a href="BayesRegression.html#cb444-1" tabindex="-1"></a><span class="fu">bayes_R2</span>(stan_mdl) <span class="sc">%&gt;%</span> <span class="fu">quantile</span>(<span class="fu">c</span>(.<span class="dv">025</span>, .<span class="dv">975</span>))</span></code></pre></div>
<pre><code>##      2.5%     97.5% 
## 0.1451410 0.2705964</code></pre>
<p>Bayesian regression has other model fit statistics.</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="BayesRegression.html#cb446-1" tabindex="-1"></a><span class="co"># Calculate posterior predictive scores</span></span>
<span id="cb446-2"><a href="BayesRegression.html#cb446-2" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">posterior_linpred</span>(stan_mdl)</span>
<span id="cb446-3"><a href="BayesRegression.html#cb446-3" tabindex="-1"></a><span class="co"># Print a summary of the 1st replication</span></span>
<span id="cb446-4"><a href="BayesRegression.html#cb446-4" tabindex="-1"></a><span class="fu">summary</span>(predictions[<span class="dv">1</span>,])</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   67.10   79.44   85.93   87.39   94.58  114.63</code></pre>
<p>Or produce a posterior predictive model check.</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="BayesRegression.html#cb448-1" tabindex="-1"></a><span class="co"># peaks of observed data and model are similar places, but there is a second </span></span>
<span id="cb448-2"><a href="BayesRegression.html#cb448-2" tabindex="-1"></a><span class="co"># mode of popularity scores around 10 that is not captured by the model.</span></span>
<span id="cb448-3"><a href="BayesRegression.html#cb448-3" tabindex="-1"></a><span class="fu">pp_check</span>(stan_mdl, <span class="st">&quot;dens_overlay&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-285-1.png" width="672" /></p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="BayesRegression.html#cb449-1" tabindex="-1"></a><span class="fu">pp_check</span>(stan_mdl, <span class="st">&quot;stat&quot;</span>)</span></code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-285-2.png" width="672" /></p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="BayesRegression.html#cb451-1" tabindex="-1"></a><span class="co"># mean and sd of the observed data in middle of expected distribution - these </span></span>
<span id="cb451-2"><a href="BayesRegression.html#cb451-2" tabindex="-1"></a><span class="co"># two characteristics are recovered well.</span></span>
<span id="cb451-3"><a href="BayesRegression.html#cb451-3" tabindex="-1"></a><span class="fu">pp_check</span>(stan_mdl, <span class="st">&quot;stat_2d&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-285-3.png" width="672" /></p>
<div id="model-comparison" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Model Comparison<a href="BayesRegression.html#model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use the <strong>loo</strong> (leave on out) package to compare models. <strong>loo</strong> approximates cross-validation. In the initial summary, 4000 is the number of iterations in the posterior, and 434 is the number of observations in the data set.</p>
<p><code>ekpd_loo</code> is the LOO estimate; <code>p_loo</code> is the effective number of parameters in the model; and <code>looic</code> is the LOO estimate converted to the deviance scale, -2 * LOO.</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="BayesRegression.html#cb452-1" tabindex="-1"></a>stan_loo <span class="ot">&lt;-</span> <span class="fu">loo</span>(stan_mdl)</span>
<span id="cb452-2"><a href="BayesRegression.html#cb452-2" tabindex="-1"></a>stan_loo</span></code></pre></div>
<pre><code>## 
## Computed from 4000 by 434 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1879.0 14.9
## p_loo         3.0  0.3
## looic      3758.0 29.8
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<p>The statistics are not useful in isolation - they should be used in comparison to competing models.</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="BayesRegression.html#cb454-1" tabindex="-1"></a>stan_mdl_2 <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(kid_score <span class="sc">~</span> mom_iq <span class="sc">*</span> mom_hs, <span class="at">data =</span> kidiq)</span></code></pre></div>
<p>Which model has more predictive power? As a rule of thumb, if the absolute value of the difference is greater than the standard error, then the result is significant. In this case, the new model is significantly better.</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a href="BayesRegression.html#cb455-1" tabindex="-1"></a>stan_loo_2 <span class="ot">&lt;-</span> <span class="fu">loo</span>(stan_mdl_2) </span>
<span id="cb455-2"><a href="BayesRegression.html#cb455-2" tabindex="-1"></a></span>
<span id="cb455-3"><a href="BayesRegression.html#cb455-3" tabindex="-1"></a><span class="fu">loo_compare</span>(stan_loo, stan_loo_2)</span></code></pre></div>
<pre><code>##            elpd_diff se_diff
## stan_mdl_2  0.0       0.0   
## stan_mdl   -6.4       4.3</code></pre>
</div>
<div id="visualization" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Visualization<a href="BayesRegression.html#visualization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use <code>tidy()</code> to pull out the intercept and slope. Use the posterior distributions to create a predicted regression line from each draw in the posterior samples. These lines will show the uncertainty around the overall line.</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="BayesRegression.html#cb457-1" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb457-2"><a href="BayesRegression.html#cb457-2" tabindex="-1"></a></span>
<span id="cb457-3"><a href="BayesRegression.html#cb457-3" tabindex="-1"></a>stan_tdy_2 <span class="ot">&lt;-</span> <span class="fu">tidy</span>(stan_mdl_2)</span>
<span id="cb457-4"><a href="BayesRegression.html#cb457-4" tabindex="-1"></a></span>
<span id="cb457-5"><a href="BayesRegression.html#cb457-5" tabindex="-1"></a>draws <span class="ot">&lt;-</span> tidybayes<span class="sc">::</span><span class="fu">spread_draws</span>(stan_mdl_2, <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>, mom_iq)</span>
<span id="cb457-6"><a href="BayesRegression.html#cb457-6" tabindex="-1"></a></span>
<span id="cb457-7"><a href="BayesRegression.html#cb457-7" tabindex="-1"></a>kidiq <span class="sc">%&gt;%</span></span>
<span id="cb457-8"><a href="BayesRegression.html#cb457-8" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> mom_iq, <span class="at">y =</span> kid_score)) <span class="sc">+</span></span>
<span id="cb457-9"><a href="BayesRegression.html#cb457-9" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb457-10"><a href="BayesRegression.html#cb457-10" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">data =</span> draws, <span class="fu">aes</span>(<span class="at">intercept =</span> <span class="st">`</span><span class="at">(Intercept)</span><span class="st">`</span>, <span class="at">slope =</span> mom_iq), </span>
<span id="cb457-11"><a href="BayesRegression.html#cb457-11" tabindex="-1"></a>              <span class="at">size =</span> <span class="fl">0.1</span>, <span class="at">alpha =</span> <span class="fl">0.2</span>, <span class="at">color =</span> <span class="st">&quot;skyblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb457-12"><a href="BayesRegression.html#cb457-12" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> stan_tdy_2<span class="sc">$</span>estimate[<span class="dv">1</span>], <span class="at">slope =</span> stan_tdy_2<span class="sc">$</span>estimate[<span class="dv">2</span>]) </span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-289-1.png" width="672" /></p>
<p>Use the model to get posterior predictions with <code>posterior_predict()</code>.</p>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="BayesRegression.html#cb458-1" tabindex="-1"></a>posteriors <span class="ot">&lt;-</span> <span class="fu">posterior_predict</span>(stan_mdl_2)</span>
<span id="cb458-2"><a href="BayesRegression.html#cb458-2" tabindex="-1"></a></span>
<span id="cb458-3"><a href="BayesRegression.html#cb458-3" tabindex="-1"></a>new_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">mom_iq =</span> <span class="dv">100</span>, <span class="at">mom_hs =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb458-4"><a href="BayesRegression.html#cb458-4" tabindex="-1"></a>posteriors_2 <span class="ot">&lt;-</span> <span class="fu">posterior_predict</span>(stan_mdl_2, <span class="at">newdata =</span> new_data) <span class="sc">%&gt;%</span></span>
<span id="cb458-5"><a href="BayesRegression.html#cb458-5" tabindex="-1"></a>  <span class="fu">as.data.frame</span>()</span>
<span id="cb458-6"><a href="BayesRegression.html#cb458-6" tabindex="-1"></a></span>
<span id="cb458-7"><a href="BayesRegression.html#cb458-7" tabindex="-1"></a><span class="fu">colnames</span>(posteriors_2) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;No HS&quot;</span>, <span class="st">&quot;Completed HS&quot;</span>)</span>
<span id="cb458-8"><a href="BayesRegression.html#cb458-8" tabindex="-1"></a>posteriors_2 <span class="sc">%&gt;%</span></span>
<span id="cb458-9"><a href="BayesRegression.html#cb458-9" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">everything</span>(), <span class="at">names_to =</span> <span class="st">&quot;HS&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;predict&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb458-10"><a href="BayesRegression.html#cb458-10" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> predict)) <span class="sc">+</span></span>
<span id="cb458-11"><a href="BayesRegression.html#cb458-11" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb458-12"><a href="BayesRegression.html#cb458-12" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>HS, <span class="at">ncol =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-290-1.png" width="672" /></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="EMMs.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["supervised-ml.pdf", "supervised-ml.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
