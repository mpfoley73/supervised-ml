<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Decision Trees | Supervised Machine Learning</title>
  <meta name="description" content="These are my personal notes related to supervised machine learning techniques." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Decision Trees | Supervised Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These are my personal notes related to supervised machine learning techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Decision Trees | Supervised Machine Learning" />
  
  <meta name="twitter:description" content="These are my personal notes related to supervised machine learning techniques." />
  

<meta name="author" content="Michael Foley" />


<meta name="date" content="2024-01-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regularization.html"/>
<link rel="next" href="support-vector-machines.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet" />
<script src="libs/tabwid-1.1.3/tabwid.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Supervised Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>1</b> Ordinary Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#parameter-estimation"><i class="fa fa-check"></i><b>1.1</b> Parameter Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#example"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-assumptions"><i class="fa fa-check"></i><b>1.2</b> Model Assumptions</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#multicollinearity"><i class="fa fa-check"></i><b>1.2.1</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#prediction"><i class="fa fa-check"></i><b>1.3</b> Prediction</a></li>
<li class="chapter" data-level="1.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#inference"><i class="fa fa-check"></i><b>1.4</b> Inference</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#t-test"><i class="fa fa-check"></i><b>1.4.1</b> <em>t</em>-Test</a></li>
<li class="chapter" data-level="1.4.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#f-test"><i class="fa fa-check"></i><b>1.4.2</b> <em>F</em>-Test</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#interpretation"><i class="fa fa-check"></i><b>1.5</b> Interpretation</a></li>
<li class="chapter" data-level="1.6" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-validation"><i class="fa fa-check"></i><b>1.6</b> Model Validation</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#accuracy-metrics"><i class="fa fa-check"></i><b>1.6.1</b> Accuracy Metrics</a></li>
<li class="chapter" data-level="1.6.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#r-squared"><i class="fa fa-check"></i><b>1.6.2</b> R-Squared</a></li>
<li class="chapter" data-level="1.6.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#cross-validation"><i class="fa fa-check"></i><b>1.6.3</b> Cross-Validation</a></li>
<li class="chapter" data-level="1.6.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#gain-curve"><i class="fa fa-check"></i><b>1.6.4</b> Gain Curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html"><i class="fa fa-check"></i><b>2</b> Generalized Linear Models (GLM)</a>
<ul>
<li class="chapter" data-level="2.1" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#binomiallogistic"><i class="fa fa-check"></i><b>2.1</b> Binomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs1"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#fit-the-model"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpretation-1"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#evaluate-the-fit"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#multinomiallogistic"><i class="fa fa-check"></i><b>2.2</b> Multinomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs2"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#fit-the-model-1"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpretation-2"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions-1"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#evaluate-the-fit-1"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting-1"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#ordinallogistic"><i class="fa fa-check"></i><b>2.3</b> Ordinal Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs3"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#fit-the-model-2"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions-2"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#evaluate-the-fit-2"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpret-results"><i class="fa fa-check"></i>Interpret Results</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting-2"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#poissonregression"><i class="fa fa-check"></i><b>2.4</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs4"><i class="fa fa-check"></i>Case Study 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html"><i class="fa fa-check"></i><b>3</b> Mixed Effects Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#lme"><i class="fa fa-check"></i><b>3.1</b> Linear Mixed Effects</a>
<ul>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#lme1"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#fit-the-model-3"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#interpretation-3"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#model-assumptions-1"><i class="fa fa-check"></i>Model Assumptions</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#evaluate-the-fit-3"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#reporting-3"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="non-linear-models.html"><a href="non-linear-models.html"><i class="fa fa-check"></i><b>4</b> Non-linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="non-linear-models.html"><a href="non-linear-models.html#splines"><i class="fa fa-check"></i><b>4.1</b> Splines</a></li>
<li class="chapter" data-level="4.2" data-path="non-linear-models.html"><a href="non-linear-models.html#mars"><i class="fa fa-check"></i><b>4.2</b> MARS</a></li>
<li class="chapter" data-level="4.3" data-path="non-linear-models.html"><a href="non-linear-models.html#gam"><i class="fa fa-check"></i><b>4.3</b> GAM</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>5</b> Regularization</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regularization.html"><a href="regularization.html#ridge"><i class="fa fa-check"></i><b>5.1</b> Ridge</a></li>
<li class="chapter" data-level="5.2" data-path="regularization.html"><a href="regularization.html#lasso"><i class="fa fa-check"></i><b>5.2</b> Lasso</a></li>
<li class="chapter" data-level="5.3" data-path="regularization.html"><a href="regularization.html#elastic-net"><i class="fa fa-check"></i><b>5.3</b> Elastic Net</a></li>
<li class="chapter" data-level="" data-path="regularization.html"><a href="regularization.html#model-summary"><i class="fa fa-check"></i>Model Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>6</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="6.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-tree"><i class="fa fa-check"></i><b>6.1</b> Classification Tree</a>
<ul>
<li class="chapter" data-level="" data-path="decision-trees.html"><a href="decision-trees.html#measuring-performance"><i class="fa fa-check"></i>Measuring Performance</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-tree"><i class="fa fa-check"></i><b>6.2</b> Regression Tree</a>
<ul>
<li class="chapter" data-level="" data-path="decision-trees.html"><a href="decision-trees.html#measuring-performance-1"><i class="fa fa-check"></i>Measuring Performance</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>6.3</b> Bagged Trees</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="decision-trees.html"><a href="decision-trees.html#bagged-classification-tree"><i class="fa fa-check"></i><b>6.3.1</b> Bagged Classification Tree</a></li>
<li class="chapter" data-level="6.3.2" data-path="decision-trees.html"><a href="decision-trees.html#bagging-regression-tree"><i class="fa fa-check"></i><b>6.3.2</b> Bagging Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="decision-trees.html"><a href="decision-trees.html#random-forests"><i class="fa fa-check"></i><b>6.4</b> Random Forests</a></li>
<li class="chapter" data-level="6.5" data-path="decision-trees.html"><a href="decision-trees.html#gradient-boosting"><i class="fa fa-check"></i><b>6.5</b> Gradient Boosting</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>7</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="7.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximal-margin-classifier"><i class="fa fa-check"></i><b>7.1</b> Maximal Margin Classifier</a></li>
<li class="chapter" data-level="7.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-classifier"><i class="fa fa-check"></i><b>7.2</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="7.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-machines-1"><i class="fa fa-check"></i><b>7.3</b> Support Vector Machines</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="BayesRegression.html"><a href="BayesRegression.html"><i class="fa fa-check"></i><b>8</b> Bayesian Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="BayesRegression.html"><a href="BayesRegression.html#compared-to-frequentist-regression"><i class="fa fa-check"></i><b>8.1</b> Compared to Frequentist Regression</a></li>
<li class="chapter" data-level="8.2" data-path="BayesRegression.html"><a href="BayesRegression.html#model-evaluation"><i class="fa fa-check"></i><b>8.2</b> Model Evaluation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="BayesRegression.html"><a href="BayesRegression.html#model-comparison"><i class="fa fa-check"></i><b>8.2.1</b> Model Comparison</a></li>
<li class="chapter" data-level="8.2.2" data-path="BayesRegression.html"><a href="BayesRegression.html#visualization"><i class="fa fa-check"></i><b>8.2.2</b> Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="EMMs.html"><a href="EMMs.html"><i class="fa fa-check"></i><b>9</b> Estimated Marginal Means</a>
<ul>
<li class="chapter" data-level="9.1" data-path="EMMs.html"><a href="EMMs.html#references"><i class="fa fa-check"></i><b>9.1</b> References</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supervised Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="decision-trees" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Decision Trees<a href="decision-trees.html#decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Decision trees, also known as classification and regression tree (CART) models, are tree-based methods for supervised machine learning.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> Simple classification trees and regression trees are easy to use and interpret, but are not competitive with the best machine learning methods. However, they form the foundation for ensemble models such as bagged trees, random forests, and boosted trees, which although less interpretable, are very accurate.</p>
<p>CART models segment the predictor space into <span class="math inline">\(K\)</span> non-overlapping terminal nodes (leaves). Each node is described by a set of rules which can be used to predict new responses. The predicted value <span class="math inline">\(\hat{y}\)</span> for each node is the mode (classification) or mean (regression).</p>
<p>CART models define the nodes through a <em>top-down greedy</em> process called <em>recursive binary splitting</em>. The process is <em>top-down</em> because it begins at the top of the tree with all observations in a single region and successively splits the predictor space. It is <em>greedy</em> because at each splitting step, the best split is made at that particular step without consideration to subsequent splits.</p>
<p>The best split is the predictor variable and cut point that minimizes a cost function. The most common cost function for regression trees is the sum of squared residuals,</p>
<p><span class="math display">\[RSS = \sum_{k=1}^K\sum_{i \in A_k}{\left(y_i - \hat{y}_{A_k} \right)^2}.\]</span></p>
<p>For classification trees, it is the Gini index,</p>
<p><span class="math display">\[G = \sum_{c=1}^C{\hat{p}_{kc}(1 - \hat{p}_{kc})},\]</span></p>
<p>and the entropy (aka information statistic)</p>
<p><span class="math display">\[D = - \sum_{c=1}^C{\hat{p}_{kc} \log \hat{p}_{kc}}\]</span></p>
<p>where <span class="math inline">\(\hat{p}_{kc}\)</span> is the proportion of training observations in node <span class="math inline">\(k\)</span> that are class <span class="math inline">\(c\)</span>. A completely <em>pure</em> node in a binary tree would have <span class="math inline">\(\hat{p} \in \{ 0, 1 \}\)</span> and <span class="math inline">\(G = D = 0\)</span>. A completely <em>impure</em> node in a binary tree would have <span class="math inline">\(\hat{p} = 0.5\)</span> and <span class="math inline">\(G = 0.5^2 \cdot 2 = 0.25\)</span> and <span class="math inline">\(D = -(0.5 \log(0.5)) \cdot 2 = 0.69\)</span>.</p>
<p>CART repeats the splitting process for each child node until a <em>stopping criterion</em> is satisfied, usually when no node size surpasses a predefined maximum, or continued splitting does not improve the model significantly. CART may also impose a minimum number of observations in each node.</p>
<p>The resulting tree likely over-fits the training data and therefore does not generalize well to test data, so CART <em>prunes</em> the tree, minimizing the cross-validated prediction error. Rather than cross-validating every possible subtree to find the one with minimum error, CART uses <em>cost-complexity pruning</em>. Cost-complexity is the trade off between error (cost) and tree size (complexity) where the trade off is quantified with cost-complexity parameter <span class="math inline">\(c_p\)</span>. The cost complexity of the tree, <span class="math inline">\(R_{c_p}(T)\)</span>, is the sum of its risk (error) plus a “cost complexity” factor <span class="math inline">\(c_p\)</span> multiple of the tree size <span class="math inline">\(|T|\)</span>.</p>
<p><span class="math display">\[R_{c_p}(T) = R(T) + c_p|T|\]</span></p>
<p><span class="math inline">\(c_p\)</span> can take on any value from <span class="math inline">\([0..\infty]\)</span>, but it turns out there is an optimal tree for <em>ranges</em> of <span class="math inline">\(c_p\)</span> values, so there are only a finite set of <em>interesting</em> values for <span class="math inline">\(c_p\)</span> <span class="citation">(<a href="#ref-James2013">James et al. 2013</a>)</span> <span class="citation">(<a href="#ref-Therneau2019">Therneau and Atkinson 2019</a>)</span> <span class="citation">(<a href="#ref-Kuhn2016">Kuhn and Johnson 2016</a>)</span>. A parametric algorithm identifies the interesting <span class="math inline">\(c_p\)</span> values and their associated pruned trees, <span class="math inline">\(T_{c_p}\)</span>. CART uses cross-validation to determine which <span class="math inline">\(c_p\)</span> is optimal.</p>
<p>The sections in this chapter work through two case studies, using the <code>ISLR::OJ</code> data set to fit classification trees and predict which brand of orange juice, Citrus Hill (CH) or Minute Maid = (MM), customers opurchase from its 17 predictor variables, and using the <code>ISLR::Carseats</code> data set to predict average sales from its 10 feature variables.</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="decision-trees.html#cb239-1" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)  <span class="co"># better formatted plots than the ones in rpart</span></span>
<span id="cb239-2"><a href="decision-trees.html#cb239-2" tabindex="-1"></a></span>
<span id="cb239-3"><a href="decision-trees.html#cb239-3" tabindex="-1"></a>oj_dat <span class="ot">&lt;-</span> ISLR<span class="sc">::</span>OJ</span>
<span id="cb239-4"><a href="decision-trees.html#cb239-4" tabindex="-1"></a></span>
<span id="cb239-5"><a href="decision-trees.html#cb239-5" tabindex="-1"></a><span class="fu">glimpse</span>(oj_dat)</span></code></pre></div>
<pre><code>## Rows: 1,070
## Columns: 18
## $ Purchase       &lt;fct&gt; CH, CH, CH, MM, CH, CH, CH, CH, CH, CH, CH, CH, CH, CH,…
## $ WeekofPurchase &lt;dbl&gt; 237, 239, 245, 227, 228, 230, 232, 234, 235, 238, 240, …
## $ StoreID        &lt;dbl&gt; 1, 1, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 2, 2…
## $ PriceCH        &lt;dbl&gt; 1.75, 1.75, 1.86, 1.69, 1.69, 1.69, 1.69, 1.75, 1.75, 1…
## $ PriceMM        &lt;dbl&gt; 1.99, 1.99, 2.09, 1.69, 1.69, 1.99, 1.99, 1.99, 1.99, 1…
## $ DiscCH         &lt;dbl&gt; 0.00, 0.00, 0.17, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…
## $ DiscMM         &lt;dbl&gt; 0.00, 0.30, 0.00, 0.00, 0.00, 0.00, 0.40, 0.40, 0.40, 0…
## $ SpecialCH      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ SpecialMM      &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0…
## $ LoyalCH        &lt;dbl&gt; 0.500000, 0.600000, 0.680000, 0.400000, 0.956535, 0.965…
## $ SalePriceMM    &lt;dbl&gt; 1.99, 1.69, 2.09, 1.69, 1.69, 1.99, 1.59, 1.59, 1.59, 1…
## $ SalePriceCH    &lt;dbl&gt; 1.75, 1.75, 1.69, 1.69, 1.69, 1.69, 1.69, 1.75, 1.75, 1…
## $ PriceDiff      &lt;dbl&gt; 0.24, -0.06, 0.40, 0.00, 0.00, 0.30, -0.10, -0.16, -0.1…
## $ Store7         &lt;fct&gt; No, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes,…
## $ PctDiscMM      &lt;dbl&gt; 0.000000, 0.150754, 0.000000, 0.000000, 0.000000, 0.000…
## $ PctDiscCH      &lt;dbl&gt; 0.000000, 0.000000, 0.091398, 0.000000, 0.000000, 0.000…
## $ ListPriceDiff  &lt;dbl&gt; 0.24, 0.24, 0.23, 0.00, 0.00, 0.30, 0.30, 0.24, 0.24, 0…
## $ STORE          &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2…</code></pre>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="decision-trees.html#cb241-1" tabindex="-1"></a>cs_dat <span class="ot">&lt;-</span> ISLR<span class="sc">::</span>Carseats</span>
<span id="cb241-2"><a href="decision-trees.html#cb241-2" tabindex="-1"></a></span>
<span id="cb241-3"><a href="decision-trees.html#cb241-3" tabindex="-1"></a><span class="fu">glimpse</span>(cs_dat)</span></code></pre></div>
<pre><code>## Rows: 400
## Columns: 11
## $ Sales       &lt;dbl&gt; 9.50, 11.22, 10.06, 7.40, 4.15, 10.81, 6.63, 11.85, 6.54, …
## $ CompPrice   &lt;dbl&gt; 138, 111, 113, 117, 141, 124, 115, 136, 132, 132, 121, 117…
## $ Income      &lt;dbl&gt; 73, 48, 35, 100, 64, 113, 105, 81, 110, 113, 78, 94, 35, 2…
## $ Advertising &lt;dbl&gt; 11, 16, 10, 4, 3, 13, 0, 15, 0, 0, 9, 4, 2, 11, 11, 5, 0, …
## $ Population  &lt;dbl&gt; 276, 260, 269, 466, 340, 501, 45, 425, 108, 131, 150, 503,…
## $ Price       &lt;dbl&gt; 120, 83, 80, 97, 128, 72, 108, 120, 124, 124, 100, 94, 136…
## $ ShelveLoc   &lt;fct&gt; Bad, Good, Medium, Medium, Bad, Bad, Medium, Good, Medium,…
## $ Age         &lt;dbl&gt; 42, 65, 59, 55, 38, 78, 71, 67, 76, 76, 26, 50, 62, 53, 52…
## $ Education   &lt;dbl&gt; 17, 10, 12, 14, 13, 16, 15, 10, 10, 17, 10, 13, 18, 18, 18…
## $ Urban       &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, No, No, No, Yes, Ye…
## $ US          &lt;fct&gt; Yes, Yes, Yes, Yes, No, Yes, No, Yes, No, Yes, Yes, Yes, N…</code></pre>
<p>Partition the data sets into training and testing sets. Use the training set to estimate parameters, compare models and feature engineering techniques, and tune models. Use the testing set as an unbiased source for measuring final model performance. The test set is held in reserve until the end of the project at which point there are only be one or two models under serious consideration.</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="decision-trees.html#cb243-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb243-2"><a href="decision-trees.html#cb243-2" tabindex="-1"></a></span>
<span id="cb243-3"><a href="decision-trees.html#cb243-3" tabindex="-1"></a>(oj_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(oj_dat, <span class="at">prop =</span> <span class="fl">0.8</span>, <span class="at">strata =</span> Purchase))</span>
<span id="cb243-4"><a href="decision-trees.html#cb243-4" tabindex="-1"></a><span class="do">## &lt;Training/Testing/Total&gt;</span></span>
<span id="cb243-5"><a href="decision-trees.html#cb243-5" tabindex="-1"></a><span class="do">## &lt;855/215/1070&gt;</span></span>
<span id="cb243-6"><a href="decision-trees.html#cb243-6" tabindex="-1"></a>oj_train <span class="ot">&lt;-</span> <span class="fu">training</span>(oj_split)</span>
<span id="cb243-7"><a href="decision-trees.html#cb243-7" tabindex="-1"></a>oj_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(oj_split)</span>
<span id="cb243-8"><a href="decision-trees.html#cb243-8" tabindex="-1"></a></span>
<span id="cb243-9"><a href="decision-trees.html#cb243-9" tabindex="-1"></a>(cs_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(cs_dat, <span class="at">prop =</span> <span class="fl">0.8</span>))</span>
<span id="cb243-10"><a href="decision-trees.html#cb243-10" tabindex="-1"></a><span class="do">## &lt;Training/Testing/Total&gt;</span></span>
<span id="cb243-11"><a href="decision-trees.html#cb243-11" tabindex="-1"></a><span class="do">## &lt;320/80/400&gt;</span></span>
<span id="cb243-12"><a href="decision-trees.html#cb243-12" tabindex="-1"></a>cs_train <span class="ot">&lt;-</span> <span class="fu">training</span>(cs_split)</span>
<span id="cb243-13"><a href="decision-trees.html#cb243-13" tabindex="-1"></a>cs_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(cs_split)</span></code></pre></div>
<div id="classification-tree" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Classification Tree<a href="decision-trees.html#classification-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The simple classification tree is uncommon, but it is the foundation for the more complex ensemble models.</p>
<p>Function <code>rpart::rpart()</code> builds a full tree, minimizing the Gini index <span class="math inline">\(G\)</span> by default (<code>parms = list(split = "gini")</code>), until the stopping criterion is satisfied. The default stopping criterion is</p>
<ul>
<li>only attempt a split if the current node has at least <code>minsplit = 20</code> observations, and</li>
<li>only accept a split if
<ul>
<li>the resulting nodes have at least <code>minbucket = round(minsplit/3)</code> observations, and</li>
<li>the resulting overall fit improves by <code>cp = 0.01</code> (i.e., <span class="math inline">\(\Delta G &lt;= 0.01\)</span>).</li>
</ul></li>
</ul>
<p>Some model parameters like <code>cp</code> (see <code>?rpart.control</code>) are hyperparameters that cannot be optimized directly from the training data set. You can tune hyperparameters by training many models on resampled data sets and exploring how well the models perform.</p>
<p>The <a href="https://www.tidymodels.org/start">tidymodels vignette</a> explains that predictive models should train with resampling methods such as cross-validation to get a short list of candidate models, then choose the best model by comparing performance on the test set. I.e., model selection is a two-phased competition. In this context, we can use CV to fit the best classification tree, then in the next section fit the best bagged tree, then random forest, and so on. The second phase would be to compare all models on the test set.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="decision-trees.html#cb244-1" tabindex="-1"></a>oj_cart <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb244-2"><a href="decision-trees.html#cb244-2" tabindex="-1"></a></span>
<span id="cb244-3"><a href="decision-trees.html#cb244-3" tabindex="-1"></a><span class="co"># `decision_tree` has 3 hyperparameters (`cost_complexity`, `tree_depth`, and</span></span>
<span id="cb244-4"><a href="decision-trees.html#cb244-4" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let&#39;s</span></span>
<span id="cb244-5"><a href="decision-trees.html#cb244-5" tabindex="-1"></a><span class="co"># optimize just `cost_complexity` and `tree_depth`.</span></span>
<span id="cb244-6"><a href="decision-trees.html#cb244-6" tabindex="-1"></a>oj_cart<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb244-7"><a href="decision-trees.html#cb244-7" tabindex="-1"></a>  <span class="fu">decision_tree</span>(</span>
<span id="cb244-8"><a href="decision-trees.html#cb244-8" tabindex="-1"></a>    <span class="at">cost_complexity =</span> <span class="fu">tune</span>(),</span>
<span id="cb244-9"><a href="decision-trees.html#cb244-9" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="fu">tune</span>()</span>
<span id="cb244-10"><a href="decision-trees.html#cb244-10" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb244-11"><a href="decision-trees.html#cb244-11" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb244-12"><a href="decision-trees.html#cb244-12" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;classification&quot;</span>)</span>
<span id="cb244-13"><a href="decision-trees.html#cb244-13" tabindex="-1"></a></span>
<span id="cb244-14"><a href="decision-trees.html#cb244-14" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb244-15"><a href="decision-trees.html#cb244-15" tabindex="-1"></a>oj_cart<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb244-16"><a href="decision-trees.html#cb244-16" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb244-17"><a href="decision-trees.html#cb244-17" tabindex="-1"></a>  <span class="fu">add_model</span>(oj_cart<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb244-18"><a href="decision-trees.html#cb244-18" tabindex="-1"></a>  <span class="fu">add_formula</span>(Purchase <span class="sc">~</span> .)</span>
<span id="cb244-19"><a href="decision-trees.html#cb244-19" tabindex="-1"></a></span>
<span id="cb244-20"><a href="decision-trees.html#cb244-20" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb244-21"><a href="decision-trees.html#cb244-21" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb244-22"><a href="decision-trees.html#cb244-22" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb244-23"><a href="decision-trees.html#cb244-23" tabindex="-1"></a>oj_cart<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb244-24"><a href="decision-trees.html#cb244-24" tabindex="-1"></a>  oj_cart<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb244-25"><a href="decision-trees.html#cb244-25" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb244-26"><a href="decision-trees.html#cb244-26" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(oj_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb244-27"><a href="decision-trees.html#cb244-27" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">cost_complexity</span>(), <span class="fu">tree_depth</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb244-28"><a href="decision-trees.html#cb244-28" tabindex="-1"></a>  )</span>
<span id="cb244-29"><a href="decision-trees.html#cb244-29" tabindex="-1"></a></span>
<span id="cb244-30"><a href="decision-trees.html#cb244-30" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb244-31"><a href="decision-trees.html#cb244-31" tabindex="-1"></a>oj_cart<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb244-32"><a href="decision-trees.html#cb244-32" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb244-33"><a href="decision-trees.html#cb244-33" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tree_depth =</span> <span class="fu">factor</span>(tree_depth)) <span class="sc">%&gt;%</span></span>
<span id="cb244-34"><a href="decision-trees.html#cb244-34" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> cost_complexity, <span class="at">y =</span> mean, <span class="at">color =</span> tree_depth)) <span class="sc">+</span></span>
<span id="cb244-35"><a href="decision-trees.html#cb244-35" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb244-36"><a href="decision-trees.html#cb244-36" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb244-37"><a href="decision-trees.html#cb244-37" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/fit-cart-class-1.png" width="672" /></p>
<p>The best models in terms of accuracy and ROC was the tree depth of 4 and any cp &lt; .01.</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="decision-trees.html#cb245-1" tabindex="-1"></a>oj_cart<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="st">&quot;accuracy&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 8
##   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    
##             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      
## 1    0.0000000001          4 accuracy binary     0.804    10  0.0149 Preprocess…
## 2    0.0000000178          4 accuracy binary     0.804    10  0.0149 Preprocess…
## 3    0.00000316            4 accuracy binary     0.804    10  0.0149 Preprocess…
## 4    0.000562              4 accuracy binary     0.804    10  0.0149 Preprocess…
## 5    0.0000000001          8 accuracy binary     0.800    10  0.0165 Preprocess…</code></pre>
<p>Select the best model in terms of accuracy and finalize the model.</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="decision-trees.html#cb247-1" tabindex="-1"></a>oj_cart<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_cart<span class="sc">$</span>tune_grid, <span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb247-2"><a href="decision-trees.html#cb247-2" tabindex="-1"></a></span>
<span id="cb247-3"><a href="decision-trees.html#cb247-3" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb247-4"><a href="decision-trees.html#cb247-4" tabindex="-1"></a>oj_cart<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(oj_cart<span class="sc">$</span>workflow, oj_cart<span class="sc">$</span>best_tune)</span>
<span id="cb247-5"><a href="decision-trees.html#cb247-5" tabindex="-1"></a></span>
<span id="cb247-6"><a href="decision-trees.html#cb247-6" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb247-7"><a href="decision-trees.html#cb247-7" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb247-8"><a href="decision-trees.html#cb247-8" tabindex="-1"></a>oj_cart<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb247-9"><a href="decision-trees.html#cb247-9" tabindex="-1"></a>  oj_cart<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb247-10"><a href="decision-trees.html#cb247-10" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span></code></pre></div>
<p>Here is the tree. The output starts with the root node. The predicted class at the root is <code>CH</code> and this prediction produces 333 errors on the 855 observations for a 61% success rate (accuracy) and 39% error rate. The child nodes of node “x” are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5), etc. Terminal nodes are labeled with an asterisk (*).</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="decision-trees.html#cb248-1" tabindex="-1"></a>oj_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">extract_workflow</span>()</span></code></pre></div>
<pre><code>## ══ Workflow [trained] ═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════
## Preprocessor: Formula
## Model: decision_tree()
## 
## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
## Purchase ~ .
## 
## ── Model ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
## n= 855 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 855 333 CH (0.61052632 0.38947368)  
##    2) LoyalCH&gt;=0.48285 534  92 CH (0.82771536 0.17228464)  
##      4) LoyalCH&gt;=0.7535455 273  13 CH (0.95238095 0.04761905) *
##      5) LoyalCH&lt; 0.7535455 261  79 CH (0.69731801 0.30268199)  
##       10) PriceDiff&gt;=-0.165 221  48 CH (0.78280543 0.21719457) *
##       11) PriceDiff&lt; -0.165 40   9 MM (0.22500000 0.77500000) *
##    3) LoyalCH&lt; 0.48285 321  80 MM (0.24922118 0.75077882)  
##      6) LoyalCH&gt;=0.2761415 148  58 MM (0.39189189 0.60810811)  
##       12) SalePriceMM&gt;=2.04 71  31 CH (0.56338028 0.43661972) *
##       13) SalePriceMM&lt; 2.04 77  18 MM (0.23376623 0.76623377)  
##         26) SpecialCH&gt;=0.5 14   6 CH (0.57142857 0.42857143) *
##         27) SpecialCH&lt; 0.5 63  10 MM (0.15873016 0.84126984) *
##      7) LoyalCH&lt; 0.2761415 173  22 MM (0.12716763 0.87283237) *</code></pre>
<p>The first split is at <code>LoyalCH</code> = 0.48285. Here is a diagram of the tree. The node label indicates the predicted value, error rate, and proportion of observations included. Below the nodes are the splitting criteria.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="decision-trees.html#cb250-1" tabindex="-1"></a>oj_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb250-2"><a href="decision-trees.html#cb250-2" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb250-3"><a href="decision-trees.html#cb250-3" tabindex="-1"></a>  <span class="fu">extract_fit_engine</span>() <span class="sc">%&gt;%</span></span>
<span id="cb250-4"><a href="decision-trees.html#cb250-4" tabindex="-1"></a>  <span class="fu">rpart.plot</span>(<span class="at">yesno =</span> <span class="cn">TRUE</span>, <span class="at">roundint =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-171-1.png" width="672" /></p>
<p>The boxes show the node classification (based on mode), the proportion of observations that are <em>not</em> <code>CH</code>, and the proportion of observations included in the node.</p>
<p><code>LoyalCH</code> was the most important variable, followed by <code>PriceDiff</code>. A variable’s importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. From the <strong>rpart</strong> <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">vignette</a> (page 12),</p>
<blockquote>
<p>“An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate.”</p>
</blockquote>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="decision-trees.html#cb251-1" tabindex="-1"></a>oj_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb251-2"><a href="decision-trees.html#cb251-2" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb251-3"><a href="decision-trees.html#cb251-3" tabindex="-1"></a>  <span class="fu">extract_fit_parsnip</span>() <span class="sc">%&gt;%</span></span>
<span id="cb251-4"><a href="decision-trees.html#cb251-4" tabindex="-1"></a>  vip<span class="sc">::</span><span class="fu">vip</span>()</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-172-1.png" width="672" /></p>
<div id="measuring-performance" class="section level3 unnumbered hasAnchor">
<h3>Measuring Performance<a href="decision-trees.html#measuring-performance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><code>collect_metrics()</code> shows the accuracy and ROC AUC metrics.</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="decision-trees.html#cb252-1" tabindex="-1"></a>oj_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 × 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.856 Preprocessor1_Model1
## 2 roc_auc  binary         0.909 Preprocessor1_Model1</code></pre>
<p>You can explore the performance by calculating the full confusion matrix and visualizing the ROC curve.</p>
<div id="confusion-matrix" class="section level4 unnumbered hasAnchor">
<h4>Confusion Matrix<a href="decision-trees.html#confusion-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The confusion matrix calculates the model performance predicting on the holdout testing data set.</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="decision-trees.html#cb254-1" tabindex="-1"></a>oj_cart<span class="sc">$</span>confmat <span class="ot">&lt;-</span></span>
<span id="cb254-2"><a href="decision-trees.html#cb254-2" tabindex="-1"></a>  oj_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb254-3"><a href="decision-trees.html#cb254-3" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb254-4"><a href="decision-trees.html#cb254-4" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> Purchase, <span class="at">estimate =</span> .pred_class)</span>
<span id="cb254-5"><a href="decision-trees.html#cb254-5" tabindex="-1"></a></span>
<span id="cb254-6"><a href="decision-trees.html#cb254-6" tabindex="-1"></a><span class="co"># oj_cart$preds &lt;- </span></span>
<span id="cb254-7"><a href="decision-trees.html#cb254-7" tabindex="-1"></a><span class="co">#   bind_cols(</span></span>
<span id="cb254-8"><a href="decision-trees.html#cb254-8" tabindex="-1"></a><span class="co">#      predict(oj_cart$fit, new_data = oj_test, type = &quot;prob&quot;),</span></span>
<span id="cb254-9"><a href="decision-trees.html#cb254-9" tabindex="-1"></a><span class="co">#      predicted = predict(oj_cart$fit, new_data = oj_test, type = &quot;class&quot;),</span></span>
<span id="cb254-10"><a href="decision-trees.html#cb254-10" tabindex="-1"></a><span class="co">#      actual = oj_test$Purchase</span></span>
<span id="cb254-11"><a href="decision-trees.html#cb254-11" tabindex="-1"></a><span class="co">#   )</span></span>
<span id="cb254-12"><a href="decision-trees.html#cb254-12" tabindex="-1"></a></span>
<span id="cb254-13"><a href="decision-trees.html#cb254-13" tabindex="-1"></a>oj_cart<span class="sc">$</span>confmat</span>
<span id="cb254-14"><a href="decision-trees.html#cb254-14" tabindex="-1"></a><span class="do">##           Truth</span></span>
<span id="cb254-15"><a href="decision-trees.html#cb254-15" tabindex="-1"></a><span class="do">## Prediction  CH  MM</span></span>
<span id="cb254-16"><a href="decision-trees.html#cb254-16" tabindex="-1"></a><span class="do">##         CH 120  20</span></span>
<span id="cb254-17"><a href="decision-trees.html#cb254-17" tabindex="-1"></a><span class="do">##         MM  11  64</span></span>
<span id="cb254-18"><a href="decision-trees.html#cb254-18" tabindex="-1"></a></span>
<span id="cb254-19"><a href="decision-trees.html#cb254-19" tabindex="-1"></a><span class="fu">summary</span>(oj_cart<span class="sc">$</span>confmat)</span>
<span id="cb254-20"><a href="decision-trees.html#cb254-20" tabindex="-1"></a><span class="do">## # A tibble: 13 × 3</span></span>
<span id="cb254-21"><a href="decision-trees.html#cb254-21" tabindex="-1"></a><span class="do">##    .metric              .estimator .estimate</span></span>
<span id="cb254-22"><a href="decision-trees.html#cb254-22" tabindex="-1"></a><span class="do">##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb254-23"><a href="decision-trees.html#cb254-23" tabindex="-1"></a><span class="do">##  1 accuracy             binary         0.856</span></span>
<span id="cb254-24"><a href="decision-trees.html#cb254-24" tabindex="-1"></a><span class="do">##  2 kap                  binary         0.691</span></span>
<span id="cb254-25"><a href="decision-trees.html#cb254-25" tabindex="-1"></a><span class="do">##  3 sens                 binary         0.916</span></span>
<span id="cb254-26"><a href="decision-trees.html#cb254-26" tabindex="-1"></a><span class="do">##  4 spec                 binary         0.762</span></span>
<span id="cb254-27"><a href="decision-trees.html#cb254-27" tabindex="-1"></a><span class="do">##  5 ppv                  binary         0.857</span></span>
<span id="cb254-28"><a href="decision-trees.html#cb254-28" tabindex="-1"></a><span class="do">##  6 npv                  binary         0.853</span></span>
<span id="cb254-29"><a href="decision-trees.html#cb254-29" tabindex="-1"></a><span class="do">##  7 mcc                  binary         0.694</span></span>
<span id="cb254-30"><a href="decision-trees.html#cb254-30" tabindex="-1"></a><span class="do">##  8 j_index              binary         0.678</span></span>
<span id="cb254-31"><a href="decision-trees.html#cb254-31" tabindex="-1"></a><span class="do">##  9 bal_accuracy         binary         0.839</span></span>
<span id="cb254-32"><a href="decision-trees.html#cb254-32" tabindex="-1"></a><span class="do">## 10 detection_prevalence binary         0.651</span></span>
<span id="cb254-33"><a href="decision-trees.html#cb254-33" tabindex="-1"></a><span class="do">## 11 precision            binary         0.857</span></span>
<span id="cb254-34"><a href="decision-trees.html#cb254-34" tabindex="-1"></a><span class="do">## 12 recall               binary         0.916</span></span>
<span id="cb254-35"><a href="decision-trees.html#cb254-35" tabindex="-1"></a><span class="do">## 13 f_meas               binary         0.886</span></span></code></pre></div>
<p>You can bound the accuracy with a 95% CI using the binomial test.</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="decision-trees.html#cb255-1" tabindex="-1"></a><span class="fu">binom.test</span>(</span>
<span id="cb255-2"><a href="decision-trees.html#cb255-2" tabindex="-1"></a>  <span class="at">x =</span> oj_cart<span class="sc">$</span>confmat<span class="sc">$</span>table <span class="sc">%&gt;%</span> <span class="fu">diag</span>() <span class="sc">%&gt;%</span> <span class="fu">sum</span>(), </span>
<span id="cb255-3"><a href="decision-trees.html#cb255-3" tabindex="-1"></a>  <span class="at">n =</span> oj_cart<span class="sc">$</span>confmat<span class="sc">$</span>table <span class="sc">%&gt;%</span> <span class="fu">sum</span>()</span>
<span id="cb255-4"><a href="decision-trees.html#cb255-4" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## 
##  Exact binomial test
## 
## data:  oj_cart$confmat$table %&gt;% diag() %&gt;% sum() and oj_cart$confmat$table %&gt;% sum()
## number of successes = 184, number of trials = 215, p-value &lt; 2.2e-16
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.8016244 0.8998833
## sample estimates:
## probability of success 
##               0.855814</code></pre>
<p>The detection prevalence (aka, no information rate (NIR)) statistic is the class rate for the largest class. In this case CH is the largest class, so NIR = 133/213 = 0.6186. The binomial test for NIR is the probability of that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH).</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="decision-trees.html#cb257-1" tabindex="-1"></a><span class="fu">binom.test</span>(</span>
<span id="cb257-2"><a href="decision-trees.html#cb257-2" tabindex="-1"></a>  <span class="at">x =</span> oj_cart<span class="sc">$</span>confmat<span class="sc">$</span>table[<span class="st">&quot;CH&quot;</span>, ] <span class="sc">%&gt;%</span> <span class="fu">sum</span>(), </span>
<span id="cb257-3"><a href="decision-trees.html#cb257-3" tabindex="-1"></a>  <span class="at">n =</span> oj_cart<span class="sc">$</span>confmat<span class="sc">$</span>table <span class="sc">%&gt;%</span> <span class="fu">sum</span>(),</span>
<span id="cb257-4"><a href="decision-trees.html#cb257-4" tabindex="-1"></a>  <span class="at">p =</span> <span class="fu">sum</span>(oj_cart<span class="sc">$</span>confmat<span class="sc">$</span>table[, <span class="st">&quot;CH&quot;</span>]) <span class="sc">/</span> <span class="fu">sum</span>(oj_cart<span class="sc">$</span>confmat<span class="sc">$</span>table),</span>
<span id="cb257-5"><a href="decision-trees.html#cb257-5" tabindex="-1"></a>  <span class="at">alternative =</span> <span class="st">&quot;greater&quot;</span></span>
<span id="cb257-6"><a href="decision-trees.html#cb257-6" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## 
##  Exact binomial test
## 
## data:  oj_cart$confmat$table[&quot;CH&quot;, ] %&gt;% sum() and oj_cart$confmat$table %&gt;% sum()
## number of successes = 140, number of trials = 215, p-value = 0.1169
## alternative hypothesis: true probability of success is greater than 0.6093023
## 95 percent confidence interval:
##  0.5940371 1.0000000
## sample estimates:
## probability of success 
##              0.6511628</code></pre>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="decision-trees.html#cb259-1" tabindex="-1"></a><span class="fu">binom.test</span>(<span class="at">x =</span> <span class="dv">116</span> <span class="sc">+</span> <span class="dv">67</span>, <span class="at">n =</span> <span class="fu">sum</span>(oj_cart<span class="sc">$</span>confmat<span class="sc">$</span>table), </span>
<span id="cb259-2"><a href="decision-trees.html#cb259-2" tabindex="-1"></a>           <span class="at">p =</span> (<span class="dv">116</span><span class="sc">+</span><span class="dv">17</span>)<span class="sc">/</span><span class="fu">sum</span>(oj_cart<span class="sc">$</span>confmat<span class="sc">$</span>table), </span>
<span id="cb259-3"><a href="decision-trees.html#cb259-3" tabindex="-1"></a>           <span class="at">alternative =</span> <span class="st">&quot;greater&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Exact binomial test
## 
## data:  116 + 67 and sum(oj_cart$confmat$table)
## number of successes = 183, number of trials = 215, p-value = 5.461e-14
## alternative hypothesis: true probability of success is greater than 0.6186047
## 95 percent confidence interval:
##  0.8052886 1.0000000
## sample estimates:
## probability of success 
##              0.8511628</code></pre>
<p>The accuracy statistic indicates the model predicts 85.6% of the observations correctly. That’s good, but less impressive when you consider the prevalence of CH is 65.1% - you could achieve that accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen’s kappa statistic. The kappa statistic is explained <a href="https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/">here</a>. It compares the accuracy to the accuracy of a “random system”. It is defined as</p>
<p><span class="math display">\[\kappa = \frac{\text{Accuracy} - RA}{1-RA}\]</span></p>
<p>where</p>
<p><span class="math display">\[RA = \frac{\text{ActFalse} \times \text{PredFalse} + \text{ActTrue} \times \text{PredTrue}}{\text{Total} \times \text{Total}}\]</span></p>
<p>is the hypothetical probability of a chance agreement.</p>
<p>The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations. In this case, a kappa statistic of 0.7064 is “substantial”. See chart <a href="https://www.statisticshowto.datasciencecentral.com/cohens-kappa-statistic/">here</a>.</p>
<p>You can remind yourself what the other confusion matrix measures are from the documentation. Visuals are almost always helpful. Here is a plot of the confusion matrix.</p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb261-1"><a href="decision-trees.html#cb261-1" tabindex="-1"></a>oj_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb261-2"><a href="decision-trees.html#cb261-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb261-3"><a href="decision-trees.html#cb261-3" tabindex="-1"></a>  <span class="fu">select</span>(Purchase, .pred_class) <span class="sc">%&gt;%</span></span>
<span id="cb261-4"><a href="decision-trees.html#cb261-4" tabindex="-1"></a>  <span class="fu">plot</span>(</span>
<span id="cb261-5"><a href="decision-trees.html#cb261-5" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">&quot;Simple Classification: Predicted vs. Actual&quot;</span>,</span>
<span id="cb261-6"><a href="decision-trees.html#cb261-6" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">&quot;Actual&quot;</span>,</span>
<span id="cb261-7"><a href="decision-trees.html#cb261-7" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;Predicted&quot;</span></span>
<span id="cb261-8"><a href="decision-trees.html#cb261-8" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-177-1.png" width="672" /></p>
</div>
<div id="roc-curve" class="section level4 unnumbered hasAnchor">
<h4>ROC Curve<a href="decision-trees.html#roc-curve" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The ROC (receiver operating characteristics) curve <span class="citation">(<a href="#ref-Fawcett2005">Fawcett 2005</a>)</span> is another measure of accuracy. The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold. <code>precrec::evalmod()</code> calculates the confusion matrix values from the model using the holdout data set. The AUC on the holdout set is 0.909. <code>pRoc::plot.roc()</code>, <code>plotROC::geom_roc()</code>, and <code>yardstick::roc_curve()</code> are options for plotting a ROC curve.</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="decision-trees.html#cb262-1" tabindex="-1"></a>oj_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb262-2"><a href="decision-trees.html#cb262-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb262-3"><a href="decision-trees.html#cb262-3" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb262-4"><a href="decision-trees.html#cb262-4" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb262-5"><a href="decision-trees.html#cb262-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;OJ CART ROC Curve&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-178-1.png" width="672" /></p>
<p>A few points on the ROC space are helpful for understanding how to use it.</p>
<ul>
<li>The lower left point (0, 0) is the result of <em>always</em> predicting “negative” or in this case “MM” if “CH” is taken as the default class. No false positives, but no true positives either.</li>
<li>The upper right point (1, 1) is the result of <em>always</em> predicting “positive” (“CH” here). You catch all true positives, but miss all the true negatives.</li>
<li>The upper left point (0, 1) is the result of perfect accuracy.</li>
<li>The lower right point (1, 0) is the result of perfect imbecility. You made the exact wrong prediction every time.</li>
<li>The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time. If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc.</li>
</ul>
<p>The goal is for all nodes to bunch up in the upper left.</p>
<p>Points to the left of the diagonal with a low TPR can be thought of as “conservative” predictors - they only make positive (CH) predictions with strong evidence. Points to the left of the diagonal with a high TPR can be thought of as “liberal” predictors - they make positive (CH) predictions with weak evidence.</p>
</div>
<div id="gain-curve-1" class="section level4 hasAnchor" number="6.1.0.1">
<h4><span class="header-section-number">6.1.0.1</span> Gain Curve<a href="decision-trees.html#gain-curve-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The gray area is the “gain” over a random prediction.</p>
<p>131 of the 215 consumers in the holdout testing set purchased CH.</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="decision-trees.html#cb263-1" tabindex="-1"></a>oj_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb263-2"><a href="decision-trees.html#cb263-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb263-3"><a href="decision-trees.html#cb263-3" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH)</span></code></pre></div>
<pre><code>## # A tibble: 8 × 4
##      .n .n_events .percent_tested .percent_found
##   &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;
## 1     0         0             0              0  
## 2    82        79            38.1           60.3
## 3   129       114            60             87.0
## 4   131       116            60.9           88.5
## 5   140       120            65.1           91.6
## 6   146       123            67.9           93.9
## 7   165       126            76.7           96.2
## 8   215       131           100            100</code></pre>
<ul>
<li><p>The gain curve encountered 79 CH purchasers (60.3%) within the first 82 observations (38.1%).</p></li>
<li><p>It encountered all 131 CH purchasers on the 215th observation (100%).</p></li>
<li><p>The bottom of the gray area is the outcome of a random model. Only half the CH purchasers would be observed within 50% of the observations. The top of the gray area is the outcome of the perfect model, the “wizard curve”. Half the CH purchasers would be observed in ~30% of the observations.</p></li>
</ul>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="decision-trees.html#cb265-1" tabindex="-1"></a>oj_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb265-2"><a href="decision-trees.html#cb265-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb265-3"><a href="decision-trees.html#cb265-3" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb265-4"><a href="decision-trees.html#cb265-4" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb265-5"><a href="decision-trees.html#cb265-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;OJ CART Gain Curve&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-181-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="regression-tree" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Regression Tree<a href="decision-trees.html#regression-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A simple regression tree is built in a manner similar to a simple classification tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic.</p>
<p>The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp). The only difference here is the <code>set_Mode("regression")</code> call to produce a regression tree.</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="decision-trees.html#cb266-1" tabindex="-1"></a>cs_cart <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb266-2"><a href="decision-trees.html#cb266-2" tabindex="-1"></a></span>
<span id="cb266-3"><a href="decision-trees.html#cb266-3" tabindex="-1"></a><span class="co"># `decision_tree` has 3 hyperparameters (`cost_complexity`, `tree_depth`, and</span></span>
<span id="cb266-4"><a href="decision-trees.html#cb266-4" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let&#39;s</span></span>
<span id="cb266-5"><a href="decision-trees.html#cb266-5" tabindex="-1"></a><span class="co"># optimize just `cost_complexity` and `tree_depth`.</span></span>
<span id="cb266-6"><a href="decision-trees.html#cb266-6" tabindex="-1"></a>cs_cart<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb266-7"><a href="decision-trees.html#cb266-7" tabindex="-1"></a>  <span class="fu">decision_tree</span>(</span>
<span id="cb266-8"><a href="decision-trees.html#cb266-8" tabindex="-1"></a>    <span class="at">cost_complexity =</span> <span class="fu">tune</span>(),</span>
<span id="cb266-9"><a href="decision-trees.html#cb266-9" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="fu">tune</span>()</span>
<span id="cb266-10"><a href="decision-trees.html#cb266-10" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb266-11"><a href="decision-trees.html#cb266-11" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb266-12"><a href="decision-trees.html#cb266-12" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb266-13"><a href="decision-trees.html#cb266-13" tabindex="-1"></a></span>
<span id="cb266-14"><a href="decision-trees.html#cb266-14" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb266-15"><a href="decision-trees.html#cb266-15" tabindex="-1"></a>cs_cart<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb266-16"><a href="decision-trees.html#cb266-16" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb266-17"><a href="decision-trees.html#cb266-17" tabindex="-1"></a>  <span class="fu">add_model</span>(cs_cart<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb266-18"><a href="decision-trees.html#cb266-18" tabindex="-1"></a>  <span class="fu">add_formula</span>(Sales <span class="sc">~</span> .)</span>
<span id="cb266-19"><a href="decision-trees.html#cb266-19" tabindex="-1"></a></span>
<span id="cb266-20"><a href="decision-trees.html#cb266-20" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb266-21"><a href="decision-trees.html#cb266-21" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb266-22"><a href="decision-trees.html#cb266-22" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb266-23"><a href="decision-trees.html#cb266-23" tabindex="-1"></a>cs_cart<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb266-24"><a href="decision-trees.html#cb266-24" tabindex="-1"></a>  cs_cart<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb266-25"><a href="decision-trees.html#cb266-25" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb266-26"><a href="decision-trees.html#cb266-26" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(cs_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb266-27"><a href="decision-trees.html#cb266-27" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">cost_complexity</span>(), <span class="fu">tree_depth</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb266-28"><a href="decision-trees.html#cb266-28" tabindex="-1"></a>  )</span>
<span id="cb266-29"><a href="decision-trees.html#cb266-29" tabindex="-1"></a></span>
<span id="cb266-30"><a href="decision-trees.html#cb266-30" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: rmse and rsq.</span></span>
<span id="cb266-31"><a href="decision-trees.html#cb266-31" tabindex="-1"></a>cs_cart<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb266-32"><a href="decision-trees.html#cb266-32" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb266-33"><a href="decision-trees.html#cb266-33" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tree_depth =</span> <span class="fu">factor</span>(tree_depth)) <span class="sc">%&gt;%</span></span>
<span id="cb266-34"><a href="decision-trees.html#cb266-34" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> cost_complexity, <span class="at">y =</span> mean, <span class="at">color =</span> tree_depth)) <span class="sc">+</span></span>
<span id="cb266-35"><a href="decision-trees.html#cb266-35" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb266-36"><a href="decision-trees.html#cb266-36" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb266-37"><a href="decision-trees.html#cb266-37" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/fit-cart-reg-1.png" width="672" /></p>
<p>The best models in terms of RMSE was the tree depth of 8 and any cp &lt; 5.6E-04.</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="decision-trees.html#cb267-1" tabindex="-1"></a>cs_cart<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 8
##   cost_complexity tree_depth .metric .estimator  mean     n std_err .config     
##             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       
## 1    0.0000000001          8 rmse    standard    2.02    10  0.0731 Preprocesso…
## 2    0.0000000178          8 rmse    standard    2.02    10  0.0731 Preprocesso…
## 3    0.00000316            8 rmse    standard    2.02    10  0.0731 Preprocesso…
## 4    0.000562              8 rmse    standard    2.02    10  0.0731 Preprocesso…
## 5    0.0000000001         11 rmse    standard    2.02    10  0.0731 Preprocesso…</code></pre>
<p>Select the best model in terms of rmse and finalize the model.</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="decision-trees.html#cb269-1" tabindex="-1"></a>cs_cart<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_cart<span class="sc">$</span>tune_grid, <span class="st">&quot;rmse&quot;</span>)</span>
<span id="cb269-2"><a href="decision-trees.html#cb269-2" tabindex="-1"></a></span>
<span id="cb269-3"><a href="decision-trees.html#cb269-3" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb269-4"><a href="decision-trees.html#cb269-4" tabindex="-1"></a>cs_cart<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_cart<span class="sc">$</span>workflow, cs_cart<span class="sc">$</span>best_tune)</span>
<span id="cb269-5"><a href="decision-trees.html#cb269-5" tabindex="-1"></a></span>
<span id="cb269-6"><a href="decision-trees.html#cb269-6" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb269-7"><a href="decision-trees.html#cb269-7" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb269-8"><a href="decision-trees.html#cb269-8" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb269-9"><a href="decision-trees.html#cb269-9" tabindex="-1"></a>  cs_cart<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb269-10"><a href="decision-trees.html#cb269-10" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span></code></pre></div>
<p>Here is the tree. The output starts with the root node. The predicted sales at the root is the mean sales in the testing data set is 7.65 (values are $000s). The deviance at the root is the SSE, 2,551. The first split is at <code>ShelveLoc</code> = [Bad, Medium] vs Good.</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="decision-trees.html#cb270-1" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">extract_workflow</span>()</span></code></pre></div>
<pre><code>## ══ Workflow [trained] ═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════
## Preprocessor: Formula
## Model: decision_tree()
## 
## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
## Sales ~ .
## 
## ── Model ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
## n= 320 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##   1) root 320 2551.272000  7.654125  
##     2) ShelveLoc=Bad,Medium 245 1430.391000  6.880531  
##       4) Price&gt;=94.5 204 1040.546000  6.447353  
##         8) Advertising&lt; 7.5 119  495.019700  5.743025  
##          16) ShelveLoc=Bad 35  107.500700  4.609714  
##            32) Population&lt; 196.5 16   48.386700  3.837500 *
##            33) Population&gt;=196.5 19   41.538400  5.260000 *
##          17) ShelveLoc=Medium 84  323.834500  6.215238  
##            34) Price&gt;=127 28  119.289000  4.952857  
##              68) Age&gt;=67 7   22.955970  2.734286 *
##              69) Age&lt; 67 21   50.393780  5.692381  
##               138) CompPrice&lt; 135 11   10.383490  4.590909 *
##               139) CompPrice&gt;=135 10   11.984440  6.904000 *
##            35) Price&lt; 127 56  137.614100  6.846429  
##              70) CompPrice&lt; 124.5 31   36.984390  6.012581  
##               140) Price&gt;=108.5 12   10.703470  5.443333 *
##               141) Price&lt; 108.5 19   19.936520  6.372105 *
##              71) CompPrice&gt;=124.5 25   52.347900  7.880400  
##               142) Age&gt;=74.5 8   11.584690  6.398750 *
##               143) Age&lt; 74.5 17   14.936310  8.577647 *
##         9) Advertising&gt;=7.5 85  403.846300  7.433412  
##          18) Age&gt;=58 34  150.180000  6.219412  
##            36) Price&gt;=135.5 7    7.098371  4.184286 *
##            37) Price&lt; 135.5 27  106.573000  6.747037  
##              74) CompPrice&lt; 123.5 15   34.053370  5.501333 *
##              75) CompPrice&gt;=123.5 12   20.147090  8.304167 *
##          19) Age&lt; 58 51  170.151200  8.242745  
##            38) Price&gt;=127 22   49.335900  6.969545  
##              76) Advertising&lt; 14 13   19.492320  6.385385 *
##              77) Advertising&gt;=14 9   18.999600  7.813333 *
##            39) Price&lt; 127 29   58.097940  9.208621  
##              78) CompPrice&lt; 130.5 16   27.176440  8.598125 *
##              79) CompPrice&gt;=130.5 13   17.618800  9.960000 *
##       5) Price&lt; 94.5 41  161.103600  9.035854  
##        10) Income&lt; 57 9   29.422620  7.285556 *
##        11) Income&gt;=57 32   96.354490  9.528125  
##          22) CompPrice&lt; 123.5 24   54.578700  8.990417  
##            44) Price&gt;=74.5 17   26.677550  8.412941 *
##            45) Price&lt; 74.5 7    8.464143 10.392860 *
##          23) CompPrice&gt;=123.5 8   14.019290 11.141250 *
##     3) ShelveLoc=Good 75  495.303800 10.181200  
##       6) Price&gt;=107.5 51  267.569700  9.221765  
##        12) Advertising&lt; 13.5 44  176.815800  8.706818  
##          24) Price&gt;=142.5 11   36.270690  7.099091 *
##          25) Price&lt; 142.5 33  102.634900  9.242727  
## 
## ...
## and 8 more lines.</code></pre>
<p>Here is a diagram of the tree. The node label indicates the predicted value (mean) and the proportion of observations that are in the node (or child nodes). Below the nodes are the splitting criteria.</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="decision-trees.html#cb272-1" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb272-2"><a href="decision-trees.html#cb272-2" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb272-3"><a href="decision-trees.html#cb272-3" tabindex="-1"></a>  <span class="fu">extract_fit_engine</span>() <span class="sc">%&gt;%</span></span>
<span id="cb272-4"><a href="decision-trees.html#cb272-4" tabindex="-1"></a>  <span class="fu">rpart.plot</span>(<span class="at">yesno =</span> <span class="cn">TRUE</span>, <span class="at">roundint =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-185-1.png" width="672" /></p>
<p><code>Price</code> and <code>ShelveLoc</code> were the most important variables.</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="decision-trees.html#cb273-1" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb273-2"><a href="decision-trees.html#cb273-2" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb273-3"><a href="decision-trees.html#cb273-3" tabindex="-1"></a>  <span class="fu">extract_fit_parsnip</span>() <span class="sc">%&gt;%</span></span>
<span id="cb273-4"><a href="decision-trees.html#cb273-4" tabindex="-1"></a>  vip<span class="sc">::</span><span class="fu">vip</span>()</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-186-1.png" width="672" /></p>
<div id="measuring-performance-1" class="section level3 unnumbered hasAnchor">
<h3>Measuring Performance<a href="decision-trees.html#measuring-performance-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><code>collect_metrics()</code> returns the RMSE, <span class="math inline">\(RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})\)</span> and the model <span class="math inline">\(R^2\)</span>. The RMSE of 2.11 in the test data set is pretty good considering the standard deviation of <code>Sales</code> is 2.74.</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="decision-trees.html#cb274-1" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 × 4
##   .metric .estimator .estimate .config             
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard       2.11  Preprocessor1_Model1
## 2 rsq     standard       0.464 Preprocessor1_Model1</code></pre>
<p>Here is a predicted vs actual plot.</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="decision-trees.html#cb276-1" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb276-2"><a href="decision-trees.html#cb276-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb276-3"><a href="decision-trees.html#cb276-3" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb276-4"><a href="decision-trees.html#cb276-4" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">&quot;cadetblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb276-5"><a href="decision-trees.html#cb276-5" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">formula =</span> <span class="st">&quot;y~x&quot;</span>) <span class="sc">+</span></span>
<span id="cb276-6"><a href="decision-trees.html#cb276-6" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb276-7"><a href="decision-trees.html#cb276-7" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Carseats CART, Predicted vs Actual&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-188-1.png" width="672" /></p>
<p>The tree nodes do a decent job of binning the observations. The predictions vs actuals plot suggests the model over-estimates at the low end and underestimates at the high end. Calculate the test data set RMSE.</p>
</div>
</div>
<div id="bagged-trees" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Bagged Trees<a href="decision-trees.html#bagged-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One drawback of decision trees is that they are high-variance estimators. A small number of additional training observations can dramatically alter the prediction performance of a learned tree.</p>
<p>Bootstrap aggregation, or <em>bagging</em>, is a general-purpose procedure for reducing the variance of a statistical learning method. The algorithm constructs <em>B</em> regression trees using <em>B</em> bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging the <em>B</em> trees reduces the variance. The predicted value for an observation is the mode (classification) or mean (regression) of the trees. <em>B</em> usually equals ~25.</p>
<p>To test the model accuracy, the out-of-bag observations are predicted from the models. For a training set of size <em>n</em>, each tree is composed of <span class="math inline">\(\sim (1 - e^{-1})n = .632n\)</span> unique observations in-bag and <span class="math inline">\(.368n\)</span> out-of-bag. For each tree in the ensemble, bagging makes predictions on the tree’s out-of-bag observations. I think (<em>see</em> page 197 of <span class="citation">(<a href="#ref-Kuhn2016">Kuhn and Johnson 2016</a>)</span>) bagging measures the performance (RMSE, Accuracy, ROC, etc.) of each tree in the ensemble and averages them to produce an overall performance estimate. (This makes no sense to me. If each tree has poor performance, then the average performance of many trees will still be poor. An ensemble of <em>B</em> trees will produce <span class="math inline">\(\sim .368 B\)</span> predictions per unique observation. Seems like you should take the mean/mode of each observation’s prediction as the final prediction. Then you have <em>n</em> predictions to compare to <em>n</em> actuals, and you assess performance on that.)</p>
<p>The downside to bagging is that there is no single tree with a set of rules to interpret. It becomes unclear which variables are more important than others.</p>
<p>The next section explains how bagged trees are a special case of random forests.</p>
<div id="bagged-classification-tree" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Bagged Classification Tree<a href="decision-trees.html#bagged-classification-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Leaning by example, I’ll predict <code>Purchase</code> from the <code>OJ</code> data set again, this time using the bagging with <code>parsnip::bag_tree()</code>.</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="decision-trees.html#cb277-1" tabindex="-1"></a>oj_bag <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb277-2"><a href="decision-trees.html#cb277-2" tabindex="-1"></a></span>
<span id="cb277-3"><a href="decision-trees.html#cb277-3" tabindex="-1"></a><span class="co"># `bag_tree` has 4 hyperparameters (`cost_complexity`, `tree_depth`, and</span></span>
<span id="cb277-4"><a href="decision-trees.html#cb277-4" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let&#39;s</span></span>
<span id="cb277-5"><a href="decision-trees.html#cb277-5" tabindex="-1"></a><span class="co"># optimize just `cost_complexity` and `tree_depth`.</span></span>
<span id="cb277-6"><a href="decision-trees.html#cb277-6" tabindex="-1"></a>oj_bag<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb277-7"><a href="decision-trees.html#cb277-7" tabindex="-1"></a>  <span class="fu">bag_tree</span>(</span>
<span id="cb277-8"><a href="decision-trees.html#cb277-8" tabindex="-1"></a>    <span class="at">cost_complexity =</span> <span class="fu">tune</span>(),</span>
<span id="cb277-9"><a href="decision-trees.html#cb277-9" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="fu">tune</span>()</span>
<span id="cb277-10"><a href="decision-trees.html#cb277-10" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb277-11"><a href="decision-trees.html#cb277-11" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb277-12"><a href="decision-trees.html#cb277-12" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;classification&quot;</span>)</span>
<span id="cb277-13"><a href="decision-trees.html#cb277-13" tabindex="-1"></a></span>
<span id="cb277-14"><a href="decision-trees.html#cb277-14" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb277-15"><a href="decision-trees.html#cb277-15" tabindex="-1"></a>oj_bag<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb277-16"><a href="decision-trees.html#cb277-16" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb277-17"><a href="decision-trees.html#cb277-17" tabindex="-1"></a>  <span class="fu">add_model</span>(oj_bag<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb277-18"><a href="decision-trees.html#cb277-18" tabindex="-1"></a>  <span class="fu">add_formula</span>(Purchase <span class="sc">~</span> .)</span>
<span id="cb277-19"><a href="decision-trees.html#cb277-19" tabindex="-1"></a></span>
<span id="cb277-20"><a href="decision-trees.html#cb277-20" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb277-21"><a href="decision-trees.html#cb277-21" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb277-22"><a href="decision-trees.html#cb277-22" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb277-23"><a href="decision-trees.html#cb277-23" tabindex="-1"></a>oj_bag<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb277-24"><a href="decision-trees.html#cb277-24" tabindex="-1"></a>  oj_bag<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb277-25"><a href="decision-trees.html#cb277-25" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb277-26"><a href="decision-trees.html#cb277-26" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(oj_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb277-27"><a href="decision-trees.html#cb277-27" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">cost_complexity</span>(), <span class="fu">tree_depth</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb277-28"><a href="decision-trees.html#cb277-28" tabindex="-1"></a>  )</span>
<span id="cb277-29"><a href="decision-trees.html#cb277-29" tabindex="-1"></a></span>
<span id="cb277-30"><a href="decision-trees.html#cb277-30" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb277-31"><a href="decision-trees.html#cb277-31" tabindex="-1"></a>oj_bag<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb277-32"><a href="decision-trees.html#cb277-32" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb277-33"><a href="decision-trees.html#cb277-33" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tree_depth =</span> <span class="fu">factor</span>(tree_depth)) <span class="sc">%&gt;%</span></span>
<span id="cb277-34"><a href="decision-trees.html#cb277-34" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> cost_complexity, <span class="at">y =</span> mean, <span class="at">color =</span> tree_depth)) <span class="sc">+</span></span>
<span id="cb277-35"><a href="decision-trees.html#cb277-35" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb277-36"><a href="decision-trees.html#cb277-36" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb277-37"><a href="decision-trees.html#cb277-37" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/fit-bag-class-1.png" width="672" /></p>
<p>The best models in terms of accuracy and ROC was the tree depth of 4 and any cp &lt;= 3.16e-06.</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="decision-trees.html#cb278-1" tabindex="-1"></a>oj_bag<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="st">&quot;accuracy&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 8
##   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    
##             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      
## 1    0.0000000001          4 accuracy binary     0.816    10 0.00944 Preprocess…
## 2    0.00000316            4 accuracy binary     0.816    10 0.0113  Preprocess…
## 3    0.000562              4 accuracy binary     0.814    10 0.00700 Preprocess…
## 4    0.0000000178          4 accuracy binary     0.813    10 0.00986 Preprocess…
## 5    0.0000000178          8 accuracy binary     0.802    10 0.0108  Preprocess…</code></pre>
<p>Select the best model in terms of accuracy and finalize the model.</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="decision-trees.html#cb280-1" tabindex="-1"></a>oj_bag<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_bag<span class="sc">$</span>tune_grid, <span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb280-2"><a href="decision-trees.html#cb280-2" tabindex="-1"></a></span>
<span id="cb280-3"><a href="decision-trees.html#cb280-3" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb280-4"><a href="decision-trees.html#cb280-4" tabindex="-1"></a>oj_bag<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(oj_bag<span class="sc">$</span>workflow, oj_bag<span class="sc">$</span>best_tune)</span>
<span id="cb280-5"><a href="decision-trees.html#cb280-5" tabindex="-1"></a></span>
<span id="cb280-6"><a href="decision-trees.html#cb280-6" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb280-7"><a href="decision-trees.html#cb280-7" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb280-8"><a href="decision-trees.html#cb280-8" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb280-9"><a href="decision-trees.html#cb280-9" tabindex="-1"></a>  oj_bag<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb280-10"><a href="decision-trees.html#cb280-10" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span></code></pre></div>
<p>I <em>think</em> tidymodels started by splitting the training set into 10 folds, then using 9 of the folds to run the bagging algorithm and collect performance measures on the hold-out fold. After repeating the process for all 10 folds, it averaged the performance measures to produce the resampling results shown below. With hyperparameters, the process is repeated for all combinations and the resampling results above are from the best performing combination.</p>
<p>There is no single tree to visualize, and you can’t even produce a VIP. Let’s look at the performance on the holdout data set. <code>collect_metrics()</code> shows the accuracy and ROC AUC metrics. The accuracy is slightly lower than the single tree, but ROC AUC is higher.</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="decision-trees.html#cb281-1" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 × 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.842 Preprocessor1_Model1
## 2 roc_auc  binary         0.925 Preprocessor1_Model1</code></pre>
<p>You can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="decision-trees.html#cb283-1" tabindex="-1"></a>oj_bag<span class="sc">$</span>confmat <span class="ot">&lt;-</span></span>
<span id="cb283-2"><a href="decision-trees.html#cb283-2" tabindex="-1"></a>  oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb283-3"><a href="decision-trees.html#cb283-3" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb283-4"><a href="decision-trees.html#cb283-4" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> Purchase, <span class="at">estimate =</span> .pred_class)</span>
<span id="cb283-5"><a href="decision-trees.html#cb283-5" tabindex="-1"></a></span>
<span id="cb283-6"><a href="decision-trees.html#cb283-6" tabindex="-1"></a>oj_bag<span class="sc">$</span>confmat</span>
<span id="cb283-7"><a href="decision-trees.html#cb283-7" tabindex="-1"></a><span class="do">##           Truth</span></span>
<span id="cb283-8"><a href="decision-trees.html#cb283-8" tabindex="-1"></a><span class="do">## Prediction  CH  MM</span></span>
<span id="cb283-9"><a href="decision-trees.html#cb283-9" tabindex="-1"></a><span class="do">##         CH 115  18</span></span>
<span id="cb283-10"><a href="decision-trees.html#cb283-10" tabindex="-1"></a><span class="do">##         MM  16  66</span></span>
<span id="cb283-11"><a href="decision-trees.html#cb283-11" tabindex="-1"></a></span>
<span id="cb283-12"><a href="decision-trees.html#cb283-12" tabindex="-1"></a><span class="fu">summary</span>(oj_bag<span class="sc">$</span>confmat)</span>
<span id="cb283-13"><a href="decision-trees.html#cb283-13" tabindex="-1"></a><span class="do">## # A tibble: 13 × 3</span></span>
<span id="cb283-14"><a href="decision-trees.html#cb283-14" tabindex="-1"></a><span class="do">##    .metric              .estimator .estimate</span></span>
<span id="cb283-15"><a href="decision-trees.html#cb283-15" tabindex="-1"></a><span class="do">##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb283-16"><a href="decision-trees.html#cb283-16" tabindex="-1"></a><span class="do">##  1 accuracy             binary         0.842</span></span>
<span id="cb283-17"><a href="decision-trees.html#cb283-17" tabindex="-1"></a><span class="do">##  2 kap                  binary         0.666</span></span>
<span id="cb283-18"><a href="decision-trees.html#cb283-18" tabindex="-1"></a><span class="do">##  3 sens                 binary         0.878</span></span>
<span id="cb283-19"><a href="decision-trees.html#cb283-19" tabindex="-1"></a><span class="do">##  4 spec                 binary         0.786</span></span>
<span id="cb283-20"><a href="decision-trees.html#cb283-20" tabindex="-1"></a><span class="do">##  5 ppv                  binary         0.865</span></span>
<span id="cb283-21"><a href="decision-trees.html#cb283-21" tabindex="-1"></a><span class="do">##  6 npv                  binary         0.805</span></span>
<span id="cb283-22"><a href="decision-trees.html#cb283-22" tabindex="-1"></a><span class="do">##  7 mcc                  binary         0.667</span></span>
<span id="cb283-23"><a href="decision-trees.html#cb283-23" tabindex="-1"></a><span class="do">##  8 j_index              binary         0.664</span></span>
<span id="cb283-24"><a href="decision-trees.html#cb283-24" tabindex="-1"></a><span class="do">##  9 bal_accuracy         binary         0.832</span></span>
<span id="cb283-25"><a href="decision-trees.html#cb283-25" tabindex="-1"></a><span class="do">## 10 detection_prevalence binary         0.619</span></span>
<span id="cb283-26"><a href="decision-trees.html#cb283-26" tabindex="-1"></a><span class="do">## 11 precision            binary         0.865</span></span>
<span id="cb283-27"><a href="decision-trees.html#cb283-27" tabindex="-1"></a><span class="do">## 12 recall               binary         0.878</span></span>
<span id="cb283-28"><a href="decision-trees.html#cb283-28" tabindex="-1"></a><span class="do">## 13 f_meas               binary         0.871</span></span>
<span id="cb283-29"><a href="decision-trees.html#cb283-29" tabindex="-1"></a></span>
<span id="cb283-30"><a href="decision-trees.html#cb283-30" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb283-31"><a href="decision-trees.html#cb283-31" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb283-32"><a href="decision-trees.html#cb283-32" tabindex="-1"></a>  <span class="fu">select</span>(Purchase, .pred_class) <span class="sc">%&gt;%</span></span>
<span id="cb283-33"><a href="decision-trees.html#cb283-33" tabindex="-1"></a>  <span class="fu">plot</span>(</span>
<span id="cb283-34"><a href="decision-trees.html#cb283-34" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">&quot;Bagged Trees: Predicted vs. Actual&quot;</span>,</span>
<span id="cb283-35"><a href="decision-trees.html#cb283-35" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">&quot;Actual&quot;</span>,</span>
<span id="cb283-36"><a href="decision-trees.html#cb283-36" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;Predicted&quot;</span></span>
<span id="cb283-37"><a href="decision-trees.html#cb283-37" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-192-1.png" width="672" /></p>
<p>The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is 0.925.</p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="decision-trees.html#cb284-1" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb284-2"><a href="decision-trees.html#cb284-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb284-3"><a href="decision-trees.html#cb284-3" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb284-4"><a href="decision-trees.html#cb284-4" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb284-5"><a href="decision-trees.html#cb284-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;OJ Bagged Trees ROC Curve&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-193-1.png" width="672" /></p>
<p>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="decision-trees.html#cb285-1" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb285-2"><a href="decision-trees.html#cb285-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb285-3"><a href="decision-trees.html#cb285-3" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb285-4"><a href="decision-trees.html#cb285-4" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb285-5"><a href="decision-trees.html#cb285-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;OJ Bagged Trees Gain Curve&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-194-1.png" width="672" /></p>
</div>
<div id="bagging-regression-tree" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Bagging Regression Tree<a href="decision-trees.html#bagging-regression-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>I’ll predict <code>Sales</code> from the <code>Carseats</code> data set again, this time with <code>parsnip::bag_tree()</code>.</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="decision-trees.html#cb286-1" tabindex="-1"></a><span class="fu">library</span>(baguette)</span>
<span id="cb286-2"><a href="decision-trees.html#cb286-2" tabindex="-1"></a></span>
<span id="cb286-3"><a href="decision-trees.html#cb286-3" tabindex="-1"></a>cs_bag <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb286-4"><a href="decision-trees.html#cb286-4" tabindex="-1"></a></span>
<span id="cb286-5"><a href="decision-trees.html#cb286-5" tabindex="-1"></a><span class="co"># `bag_tree` has 4 hyperparameters (`cost_complexity`, `tree_depth`, and</span></span>
<span id="cb286-6"><a href="decision-trees.html#cb286-6" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let&#39;s</span></span>
<span id="cb286-7"><a href="decision-trees.html#cb286-7" tabindex="-1"></a><span class="co"># optimize just `cost_complexity` and `tree_depth`.</span></span>
<span id="cb286-8"><a href="decision-trees.html#cb286-8" tabindex="-1"></a>cs_bag<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb286-9"><a href="decision-trees.html#cb286-9" tabindex="-1"></a>  <span class="fu">bag_tree</span>(</span>
<span id="cb286-10"><a href="decision-trees.html#cb286-10" tabindex="-1"></a>    <span class="at">cost_complexity =</span> <span class="fu">tune</span>(),</span>
<span id="cb286-11"><a href="decision-trees.html#cb286-11" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="fu">tune</span>()</span>
<span id="cb286-12"><a href="decision-trees.html#cb286-12" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb286-13"><a href="decision-trees.html#cb286-13" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb286-14"><a href="decision-trees.html#cb286-14" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb286-15"><a href="decision-trees.html#cb286-15" tabindex="-1"></a></span>
<span id="cb286-16"><a href="decision-trees.html#cb286-16" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb286-17"><a href="decision-trees.html#cb286-17" tabindex="-1"></a>cs_bag<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb286-18"><a href="decision-trees.html#cb286-18" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb286-19"><a href="decision-trees.html#cb286-19" tabindex="-1"></a>  <span class="fu">add_model</span>(cs_bag<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb286-20"><a href="decision-trees.html#cb286-20" tabindex="-1"></a>  <span class="fu">add_formula</span>(Sales <span class="sc">~</span> .)</span>
<span id="cb286-21"><a href="decision-trees.html#cb286-21" tabindex="-1"></a></span>
<span id="cb286-22"><a href="decision-trees.html#cb286-22" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb286-23"><a href="decision-trees.html#cb286-23" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb286-24"><a href="decision-trees.html#cb286-24" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb286-25"><a href="decision-trees.html#cb286-25" tabindex="-1"></a>cs_bag<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb286-26"><a href="decision-trees.html#cb286-26" tabindex="-1"></a>  cs_bag<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb286-27"><a href="decision-trees.html#cb286-27" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb286-28"><a href="decision-trees.html#cb286-28" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(cs_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb286-29"><a href="decision-trees.html#cb286-29" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">cost_complexity</span>(), <span class="fu">tree_depth</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb286-30"><a href="decision-trees.html#cb286-30" tabindex="-1"></a>  )</span>
<span id="cb286-31"><a href="decision-trees.html#cb286-31" tabindex="-1"></a></span>
<span id="cb286-32"><a href="decision-trees.html#cb286-32" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb286-33"><a href="decision-trees.html#cb286-33" tabindex="-1"></a>cs_bag<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb286-34"><a href="decision-trees.html#cb286-34" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb286-35"><a href="decision-trees.html#cb286-35" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tree_depth =</span> <span class="fu">factor</span>(tree_depth)) <span class="sc">%&gt;%</span></span>
<span id="cb286-36"><a href="decision-trees.html#cb286-36" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> cost_complexity, <span class="at">y =</span> mean, <span class="at">color =</span> tree_depth)) <span class="sc">+</span></span>
<span id="cb286-37"><a href="decision-trees.html#cb286-37" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb286-38"><a href="decision-trees.html#cb286-38" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb286-39"><a href="decision-trees.html#cb286-39" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/fit-bag-reg-1.png" width="672" /></p>
<p>The best models in terms of RMSE was the tree depth of 8 and any cp &lt; 5.6E-04.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="decision-trees.html#cb287-1" tabindex="-1"></a>cs_bag<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 8
##   cost_complexity tree_depth .metric .estimator  mean     n std_err .config     
##             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       
## 1    0.000562             11 rmse    standard    1.58    10  0.0416 Preprocesso…
## 2    0.0000000178         15 rmse    standard    1.58    10  0.0585 Preprocesso…
## 3    0.000562             15 rmse    standard    1.59    10  0.0639 Preprocesso…
## 4    0.0000000001         11 rmse    standard    1.60    10  0.0652 Preprocesso…
## 5    0.00000316            8 rmse    standard    1.63    10  0.0618 Preprocesso…</code></pre>
<p>Select the best model in terms of rmse and finalize the model.</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="decision-trees.html#cb289-1" tabindex="-1"></a>cs_bag<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_bag<span class="sc">$</span>tune_grid, <span class="st">&quot;rmse&quot;</span>)</span>
<span id="cb289-2"><a href="decision-trees.html#cb289-2" tabindex="-1"></a></span>
<span id="cb289-3"><a href="decision-trees.html#cb289-3" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb289-4"><a href="decision-trees.html#cb289-4" tabindex="-1"></a>cs_bag<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_bag<span class="sc">$</span>workflow, cs_bag<span class="sc">$</span>best_tune)</span>
<span id="cb289-5"><a href="decision-trees.html#cb289-5" tabindex="-1"></a></span>
<span id="cb289-6"><a href="decision-trees.html#cb289-6" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb289-7"><a href="decision-trees.html#cb289-7" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb289-8"><a href="decision-trees.html#cb289-8" tabindex="-1"></a>cs_bag<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb289-9"><a href="decision-trees.html#cb289-9" tabindex="-1"></a>  cs_bag<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb289-10"><a href="decision-trees.html#cb289-10" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span></code></pre></div>
<p><code>collect_metrics()</code> returns the RMSE, <span class="math inline">\(RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})\)</span> and the model <span class="math inline">\(R^2\)</span>. The RMSE of 1.68 in the test data set is pretty good considering the standard deviation of <code>Sales</code> is 2.74.</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="decision-trees.html#cb290-1" tabindex="-1"></a>cs_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 × 4
##   .metric .estimator .estimate .config             
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard       1.68  Preprocessor1_Model1
## 2 rsq     standard       0.643 Preprocessor1_Model1</code></pre>
<p>Here is a predicted vs actual plot.</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="decision-trees.html#cb292-1" tabindex="-1"></a>cs_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb292-2"><a href="decision-trees.html#cb292-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb292-3"><a href="decision-trees.html#cb292-3" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb292-4"><a href="decision-trees.html#cb292-4" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">&quot;cadetblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb292-5"><a href="decision-trees.html#cb292-5" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">formula =</span> <span class="st">&quot;y~x&quot;</span>) <span class="sc">+</span></span>
<span id="cb292-6"><a href="decision-trees.html#cb292-6" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb292-7"><a href="decision-trees.html#cb292-7" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Carseats Bagged Trees, Predicted vs Actual&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-198-1.png" width="672" /></p>
</div>
</div>
<div id="random-forests" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Random Forests<a href="decision-trees.html#random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random forests improve bagged trees by way of a small tweak that de-correlates the trees. As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of <em>mtry</em> predictors is chosen as split candidates from the full set of <em>p</em> predictors. A fresh sample of <em>mtry</em> predictors is taken at each split. Typically <span class="math inline">\(mtry \sim \sqrt{p}\)</span>. Bagged trees are thus a special case of random forests where <em>mtry = p</em>.</p>
<div id="random-forest-classification-tree" class="section level4 hasAnchor" number="6.4.0.1">
<h4><span class="header-section-number">6.4.0.1</span> Random Forest Classification Tree<a href="decision-trees.html#random-forest-classification-tree" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Leaning by example, I’ll predict <code>Purchase</code> from the <code>OJ</code> data set again, this time using the bagging with <code>parsnip::rand_forest()</code>.</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="decision-trees.html#cb293-1" tabindex="-1"></a>oj_rf <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb293-2"><a href="decision-trees.html#cb293-2" tabindex="-1"></a></span>
<span id="cb293-3"><a href="decision-trees.html#cb293-3" tabindex="-1"></a><span class="co"># `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and</span></span>
<span id="cb293-4"><a href="decision-trees.html#cb293-4" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let&#39;s</span></span>
<span id="cb293-5"><a href="decision-trees.html#cb293-5" tabindex="-1"></a><span class="co"># optimize just `trees` and `min_n`.</span></span>
<span id="cb293-6"><a href="decision-trees.html#cb293-6" tabindex="-1"></a>oj_rf<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb293-7"><a href="decision-trees.html#cb293-7" tabindex="-1"></a>  <span class="fu">rand_forest</span>(</span>
<span id="cb293-8"><a href="decision-trees.html#cb293-8" tabindex="-1"></a>    <span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb293-9"><a href="decision-trees.html#cb293-9" tabindex="-1"></a>    <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb293-10"><a href="decision-trees.html#cb293-10" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb293-11"><a href="decision-trees.html#cb293-11" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;ranger&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb293-12"><a href="decision-trees.html#cb293-12" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;classification&quot;</span>)</span>
<span id="cb293-13"><a href="decision-trees.html#cb293-13" tabindex="-1"></a></span>
<span id="cb293-14"><a href="decision-trees.html#cb293-14" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb293-15"><a href="decision-trees.html#cb293-15" tabindex="-1"></a>oj_rf<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb293-16"><a href="decision-trees.html#cb293-16" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb293-17"><a href="decision-trees.html#cb293-17" tabindex="-1"></a>  <span class="fu">add_model</span>(oj_rf<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb293-18"><a href="decision-trees.html#cb293-18" tabindex="-1"></a>  <span class="fu">add_formula</span>(Purchase <span class="sc">~</span> .)</span>
<span id="cb293-19"><a href="decision-trees.html#cb293-19" tabindex="-1"></a></span>
<span id="cb293-20"><a href="decision-trees.html#cb293-20" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb293-21"><a href="decision-trees.html#cb293-21" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb293-22"><a href="decision-trees.html#cb293-22" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb293-23"><a href="decision-trees.html#cb293-23" tabindex="-1"></a>oj_rf<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb293-24"><a href="decision-trees.html#cb293-24" tabindex="-1"></a>  oj_rf<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb293-25"><a href="decision-trees.html#cb293-25" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb293-26"><a href="decision-trees.html#cb293-26" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(oj_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb293-27"><a href="decision-trees.html#cb293-27" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">trees</span>(), <span class="fu">min_n</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb293-28"><a href="decision-trees.html#cb293-28" tabindex="-1"></a>  )</span>
<span id="cb293-29"><a href="decision-trees.html#cb293-29" tabindex="-1"></a></span>
<span id="cb293-30"><a href="decision-trees.html#cb293-30" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb293-31"><a href="decision-trees.html#cb293-31" tabindex="-1"></a>oj_rf<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb293-32"><a href="decision-trees.html#cb293-32" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb293-33"><a href="decision-trees.html#cb293-33" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trees =</span> <span class="fu">factor</span>(trees)) <span class="sc">%&gt;%</span></span>
<span id="cb293-34"><a href="decision-trees.html#cb293-34" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> min_n, <span class="at">y =</span> mean, <span class="at">color =</span> trees)) <span class="sc">+</span></span>
<span id="cb293-35"><a href="decision-trees.html#cb293-35" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb293-36"><a href="decision-trees.html#cb293-36" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb293-37"><a href="decision-trees.html#cb293-37" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/fit-rf-class-1.png" width="672" /></p>
<p>The best models in terms of accuracy and ROC was the trees of 1000 and any minimum node size of 40.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="decision-trees.html#cb294-1" tabindex="-1"></a>oj_rf<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="st">&quot;accuracy&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 8
##   trees min_n .metric  .estimator  mean     n std_err .config              
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1  1500    30 accuracy binary     0.813    10  0.0122 Preprocessor1_Model19
## 2  1000    40 accuracy binary     0.812    10  0.0111 Preprocessor1_Model23
## 3   500    40 accuracy binary     0.812    10  0.0125 Preprocessor1_Model22
## 4  1000    30 accuracy binary     0.812    10  0.0130 Preprocessor1_Model18
## 5   500    30 accuracy binary     0.812    10  0.0127 Preprocessor1_Model17</code></pre>
<p>Select the best model in terms of accuracy and finalize the model.</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="decision-trees.html#cb296-1" tabindex="-1"></a>oj_rf<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_rf<span class="sc">$</span>tune_grid, <span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb296-2"><a href="decision-trees.html#cb296-2" tabindex="-1"></a></span>
<span id="cb296-3"><a href="decision-trees.html#cb296-3" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb296-4"><a href="decision-trees.html#cb296-4" tabindex="-1"></a>oj_rf<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(oj_rf<span class="sc">$</span>workflow, oj_rf<span class="sc">$</span>best_tune)</span>
<span id="cb296-5"><a href="decision-trees.html#cb296-5" tabindex="-1"></a></span>
<span id="cb296-6"><a href="decision-trees.html#cb296-6" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb296-7"><a href="decision-trees.html#cb296-7" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb296-8"><a href="decision-trees.html#cb296-8" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb296-9"><a href="decision-trees.html#cb296-9" tabindex="-1"></a>  oj_rf<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb296-10"><a href="decision-trees.html#cb296-10" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span></code></pre></div>
<p>There is no single tree to visualize, and you can’t even produce a VIP. Let’s look at the performance on the holdout data set. <code>collect_metrics()</code> shows the accuracy and ROC AUC metrics. The accuracy is slightly lower than the single tree, but ROC AUC is higher than both the single tree and bagged trees.</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="decision-trees.html#cb297-1" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 × 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.847 Preprocessor1_Model1
## 2 roc_auc  binary         0.935 Preprocessor1_Model1</code></pre>
<p>You can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="decision-trees.html#cb299-1" tabindex="-1"></a>oj_rf<span class="sc">$</span>confmat <span class="ot">&lt;-</span></span>
<span id="cb299-2"><a href="decision-trees.html#cb299-2" tabindex="-1"></a>  oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb299-3"><a href="decision-trees.html#cb299-3" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb299-4"><a href="decision-trees.html#cb299-4" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> Purchase, <span class="at">estimate =</span> .pred_class)</span>
<span id="cb299-5"><a href="decision-trees.html#cb299-5" tabindex="-1"></a></span>
<span id="cb299-6"><a href="decision-trees.html#cb299-6" tabindex="-1"></a>oj_rf<span class="sc">$</span>confmat</span>
<span id="cb299-7"><a href="decision-trees.html#cb299-7" tabindex="-1"></a><span class="do">##           Truth</span></span>
<span id="cb299-8"><a href="decision-trees.html#cb299-8" tabindex="-1"></a><span class="do">## Prediction  CH  MM</span></span>
<span id="cb299-9"><a href="decision-trees.html#cb299-9" tabindex="-1"></a><span class="do">##         CH 116  18</span></span>
<span id="cb299-10"><a href="decision-trees.html#cb299-10" tabindex="-1"></a><span class="do">##         MM  15  66</span></span>
<span id="cb299-11"><a href="decision-trees.html#cb299-11" tabindex="-1"></a></span>
<span id="cb299-12"><a href="decision-trees.html#cb299-12" tabindex="-1"></a><span class="fu">summary</span>(oj_rf<span class="sc">$</span>confmat)</span>
<span id="cb299-13"><a href="decision-trees.html#cb299-13" tabindex="-1"></a><span class="do">## # A tibble: 13 × 3</span></span>
<span id="cb299-14"><a href="decision-trees.html#cb299-14" tabindex="-1"></a><span class="do">##    .metric              .estimator .estimate</span></span>
<span id="cb299-15"><a href="decision-trees.html#cb299-15" tabindex="-1"></a><span class="do">##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb299-16"><a href="decision-trees.html#cb299-16" tabindex="-1"></a><span class="do">##  1 accuracy             binary         0.847</span></span>
<span id="cb299-17"><a href="decision-trees.html#cb299-17" tabindex="-1"></a><span class="do">##  2 kap                  binary         0.676</span></span>
<span id="cb299-18"><a href="decision-trees.html#cb299-18" tabindex="-1"></a><span class="do">##  3 sens                 binary         0.885</span></span>
<span id="cb299-19"><a href="decision-trees.html#cb299-19" tabindex="-1"></a><span class="do">##  4 spec                 binary         0.786</span></span>
<span id="cb299-20"><a href="decision-trees.html#cb299-20" tabindex="-1"></a><span class="do">##  5 ppv                  binary         0.866</span></span>
<span id="cb299-21"><a href="decision-trees.html#cb299-21" tabindex="-1"></a><span class="do">##  6 npv                  binary         0.815</span></span>
<span id="cb299-22"><a href="decision-trees.html#cb299-22" tabindex="-1"></a><span class="do">##  7 mcc                  binary         0.676</span></span>
<span id="cb299-23"><a href="decision-trees.html#cb299-23" tabindex="-1"></a><span class="do">##  8 j_index              binary         0.671</span></span>
<span id="cb299-24"><a href="decision-trees.html#cb299-24" tabindex="-1"></a><span class="do">##  9 bal_accuracy         binary         0.836</span></span>
<span id="cb299-25"><a href="decision-trees.html#cb299-25" tabindex="-1"></a><span class="do">## 10 detection_prevalence binary         0.623</span></span>
<span id="cb299-26"><a href="decision-trees.html#cb299-26" tabindex="-1"></a><span class="do">## 11 precision            binary         0.866</span></span>
<span id="cb299-27"><a href="decision-trees.html#cb299-27" tabindex="-1"></a><span class="do">## 12 recall               binary         0.885</span></span>
<span id="cb299-28"><a href="decision-trees.html#cb299-28" tabindex="-1"></a><span class="do">## 13 f_meas               binary         0.875</span></span>
<span id="cb299-29"><a href="decision-trees.html#cb299-29" tabindex="-1"></a></span>
<span id="cb299-30"><a href="decision-trees.html#cb299-30" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb299-31"><a href="decision-trees.html#cb299-31" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb299-32"><a href="decision-trees.html#cb299-32" tabindex="-1"></a>  <span class="fu">select</span>(Purchase, .pred_class) <span class="sc">%&gt;%</span></span>
<span id="cb299-33"><a href="decision-trees.html#cb299-33" tabindex="-1"></a>  <span class="fu">plot</span>(</span>
<span id="cb299-34"><a href="decision-trees.html#cb299-34" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">&quot;Random Forest: Predicted vs. Actual&quot;</span>,</span>
<span id="cb299-35"><a href="decision-trees.html#cb299-35" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">&quot;Actual&quot;</span>,</span>
<span id="cb299-36"><a href="decision-trees.html#cb299-36" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;Predicted&quot;</span></span>
<span id="cb299-37"><a href="decision-trees.html#cb299-37" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-202-1.png" width="672" /></p>
<p>The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is 0.935.</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="decision-trees.html#cb300-1" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb300-2"><a href="decision-trees.html#cb300-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb300-3"><a href="decision-trees.html#cb300-3" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb300-4"><a href="decision-trees.html#cb300-4" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb300-5"><a href="decision-trees.html#cb300-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;OJ Random Forest ROC Curve&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-203-1.png" width="672" /></p>
<p>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="decision-trees.html#cb301-1" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb301-2"><a href="decision-trees.html#cb301-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb301-3"><a href="decision-trees.html#cb301-3" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb301-4"><a href="decision-trees.html#cb301-4" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb301-5"><a href="decision-trees.html#cb301-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;OJ Random Forest Gain Curve&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-204-1.png" width="672" /></p>
</div>
<div id="random-forest-regression-tree" class="section level4 hasAnchor" number="6.4.0.2">
<h4><span class="header-section-number">6.4.0.2</span> Random Forest Regression Tree<a href="decision-trees.html#random-forest-regression-tree" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>I’ll predict <code>Sales</code> from the <code>Carseats</code> data set again, this time with <code>parsnip::rand_forest()</code>.</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="decision-trees.html#cb302-1" tabindex="-1"></a>cs_rf <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb302-2"><a href="decision-trees.html#cb302-2" tabindex="-1"></a></span>
<span id="cb302-3"><a href="decision-trees.html#cb302-3" tabindex="-1"></a><span class="co"># `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and</span></span>
<span id="cb302-4"><a href="decision-trees.html#cb302-4" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let&#39;s</span></span>
<span id="cb302-5"><a href="decision-trees.html#cb302-5" tabindex="-1"></a><span class="co"># optimize just `trees` and `min_n`.</span></span>
<span id="cb302-6"><a href="decision-trees.html#cb302-6" tabindex="-1"></a>cs_rf<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb302-7"><a href="decision-trees.html#cb302-7" tabindex="-1"></a>  <span class="fu">rand_forest</span>(</span>
<span id="cb302-8"><a href="decision-trees.html#cb302-8" tabindex="-1"></a>    <span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb302-9"><a href="decision-trees.html#cb302-9" tabindex="-1"></a>    <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb302-10"><a href="decision-trees.html#cb302-10" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb302-11"><a href="decision-trees.html#cb302-11" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;ranger&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb302-12"><a href="decision-trees.html#cb302-12" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb302-13"><a href="decision-trees.html#cb302-13" tabindex="-1"></a></span>
<span id="cb302-14"><a href="decision-trees.html#cb302-14" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb302-15"><a href="decision-trees.html#cb302-15" tabindex="-1"></a>cs_rf<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb302-16"><a href="decision-trees.html#cb302-16" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb302-17"><a href="decision-trees.html#cb302-17" tabindex="-1"></a>  <span class="fu">add_model</span>(cs_rf<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb302-18"><a href="decision-trees.html#cb302-18" tabindex="-1"></a>  <span class="fu">add_formula</span>(Sales <span class="sc">~</span> .)</span>
<span id="cb302-19"><a href="decision-trees.html#cb302-19" tabindex="-1"></a></span>
<span id="cb302-20"><a href="decision-trees.html#cb302-20" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb302-21"><a href="decision-trees.html#cb302-21" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb302-22"><a href="decision-trees.html#cb302-22" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb302-23"><a href="decision-trees.html#cb302-23" tabindex="-1"></a>cs_rf<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb302-24"><a href="decision-trees.html#cb302-24" tabindex="-1"></a>  cs_rf<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb302-25"><a href="decision-trees.html#cb302-25" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb302-26"><a href="decision-trees.html#cb302-26" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(cs_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb302-27"><a href="decision-trees.html#cb302-27" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">trees</span>(), <span class="fu">min_n</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb302-28"><a href="decision-trees.html#cb302-28" tabindex="-1"></a>  )</span>
<span id="cb302-29"><a href="decision-trees.html#cb302-29" tabindex="-1"></a></span>
<span id="cb302-30"><a href="decision-trees.html#cb302-30" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb302-31"><a href="decision-trees.html#cb302-31" tabindex="-1"></a>cs_rf<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb302-32"><a href="decision-trees.html#cb302-32" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb302-33"><a href="decision-trees.html#cb302-33" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trees =</span> <span class="fu">factor</span>(trees)) <span class="sc">%&gt;%</span></span>
<span id="cb302-34"><a href="decision-trees.html#cb302-34" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> min_n, <span class="at">y =</span> mean, <span class="at">color =</span> trees)) <span class="sc">+</span></span>
<span id="cb302-35"><a href="decision-trees.html#cb302-35" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb302-36"><a href="decision-trees.html#cb302-36" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb302-37"><a href="decision-trees.html#cb302-37" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/fit-rf-reg-1.png" width="672" /></p>
<p>The best models in terms of RMSE was 2000 tree with at least 2 data points per node.</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="decision-trees.html#cb303-1" tabindex="-1"></a>cs_rf<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 8
##   trees min_n .metric .estimator  mean     n std_err .config              
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1  1000     2 rmse    standard    1.72    10  0.0707 Preprocessor1_Model03
## 2   500     2 rmse    standard    1.73    10  0.0724 Preprocessor1_Model02
## 3  1500     2 rmse    standard    1.73    10  0.0696 Preprocessor1_Model04
## 4  2000     2 rmse    standard    1.73    10  0.0687 Preprocessor1_Model05
## 5  1500    11 rmse    standard    1.77    10  0.0681 Preprocessor1_Model09</code></pre>
<p>Select the best model in terms of rmse and finalize the model.</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="decision-trees.html#cb305-1" tabindex="-1"></a>cs_rf<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_rf<span class="sc">$</span>tune_grid, <span class="st">&quot;rmse&quot;</span>)</span>
<span id="cb305-2"><a href="decision-trees.html#cb305-2" tabindex="-1"></a></span>
<span id="cb305-3"><a href="decision-trees.html#cb305-3" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb305-4"><a href="decision-trees.html#cb305-4" tabindex="-1"></a>cs_rf<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_rf<span class="sc">$</span>workflow, cs_rf<span class="sc">$</span>best_tune)</span>
<span id="cb305-5"><a href="decision-trees.html#cb305-5" tabindex="-1"></a></span>
<span id="cb305-6"><a href="decision-trees.html#cb305-6" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb305-7"><a href="decision-trees.html#cb305-7" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb305-8"><a href="decision-trees.html#cb305-8" tabindex="-1"></a>cs_rf<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb305-9"><a href="decision-trees.html#cb305-9" tabindex="-1"></a>  cs_rf<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb305-10"><a href="decision-trees.html#cb305-10" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span></code></pre></div>
<p><code>collect_metrics()</code> returns the RMSE, <span class="math inline">\(RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})\)</span> and the model <span class="math inline">\(R^2\)</span>. The RMSE of 1.78 in the test data set is pretty good considering the standard deviation of <code>Sales</code> is 2.74.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="decision-trees.html#cb306-1" tabindex="-1"></a>cs_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 × 4
##   .metric .estimator .estimate .config             
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard       1.78  Preprocessor1_Model1
## 2 rsq     standard       0.665 Preprocessor1_Model1</code></pre>
<p>Here is a predicted vs actual plot.</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="decision-trees.html#cb308-1" tabindex="-1"></a>cs_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb308-2"><a href="decision-trees.html#cb308-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb308-3"><a href="decision-trees.html#cb308-3" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb308-4"><a href="decision-trees.html#cb308-4" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">&quot;cadetblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb308-5"><a href="decision-trees.html#cb308-5" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">formula =</span> <span class="st">&quot;y~x&quot;</span>) <span class="sc">+</span></span>
<span id="cb308-6"><a href="decision-trees.html#cb308-6" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb308-7"><a href="decision-trees.html#cb308-7" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Carseats Random Forest, Predicted vs Actual&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-208-1.png" width="672" /></p>
</div>
</div>
<div id="gradient-boosting" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Gradient Boosting<a href="decision-trees.html#gradient-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Note</strong>: I learned gradient boosting from <a href="https://explained.ai/gradient-boosting/L2-loss.html">explained.ai</a>.</p>
<p>Gradient boosting machine (GBM) is an additive modeling algorithm that gradually builds a composite model by iteratively adding <em>M</em> weak sub-models based on the performance of the prior iteration’s composite,</p>
<p><span class="math display">\[F_M(x) = \sum_m^M f_m(x).\]</span></p>
<p>The idea is to fit a weak model, then replace the response values with the residuals from that model, and fit another model. Adding the residual prediction model to the original response prediction model produces a more accurate model. GBM repeats this process over and over, running new models to predict the residuals of the previous composite models, and adding the results to produce new composites. With each iteration, the model becomes stronger and stronger. The successive trees are usually weighted to slow down the learning rate. “Shrinkage” reduces the influence of each individual tree and leaves space for future trees to improve the model.</p>
<p><span class="math display">\[F_M(x) = f_0 + \eta\sum_{m = 1}^M f_m(x).\]</span></p>
<p>The smaller the learning rate, <span class="math inline">\(\eta\)</span>, the larger the number of trees, <span class="math inline">\(M\)</span>. <span class="math inline">\(\eta\)</span> and <span class="math inline">\(M\)</span> are hyperparameters. Other constraints to the trees are usually applied as additional hyperparameters, including, tree depth, number of nodes, minimum observations per split, and minimum improvement to loss.</p>
<p>The name “gradient boosting” refers to the <em>boosting</em> of a model with a <em>gradient</em>. Each round of training builds a <em>weak learner</em> and uses the residuals to calculate a gradient, the partial derivative of the loss function. Gradient boosting “descends the gradient” to adjust the model parameters to reduce the error in the next round of training.</p>
<p>In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error. GBM continues until it reaches maximum number of trees or an acceptable error level.</p>
<div id="gradient-boosting-classification-tree" class="section level4 hasAnchor" number="6.5.0.1">
<h4><span class="header-section-number">6.5.0.1</span> Gradient Boosting Classification Tree<a href="decision-trees.html#gradient-boosting-classification-tree" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Leaning by example, I’ll predict <code>Purchase</code> from the <code>OJ</code> data set again, this time using the bagging with <code>parsnip::boost_tree()</code></p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="decision-trees.html#cb309-1" tabindex="-1"></a>oj_xgb <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb309-2"><a href="decision-trees.html#cb309-2" tabindex="-1"></a></span>
<span id="cb309-3"><a href="decision-trees.html#cb309-3" tabindex="-1"></a><span class="co"># `boost_tree` has 8 hyperparameters. Let&#39;s optimize just `trees` and `min_n`.</span></span>
<span id="cb309-4"><a href="decision-trees.html#cb309-4" tabindex="-1"></a>oj_xgb<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb309-5"><a href="decision-trees.html#cb309-5" tabindex="-1"></a>  <span class="fu">boost_tree</span>(</span>
<span id="cb309-6"><a href="decision-trees.html#cb309-6" tabindex="-1"></a>    <span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb309-7"><a href="decision-trees.html#cb309-7" tabindex="-1"></a>    <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb309-8"><a href="decision-trees.html#cb309-8" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb309-9"><a href="decision-trees.html#cb309-9" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;xgboost&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb309-10"><a href="decision-trees.html#cb309-10" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;classification&quot;</span>)</span>
<span id="cb309-11"><a href="decision-trees.html#cb309-11" tabindex="-1"></a></span>
<span id="cb309-12"><a href="decision-trees.html#cb309-12" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb309-13"><a href="decision-trees.html#cb309-13" tabindex="-1"></a>oj_xgb<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb309-14"><a href="decision-trees.html#cb309-14" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb309-15"><a href="decision-trees.html#cb309-15" tabindex="-1"></a>  <span class="fu">add_model</span>(oj_xgb<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb309-16"><a href="decision-trees.html#cb309-16" tabindex="-1"></a>  <span class="fu">add_formula</span>(Purchase <span class="sc">~</span> .)</span>
<span id="cb309-17"><a href="decision-trees.html#cb309-17" tabindex="-1"></a></span>
<span id="cb309-18"><a href="decision-trees.html#cb309-18" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb309-19"><a href="decision-trees.html#cb309-19" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb309-20"><a href="decision-trees.html#cb309-20" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb309-21"><a href="decision-trees.html#cb309-21" tabindex="-1"></a>oj_xgb<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb309-22"><a href="decision-trees.html#cb309-22" tabindex="-1"></a>  oj_xgb<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb309-23"><a href="decision-trees.html#cb309-23" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb309-24"><a href="decision-trees.html#cb309-24" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(oj_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb309-25"><a href="decision-trees.html#cb309-25" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">trees</span>(), <span class="fu">min_n</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb309-26"><a href="decision-trees.html#cb309-26" tabindex="-1"></a>  )</span>
<span id="cb309-27"><a href="decision-trees.html#cb309-27" tabindex="-1"></a></span>
<span id="cb309-28"><a href="decision-trees.html#cb309-28" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb309-29"><a href="decision-trees.html#cb309-29" tabindex="-1"></a>oj_xgb<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb309-30"><a href="decision-trees.html#cb309-30" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb309-31"><a href="decision-trees.html#cb309-31" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trees =</span> <span class="fu">factor</span>(trees)) <span class="sc">%&gt;%</span></span>
<span id="cb309-32"><a href="decision-trees.html#cb309-32" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> min_n, <span class="at">y =</span> mean, <span class="at">color =</span> trees)) <span class="sc">+</span></span>
<span id="cb309-33"><a href="decision-trees.html#cb309-33" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb309-34"><a href="decision-trees.html#cb309-34" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb309-35"><a href="decision-trees.html#cb309-35" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/fit-xgboost-class-1.png" width="672" /></p>
<p>The best models in terms of accuracy and ROC was the trees of 1000 and any minimum node size of 40.</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="decision-trees.html#cb310-1" tabindex="-1"></a>oj_xgb<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="st">&quot;accuracy&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 8
##   trees min_n .metric  .estimator  mean     n std_err .config              
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1   500    30 accuracy binary     0.814    10 0.0146  Preprocessor1_Model17
## 2  1500    30 accuracy binary     0.814    10 0.0121  Preprocessor1_Model19
## 3  2000    30 accuracy binary     0.814    10 0.0121  Preprocessor1_Model20
## 4     1     2 accuracy binary     0.814    10 0.00999 Preprocessor1_Model01
## 5  1000    30 accuracy binary     0.811    10 0.0149  Preprocessor1_Model18</code></pre>
<p>Select the best model in terms of accuracy and finalize the model.</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="decision-trees.html#cb312-1" tabindex="-1"></a>oj_xgb<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_xgb<span class="sc">$</span>tune_grid, <span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb312-2"><a href="decision-trees.html#cb312-2" tabindex="-1"></a></span>
<span id="cb312-3"><a href="decision-trees.html#cb312-3" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb312-4"><a href="decision-trees.html#cb312-4" tabindex="-1"></a>oj_xgb<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(oj_xgb<span class="sc">$</span>workflow, oj_xgb<span class="sc">$</span>best_tune)</span>
<span id="cb312-5"><a href="decision-trees.html#cb312-5" tabindex="-1"></a></span>
<span id="cb312-6"><a href="decision-trees.html#cb312-6" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb312-7"><a href="decision-trees.html#cb312-7" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb312-8"><a href="decision-trees.html#cb312-8" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb312-9"><a href="decision-trees.html#cb312-9" tabindex="-1"></a>  oj_xgb<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb312-10"><a href="decision-trees.html#cb312-10" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span></code></pre></div>
<p>There is no single tree to visualize, and you can’t even produce a VIP. Let’s look at the performance on the holdout data set. <code>collect_metrics()</code> shows the accuracy and ROC AUC metrics.</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="decision-trees.html#cb313-1" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 × 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.842 Preprocessor1_Model1
## 2 roc_auc  binary         0.932 Preprocessor1_Model1</code></pre>
<p>You can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="decision-trees.html#cb315-1" tabindex="-1"></a>oj_xgb<span class="sc">$</span>confmat <span class="ot">&lt;-</span></span>
<span id="cb315-2"><a href="decision-trees.html#cb315-2" tabindex="-1"></a>  oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb315-3"><a href="decision-trees.html#cb315-3" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb315-4"><a href="decision-trees.html#cb315-4" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> Purchase, <span class="at">estimate =</span> .pred_class)</span>
<span id="cb315-5"><a href="decision-trees.html#cb315-5" tabindex="-1"></a></span>
<span id="cb315-6"><a href="decision-trees.html#cb315-6" tabindex="-1"></a>oj_xgb<span class="sc">$</span>confmat</span>
<span id="cb315-7"><a href="decision-trees.html#cb315-7" tabindex="-1"></a><span class="do">##           Truth</span></span>
<span id="cb315-8"><a href="decision-trees.html#cb315-8" tabindex="-1"></a><span class="do">## Prediction  CH  MM</span></span>
<span id="cb315-9"><a href="decision-trees.html#cb315-9" tabindex="-1"></a><span class="do">##         CH 114  17</span></span>
<span id="cb315-10"><a href="decision-trees.html#cb315-10" tabindex="-1"></a><span class="do">##         MM  17  67</span></span>
<span id="cb315-11"><a href="decision-trees.html#cb315-11" tabindex="-1"></a></span>
<span id="cb315-12"><a href="decision-trees.html#cb315-12" tabindex="-1"></a><span class="fu">summary</span>(oj_xgb<span class="sc">$</span>confmat)</span>
<span id="cb315-13"><a href="decision-trees.html#cb315-13" tabindex="-1"></a><span class="do">## # A tibble: 13 × 3</span></span>
<span id="cb315-14"><a href="decision-trees.html#cb315-14" tabindex="-1"></a><span class="do">##    .metric              .estimator .estimate</span></span>
<span id="cb315-15"><a href="decision-trees.html#cb315-15" tabindex="-1"></a><span class="do">##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb315-16"><a href="decision-trees.html#cb315-16" tabindex="-1"></a><span class="do">##  1 accuracy             binary         0.842</span></span>
<span id="cb315-17"><a href="decision-trees.html#cb315-17" tabindex="-1"></a><span class="do">##  2 kap                  binary         0.668</span></span>
<span id="cb315-18"><a href="decision-trees.html#cb315-18" tabindex="-1"></a><span class="do">##  3 sens                 binary         0.870</span></span>
<span id="cb315-19"><a href="decision-trees.html#cb315-19" tabindex="-1"></a><span class="do">##  4 spec                 binary         0.798</span></span>
<span id="cb315-20"><a href="decision-trees.html#cb315-20" tabindex="-1"></a><span class="do">##  5 ppv                  binary         0.870</span></span>
<span id="cb315-21"><a href="decision-trees.html#cb315-21" tabindex="-1"></a><span class="do">##  6 npv                  binary         0.798</span></span>
<span id="cb315-22"><a href="decision-trees.html#cb315-22" tabindex="-1"></a><span class="do">##  7 mcc                  binary         0.668</span></span>
<span id="cb315-23"><a href="decision-trees.html#cb315-23" tabindex="-1"></a><span class="do">##  8 j_index              binary         0.668</span></span>
<span id="cb315-24"><a href="decision-trees.html#cb315-24" tabindex="-1"></a><span class="do">##  9 bal_accuracy         binary         0.834</span></span>
<span id="cb315-25"><a href="decision-trees.html#cb315-25" tabindex="-1"></a><span class="do">## 10 detection_prevalence binary         0.609</span></span>
<span id="cb315-26"><a href="decision-trees.html#cb315-26" tabindex="-1"></a><span class="do">## 11 precision            binary         0.870</span></span>
<span id="cb315-27"><a href="decision-trees.html#cb315-27" tabindex="-1"></a><span class="do">## 12 recall               binary         0.870</span></span>
<span id="cb315-28"><a href="decision-trees.html#cb315-28" tabindex="-1"></a><span class="do">## 13 f_meas               binary         0.870</span></span>
<span id="cb315-29"><a href="decision-trees.html#cb315-29" tabindex="-1"></a></span>
<span id="cb315-30"><a href="decision-trees.html#cb315-30" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb315-31"><a href="decision-trees.html#cb315-31" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb315-32"><a href="decision-trees.html#cb315-32" tabindex="-1"></a>  <span class="fu">select</span>(Purchase, .pred_class) <span class="sc">%&gt;%</span></span>
<span id="cb315-33"><a href="decision-trees.html#cb315-33" tabindex="-1"></a>  <span class="fu">plot</span>(</span>
<span id="cb315-34"><a href="decision-trees.html#cb315-34" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">&quot;XGBoost: Predicted vs. Actual&quot;</span>,</span>
<span id="cb315-35"><a href="decision-trees.html#cb315-35" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">&quot;Actual&quot;</span>,</span>
<span id="cb315-36"><a href="decision-trees.html#cb315-36" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;Predicted&quot;</span></span>
<span id="cb315-37"><a href="decision-trees.html#cb315-37" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-212-1.png" width="672" /></p>
<p>The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is 0.932.</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="decision-trees.html#cb316-1" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb316-2"><a href="decision-trees.html#cb316-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb316-3"><a href="decision-trees.html#cb316-3" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb316-4"><a href="decision-trees.html#cb316-4" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb316-5"><a href="decision-trees.html#cb316-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;OJ XGBoost ROC Curve&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-213-1.png" width="672" /></p>
<p>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="decision-trees.html#cb317-1" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb317-2"><a href="decision-trees.html#cb317-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb317-3"><a href="decision-trees.html#cb317-3" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb317-4"><a href="decision-trees.html#cb317-4" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb317-5"><a href="decision-trees.html#cb317-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;OJ XGBoost Gain Curve&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-214-1.png" width="672" /></p>
</div>
<div id="gradient-boosting-regression-tree" class="section level4 hasAnchor" number="6.5.0.2">
<h4><span class="header-section-number">6.5.0.2</span> Gradient Boosting Regression Tree<a href="decision-trees.html#gradient-boosting-regression-tree" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>I’ll predict <code>Sales</code> from the <code>Carseats</code> data set again, this time with <code>parsnip::rand_forest()</code>.</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="decision-trees.html#cb318-1" tabindex="-1"></a>cs_xgb <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb318-2"><a href="decision-trees.html#cb318-2" tabindex="-1"></a></span>
<span id="cb318-3"><a href="decision-trees.html#cb318-3" tabindex="-1"></a><span class="co"># `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and</span></span>
<span id="cb318-4"><a href="decision-trees.html#cb318-4" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let&#39;s</span></span>
<span id="cb318-5"><a href="decision-trees.html#cb318-5" tabindex="-1"></a><span class="co"># optimize just `trees` and `min_n`.</span></span>
<span id="cb318-6"><a href="decision-trees.html#cb318-6" tabindex="-1"></a>cs_xgb<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb318-7"><a href="decision-trees.html#cb318-7" tabindex="-1"></a>  <span class="fu">boost_tree</span>(</span>
<span id="cb318-8"><a href="decision-trees.html#cb318-8" tabindex="-1"></a>    <span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb318-9"><a href="decision-trees.html#cb318-9" tabindex="-1"></a>    <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb318-10"><a href="decision-trees.html#cb318-10" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb318-11"><a href="decision-trees.html#cb318-11" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;xgboost&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb318-12"><a href="decision-trees.html#cb318-12" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb318-13"><a href="decision-trees.html#cb318-13" tabindex="-1"></a></span>
<span id="cb318-14"><a href="decision-trees.html#cb318-14" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb318-15"><a href="decision-trees.html#cb318-15" tabindex="-1"></a>cs_xgb<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb318-16"><a href="decision-trees.html#cb318-16" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb318-17"><a href="decision-trees.html#cb318-17" tabindex="-1"></a>  <span class="fu">add_model</span>(cs_xgb<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb318-18"><a href="decision-trees.html#cb318-18" tabindex="-1"></a>  <span class="fu">add_formula</span>(Sales <span class="sc">~</span> .)</span>
<span id="cb318-19"><a href="decision-trees.html#cb318-19" tabindex="-1"></a></span>
<span id="cb318-20"><a href="decision-trees.html#cb318-20" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb318-21"><a href="decision-trees.html#cb318-21" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb318-22"><a href="decision-trees.html#cb318-22" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb318-23"><a href="decision-trees.html#cb318-23" tabindex="-1"></a>cs_xgb<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb318-24"><a href="decision-trees.html#cb318-24" tabindex="-1"></a>  cs_xgb<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb318-25"><a href="decision-trees.html#cb318-25" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb318-26"><a href="decision-trees.html#cb318-26" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(cs_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb318-27"><a href="decision-trees.html#cb318-27" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">trees</span>(), <span class="fu">min_n</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb318-28"><a href="decision-trees.html#cb318-28" tabindex="-1"></a>  )</span>
<span id="cb318-29"><a href="decision-trees.html#cb318-29" tabindex="-1"></a></span>
<span id="cb318-30"><a href="decision-trees.html#cb318-30" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb318-31"><a href="decision-trees.html#cb318-31" tabindex="-1"></a>cs_xgb<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb318-32"><a href="decision-trees.html#cb318-32" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb318-33"><a href="decision-trees.html#cb318-33" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trees =</span> <span class="fu">factor</span>(trees)) <span class="sc">%&gt;%</span></span>
<span id="cb318-34"><a href="decision-trees.html#cb318-34" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> min_n, <span class="at">y =</span> mean, <span class="at">color =</span> trees)) <span class="sc">+</span></span>
<span id="cb318-35"><a href="decision-trees.html#cb318-35" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb318-36"><a href="decision-trees.html#cb318-36" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb318-37"><a href="decision-trees.html#cb318-37" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/fit-xgboost-reg-1.png" width="672" /></p>
<p>The best models in terms of RMSE was 500 trees with at least 21 data points per node.</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="decision-trees.html#cb319-1" tabindex="-1"></a>cs_xgb<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="st">&quot;rmse&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 8
##   trees min_n .metric .estimator  mean     n std_err .config              
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1   500    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model07
## 2  1000    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model08
## 3  1500    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model09
## 4  2000    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model10
## 5   500    40 rmse    standard    1.38    10  0.0759 Preprocessor1_Model22</code></pre>
<p>Select the best model in terms of rmse and finalize the model.</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="decision-trees.html#cb321-1" tabindex="-1"></a>cs_xgb<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_xgb<span class="sc">$</span>tune_grid, <span class="st">&quot;rmse&quot;</span>)</span>
<span id="cb321-2"><a href="decision-trees.html#cb321-2" tabindex="-1"></a></span>
<span id="cb321-3"><a href="decision-trees.html#cb321-3" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb321-4"><a href="decision-trees.html#cb321-4" tabindex="-1"></a>cs_xgb<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_xgb<span class="sc">$</span>workflow, cs_xgb<span class="sc">$</span>best_tune)</span>
<span id="cb321-5"><a href="decision-trees.html#cb321-5" tabindex="-1"></a></span>
<span id="cb321-6"><a href="decision-trees.html#cb321-6" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb321-7"><a href="decision-trees.html#cb321-7" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb321-8"><a href="decision-trees.html#cb321-8" tabindex="-1"></a>cs_xgb<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb321-9"><a href="decision-trees.html#cb321-9" tabindex="-1"></a>  cs_rf<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb321-10"><a href="decision-trees.html#cb321-10" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span></code></pre></div>
<p><code>collect_metrics()</code> returns the RMSE, <span class="math inline">\(RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})\)</span> and the model <span class="math inline">\(R^2\)</span>. The RMSE of 1.79 in the test data set is pretty good considering the standard deviation of <code>Sales</code> is 2.74.</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="decision-trees.html#cb322-1" tabindex="-1"></a>cs_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 × 4
##   .metric .estimator .estimate .config             
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 rmse    standard       1.79  Preprocessor1_Model1
## 2 rsq     standard       0.660 Preprocessor1_Model1</code></pre>
<p>Here is a predicted vs actual plot.</p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="decision-trees.html#cb324-1" tabindex="-1"></a>cs_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb324-2"><a href="decision-trees.html#cb324-2" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb324-3"><a href="decision-trees.html#cb324-3" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb324-4"><a href="decision-trees.html#cb324-4" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">&quot;cadetblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb324-5"><a href="decision-trees.html#cb324-5" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">formula =</span> <span class="st">&quot;y~x&quot;</span>) <span class="sc">+</span></span>
<span id="cb324-6"><a href="decision-trees.html#cb324-6" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb324-7"><a href="decision-trees.html#cb324-7" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Carseats Random Forest, Predicted vs Actual&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-218-1.png" width="672" /></p>

</div>
</div>
</div>
<h3>References<a href="references-1.html#references-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Fawcett2005" class="csl-entry">
Fawcett, Tom. 2005. <em>An Introduction to ROC Analysis</em>. ELSEVIER. <a href="https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf">https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf</a>.
</div>
<div id="ref-James2013" class="csl-entry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning: With Applications in r</em>. 1st ed. New York, NY: Springer. <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/book.html">http://faculty.marshall.usc.edu/gareth-james/ISL/book.html</a>.
</div>
<div id="ref-Kuhn2016" class="csl-entry">
Kuhn, Max, and Kjell Johnson. 2016. <em>Applied Predictive Modeling</em>. 1st ed. New York, NY: Springer. <a href="http://appliedpredictivemodeling.com/">http://appliedpredictivemodeling.com/</a>.
</div>
<div id="ref-Therneau2019" class="csl-entry">
Therneau, Terry, and Elizabeth Atkinson. 2019. <em>An Introduction to Recursive Partitioning Using the RPART Routines</em>. Boca Raton, Florida: Chapman; Hall/CRC. <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="15">
<li id="fn15"><p>These notes rely on <a href="https://online.stat.psu.edu/stat508/">PSU STAT 508</a> and on the <a href="https://www.tidymodels.org/start/">tidymodels vignettes</a> for model building.<a href="decision-trees.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>I think for a production system, you would take one more step: re-fit the final model to the entire data set using the hyperparemeter settings from the winning model.<a href="decision-trees.html#fnref16" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regularization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="support-vector-machines.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["supervised-ml.pdf", "supervised-ml.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
