<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Regularization | Supervised Machine Learning</title>
  <meta name="description" content="These are my personal notes related to supervised machine learning techniques." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Regularization | Supervised Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These are my personal notes related to supervised machine learning techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Regularization | Supervised Machine Learning" />
  
  <meta name="twitter:description" content="These are my personal notes related to supervised machine learning techniques." />
  

<meta name="author" content="Michael Foley" />


<meta name="date" content="2024-01-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="non-linear-models.html"/>
<link rel="next" href="decision-trees.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet" />
<script src="libs/tabwid-1.1.3/tabwid.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Supervised Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>1</b> Ordinary Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#parameter-estimation"><i class="fa fa-check"></i><b>1.1</b> Parameter Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#example"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-assumptions"><i class="fa fa-check"></i><b>1.2</b> Model Assumptions</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#multicollinearity"><i class="fa fa-check"></i><b>1.2.1</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#prediction"><i class="fa fa-check"></i><b>1.3</b> Prediction</a></li>
<li class="chapter" data-level="1.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#inference"><i class="fa fa-check"></i><b>1.4</b> Inference</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#t-test"><i class="fa fa-check"></i><b>1.4.1</b> <em>t</em>-Test</a></li>
<li class="chapter" data-level="1.4.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#f-test"><i class="fa fa-check"></i><b>1.4.2</b> <em>F</em>-Test</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#interpretation"><i class="fa fa-check"></i><b>1.5</b> Interpretation</a></li>
<li class="chapter" data-level="1.6" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-validation"><i class="fa fa-check"></i><b>1.6</b> Model Validation</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#accuracy-metrics"><i class="fa fa-check"></i><b>1.6.1</b> Accuracy Metrics</a></li>
<li class="chapter" data-level="1.6.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#r-squared"><i class="fa fa-check"></i><b>1.6.2</b> R-Squared</a></li>
<li class="chapter" data-level="1.6.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#cross-validation"><i class="fa fa-check"></i><b>1.6.3</b> Cross-Validation</a></li>
<li class="chapter" data-level="1.6.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#gain-curve"><i class="fa fa-check"></i><b>1.6.4</b> Gain Curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html"><i class="fa fa-check"></i><b>2</b> Generalized Linear Models (GLM)</a>
<ul>
<li class="chapter" data-level="2.1" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#binomiallogistic"><i class="fa fa-check"></i><b>2.1</b> Binomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs1"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#fit-the-model"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpretation-1"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#evaluate-the-fit"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#multinomiallogistic"><i class="fa fa-check"></i><b>2.2</b> Multinomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs2"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#fit-the-model-1"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpretation-2"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions-1"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#evaluate-the-fit-1"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting-1"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#ordinallogistic"><i class="fa fa-check"></i><b>2.3</b> Ordinal Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs3"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#fit-the-model-2"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions-2"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#evaluate-the-fit-2"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpret-results"><i class="fa fa-check"></i>Interpret Results</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting-2"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#poissonregression"><i class="fa fa-check"></i><b>2.4</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs4"><i class="fa fa-check"></i>Case Study 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html"><i class="fa fa-check"></i><b>3</b> Mixed Effects Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#lme"><i class="fa fa-check"></i><b>3.1</b> Linear Mixed Effects</a>
<ul>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#lme1"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#fit-the-model-3"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#interpretation-3"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#model-assumptions-1"><i class="fa fa-check"></i>Model Assumptions</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#evaluate-the-fit-3"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#reporting-3"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="non-linear-models.html"><a href="non-linear-models.html"><i class="fa fa-check"></i><b>4</b> Non-linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="non-linear-models.html"><a href="non-linear-models.html#splines"><i class="fa fa-check"></i><b>4.1</b> Splines</a></li>
<li class="chapter" data-level="4.2" data-path="non-linear-models.html"><a href="non-linear-models.html#mars"><i class="fa fa-check"></i><b>4.2</b> MARS</a></li>
<li class="chapter" data-level="4.3" data-path="non-linear-models.html"><a href="non-linear-models.html#gam"><i class="fa fa-check"></i><b>4.3</b> GAM</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>5</b> Regularization</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regularization.html"><a href="regularization.html#ridge"><i class="fa fa-check"></i><b>5.1</b> Ridge</a></li>
<li class="chapter" data-level="5.2" data-path="regularization.html"><a href="regularization.html#lasso"><i class="fa fa-check"></i><b>5.2</b> Lasso</a></li>
<li class="chapter" data-level="5.3" data-path="regularization.html"><a href="regularization.html#elastic-net"><i class="fa fa-check"></i><b>5.3</b> Elastic Net</a></li>
<li class="chapter" data-level="" data-path="regularization.html"><a href="regularization.html#model-summary"><i class="fa fa-check"></i>Model Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>6</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="6.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-tree"><i class="fa fa-check"></i><b>6.1</b> Classification Tree</a>
<ul>
<li class="chapter" data-level="" data-path="decision-trees.html"><a href="decision-trees.html#measuring-performance"><i class="fa fa-check"></i>Measuring Performance</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-tree"><i class="fa fa-check"></i><b>6.2</b> Regression Tree</a>
<ul>
<li class="chapter" data-level="" data-path="decision-trees.html"><a href="decision-trees.html#measuring-performance-1"><i class="fa fa-check"></i>Measuring Performance</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>6.3</b> Bagged Trees</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="decision-trees.html"><a href="decision-trees.html#bagged-classification-tree"><i class="fa fa-check"></i><b>6.3.1</b> Bagged Classification Tree</a></li>
<li class="chapter" data-level="6.3.2" data-path="decision-trees.html"><a href="decision-trees.html#bagging-regression-tree"><i class="fa fa-check"></i><b>6.3.2</b> Bagging Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="decision-trees.html"><a href="decision-trees.html#random-forests"><i class="fa fa-check"></i><b>6.4</b> Random Forests</a></li>
<li class="chapter" data-level="6.5" data-path="decision-trees.html"><a href="decision-trees.html#gradient-boosting"><i class="fa fa-check"></i><b>6.5</b> Gradient Boosting</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>7</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="7.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximal-margin-classifier"><i class="fa fa-check"></i><b>7.1</b> Maximal Margin Classifier</a></li>
<li class="chapter" data-level="7.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-classifier"><i class="fa fa-check"></i><b>7.2</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="7.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-machines-1"><i class="fa fa-check"></i><b>7.3</b> Support Vector Machines</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="BayesRegression.html"><a href="BayesRegression.html"><i class="fa fa-check"></i><b>8</b> Bayesian Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="BayesRegression.html"><a href="BayesRegression.html#compared-to-frequentist-regression"><i class="fa fa-check"></i><b>8.1</b> Compared to Frequentist Regression</a></li>
<li class="chapter" data-level="8.2" data-path="BayesRegression.html"><a href="BayesRegression.html#model-evaluation"><i class="fa fa-check"></i><b>8.2</b> Model Evaluation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="BayesRegression.html"><a href="BayesRegression.html#model-comparison"><i class="fa fa-check"></i><b>8.2.1</b> Model Comparison</a></li>
<li class="chapter" data-level="8.2.2" data-path="BayesRegression.html"><a href="BayesRegression.html#visualization"><i class="fa fa-check"></i><b>8.2.2</b> Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="EMMs.html"><a href="EMMs.html"><i class="fa fa-check"></i><b>9</b> Estimated Marginal Means</a>
<ul>
<li class="chapter" data-level="9.1" data-path="EMMs.html"><a href="EMMs.html#references"><i class="fa fa-check"></i><b>9.1</b> References</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supervised Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regularization" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Regularization<a href="regularization.html#regularization" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>These notes are from this <a href="https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net">tutorial</a> on DataCamp, the <a href="https://campus.datacamp.com/courses/machine-learning-toolbox">Machine Learning Toolbox</a> DataCamp class, and Interpretable Machine Learning <span class="citation">(<a href="#ref-Molner2020">Molnar 2020</a>)</span>.</p>
<p>Regularization is a set of methods that manage the bias-variance trade-off problem in linear regression.</p>
<p>The linear regression model is <span class="math inline">\(Y = X \beta + \epsilon\)</span>, where <span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span>. OLS estimates the coefficients by minimizing the loss function</p>
<p><span class="math display">\[L = \sum_{i = 1}^n \left(y_i - x_i^{&#39;} \hat\beta \right)^2.\]</span></p>
<p>The resulting estimate for the coefficients is</p>
<p><span class="math display">\[\hat{\beta} = \left(X&#39;X\right)^{-1}\left(X&#39;Y\right).\]</span></p>
<p>There are two important characteristics of any estimator: its <em>bias</em> and its <em>variance</em>. For OLS, these are</p>
<p><span class="math display">\[Bias(\hat{\beta}) = E(\hat{\beta}) - \beta = 0\]</span>
and</p>
<p><span class="math display">\[Var(\hat{\beta}) = \sigma^2(X&#39;X)^{-1}\]</span></p>
<p>where the unknown population variance <span class="math inline">\(\sigma^2\)</span> is estimated from the residuals</p>
<p><span class="math display">\[\hat\sigma^2 = \frac{\epsilon&#39; \epsilon}{n - k}.\]</span></p>
<p>The OLS estimator is unbiased, but can have a large variance when the predictor variables are highly correlated with each other, or when there are many predictors (notice how <span class="math inline">\(\hat{\sigma}^2\)</span> increases as <span class="math inline">\(k \rightarrow n\)</span>). Stepwise selection balances the trade-off by eliminating variables, but this throws away information. <em>Regularization</em> keeps all the predictors, but reduces coefficient magnitudes to reduce variance at the expense of some bias.</p>
<p>In the sections below, I’ll use the <code>mtcars</code> data set to predict <code>mpg</code> from the other variables using the <code>caret::glmnet()</code> function. <code>glmnet()</code> uses penalized maximum likelihood to fit generalized linear models such as ridge, lasso, and elastic net. I’ll compare the model performances by creating a training and validation set, and a common <code>trainControl</code> object to make sure the models use the same observations in the cross-validation folds.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="regularization.html#cb214-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb214-2"><a href="regularization.html#cb214-2" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb214-3"><a href="regularization.html#cb214-3" tabindex="-1"></a></span>
<span id="cb214-4"><a href="regularization.html#cb214-4" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;mtcars&quot;</span>)</span>
<span id="cb214-5"><a href="regularization.html#cb214-5" tabindex="-1"></a></span>
<span id="cb214-6"><a href="regularization.html#cb214-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb214-7"><a href="regularization.html#cb214-7" tabindex="-1"></a>partition <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(mtcars<span class="sc">$</span>mpg, <span class="at">p =</span> <span class="fl">0.8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb214-8"><a href="regularization.html#cb214-8" tabindex="-1"></a>training <span class="ot">&lt;-</span> mtcars[partition, ]</span>
<span id="cb214-9"><a href="regularization.html#cb214-9" tabindex="-1"></a>testing <span class="ot">&lt;-</span> mtcars[<span class="sc">-</span>partition, ]</span>
<span id="cb214-10"><a href="regularization.html#cb214-10" tabindex="-1"></a></span>
<span id="cb214-11"><a href="regularization.html#cb214-11" tabindex="-1"></a>train_control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(</span>
<span id="cb214-12"><a href="regularization.html#cb214-12" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb214-13"><a href="regularization.html#cb214-13" tabindex="-1"></a>  <span class="at">number =</span> <span class="dv">5</span>,  </span>
<span id="cb214-14"><a href="regularization.html#cb214-14" tabindex="-1"></a>  <span class="at">repeats =</span> <span class="dv">5</span>,</span>
<span id="cb214-15"><a href="regularization.html#cb214-15" tabindex="-1"></a>  <span class="at">savePredictions =</span> <span class="st">&quot;final&quot;</span>  <span class="co"># saves predictions from optimal tuning parameters</span></span>
<span id="cb214-16"><a href="regularization.html#cb214-16" tabindex="-1"></a>)</span></code></pre></div>
<div id="ridge" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Ridge<a href="regularization.html#ridge" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ridge regression estimates the linear model coefficients by minimizing an augmented loss function which includes a term, <span class="math inline">\(\lambda\)</span>, that penalizes the magnitude of the coefficient estimates,</p>
<p><span class="math display">\[L = \sum_{i = 1}^n \left(y_i - x_i^{&#39;} \hat\beta \right)^2 + \lambda \sum_{j=1}^k \hat{\beta}_j^2.\]</span></p>
<p>The resulting estimate for the coefficients is</p>
<p><span class="math display">\[\hat{\beta} = \left(X&#39;X + \lambda I\right)^{-1}\left(X&#39;Y \right).\]</span></p>
<p>As <span class="math inline">\(\lambda \rightarrow 0\)</span>, ridge regression approaches OLS. The bias and variance for the ridge estimator are</p>
<p><span class="math display">\[Bias(\hat{\beta}) = -\lambda \left(X&#39;X + \lambda I \right)^{-1} \beta\]</span>
<span class="math display">\[Var(\hat{\beta}) = \sigma^2 \left(X&#39;X + \lambda I \right)^{-1}X&#39;X \left(X&#39;X + \lambda I \right)^{-1}\]</span></p>
<p>The estimator bias increases with <span class="math inline">\(\lambda\)</span> and the estimator variance decreases with <span class="math inline">\(\lambda\)</span>. The optimal level for <span class="math inline">\(\lambda\)</span> is the one that minimizes the root mean squared error (RMSE) or the Akaike or Bayesian Information Criterion (AIC or BIC), or R-squared.</p>
<div id="example-1" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="regularization.html#example-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Specify <code>alpha = 0</code> in a tuning grid for ridge regression (the following sections reveal how alpha distinguishes ridge, lasso, and elastic net). Note that I standardize the predictors in the <code>preProcess</code> step - ridge regression requires standardization.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="regularization.html#cb215-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb215-2"><a href="regularization.html#cb215-2" tabindex="-1"></a>mdl_ridge <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb215-3"><a href="regularization.html#cb215-3" tabindex="-1"></a>  mpg <span class="sc">~</span> .,</span>
<span id="cb215-4"><a href="regularization.html#cb215-4" tabindex="-1"></a>  <span class="at">data =</span> training,</span>
<span id="cb215-5"><a href="regularization.html#cb215-5" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>,</span>
<span id="cb215-6"><a href="regularization.html#cb215-6" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,  <span class="co"># Choose from RMSE, RSquared, AIC, BIC, ...others?</span></span>
<span id="cb215-7"><a href="regularization.html#cb215-7" tabindex="-1"></a>  <span class="at">preProcess =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</span>
<span id="cb215-8"><a href="regularization.html#cb215-8" tabindex="-1"></a>  <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(</span>
<span id="cb215-9"><a href="regularization.html#cb215-9" tabindex="-1"></a>    <span class="at">.alpha =</span> <span class="dv">0</span>,  <span class="co"># optimize a ridge regression</span></span>
<span id="cb215-10"><a href="regularization.html#cb215-10" tabindex="-1"></a>    <span class="at">.lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">101</span>)</span>
<span id="cb215-11"><a href="regularization.html#cb215-11" tabindex="-1"></a>  ),</span>
<span id="cb215-12"><a href="regularization.html#cb215-12" tabindex="-1"></a>  <span class="at">trControl =</span> train_control</span>
<span id="cb215-13"><a href="regularization.html#cb215-13" tabindex="-1"></a>  )</span>
<span id="cb215-14"><a href="regularization.html#cb215-14" tabindex="-1"></a>mdl_ridge</span></code></pre></div>
<pre><code>## glmnet 
## 
## 28 samples
## 10 predictors
## 
## Pre-processing: centered (10), scaled (10) 
## Resampling: Cross-Validated (5 fold, repeated 5 times) 
## Summary of sample sizes: 21, 24, 22, 21, 24, 21, ... 
## Resampling results across tuning parameters:
## 
##   lambda  RMSE      Rsquared   MAE     
##   0.00    2.554751  0.8788756  2.234470
##   0.05    2.554751  0.8788756  2.234470
##   0.10    2.554751  0.8788756  2.234470
##   0.15    2.554751  0.8788756  2.234470
##   0.20    2.554751  0.8788756  2.234470
##   0.25    2.554751  0.8788756  2.234470
##   0.30    2.554751  0.8788756  2.234470
##   0.35    2.554751  0.8788756  2.234470
##   0.40    2.554751  0.8788756  2.234470
##   0.45    2.554594  0.8789255  2.234683
##   0.50    2.552197  0.8791114  2.233333
##   0.55    2.541833  0.8793654  2.225176
##   0.60    2.529913  0.8798195  2.215643
##   0.65    2.519126  0.8803007  2.206810
##   0.70    2.509440  0.8807077  2.198506
##   0.75    2.500730  0.8810505  2.190795
##   0.80    2.492807  0.8813491  2.183807
##   0.85    2.485563  0.8816140  2.177294
##   0.90    2.478997  0.8818405  2.171425
##   0.95    2.472966  0.8820389  2.165789
##   1.00    2.467383  0.8822118  2.160407
##   1.05    2.462264  0.8823596  2.155277
##   1.10    2.457491  0.8824959  2.150329
##   1.15    2.453096  0.8826143  2.145577
##   1.20    2.448994  0.8827201  2.141010
##   1.25    2.445283  0.8828075  2.136689
##   1.30    2.441821  0.8828813  2.132518
##   1.35    2.438632  0.8829440  2.128488
##   1.40    2.435595  0.8829971  2.124567
##   1.45    2.432766  0.8830499  2.120776
##   1.50    2.430196  0.8830912  2.117202
##   1.55    2.427779  0.8831246  2.113911
##   1.60    2.425544  0.8831528  2.110735
##   1.65    2.423506  0.8831710  2.107696
##   1.70    2.421585  0.8831843  2.104720
##   1.75    2.419833  0.8831973  2.101850
##   1.80    2.418232  0.8832041  2.099074
##   1.85    2.416707  0.8832052  2.096356
##   1.90    2.415308  0.8832049  2.093724
##   1.95    2.414077  0.8831966  2.091421
##   2.00    2.412915  0.8831858  2.089232
##   2.05    2.411832  0.8831731  2.087084
##   2.10    2.410862  0.8831612  2.084999
##   2.15    2.410002  0.8831449  2.082982
##   2.20    2.409207  0.8831253  2.081117
##   2.25    2.408493  0.8831033  2.079633
##   2.30    2.407877  0.8830816  2.078216
##   2.35    2.407362  0.8830560  2.076947
##   2.40    2.406888  0.8830284  2.075738
##   2.45    2.406463  0.8829988  2.074538
##   2.50    2.406109  0.8829717  2.073390
##   2.55    2.405864  0.8829408  2.072316
##   2.60    2.405693  0.8829069  2.071291
##   2.65    2.405542  0.8828717  2.070256
##   2.70    2.405448  0.8828350  2.069254
##   2.75    2.405415  0.8827999  2.068291
##   2.80    2.405452  0.8827624  2.067369
##   2.85    2.405539  0.8827235  2.066542
##   2.90    2.405637  0.8826837  2.065770
##   2.95    2.405780  0.8826424  2.065242
##   3.00    2.405964  0.8826039  2.064761
##   3.05    2.406229  0.8825637  2.064336
##   3.10    2.406541  0.8825222  2.063945
##   3.15    2.406876  0.8824799  2.063540
##   3.20    2.407231  0.8824370  2.063120
##   3.25    2.407628  0.8823943  2.062722
##   3.30    2.408062  0.8823537  2.062348
##   3.35    2.408562  0.8823110  2.062118
##   3.40    2.409100  0.8822673  2.062037
##   3.45    2.409655  0.8822228  2.061944
##   3.50    2.410216  0.8821782  2.061828
##   3.55    2.410808  0.8821331  2.061723
##   3.60    2.411432  0.8820899  2.061639
##   3.65    2.412102  0.8820470  2.061577
##   3.70    2.412809  0.8820035  2.061543
##   3.75    2.413551  0.8819589  2.061568
##   3.80    2.414295  0.8819141  2.061655
##   3.85    2.415055  0.8818687  2.061734
##   3.90    2.415843  0.8818234  2.062062
##   3.95    2.416658  0.8817793  2.062424
##   4.00    2.417511  0.8817359  2.062798
##   4.05    2.418400  0.8816915  2.063183
##   4.10    2.419316  0.8816465  2.063584
##   4.15    2.420238  0.8816012  2.063970
##   4.20    2.421163  0.8815562  2.064598
##   4.25    2.422110  0.8815102  2.065248
##   4.30    2.423081  0.8814658  2.065910
##   4.35    2.424072  0.8814226  2.066657
##   4.40    2.425104  0.8813791  2.067521
##   4.45    2.426155  0.8813356  2.068385
##   4.50    2.427230  0.8812914  2.069250
##   4.55    2.428308  0.8812470  2.070099
##   4.60    2.429387  0.8812029  2.070929
##   4.65    2.430482  0.8811578  2.071757
##   4.70    2.431595  0.8811139  2.072827
##   4.75    2.432731  0.8810710  2.073935
##   4.80    2.433886  0.8810292  2.075036
##   4.85    2.435074  0.8809864  2.076150
##   4.90    2.436276  0.8809438  2.077259
##   4.95    2.437498  0.8809006  2.078371
##   5.00    2.438716  0.8808576  2.079462
## 
## Tuning parameter &#39;alpha&#39; was held constant at a value of 0
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 0 and lambda = 2.75.</code></pre>
<p>The model printout shows the RMSE, R-Squared, and mean absolute error (MAE) values at each lambda specified in the tuning grid. The final three lines summarize what happened. It did not tune alpha because I held it at 0 for ridge regression; it optimized using RMSE; and the optimal tuning values (at the minimum RMSE) were alpha = 0 and lambda = 2.75. You plot the model to see the tuning results.</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="regularization.html#cb217-1" tabindex="-1"></a><span class="fu">ggplot</span>(mdl_ridge) <span class="sc">+</span></span>
<span id="cb217-2"><a href="regularization.html#cb217-2" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Ridge Regression Parameter Tuning&quot;</span>, <span class="at">x =</span> <span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-154-1.png" width="672" /></p>
<p><code>varImp()</code> ranks the predictors by the absolute value of the coefficients in the tuned model. The most important variables here were <code>wt</code>, <code>disp</code>, and <code>am</code>.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="regularization.html#cb218-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">varImp</span>(mdl_ridge))</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-155-1.png" width="672" /></p>
</div>
</div>
<div id="lasso" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Lasso<a href="regularization.html#lasso" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lasso stands for “least absolute shrinkage and selection operator”. Like ridge, lasso adds a penalty for coefficients, but instead of penalizing the sum of squared coefficients (L2 penalty), lasso penalizes the sum of absolute values (L1 penalty). As a result, for high values of <span class="math inline">\(\lambda\)</span>, coefficients can be zeroed under lasso.</p>
<p>The loss function for lasso is</p>
<p><span class="math display">\[L = \sum_{i = 1}^n \left(y_i - x_i^{&#39;} \hat\beta \right)^2 + \lambda \sum_{j=1}^k \left| \hat{\beta}_j \right|.\]</span></p>
<div id="example-2" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="regularization.html#example-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Continuing with prediction of <code>mpg</code> from the other variables in the <code>mtcars</code> data set, follow the same steps as before, but with ridge regression. This time specify parameter <code>alpha = 1</code> for ridge regression (it was 0 for ridge, and for elastic net it will be something in between and require optimization).</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="regularization.html#cb219-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb219-2"><a href="regularization.html#cb219-2" tabindex="-1"></a>mdl_lasso <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb219-3"><a href="regularization.html#cb219-3" tabindex="-1"></a>  mpg <span class="sc">~</span> .,</span>
<span id="cb219-4"><a href="regularization.html#cb219-4" tabindex="-1"></a>  <span class="at">data =</span> training,</span>
<span id="cb219-5"><a href="regularization.html#cb219-5" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>,</span>
<span id="cb219-6"><a href="regularization.html#cb219-6" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb219-7"><a href="regularization.html#cb219-7" tabindex="-1"></a>  <span class="at">preProcess =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</span>
<span id="cb219-8"><a href="regularization.html#cb219-8" tabindex="-1"></a>  <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(</span>
<span id="cb219-9"><a href="regularization.html#cb219-9" tabindex="-1"></a>    <span class="at">.alpha =</span> <span class="dv">1</span>,  <span class="co"># optimize a lasso regression</span></span>
<span id="cb219-10"><a href="regularization.html#cb219-10" tabindex="-1"></a>    <span class="at">.lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">101</span>)</span>
<span id="cb219-11"><a href="regularization.html#cb219-11" tabindex="-1"></a>  ),</span>
<span id="cb219-12"><a href="regularization.html#cb219-12" tabindex="-1"></a>  <span class="at">trControl =</span> train_control</span>
<span id="cb219-13"><a href="regularization.html#cb219-13" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,
## : There were missing values in resampled performance measures.</code></pre>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="regularization.html#cb221-1" tabindex="-1"></a>mdl_lasso<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##    alpha lambda
## 14     1   0.65</code></pre>
<p>The summary output shows the model did not tune alpha because I held it at 1 for lasso regression. The optimal tuning values (at the minimum RMSE) were alpha = 1 and lambda = 0.65. You can see the RMSE minimum on the the plot.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="regularization.html#cb223-1" tabindex="-1"></a><span class="fu">ggplot</span>(mdl_ridge) <span class="sc">+</span></span>
<span id="cb223-2"><a href="regularization.html#cb223-2" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Lasso Regression Parameter Tuning&quot;</span>, <span class="at">x =</span> <span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-157-1.png" width="672" /></p>
</div>
</div>
<div id="elastic-net" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Elastic Net<a href="regularization.html#elastic-net" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Elastic Net combines the penalties of ridge and lasso to get the best of both worlds. The loss function for elastic net is</p>
<p><span class="math display">\[L = \frac{\sum_{i = 1}^n \left(y_i - x_i^{&#39;} \hat\beta \right)^2}{2n} + \lambda \frac{1 - \alpha}{2}\sum_{j=1}^k \hat{\beta}_j^2 + \lambda \alpha\left| \hat{\beta}_j \right|.\]</span></p>
<p>In this loss function, new parameter <span class="math inline">\(\alpha\)</span> is a “mixing” parameter that balances the two approaches. If <span class="math inline">\(\alpha\)</span> is zero, you are back to ridge regression, and if <span class="math inline">\(\alpha\)</span> is one, you are back to lasso.</p>
<div id="example-3" class="section level4 unnumbered hasAnchor">
<h4>Example<a href="regularization.html#example-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Continuing with prediction of <code>mpg</code> from the other variables in the <code>mtcars</code> data set, follow the same steps as before, but with elastic net regression there are two parameters to optimize: <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\alpha\)</span>.</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="regularization.html#cb224-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb224-2"><a href="regularization.html#cb224-2" tabindex="-1"></a>mdl_elnet <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb224-3"><a href="regularization.html#cb224-3" tabindex="-1"></a>  mpg <span class="sc">~</span> .,</span>
<span id="cb224-4"><a href="regularization.html#cb224-4" tabindex="-1"></a>  <span class="at">data =</span> training,</span>
<span id="cb224-5"><a href="regularization.html#cb224-5" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>,</span>
<span id="cb224-6"><a href="regularization.html#cb224-6" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb224-7"><a href="regularization.html#cb224-7" tabindex="-1"></a>  <span class="at">preProcess =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</span>
<span id="cb224-8"><a href="regularization.html#cb224-8" tabindex="-1"></a>  <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(</span>
<span id="cb224-9"><a href="regularization.html#cb224-9" tabindex="-1"></a>    <span class="at">.alpha =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">10</span>),  <span class="co"># optimize an elnet regression</span></span>
<span id="cb224-10"><a href="regularization.html#cb224-10" tabindex="-1"></a>    <span class="at">.lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">101</span>)</span>
<span id="cb224-11"><a href="regularization.html#cb224-11" tabindex="-1"></a>  ),</span>
<span id="cb224-12"><a href="regularization.html#cb224-12" tabindex="-1"></a>  <span class="at">trControl =</span> train_control</span>
<span id="cb224-13"><a href="regularization.html#cb224-13" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,
## : There were missing values in resampled performance measures.</code></pre>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="regularization.html#cb226-1" tabindex="-1"></a>mdl_elnet<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##    alpha lambda
## 56     0   2.75</code></pre>
<p>The optimal tuning values (at the mininum RMSE) were alpha = 0.0 and lambda = 2.75, so the mix is 100% ridge, 0% lasso. You can see the RMSE minimum on the the plot. Alpha is on the horizontal axis and the different lambdas are shown as separate series.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="regularization.html#cb228-1" tabindex="-1"></a><span class="fu">ggplot</span>(mdl_elnet) <span class="sc">+</span></span>
<span id="cb228-2"><a href="regularization.html#cb228-2" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Elastic Net Regression Parameter Tuning&quot;</span>, <span class="at">x =</span> <span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<pre><code>## Warning: The shape palette can deal with a maximum of 6 discrete values because
## more than 6 becomes difficult to discriminate; you have 10. Consider
## specifying shapes manually if you must have them.</code></pre>
<pre><code>## Warning: Removed 404 rows containing missing values (`geom_point()`).</code></pre>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-159-1.png" width="672" /></p>
</div>
</div>
<div id="model-summary" class="section level2 unnumbered hasAnchor">
<h2>Model Summary<a href="regularization.html#model-summary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Make predictions on the validation data set for each of the three models.</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="regularization.html#cb231-1" tabindex="-1"></a>pr_ridge <span class="ot">&lt;-</span> <span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(mdl_ridge, <span class="at">newdata =</span> testing), <span class="at">obs =</span> testing<span class="sc">$</span>mpg)</span>
<span id="cb231-2"><a href="regularization.html#cb231-2" tabindex="-1"></a>pr_lasso <span class="ot">&lt;-</span> <span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(mdl_lasso, <span class="at">newdata =</span> testing), <span class="at">obs =</span> testing<span class="sc">$</span>mpg)</span>
<span id="cb231-3"><a href="regularization.html#cb231-3" tabindex="-1"></a>pr_elnet <span class="ot">&lt;-</span> <span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(mdl_elnet, <span class="at">newdata =</span> testing), <span class="at">obs =</span> testing<span class="sc">$</span>mpg)</span></code></pre></div>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="regularization.html#cb232-1" tabindex="-1"></a><span class="fu">rbind</span>(pr_ridge, pr_lasso, pr_elnet)</span></code></pre></div>
<pre><code>##              RMSE  Rsquared      MAE
## pr_ridge 3.745350 0.9007955 2.761726
## pr_lasso 4.032868 0.9734421 3.009135
## pr_elnet 3.745350 0.9007955 2.761726</code></pre>
<p>It looks like ridge/elnet was the big winner today based on RMSE and MAE. Lasso had the best Rsquared though. On average, ridge/elnet will miss the true value of <code>mpg</code> by 3.75 mpg (RMSE) or 2.76 mpg (MAE). The model explains about 90% of the variation in <code>mpg</code>.</p>
<p>You can also compare the models by resampling.</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="regularization.html#cb234-1" tabindex="-1"></a>model.resamples <span class="ot">&lt;-</span> <span class="fu">resamples</span>(<span class="fu">list</span>(<span class="at">Ridge =</span> mdl_ridge,</span>
<span id="cb234-2"><a href="regularization.html#cb234-2" tabindex="-1"></a>                                  <span class="at">Lasso =</span> mdl_lasso,</span>
<span id="cb234-3"><a href="regularization.html#cb234-3" tabindex="-1"></a>                                  <span class="at">ELNet =</span> mdl_elnet))</span>
<span id="cb234-4"><a href="regularization.html#cb234-4" tabindex="-1"></a><span class="fu">summary</span>(model.resamples)</span></code></pre></div>
<pre><code>## 
## Call:
## summary.resamples(object = model.resamples)
## 
## Models: Ridge, Lasso, ELNet 
## Number of resamples: 25 
## 
## MAE 
##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## Ridge 0.6614059 1.587531 2.179524 2.068291 2.523411 3.527409    0
## Lasso 0.8148722 1.862340 2.237722 2.272930 2.596078 3.985098    0
## ELNet 0.6614059 1.587531 2.179524 2.068291 2.523411 3.527409    0
## 
## RMSE 
##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## Ridge 0.7325538 1.935884 2.462086 2.405415 2.820892 4.264528    0
## Lasso 0.9083703 2.138883 2.521894 2.623880 2.934442 4.506856    0
## ELNet 0.7325538 1.935884 2.462086 2.405415 2.820892 4.264528    0
## 
## Rsquared 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## Ridge 0.6919630 0.8357446 0.8904019 0.8827999 0.9382490 0.9791672    0
## Lasso 0.6263831 0.8100574 0.8656579 0.8593787 0.9407846 0.9955706    0
## ELNet 0.6919630 0.8357446 0.8904019 0.8827999 0.9382490 0.9791672    0</code></pre>
<p>You want the smallest mean RMSE, and a small range of RMSEs. Ridge/elnet had the smallest mean, and a relatively small range. Boxplots are a common way to visualize this information.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="regularization.html#cb236-1" tabindex="-1"></a><span class="fu">bwplot</span>(model.resamples, <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Model Comparison on Resamples&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-163-1.png" width="672" /></p>
<p>Now that you have identified the optimal model, capture its tuning parameters and refit the model to the entire data set.</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="regularization.html#cb237-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb237-2"><a href="regularization.html#cb237-2" tabindex="-1"></a>mdl_final <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb237-3"><a href="regularization.html#cb237-3" tabindex="-1"></a>  mpg <span class="sc">~</span> .,</span>
<span id="cb237-4"><a href="regularization.html#cb237-4" tabindex="-1"></a>  <span class="at">data =</span> training,</span>
<span id="cb237-5"><a href="regularization.html#cb237-5" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>,</span>
<span id="cb237-6"><a href="regularization.html#cb237-6" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb237-7"><a href="regularization.html#cb237-7" tabindex="-1"></a>  <span class="at">preProcess =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</span>
<span id="cb237-8"><a href="regularization.html#cb237-8" tabindex="-1"></a>  <span class="at">tuneGrid =</span> <span class="fu">data.frame</span>(</span>
<span id="cb237-9"><a href="regularization.html#cb237-9" tabindex="-1"></a>    <span class="at">.alpha =</span> mdl_ridge<span class="sc">$</span>bestTune<span class="sc">$</span>alpha,  <span class="co"># optimized hyperparameters</span></span>
<span id="cb237-10"><a href="regularization.html#cb237-10" tabindex="-1"></a>    <span class="at">.lambda =</span> mdl_ridge<span class="sc">$</span>bestTune<span class="sc">$</span>lambda),  <span class="co"># optimized hyperparameters</span></span>
<span id="cb237-11"><a href="regularization.html#cb237-11" tabindex="-1"></a>  <span class="at">trControl =</span> train_control</span>
<span id="cb237-12"><a href="regularization.html#cb237-12" tabindex="-1"></a>  )</span>
<span id="cb237-13"><a href="regularization.html#cb237-13" tabindex="-1"></a>mdl_final</span></code></pre></div>
<pre><code>## glmnet 
## 
## 28 samples
## 10 predictors
## 
## Pre-processing: centered (10), scaled (10) 
## Resampling: Cross-Validated (5 fold, repeated 5 times) 
## Summary of sample sizes: 22, 22, 23, 22, 23, 23, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   2.441704  0.8884221  2.115503
## 
## Tuning parameter &#39;alpha&#39; was held constant at a value of 0
## Tuning
##  parameter &#39;lambda&#39; was held constant at a value of 2.75</code></pre>
<p>The model is ready to predict on new data! Here are some final conclusions on the models.</p>
<ul>
<li>Lasso can set some coefficients to zero, thus performing variable selection.</li>
<li>Lasso and Ridge address multicollinearity differently: in ridge regression, the coefficients of correlated predictors are similar; In lasso, one of the correlated predictors has a larger coefficient, while the rest are (nearly) zeroed.</li>
<li>Lasso tends to do well if there are a small number of significant parameters and the others are close to zero. Ridge tends to work well if there are many large parameters of about the same value.</li>
<li>In practice, you don’t know which will be best, so run cross-validation pick the best.</li>
</ul>

</div>
</div>
<h3>References<a href="references-1.html#references-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Molner2020" class="csl-entry">
Molnar, Christoph. 2020. <em>Interpretable Machine Learning</em>. <a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="non-linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="decision-trees.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["supervised-ml.pdf", "supervised-ml.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
