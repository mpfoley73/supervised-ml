<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Support Vector Machines | Supervised Machine Learning</title>
  <meta name="description" content="These are my personal notes related to supervised machine learning techniques." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Support Vector Machines | Supervised Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These are my personal notes related to supervised machine learning techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Support Vector Machines | Supervised Machine Learning" />
  
  <meta name="twitter:description" content="These are my personal notes related to supervised machine learning techniques." />
  

<meta name="author" content="Michael Foley" />


<meta name="date" content="2024-01-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="decision-trees.html"/>
<link rel="next" href="BayesRegression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Supervised Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>1</b> Ordinary Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#parameter-estimation"><i class="fa fa-check"></i><b>1.1</b> Parameter Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#example"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-assumptions"><i class="fa fa-check"></i><b>1.2</b> Model Assumptions</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#multicollinearity"><i class="fa fa-check"></i><b>1.2.1</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#prediction"><i class="fa fa-check"></i><b>1.3</b> Prediction</a></li>
<li class="chapter" data-level="1.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#inference"><i class="fa fa-check"></i><b>1.4</b> Inference</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#t-test"><i class="fa fa-check"></i><b>1.4.1</b> <em>t</em>-Test</a></li>
<li class="chapter" data-level="1.4.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#f-test"><i class="fa fa-check"></i><b>1.4.2</b> <em>F</em>-Test</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#interpretation"><i class="fa fa-check"></i><b>1.5</b> Interpretation</a></li>
<li class="chapter" data-level="1.6" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-validation"><i class="fa fa-check"></i><b>1.6</b> Model Validation</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#accuracy-metrics"><i class="fa fa-check"></i><b>1.6.1</b> Accuracy Metrics</a></li>
<li class="chapter" data-level="1.6.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#r-squared"><i class="fa fa-check"></i><b>1.6.2</b> R-Squared</a></li>
<li class="chapter" data-level="1.6.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#cross-validation"><i class="fa fa-check"></i><b>1.6.3</b> Cross-Validation</a></li>
<li class="chapter" data-level="1.6.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#gain-curve"><i class="fa fa-check"></i><b>1.6.4</b> Gain Curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html"><i class="fa fa-check"></i><b>2</b> Generalized Linear Models (GLM)</a>
<ul>
<li class="chapter" data-level="2.1" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#binomiallogistic"><i class="fa fa-check"></i><b>2.1</b> Binomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs1"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#fit-the-model"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpretation-1"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#evaluate-the-fit"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#multinomiallogistic"><i class="fa fa-check"></i><b>2.2</b> Multinomial Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs2"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#fit-the-model-1"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpretation-2"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions-1"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#evaluate-the-fit-1"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting-1"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#ordinallogistic"><i class="fa fa-check"></i><b>2.3</b> Ordinal Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs3"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#fit-the-model-2"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions-2"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#evaluate-the-fit-2"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpret-results"><i class="fa fa-check"></i>Interpret Results</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting-2"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#poissonregression"><i class="fa fa-check"></i><b>2.4</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs4"><i class="fa fa-check"></i>Case Study 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html"><i class="fa fa-check"></i><b>3</b> Mixed Effects Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#lme"><i class="fa fa-check"></i><b>3.1</b> Linear Mixed Effects</a>
<ul>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#lme1"><i class="fa fa-check"></i>Case Study</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#fit-the-model-3"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#interpretation-3"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#model-assumptions-1"><i class="fa fa-check"></i>Model Assumptions</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#evaluate-the-fit-3"><i class="fa fa-check"></i>Evaluate the Fit</a></li>
<li class="chapter" data-level="" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html#reporting-3"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="non-linear-models.html"><a href="non-linear-models.html"><i class="fa fa-check"></i><b>4</b> Non-linear Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="non-linear-models.html"><a href="non-linear-models.html#splines"><i class="fa fa-check"></i><b>4.1</b> Splines</a></li>
<li class="chapter" data-level="4.2" data-path="non-linear-models.html"><a href="non-linear-models.html#mars"><i class="fa fa-check"></i><b>4.2</b> MARS</a></li>
<li class="chapter" data-level="4.3" data-path="non-linear-models.html"><a href="non-linear-models.html#gam"><i class="fa fa-check"></i><b>4.3</b> GAM</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>5</b> Regularization</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regularization.html"><a href="regularization.html#ridge"><i class="fa fa-check"></i><b>5.1</b> Ridge</a></li>
<li class="chapter" data-level="5.2" data-path="regularization.html"><a href="regularization.html#lasso"><i class="fa fa-check"></i><b>5.2</b> Lasso</a></li>
<li class="chapter" data-level="5.3" data-path="regularization.html"><a href="regularization.html#elastic-net"><i class="fa fa-check"></i><b>5.3</b> Elastic Net</a></li>
<li class="chapter" data-level="" data-path="regularization.html"><a href="regularization.html#model-summary"><i class="fa fa-check"></i>Model Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>6</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="6.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-tree"><i class="fa fa-check"></i><b>6.1</b> Classification Tree</a>
<ul>
<li class="chapter" data-level="" data-path="decision-trees.html"><a href="decision-trees.html#measuring-performance"><i class="fa fa-check"></i>Measuring Performance</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-tree"><i class="fa fa-check"></i><b>6.2</b> Regression Tree</a>
<ul>
<li class="chapter" data-level="" data-path="decision-trees.html"><a href="decision-trees.html#measuring-performance-1"><i class="fa fa-check"></i>Measuring Performance</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>6.3</b> Bagged Trees</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="decision-trees.html"><a href="decision-trees.html#bagged-classification-tree"><i class="fa fa-check"></i><b>6.3.1</b> Bagged Classification Tree</a></li>
<li class="chapter" data-level="6.3.2" data-path="decision-trees.html"><a href="decision-trees.html#bagging-regression-tree"><i class="fa fa-check"></i><b>6.3.2</b> Bagging Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="decision-trees.html"><a href="decision-trees.html#random-forests"><i class="fa fa-check"></i><b>6.4</b> Random Forests</a></li>
<li class="chapter" data-level="6.5" data-path="decision-trees.html"><a href="decision-trees.html#gradient-boosting"><i class="fa fa-check"></i><b>6.5</b> Gradient Boosting</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>7</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="7.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximal-margin-classifier"><i class="fa fa-check"></i><b>7.1</b> Maximal Margin Classifier</a></li>
<li class="chapter" data-level="7.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-classifier"><i class="fa fa-check"></i><b>7.2</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="7.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-machines-1"><i class="fa fa-check"></i><b>7.3</b> Support Vector Machines</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="BayesRegression.html"><a href="BayesRegression.html"><i class="fa fa-check"></i><b>8</b> Bayesian Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="BayesRegression.html"><a href="BayesRegression.html#compared-to-frequentist-regression"><i class="fa fa-check"></i><b>8.1</b> Compared to Frequentist Regression</a></li>
<li class="chapter" data-level="8.2" data-path="BayesRegression.html"><a href="BayesRegression.html#model-evaluation"><i class="fa fa-check"></i><b>8.2</b> Model Evaluation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="BayesRegression.html"><a href="BayesRegression.html#model-comparison"><i class="fa fa-check"></i><b>8.2.1</b> Model Comparison</a></li>
<li class="chapter" data-level="8.2.2" data-path="BayesRegression.html"><a href="BayesRegression.html#visualization"><i class="fa fa-check"></i><b>8.2.2</b> Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="EMMs.html"><a href="EMMs.html"><i class="fa fa-check"></i><b>9</b> Estimated Marginal Means</a>
<ul>
<li class="chapter" data-level="9.1" data-path="EMMs.html"><a href="EMMs.html#references"><i class="fa fa-check"></i><b>9.1</b> References</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supervised Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="support-vector-machines" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Support Vector Machines<a href="support-vector-machines.html#support-vector-machines" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>These notes rely on <span class="citation">(<a href="#ref-James2013">James et al. 2013</a>)</span>, <span class="citation">(<a href="#ref-Hastie2017">Hastie, Tibshirani, and Friedman 2017</a>)</span>, <span class="citation">(<a href="#ref-Kuhn2016">Kuhn and Johnson 2016</a>)</span>, <a href="https://online.stat.psu.edu/stat508/">PSU STAT 508</a>, and the <a href="https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf">e1071 SVM vignette</a>.</p>
<p>Support Vector Machines (SVM) is a classification model that maps observations as points in space so that the categories are divided by as wide a gap as possible. New observations can then be mapped into the space for prediction. The SVM algorithm finds the optimal separating hyperplane using a nonlinear mapping. The hyperplane is defined by the observations that lie within a margin optimized by a cost hyperparameter. These observations are called the <em>support vectors</em>.</p>
<p>SVM is an extension of the <em>support vector classifier</em> which in turn is a generalization of the simple and intuitive <em>maximal margin classifier</em>. The maximal margin classifier is defined for cases where the data can be separated by a linear boundary (uncommon). The support vector classifier generalizes the maximal margin classifier by introducing a margin which permits some observations to land on wrong side of the hyperplane. The support vector machine generalizes still more by introducing non-linear hyperplanes. The best way to understand SVM is to start with the maximal margin classifier and work up.</p>
<p>I’ll learn by example, using the <code>ISLR::Default</code> data set to predict which customers will default on their credit card debt from its 3 predictor variables. I’m using this <a href="https://dataaspirant.com/support-vector-machine-classifier-implementation-r-caret-package/#:~:text=For%20machine%20learning%2C%20caret%20package%20is%20a%20nice,build%20a%20hyperplane%20separating%20data%20for%20different%20classes.">Dataaspirant</a> tutorial for guidance. The three predictors are <code>student</code> (Yes|No), current credit card <code>balance</code>, and annual <code>income</code>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="support-vector-machines.html#cb1-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="support-vector-machines.html#cb1-2" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb1-3"><a href="support-vector-machines.html#cb1-3" tabindex="-1"></a><span class="fu">library</span>(janitor)</span>
<span id="cb1-4"><a href="support-vector-machines.html#cb1-4" tabindex="-1"></a><span class="fu">library</span>(scales)</span>
<span id="cb1-5"><a href="support-vector-machines.html#cb1-5" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb1-6"><a href="support-vector-machines.html#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="support-vector-machines.html#cb1-7" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1-8"><a href="support-vector-machines.html#cb1-8" tabindex="-1"></a><span class="fu">library</span>(recipes)</span>
<span id="cb1-9"><a href="support-vector-machines.html#cb1-9" tabindex="-1"></a><span class="fu">library</span>(tictoc)</span>
<span id="cb1-10"><a href="support-vector-machines.html#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="support-vector-machines.html#cb1-11" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-12"><a href="support-vector-machines.html#cb1-12" tabindex="-1"></a>dat <span class="ot">&lt;-</span> ISLR<span class="sc">::</span>Default <span class="sc">%&gt;%</span> <span class="fu">slice_sample</span>(<span class="at">n =</span> <span class="dv">1000</span>)</span>
<span id="cb1-13"><a href="support-vector-machines.html#cb1-13" tabindex="-1"></a><span class="fu">glimpse</span>(dat)</span></code></pre></div>
<pre><code>## Rows: 1,000
## Columns: 4
## $ default &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, No…
## $ student &lt;fct&gt; No, No, Yes, No, Yes, Yes, No, No, No, No, No, Yes, No, No, Ye…
## $ balance &lt;dbl&gt; 559.4686, 905.6415, 1806.5517, 1615.2250, 1229.4415, 1723.2161…
## $ income  &lt;dbl&gt; 61187.81, 47271.35, 17648.20, 55219.52, 12158.04, 23279.00, 40…</code></pre>
<p>I’ll build and compare models the customary way, splitting <code>dat</code> (<em>n</em> = 1,000) into <code>dat_train</code> (80%, <em>n</em> = 800) to fit models, and <code>dat_test</code> (20%, <em>n</em> = 200) to evaluate.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="support-vector-machines.html#cb3-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb3-2"><a href="support-vector-machines.html#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="support-vector-machines.html#cb3-3" tabindex="-1"></a><span class="co"># initial_split() partitions by `strata`, then samples `prop` percent. This </span></span>
<span id="cb3-4"><a href="support-vector-machines.html#cb3-4" tabindex="-1"></a><span class="co"># ensures the outcome is proportionally represented in data sets.</span></span>
<span id="cb3-5"><a href="support-vector-machines.html#cb3-5" tabindex="-1"></a>dat_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(dat, <span class="at">prop =</span> .<span class="dv">8</span>, <span class="at">strata =</span> default)</span>
<span id="cb3-6"><a href="support-vector-machines.html#cb3-6" tabindex="-1"></a>dat_train <span class="ot">&lt;-</span> <span class="fu">training</span>(dat_split)</span>
<span id="cb3-7"><a href="support-vector-machines.html#cb3-7" tabindex="-1"></a>dat_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(dat_split)</span></code></pre></div>
<p>Only 3.0% of applicants default, so this is a difficult prediction problem.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="support-vector-machines.html#cb4-1" tabindex="-1"></a><span class="fu">tabyl</span>(dat<span class="sc">$</span>default)</span></code></pre></div>
<pre><code>##  dat$default   n percent
##           No 970    0.97
##          Yes  30    0.03</code></pre>
<div id="maximal-margin-classifier" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Maximal Margin Classifier<a href="support-vector-machines.html#maximal-margin-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The maximal margin classifier is the optimal hyperplane defined in the (rare) case where two classes are <em>linearly separable</em>. Given an <span class="math inline">\(X_{n \times p}\)</span> predictor matrix with a binary response variable <span class="math inline">\(y \in \{-1, 1\}\)</span> it <em>might</em> be possible to define a <em>p</em>-dimensional hyperplane <span class="math inline">\(h(x) = \beta_0 + \beta_1x_1 + \beta_2x_2 \dots + \beta_px_p = X_i^{&#39;} \beta + \beta_0 = 0\)</span> such that all of the <span class="math inline">\(y_i = -1\)</span> observations fall on the negative side of the hyperplane and the <span class="math inline">\(y_i = +1\)</span> observations fall on the positive side:</p>
<p><span class="math display">\[y_i \left(x_i^{&#39;} \beta + \beta_0 \right) &gt; 0\]</span></p>
<p>This <em>separating hyperplane</em> is a simple classifier, and the magnitude of <span class="math inline">\(\left(x_i^{&#39;} \beta + \beta_0 \right)\)</span> is an indicator of confidence in the predicted classification.</p>
<p>If you constrain <span class="math inline">\(\beta\)</span> to be a unit vector, <span class="math inline">\(||\beta|| = \sum\beta^2 = 1\)</span>, then the products of the hyperplane and response variables, <span class="math inline">\(\left(x_i^{&#39;} \beta + \beta_0 \right)\)</span>, are the positive perpendicular distances from the hyperplane. If a separating hyperplane exists, there are an infinite number of possible hyperplanes. Evaluate a hyperplane by its <em>margin</em>, <span class="math inline">\(M\)</span>, the perpendicular distance to the closest observation.</p>
<p><span class="math display">\[M = \min \left\{y_i (x_i^{&#39;} \beta + \beta_0) \right\}.\]</span></p>
<p>The <em>maximal margin classifier</em> is the hyperplane that maximizes <span class="math inline">\(M.\)</span> The figure below (adapted from figure 9.3 from <span class="citation">(<a href="#ref-James2013">James et al. 2013</a>)</span>) shows a maximal marginal classifier. The three vectors shown in the figure anchor the hyperplane and are called the <em>support vectors</em>. Interestingly, it is only these three observations that factor into the determination of the maximal marginal classifier.</p>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-4-1.png" width="480" /></p>
<p>So, to put it all together, if a separating hyperplane exists, one could calculate it by maximizing <span class="math inline">\(M\)</span> subject to <span class="math inline">\(||\beta|| = 1\)</span> and <span class="math inline">\(y_i (x_i^{&#39;} \beta + \beta_0) \ge M\)</span> for all <span class="math inline">\(i\)</span>. However, a separating hyperplane rarely exists. In fact, even if a separating hyperplane does exist, its maximal margin classifier is probably undesirably narrow. A maximal margin classifier is sensitive to outliers so it tends to overfit data.</p>
</div>
<div id="support-vector-classifier" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Support Vector Classifier<a href="support-vector-machines.html#support-vector-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The maximal margin classifier can be generalized to non-separable cases using a so-called <em>soft margin</em>. The generalization is called the <em>support vector classifier</em>. The soft margin allows some misclassification in the interest of greater robustness to individual observations.</p>
<p>The support vector classifier maximizes <span class="math inline">\(M\)</span> subject to <span class="math inline">\(||\beta|| = 1\)</span> and <span class="math inline">\(y_i (x_i^{&#39;} \beta + \beta_0) \ge M(1 - \xi_i)\)</span> and <span class="math inline">\(\sum \xi_i \le \Xi\)</span> for all <span class="math inline">\(i\)</span>. The <span class="math inline">\(\xi_i\)</span> are <em>slack variables</em> whose sum is bounded by some constant tuning parameter <span class="math inline">\(\Xi\)</span>. The slack variable values indicate where the observation lies: <span class="math inline">\(\xi_i = 0\)</span> observations lie on the correct side of the margin; <span class="math inline">\(\xi_i &gt; 0\)</span> observation lie on the wrong side of the margin; <span class="math inline">\(\xi_i &gt; 1\)</span> observations lie on the wrong side of the hyperplane. <span class="math inline">\(\Xi\)</span> sets the tolerance for margin violation. If <span class="math inline">\(\Xi = 0\)</span>, then all observations must reside on the correct side of the margin, as in the maximal margin classifier. <span class="math inline">\(\Xi\)</span> controls the bias-variance trade-off: as <span class="math inline">\(\Xi\)</span> increases, the margin widens and allows more violations, increasing bias and decreasing variance. Similar to the maximal margin classifier, only the observations that are on the margin or that violate the margin factor into the determination of the support vector classifier. These observations are the support vectors.</p>
<p>The figure below (adaptation of figure 9.7 from <span class="citation">(<a href="#ref-James2013">James et al. 2013</a>)</span>) shows two support vector classifiers. The one on the left uses a large <span class="math inline">\(\Xi\)</span> and as a result includes many support vectors. The one on the right uses a smaller <span class="math inline">\(\Xi.\)</span></p>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>As <span class="math inline">\(\Xi\)</span> increases, the number of violating observations increase, and thus the number of support vectors increases. This property makes the algorithm robust to the extreme observations far away from the hyperplane. The only shortcoming with the algorithm is that it presumes a linear decision boundary.</p>
<p>Let’s build a support vector classifier model to predict credit <code>default</code> in the <code>ISLM:Default</code> data set. I’ll build the model in <strong>caret</strong> with the <code>svmLinear</code> method using 10-fold cross-validation (CV-10) to optimize the hyperparameters. There is only one hyperparameter for this model, the cost parameter:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="support-vector-machines.html#cb6-1" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">modelLookup</span>(<span class="st">&quot;svmLinear&quot;</span>)</span></code></pre></div>
<pre><code>##       model parameter label forReg forClass probModel
## 1 svmLinear         C  Cost   TRUE     TRUE      TRUE</code></pre>
<p>I’m not sure if <span class="math inline">\(C = Xi\)</span>, but it seems to function the same way. The documentation notes in <code>e1071::svm()</code> say <em>C</em> is the “<em>cost of constraints violation (default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.</em>” CV-10 will fit 10 models for each candidate value of <em>C</em> and keep the model with the best performance on resamples according to our evaluation metric. I can evaluate with “Accuracy”, “Kappa”, or “ROC”. I’ll use ROC, so I need to also set <code>summaryFunction = twoClassSummary</code> and <code>classProbs = TRUE</code>.</p>
<p><code>svmLinear</code> expects the response variable to be a factor with labels that can double as R variable names. This data set is fine because <code>default</code> is a factor with labels “No” and “Yes”. The predictor variables should be of comparable scale, but that is <em>not</em> the case here: <code>student</code> is binary, <code>balance</code> has a range of $0 - $2654, and <code>income</code> has a range of $772 - $73,554. I’ll one-hot encode <code>student</code> and standardize <code>balance</code> and <code>income</code> inside a <code>recipe</code> object.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="support-vector-machines.html#cb8-1" tabindex="-1"></a>mdl_ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(</span>
<span id="cb8-2"><a href="support-vector-machines.html#cb8-2" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>,</span>
<span id="cb8-3"><a href="support-vector-machines.html#cb8-3" tabindex="-1"></a>  <span class="at">summaryFunction =</span> twoClassSummary, <span class="at">classProbs =</span> <span class="cn">TRUE</span></span>
<span id="cb8-4"><a href="support-vector-machines.html#cb8-4" tabindex="-1"></a>)</span>
<span id="cb8-5"><a href="support-vector-machines.html#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="support-vector-machines.html#cb8-6" tabindex="-1"></a>rcpe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(default <span class="sc">~</span> ., <span class="at">data =</span> dat_train) <span class="sc">%&gt;%</span></span>
<span id="cb8-7"><a href="support-vector-machines.html#cb8-7" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb8-8"><a href="support-vector-machines.html#cb8-8" tabindex="-1"></a>  <span class="fu">step_center</span>(balance, income) <span class="sc">%&gt;%</span></span>
<span id="cb8-9"><a href="support-vector-machines.html#cb8-9" tabindex="-1"></a>  <span class="fu">step_scale</span>(balance, income)</span>
<span id="cb8-10"><a href="support-vector-machines.html#cb8-10" tabindex="-1"></a></span>
<span id="cb8-11"><a href="support-vector-machines.html#cb8-11" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb8-12"><a href="support-vector-machines.html#cb8-12" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb8-13"><a href="support-vector-machines.html#cb8-13" tabindex="-1"></a><span class="fu">capture.output</span>(</span>
<span id="cb8-14"><a href="support-vector-machines.html#cb8-14" tabindex="-1"></a>  mdl_svm_linear <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb8-15"><a href="support-vector-machines.html#cb8-15" tabindex="-1"></a>    rcpe,</span>
<span id="cb8-16"><a href="support-vector-machines.html#cb8-16" tabindex="-1"></a>    <span class="at">data =</span> dat_train,</span>
<span id="cb8-17"><a href="support-vector-machines.html#cb8-17" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;svmLinear&quot;</span>,</span>
<span id="cb8-18"><a href="support-vector-machines.html#cb8-18" tabindex="-1"></a>    <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>,</span>
<span id="cb8-19"><a href="support-vector-machines.html#cb8-19" tabindex="-1"></a>    <span class="at">trControl =</span> mdl_ctrl,</span>
<span id="cb8-20"><a href="support-vector-machines.html#cb8-20" tabindex="-1"></a>    <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">C =</span> <span class="fu">c</span>(<span class="fl">1e-1</span>, <span class="fl">1e0</span>, <span class="fl">1e1</span>, <span class="fl">1e2</span>, <span class="fl">1e3</span>, <span class="fl">1e4</span>))</span>
<span id="cb8-21"><a href="support-vector-machines.html#cb8-21" tabindex="-1"></a>  )</span>
<span id="cb8-22"><a href="support-vector-machines.html#cb8-22" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Loading required namespace: kernlab</code></pre>
<pre><code>## 
## Attaching package: &#39;kernlab&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ordinal&#39;:
## 
##     convergence</code></pre>
<pre><code>## The following object is masked from &#39;package:VGAM&#39;:
## 
##     nvar</code></pre>
<pre><code>## The following object is masked from &#39;package:tictoc&#39;:
## 
##     size</code></pre>
<pre><code>## The following object is masked from &#39;package:scales&#39;:
## 
##     alpha</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     alpha</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="support-vector-machines.html#cb17-1" tabindex="-1"></a><span class="fu">toc</span>()</span></code></pre></div>
<p>I experimented with the <code>tuneGrid</code> and found that smaller values for <em>C</em> (C &lt;= 10) produced poorer performance on resamples and on the holdout set. Unfortunately, the time to fit the models increased with <em>C</em> so that <code>expand.grid(C = c(1e-1, 1e0, 1e1, 1e2, 1e3, 1e4)</code> ran ~6 minutes.</p>
<p>The cross-validation maximized ROC with <em>C</em> = 10,000. The first three values (.1, 1, and 10) resulted in models that predicted no default every time.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="support-vector-machines.html#cb18-1" tabindex="-1"></a>mdl_svm_linear</span></code></pre></div>
<pre><code>## Support Vector Machines with Linear Kernel 
## 
## 800 samples
##   3 predictor
##   2 classes: &#39;No&#39;, &#39;Yes&#39; 
## 
## Recipe steps: dummy, center, scale 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 720, 720, 720, 720, 720, 721, ... 
## Resampling results across tuning parameters:
## 
##   C      ROC        Sens       Spec     
##   1e-01  0.9791792  0.9974192  0.1333333
##   1e+00  0.9785381  0.9961372  0.1333333
##   1e+01  0.9785381  0.9987013  0.2333333
##   1e+02  0.9785381  0.9987179  0.1333333
##   1e+03  0.9785381  0.9987013  0.2333333
##   1e+04  0.9785381  1.0000000  0.1833333
## 
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was C = 0.1.</code></pre>
<p>As <em>C</em> increases, the model variance decreases at the expense of more bias. The plot of the optimization results makes you wonder if <em>C</em> = 100 is basically just as good as <em>C</em> = 10,000 at a fraction of the fitting time.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="support-vector-machines.html#cb20-1" tabindex="-1"></a><span class="fu">ggplot</span>(mdl_svm_linear)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Predictions on the holdout set yield 96.8% accuracy. It found 15 of the 66 defaulters (sensitivity = 0.227), and misclassified 13 of the 1933 non-defaulters (specificity = 0.993).</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="support-vector-machines.html#cb21-1" tabindex="-1"></a>preds_svm_linear <span class="ot">&lt;-</span> <span class="fu">bind_cols</span>(</span>
<span id="cb21-2"><a href="support-vector-machines.html#cb21-2" tabindex="-1"></a>  dat_test,</span>
<span id="cb21-3"><a href="support-vector-machines.html#cb21-3" tabindex="-1"></a>  <span class="fu">predict</span>(mdl_svm_linear, <span class="at">newdata =</span> dat_test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>),</span>
<span id="cb21-4"><a href="support-vector-machines.html#cb21-4" tabindex="-1"></a>  <span class="at">Predicted =</span> <span class="fu">predict</span>(mdl_svm_linear, <span class="at">newdata =</span> dat_test, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>)</span>
<span id="cb21-5"><a href="support-vector-machines.html#cb21-5" tabindex="-1"></a>)</span>
<span id="cb21-6"><a href="support-vector-machines.html#cb21-6" tabindex="-1"></a><span class="fu">confusionMatrix</span>(preds_svm_linear<span class="sc">$</span>Predicted, <span class="at">reference =</span> preds_svm_linear<span class="sc">$</span>default, <span class="at">positive =</span> <span class="st">&quot;Yes&quot;</span>)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  No Yes
##        No  192   7
##        Yes   0   1
##                                           
##                Accuracy : 0.965           
##                  95% CI : (0.9292, 0.9858)
##     No Information Rate : 0.96            
##     P-Value [Acc &gt; NIR] : 0.45010         
##                                           
##                   Kappa : 0.2152          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.02334         
##                                           
##             Sensitivity : 0.1250          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.9648          
##              Prevalence : 0.0400          
##          Detection Rate : 0.0050          
##    Detection Prevalence : 0.0050          
##       Balanced Accuracy : 0.5625          
##                                           
##        &#39;Positive&#39; Class : Yes             
## </code></pre>
<p><code>Metrics::auc()</code> will calculate the confusion matrix values from the model using the holdout data set. The AUC on the holdout set is 0.9541.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="support-vector-machines.html#cb23-1" tabindex="-1"></a>mdl_svm_linear_auc <span class="ot">&lt;-</span> Metrics<span class="sc">::</span><span class="fu">auc</span>(<span class="at">actual =</span> preds_svm_linear<span class="sc">$</span>default <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>, preds_svm_linear<span class="sc">$</span>Yes)</span>
<span id="cb23-2"><a href="support-vector-machines.html#cb23-2" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(preds_svm_linear, default, Yes) <span class="sc">%&gt;%</span></span>
<span id="cb23-3"><a href="support-vector-machines.html#cb23-3" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb23-4"><a href="support-vector-machines.html#cb23-4" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb23-5"><a href="support-vector-machines.html#cb23-5" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;SVM Linear Model ROC Curve, Test Data&quot;</span>,</span>
<span id="cb23-6"><a href="support-vector-machines.html#cb23-6" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="fu">paste0</span>(<span class="st">&quot;AUC = &quot;</span>, <span class="fu">round</span>(mdl_svm_linear_auc, <span class="dv">4</span>))</span>
<span id="cb23-7"><a href="support-vector-machines.html#cb23-7" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>There are just a few predictors in this model, so there is a chance I can visualize the model.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="support-vector-machines.html#cb24-1" tabindex="-1"></a>fits_svm_linear <span class="ot">&lt;-</span> <span class="fu">bind_cols</span>(</span>
<span id="cb24-2"><a href="support-vector-machines.html#cb24-2" tabindex="-1"></a>  dat_train,</span>
<span id="cb24-3"><a href="support-vector-machines.html#cb24-3" tabindex="-1"></a>  <span class="fu">predict</span>(mdl_svm_linear, <span class="at">newdata =</span> dat_train, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>),</span>
<span id="cb24-4"><a href="support-vector-machines.html#cb24-4" tabindex="-1"></a>  <span class="at">Predicted =</span> <span class="fu">predict</span>(mdl_svm_linear, <span class="at">newdata =</span> dat_train, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb24-5"><a href="support-vector-machines.html#cb24-5" tabindex="-1"></a>)</span>
<span id="cb24-6"><a href="support-vector-machines.html#cb24-6" tabindex="-1"></a></span>
<span id="cb24-7"><a href="support-vector-machines.html#cb24-7" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb24-8"><a href="support-vector-machines.html#cb24-8" tabindex="-1"></a>  fits_svm_linear <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">set =</span> <span class="st">&quot;Actual&quot;</span>, <span class="at">outcome =</span> default),</span>
<span id="cb24-9"><a href="support-vector-machines.html#cb24-9" tabindex="-1"></a>  fits_svm_linear <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">set =</span> <span class="st">&quot;Fitted&quot;</span>, <span class="at">outcome =</span> Predicted)</span>
<span id="cb24-10"><a href="support-vector-machines.html#cb24-10" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb24-11"><a href="support-vector-machines.html#cb24-11" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> balance, <span class="at">y =</span> income, <span class="at">color =</span> outcome)) <span class="sc">+</span></span>
<span id="cb24-12"><a href="support-vector-machines.html#cb24-12" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">shape =</span> student), <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb24-13"><a href="support-vector-machines.html#cb24-13" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb24-14"><a href="support-vector-machines.html#cb24-14" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span>) <span class="sc">+</span></span>
<span id="cb24-15"><a href="support-vector-machines.html#cb24-15" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">labels=</span>scales<span class="sc">::</span><span class="fu">dollar_format</span>()) <span class="sc">+</span></span>
<span id="cb24-16"><a href="support-vector-machines.html#cb24-16" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">labels=</span>scales<span class="sc">::</span><span class="fu">dollar_format</span>()) <span class="sc">+</span></span>
<span id="cb24-17"><a href="support-vector-machines.html#cb24-17" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">list</span>(<span class="at">No =</span> <span class="st">&quot;#B6E2D3&quot;</span>, <span class="at">Yes =</span> <span class="st">&quot;#EF7C8E&quot;</span>)) <span class="sc">+</span></span>
<span id="cb24-18"><a href="support-vector-machines.html#cb24-18" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Training Set&quot;</span>) <span class="sc">+</span></span>
<span id="cb24-19"><a href="support-vector-machines.html#cb24-19" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="fu">vars</span>(set))</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Looks like the hyperplane slopes slightly right, so high credit card balances are a little less likely to fall into default if income is high. Distinguishing students is difficult, but they are generally at the low end of the income scale, and they seem to exert a positive association with default. Let’s look at the same figure with the training set.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="support-vector-machines.html#cb25-1" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb25-2"><a href="support-vector-machines.html#cb25-2" tabindex="-1"></a>  preds_svm_linear <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">set =</span> <span class="st">&quot;Actual&quot;</span>, <span class="at">outcome =</span> default),</span>
<span id="cb25-3"><a href="support-vector-machines.html#cb25-3" tabindex="-1"></a>  preds_svm_linear <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">set =</span> <span class="st">&quot;Fitted&quot;</span>, <span class="at">outcome =</span> Predicted)</span>
<span id="cb25-4"><a href="support-vector-machines.html#cb25-4" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb25-5"><a href="support-vector-machines.html#cb25-5" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> balance, <span class="at">y =</span> income, <span class="at">color =</span> outcome)) <span class="sc">+</span></span>
<span id="cb25-6"><a href="support-vector-machines.html#cb25-6" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">shape =</span> student), <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb25-7"><a href="support-vector-machines.html#cb25-7" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-8"><a href="support-vector-machines.html#cb25-8" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span>) <span class="sc">+</span></span>
<span id="cb25-9"><a href="support-vector-machines.html#cb25-9" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">labels=</span>scales<span class="sc">::</span><span class="fu">dollar_format</span>()) <span class="sc">+</span></span>
<span id="cb25-10"><a href="support-vector-machines.html#cb25-10" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">labels=</span>scales<span class="sc">::</span><span class="fu">dollar_format</span>()) <span class="sc">+</span></span>
<span id="cb25-11"><a href="support-vector-machines.html#cb25-11" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">list</span>(<span class="at">No =</span> <span class="st">&quot;#B6E2D3&quot;</span>, <span class="at">Yes =</span> <span class="st">&quot;#EF7C8E&quot;</span>)) <span class="sc">+</span></span>
<span id="cb25-12"><a href="support-vector-machines.html#cb25-12" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Test Set&quot;</span>) <span class="sc">+</span></span>
<span id="cb25-13"><a href="support-vector-machines.html#cb25-13" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="fu">vars</span>(set))</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Visually, the model performed consistently between the training and test sets.</p>
</div>
<div id="support-vector-machines-1" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Support Vector Machines<a href="support-vector-machines.html#support-vector-machines-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Enlarging the feature space of the support vector classifier accommodates nonlinear relationships. Support vector machines do this in a specific way, using <em>kernels</em>. Before diving into kernels, you need to understand (somewhat) the solution to the support vector classifier optimization problem.</p>
<p>The linear support vector classifier can be represented as</p>
<p><span class="math display">\[f(x) = \beta_0 + \sum_i^n \alpha_i \langle x, x_i \rangle.\]</span></p>
<p>That is, the classification of test observation <span class="math inline">\(x\)</span> is the sum of the dot products of <span class="math inline">\(x\)</span> with all the <span class="math inline">\(n\)</span> observations in the training set, multiplied by the vector <span class="math inline">\(\alpha\)</span> (plus the constant <span class="math inline">\(\beta_0\)</span>). The <span class="math inline">\(\alpha\)</span> vector is calculated from the <span class="math inline">\(n \choose 2\)</span> dot products of the training data set. Actually, the classification is simpler than that because <span class="math inline">\(\alpha_i = 0\)</span> for all observation that are not support vectors, so you can actually represent the solution as</p>
<p><span class="math display">\[f(x) = \beta_0 + \sum_{i \in S} \alpha_i \langle x, x_i \rangle\]</span>
where <span class="math inline">\(S\)</span> is the set of support vector indices.</p>
<p>Now, you can generalize the inner dot product with a wrapper function, called a <em>kernel</em>, <span class="math inline">\(K(x_i, x_{i^{&#39;}})\)</span>.</p>
<p><span class="math display">\[f(x) = \beta_0 + \sum_{i \in S} \alpha_i K(x, x_i)\]</span></p>
<p>To get the the support vector classifier, you’d defined <span class="math inline">\(K\)</span> to be a <em>linear</em> kernel:</p>
<p><span class="math display">\[K(x_i, x_i^{&#39;}) = \langle x, x_i \rangle\]</span></p>
<p>But you could also use other kernels, like the polynomial of degree <span class="math inline">\(d\)</span>,</p>
<p><span class="math display">\[K(x, x&#39;) = (1 + \langle x, x&#39; \rangle)^d\]</span></p>
<p>or radial</p>
<p><span class="math display">\[K(x, x&#39;) = \exp\{-\gamma ||x - x&#39;||^2\}.\]</span></p>
<p>The figure below (figure 9.9 from <span class="citation">(<a href="#ref-James2013">James et al. 2013</a>)</span>) shows two support vector classifiers. The one on the left uses a polynomial kernel and the one on the right uses a radial kernel.</p>
<div class="float">
<img src="./images/svm_svm.png" alt="FIGURE 9.9 from An Introduction to Statistical Learning" />
<div class="figcaption">FIGURE 9.9 from An Introduction to Statistical Learning</div>
</div>
<p>The SVM model can be expressed in the familiar “loss + penalty” minimization structure, <span class="math inline">\(\min_{\beta} \left \{ L(X,y,\beta) + \lambda P(\beta) \right \}\)</span> as</p>
<p><span class="math display">\[\min_\beta \left \{ \sum_{i=1}^n \max [0, 1-y_i f(x_i)] + \lambda \sum_{j=1}^p \beta_j^2 \right \}\]</span></p>
<p>Increasing <span class="math inline">\(\lambda\)</span>, shrinks <span class="math inline">\(\beta\)</span> and more violations to the margin are tolerated, resulting in a lower-variance/higher-bias model. The loss function above is known as a <em>hinge loss</em>.</p>
<p>Let’s build a support vector machine model to predict credit <code>default</code> in the <code>ISLM:Default</code> data set again. I’ll try a polynomial kernel with the <code>svmPoly</code> method and a radial kernal with <code>svmRadial</code>. <code>svmPoly</code> has three hyperparameters. <em>What is the Scale parameter?</em></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="support-vector-machines.html#cb26-1" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">modelLookup</span>(<span class="st">&quot;svmPoly&quot;</span>)</span></code></pre></div>
<pre><code>##     model parameter             label forReg forClass probModel
## 1 svmPoly    degree Polynomial Degree   TRUE     TRUE      TRUE
## 2 svmPoly     scale             Scale   TRUE     TRUE      TRUE
## 3 svmPoly         C              Cost   TRUE     TRUE      TRUE</code></pre>
<p><code>svmRadial</code> has two hyperparameters:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="support-vector-machines.html#cb28-1" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">modelLookup</span>(<span class="st">&quot;svmRadial&quot;</span>)</span></code></pre></div>
<pre><code>##       model parameter label forReg forClass probModel
## 1 svmRadial     sigma Sigma   TRUE     TRUE      TRUE
## 2 svmRadial         C  Cost   TRUE     TRUE      TRUE</code></pre>
<p>I’ll use the same <code>trControl</code> object as with the support vector classifier, and I’ll use the same one-hot encoded binaries and scaled and centered data. I fixed <code>scale</code> at its default value and tried polynomials of degree 1-3. I used the same candidate <code>cost</code> values.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="support-vector-machines.html#cb30-1" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb30-2"><a href="support-vector-machines.html#cb30-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb30-3"><a href="support-vector-machines.html#cb30-3" tabindex="-1"></a><span class="fu">capture.output</span>(</span>
<span id="cb30-4"><a href="support-vector-machines.html#cb30-4" tabindex="-1"></a>  mdl_svm_poly <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb30-5"><a href="support-vector-machines.html#cb30-5" tabindex="-1"></a>    rcpe,</span>
<span id="cb30-6"><a href="support-vector-machines.html#cb30-6" tabindex="-1"></a>    <span class="at">data =</span> dat_train,</span>
<span id="cb30-7"><a href="support-vector-machines.html#cb30-7" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;svmPoly&quot;</span>,</span>
<span id="cb30-8"><a href="support-vector-machines.html#cb30-8" tabindex="-1"></a>    <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>,</span>
<span id="cb30-9"><a href="support-vector-machines.html#cb30-9" tabindex="-1"></a>    <span class="at">trControl =</span> mdl_ctrl,</span>
<span id="cb30-10"><a href="support-vector-machines.html#cb30-10" tabindex="-1"></a>    <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(</span>
<span id="cb30-11"><a href="support-vector-machines.html#cb30-11" tabindex="-1"></a>      <span class="at">C =</span> <span class="fu">c</span>(<span class="fl">1e-1</span>, <span class="fl">1e0</span>, <span class="fl">1e1</span>, <span class="fl">1e2</span>, <span class="fl">1e3</span>, <span class="fl">1e4</span>),</span>
<span id="cb30-12"><a href="support-vector-machines.html#cb30-12" tabindex="-1"></a>      <span class="at">degree =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>),</span>
<span id="cb30-13"><a href="support-vector-machines.html#cb30-13" tabindex="-1"></a>      <span class="at">scale =</span> <span class="fl">0.001</span>)</span>
<span id="cb30-14"><a href="support-vector-machines.html#cb30-14" tabindex="-1"></a>  )</span>
<span id="cb30-15"><a href="support-vector-machines.html#cb30-15" tabindex="-1"></a>)</span>
<span id="cb30-16"><a href="support-vector-machines.html#cb30-16" tabindex="-1"></a><span class="fu">toc</span>()</span></code></pre></div>
<p>The model ran ~3 minutes. The cross-validation maximized ROC with <em>degree</em> = 2 and <em>C</em> = 0.1.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="support-vector-machines.html#cb31-1" tabindex="-1"></a>mdl_svm_poly</span></code></pre></div>
<pre><code>## Support Vector Machines with Polynomial Kernel 
## 
## 800 samples
##   3 predictor
##   2 classes: &#39;No&#39;, &#39;Yes&#39; 
## 
## Recipe steps: dummy, center, scale 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 720, 720, 720, 720, 720, 721, ... 
## Resampling results across tuning parameters:
## 
##   C      degree  ROC        Sens       Spec      
##   1e-01  1       0.9721029  1.0000000  0.00000000
##   1e-01  2       0.9714619  1.0000000  0.00000000
##   1e-01  3       0.9772561  1.0000000  0.03333333
##   1e+00  1       0.9753247  0.9961372  0.18333333
##   1e+00  2       0.9785381  0.9961205  0.18333333
##   1e+00  3       0.9785298  0.9987013  0.21666667
##   1e+01  1       0.9791708  0.9987013  0.18333333
##   1e+01  2       0.9798202  0.9987179  0.18333333
##   1e+01  3       0.9785298  0.9948551  0.13333333
##   1e+02  1       0.9791792  0.9974192  0.15000000
##   1e+02  2       0.9791792  0.9974359  0.28333333
##   1e+02  3       0.9791792  0.9922744  0.23333333
##   1e+03  1       0.9785381  0.9948551  0.13333333
##   1e+03  2       0.9791792  0.9974026  0.08333333
##   1e+03  3       0.9791792  0.9974192  0.08333333
##   1e+04  1       0.9785381  1.0000000  0.18333333
##   1e+04  2       0.9804612  0.9987179  0.18333333
##   1e+04  3       0.9785132  1.0000000  0.13333333
## 
## Tuning parameter &#39;scale&#39; was held constant at a value of 0.001
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were degree = 2, scale = 0.001 and C
##  = 10000.</code></pre>
<p>Here is <code>svmRadial</code>. At this point, I do not know how the sigma tuning parameter works, so I expanded around the default value used from <code>tuneLength = 1</code>.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="support-vector-machines.html#cb33-1" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb33-2"><a href="support-vector-machines.html#cb33-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb33-3"><a href="support-vector-machines.html#cb33-3" tabindex="-1"></a><span class="fu">capture.output</span>(</span>
<span id="cb33-4"><a href="support-vector-machines.html#cb33-4" tabindex="-1"></a>  mdl_svm_radial <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb33-5"><a href="support-vector-machines.html#cb33-5" tabindex="-1"></a>    rcpe,</span>
<span id="cb33-6"><a href="support-vector-machines.html#cb33-6" tabindex="-1"></a>    <span class="at">data =</span> dat_train,</span>
<span id="cb33-7"><a href="support-vector-machines.html#cb33-7" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;svmRadial&quot;</span>,</span>
<span id="cb33-8"><a href="support-vector-machines.html#cb33-8" tabindex="-1"></a>    <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>,</span>
<span id="cb33-9"><a href="support-vector-machines.html#cb33-9" tabindex="-1"></a>    <span class="at">trControl =</span> mdl_ctrl,</span>
<span id="cb33-10"><a href="support-vector-machines.html#cb33-10" tabindex="-1"></a>    <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(</span>
<span id="cb33-11"><a href="support-vector-machines.html#cb33-11" tabindex="-1"></a>      <span class="at">C =</span> <span class="fu">c</span>(<span class="fl">1e-1</span>, <span class="fl">1e0</span>, <span class="fl">1e1</span>, <span class="fl">1e2</span>, <span class="fl">1e3</span>),</span>
<span id="cb33-12"><a href="support-vector-machines.html#cb33-12" tabindex="-1"></a>      <span class="at">sigma =</span> <span class="fu">c</span>(.<span class="dv">01</span>, .<span class="dv">1</span>, <span class="fl">1.0</span>)</span>
<span id="cb33-13"><a href="support-vector-machines.html#cb33-13" tabindex="-1"></a>    )</span>
<span id="cb33-14"><a href="support-vector-machines.html#cb33-14" tabindex="-1"></a>  )</span>
<span id="cb33-15"><a href="support-vector-machines.html#cb33-15" tabindex="-1"></a>)</span>
<span id="cb33-16"><a href="support-vector-machines.html#cb33-16" tabindex="-1"></a><span class="fu">toc</span>()</span></code></pre></div>
<p>This model ran in ~11 minutes and optimized at <em>C</em> = 0.1 and <em>sigma</em> = 0.01.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="support-vector-machines.html#cb34-1" tabindex="-1"></a>mdl_svm_radial</span></code></pre></div>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 800 samples
##   3 predictor
##   2 classes: &#39;No&#39;, &#39;Yes&#39; 
## 
## Recipe steps: dummy, center, scale 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 720, 720, 720, 720, 720, 721, ... 
## Resampling results across tuning parameters:
## 
##   C      sigma  ROC        Sens       Spec      
##   1e-01  0.01   0.9772561  0.9948385  0.18333333
##   1e-01  0.10   0.9393690  0.9961372  0.23333333
##   1e-01  1.00   0.7557859  0.9974192  0.13333333
##   1e+00  0.01   0.9791792  0.9948551  0.18333333
##   1e+00  0.10   0.9393690  0.9961538  0.23333333
##   1e+00  1.00   0.7570763  0.9961372  0.18333333
##   1e+01  0.01   0.9791792  0.9974192  0.15000000
##   1e+01  0.10   0.8993923  0.9987013  0.13333333
##   1e+01  1.00   0.7878483  0.9961372  0.13333333
##   1e+02  0.01   0.9804446  0.9987179  0.23333333
##   1e+02  0.10   0.8865551  0.9987013  0.13333333
##   1e+02  1.00   0.7990870  0.9987013  0.18333333
##   1e+03  0.01   0.9476607  0.9974192  0.08333333
##   1e+03  0.10   0.9039461  0.9987013  0.13333333
##   1e+03  1.00   0.8748224  0.9987179  0.15000000
## 
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were sigma = 0.01 and C = 100.</code></pre>
<p>Here are the optimization plots I used to help tune the models.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="support-vector-machines.html#cb36-1" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(mdl_svm_poly) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;SVMPoly Tuning&quot;</span>) <span class="sc">+</span> </span>
<span id="cb36-2"><a href="support-vector-machines.html#cb36-2" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</span>
<span id="cb36-3"><a href="support-vector-machines.html#cb36-3" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(mdl_svm_radial) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;SVMRadial Tuning&quot;</span>) <span class="sc">+</span> </span>
<span id="cb36-4"><a href="support-vector-machines.html#cb36-4" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</span>
<span id="cb36-5"><a href="support-vector-machines.html#cb36-5" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1, p2, <span class="at">nrow =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The polynomial model predictions on the holdout set yielded 96.85% accuracy. It found 20 of the 66 defaulters (sensitivity = 0.303), and misclassified 17 of the 1933 non-defaulters (specificity = 0.991). So the polynomial model found a few more defaulters at the expense of a few more mistakes.</p>
<p>The radial model predictions on the holdout set yielded 97% accuracy. It found 17 defaulters (sensitivity = 0.258), and misclassified 11 non-defaulters (specificity = 0.994). So the radial was somewhere between the linear and polynomial models.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="support-vector-machines.html#cb37-1" tabindex="-1"></a>preds_svm_poly <span class="ot">&lt;-</span> <span class="fu">bind_cols</span>(</span>
<span id="cb37-2"><a href="support-vector-machines.html#cb37-2" tabindex="-1"></a>  dat_test,</span>
<span id="cb37-3"><a href="support-vector-machines.html#cb37-3" tabindex="-1"></a>  <span class="fu">predict</span>(mdl_svm_poly, <span class="at">newdata =</span> dat_test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>),</span>
<span id="cb37-4"><a href="support-vector-machines.html#cb37-4" tabindex="-1"></a>  <span class="at">Predicted =</span> <span class="fu">predict</span>(mdl_svm_poly, <span class="at">newdata =</span> dat_test, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>)</span>
<span id="cb37-5"><a href="support-vector-machines.html#cb37-5" tabindex="-1"></a>)</span>
<span id="cb37-6"><a href="support-vector-machines.html#cb37-6" tabindex="-1"></a><span class="fu">confusionMatrix</span>(preds_svm_poly<span class="sc">$</span>Predicted, <span class="at">reference =</span> preds_svm_poly<span class="sc">$</span>default, <span class="at">positive =</span> <span class="st">&quot;Yes&quot;</span>)</span>
<span id="cb37-7"><a href="support-vector-machines.html#cb37-7" tabindex="-1"></a></span>
<span id="cb37-8"><a href="support-vector-machines.html#cb37-8" tabindex="-1"></a>preds_svm_radial <span class="ot">&lt;-</span> <span class="fu">bind_cols</span>(</span>
<span id="cb37-9"><a href="support-vector-machines.html#cb37-9" tabindex="-1"></a>  dat_test,</span>
<span id="cb37-10"><a href="support-vector-machines.html#cb37-10" tabindex="-1"></a>  <span class="fu">predict</span>(mdl_svm_radial, <span class="at">newdata =</span> dat_test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>),</span>
<span id="cb37-11"><a href="support-vector-machines.html#cb37-11" tabindex="-1"></a>  <span class="at">Predicted =</span> <span class="fu">predict</span>(mdl_svm_radial, <span class="at">newdata =</span> dat_test, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>)</span>
<span id="cb37-12"><a href="support-vector-machines.html#cb37-12" tabindex="-1"></a>)</span>
<span id="cb37-13"><a href="support-vector-machines.html#cb37-13" tabindex="-1"></a><span class="fu">confusionMatrix</span>(preds_svm_radial<span class="sc">$</span>Predicted, <span class="at">reference =</span> preds_svm_radial<span class="sc">$</span>default, <span class="at">positive =</span> <span class="st">&quot;Yes&quot;</span>)</span></code></pre></div>
<p>The AUCs on the holdout set is where 0.9536 for the polynmomial and 0.8836 for the radial.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="support-vector-machines.html#cb38-1" tabindex="-1"></a>Metrics<span class="sc">::</span><span class="fu">auc</span>(<span class="at">actual =</span> preds_svm_poly<span class="sc">$</span>default <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>, preds_svm_poly<span class="sc">$</span>Yes)</span></code></pre></div>
<pre><code>## [1] 0.906901</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="support-vector-machines.html#cb40-1" tabindex="-1"></a>Metrics<span class="sc">::</span><span class="fu">auc</span>(<span class="at">actual =</span> preds_svm_radial<span class="sc">$</span>default <span class="sc">==</span> <span class="st">&quot;Yes&quot;</span>, preds_svm_radial<span class="sc">$</span>Yes)</span></code></pre></div>
<pre><code>## [1] 0.8873698</code></pre>
<p>Let’s see what the two models look like on the training data.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="support-vector-machines.html#cb42-1" tabindex="-1"></a>fits_svm_poly <span class="ot">&lt;-</span> <span class="fu">bind_cols</span>(</span>
<span id="cb42-2"><a href="support-vector-machines.html#cb42-2" tabindex="-1"></a>  dat_train,</span>
<span id="cb42-3"><a href="support-vector-machines.html#cb42-3" tabindex="-1"></a>  <span class="fu">predict</span>(mdl_svm_poly, <span class="at">newdata =</span> dat_train, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>),</span>
<span id="cb42-4"><a href="support-vector-machines.html#cb42-4" tabindex="-1"></a>  <span class="at">Predicted =</span> <span class="fu">predict</span>(mdl_svm_poly, <span class="at">newdata =</span> dat_train, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb42-5"><a href="support-vector-machines.html#cb42-5" tabindex="-1"></a>)</span>
<span id="cb42-6"><a href="support-vector-machines.html#cb42-6" tabindex="-1"></a></span>
<span id="cb42-7"><a href="support-vector-machines.html#cb42-7" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb42-8"><a href="support-vector-machines.html#cb42-8" tabindex="-1"></a>  fits_svm_poly <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">set =</span> <span class="st">&quot;Actual&quot;</span>, <span class="at">outcome =</span> default),</span>
<span id="cb42-9"><a href="support-vector-machines.html#cb42-9" tabindex="-1"></a>  fits_svm_poly <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">set =</span> <span class="st">&quot;Fitted&quot;</span>, <span class="at">outcome =</span> Predicted)</span>
<span id="cb42-10"><a href="support-vector-machines.html#cb42-10" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb42-11"><a href="support-vector-machines.html#cb42-11" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> balance, <span class="at">y =</span> income, <span class="at">color =</span> outcome)) <span class="sc">+</span></span>
<span id="cb42-12"><a href="support-vector-machines.html#cb42-12" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">shape =</span> student), <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb42-13"><a href="support-vector-machines.html#cb42-13" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb42-14"><a href="support-vector-machines.html#cb42-14" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span>) <span class="sc">+</span></span>
<span id="cb42-15"><a href="support-vector-machines.html#cb42-15" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">labels=</span>scales<span class="sc">::</span><span class="fu">dollar_format</span>()) <span class="sc">+</span></span>
<span id="cb42-16"><a href="support-vector-machines.html#cb42-16" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">labels=</span>scales<span class="sc">::</span><span class="fu">dollar_format</span>()) <span class="sc">+</span></span>
<span id="cb42-17"><a href="support-vector-machines.html#cb42-17" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">list</span>(<span class="at">No =</span> <span class="st">&quot;#B6E2D3&quot;</span>, <span class="at">Yes =</span> <span class="st">&quot;#EF7C8E&quot;</span>)) <span class="sc">+</span></span>
<span id="cb42-18"><a href="support-vector-machines.html#cb42-18" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Training Set, Polynomial Kernel&quot;</span>) <span class="sc">+</span></span>
<span id="cb42-19"><a href="support-vector-machines.html#cb42-19" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="fu">vars</span>(set))</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="support-vector-machines.html#cb43-1" tabindex="-1"></a>fits_svm_radial <span class="ot">&lt;-</span> <span class="fu">bind_cols</span>(</span>
<span id="cb43-2"><a href="support-vector-machines.html#cb43-2" tabindex="-1"></a>  dat_train,</span>
<span id="cb43-3"><a href="support-vector-machines.html#cb43-3" tabindex="-1"></a>  <span class="fu">predict</span>(mdl_svm_radial, <span class="at">newdata =</span> dat_train, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>),</span>
<span id="cb43-4"><a href="support-vector-machines.html#cb43-4" tabindex="-1"></a>  <span class="at">Predicted =</span> <span class="fu">predict</span>(mdl_svm_radial, <span class="at">newdata =</span> dat_train, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb43-5"><a href="support-vector-machines.html#cb43-5" tabindex="-1"></a>)</span>
<span id="cb43-6"><a href="support-vector-machines.html#cb43-6" tabindex="-1"></a></span>
<span id="cb43-7"><a href="support-vector-machines.html#cb43-7" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb43-8"><a href="support-vector-machines.html#cb43-8" tabindex="-1"></a>  fits_svm_radial <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">set =</span> <span class="st">&quot;Actual&quot;</span>, <span class="at">outcome =</span> default),</span>
<span id="cb43-9"><a href="support-vector-machines.html#cb43-9" tabindex="-1"></a>  fits_svm_radial <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">set =</span> <span class="st">&quot;Fitted&quot;</span>, <span class="at">outcome =</span> Predicted)</span>
<span id="cb43-10"><a href="support-vector-machines.html#cb43-10" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb43-11"><a href="support-vector-machines.html#cb43-11" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> balance, <span class="at">y =</span> income, <span class="at">color =</span> outcome)) <span class="sc">+</span></span>
<span id="cb43-12"><a href="support-vector-machines.html#cb43-12" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">shape =</span> student), <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb43-13"><a href="support-vector-machines.html#cb43-13" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb43-14"><a href="support-vector-machines.html#cb43-14" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span>) <span class="sc">+</span></span>
<span id="cb43-15"><a href="support-vector-machines.html#cb43-15" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">labels=</span>scales<span class="sc">::</span><span class="fu">dollar_format</span>()) <span class="sc">+</span></span>
<span id="cb43-16"><a href="support-vector-machines.html#cb43-16" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">labels=</span>scales<span class="sc">::</span><span class="fu">dollar_format</span>()) <span class="sc">+</span></span>
<span id="cb43-17"><a href="support-vector-machines.html#cb43-17" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">list</span>(<span class="at">No =</span> <span class="st">&quot;#B6E2D3&quot;</span>, <span class="at">Yes =</span> <span class="st">&quot;#EF7C8E&quot;</span>)) <span class="sc">+</span></span>
<span id="cb43-18"><a href="support-vector-machines.html#cb43-18" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Training Set, Radial Kernel&quot;</span>) <span class="sc">+</span></span>
<span id="cb43-19"><a href="support-vector-machines.html#cb43-19" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="fu">vars</span>(set))</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>You can see the slight curvature in the hyperplane now. The polynomial and the radial models look pretty much identical to me.</p>

</div>
</div>
<h3>References<a href="references-1.html#references-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Hastie2017" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. <em>The Elements of Statistical Learning</em>. 2nd ed. New York, NY: Springer. <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">https://web.stanford.edu/~hastie/ElemStatLearn/</a>.
</div>
<div id="ref-James2013" class="csl-entry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning: With Applications in r</em>. 1st ed. New York, NY: Springer. <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/book.html">http://faculty.marshall.usc.edu/gareth-james/ISL/book.html</a>.
</div>
<div id="ref-Kuhn2016" class="csl-entry">
Kuhn, Max, and Kjell Johnson. 2016. <em>Applied Predictive Modeling</em>. 1st ed. New York, NY: Springer. <a href="http://appliedpredictivemodeling.com/">http://appliedpredictivemodeling.com/</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="decision-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="BayesRegression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["supervised-ml.pdf", "supervised-ml.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
