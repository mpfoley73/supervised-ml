<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Ordinary Least Squares | Supervised Machine Learning</title>
  <meta name="description" content="These are my personal notes related to supervised machine learning techniques." />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Ordinary Least Squares | Supervised Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These are my personal notes related to supervised machine learning techniques." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Ordinary Least Squares | Supervised Machine Learning" />
  
  <meta name="twitter:description" content="These are my personal notes related to supervised machine learning techniques." />
  

<meta name="author" content="Michael Foley" />


<meta name="date" content="2023-07-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="generalized-linear-models-glm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet" />
<script src="libs/tabwid-1.1.3/tabwid.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Supervised Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>1</b> Ordinary Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#linear-regression-model"><i class="fa fa-check"></i><b>1.1</b> Linear Regression Model</a></li>
<li class="chapter" data-level="1.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#parameter-estimation"><i class="fa fa-check"></i><b>1.2</b> Parameter Estimation</a></li>
<li class="chapter" data-level="1.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-assumptions"><i class="fa fa-check"></i><b>1.3</b> Model Assumptions</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#linearity"><i class="fa fa-check"></i><b>1.3.1</b> Linearity</a></li>
<li class="chapter" data-level="1.3.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#multicollinearity"><i class="fa fa-check"></i><b>1.3.2</b> Multicollinearity</a></li>
<li class="chapter" data-level="1.3.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#normality"><i class="fa fa-check"></i><b>1.3.3</b> Normality</a></li>
<li class="chapter" data-level="1.3.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#equal-variances"><i class="fa fa-check"></i><b>1.3.4</b> Equal Variances</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#prediction"><i class="fa fa-check"></i><b>1.4</b> Prediction</a></li>
<li class="chapter" data-level="1.5" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#inference"><i class="fa fa-check"></i><b>1.5</b> Inference</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#t-test"><i class="fa fa-check"></i><b>1.5.1</b> <em>t</em>-Test</a></li>
<li class="chapter" data-level="1.5.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#f-test"><i class="fa fa-check"></i><b>1.5.2</b> <em>F</em>-Test</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#interpretation"><i class="fa fa-check"></i><b>1.6</b> Interpretation</a></li>
<li class="chapter" data-level="1.7" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-validation"><i class="fa fa-check"></i><b>1.7</b> Model Validation</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#accuracy-metrics"><i class="fa fa-check"></i><b>1.7.1</b> Accuracy Metrics</a></li>
<li class="chapter" data-level="1.7.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#cross-validation"><i class="fa fa-check"></i><b>1.7.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="1.7.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#gain-curve"><i class="fa fa-check"></i><b>1.7.3</b> Gain Curve</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#ols-reference"><i class="fa fa-check"></i><b>1.8</b> OLS Reference</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html"><i class="fa fa-check"></i><b>2</b> Generalized Linear Models (GLM)</a>
<ul>
<li class="chapter" data-level="2.1" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#binomiallogistic"><i class="fa fa-check"></i><b>2.1</b> Binomial Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs1"><i class="fa fa-check"></i>Case Study 1</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#the-model"><i class="fa fa-check"></i>The Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpretation-1"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#model-fit"><i class="fa fa-check"></i>Model Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#multinomiallogistic"><i class="fa fa-check"></i><b>2.2</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs2"><i class="fa fa-check"></i>Case Study 2</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#the-model-1"><i class="fa fa-check"></i>The Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpretation-2"><i class="fa fa-check"></i>Interpretation</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assumptions-1"><i class="fa fa-check"></i>Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#model-fit-1"><i class="fa fa-check"></i>Model Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting-1"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#ordinallogistic"><i class="fa fa-check"></i><b>2.3</b> Ordinal Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs3"><i class="fa fa-check"></i>Case Study 3</a>
<ul>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#fit-the-model"><i class="fa fa-check"></i>Fit the Model</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#verify-assumptions"><i class="fa fa-check"></i>Verify Assumptions</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#assess-the-model-fit"><i class="fa fa-check"></i>Assess the Model Fit</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#interpret-results"><i class="fa fa-check"></i>Interpret Results</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#reporting-2"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#poissonregression"><i class="fa fa-check"></i><b>2.4</b> Poisson Regression</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-models-glm.html"><a href="generalized-linear-models-glm.html#cs4"><i class="fa fa-check"></i>Case Study 4</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>3</b> Regularization</a>
<ul>
<li class="chapter" data-level="3.1" data-path="regularization.html"><a href="regularization.html#ridge"><i class="fa fa-check"></i><b>3.1</b> Ridge</a></li>
<li class="chapter" data-level="3.2" data-path="regularization.html"><a href="regularization.html#lasso"><i class="fa fa-check"></i><b>3.2</b> Lasso</a></li>
<li class="chapter" data-level="3.3" data-path="regularization.html"><a href="regularization.html#elastic-net"><i class="fa fa-check"></i><b>3.3</b> Elastic Net</a></li>
<li class="chapter" data-level="" data-path="regularization.html"><a href="regularization.html#model-summary"><i class="fa fa-check"></i>Model Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>4</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="4.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-tree"><i class="fa fa-check"></i><b>4.1</b> Classification Tree</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="decision-trees.html"><a href="decision-trees.html#measuring-performance"><i class="fa fa-check"></i><b>4.1.1</b> Measuring Performance</a></li>
<li class="chapter" data-level="4.1.2" data-path="decision-trees.html"><a href="decision-trees.html#training-with-caret"><i class="fa fa-check"></i><b>4.1.2</b> Training with Caret</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-tree"><i class="fa fa-check"></i><b>4.2</b> Regression Tree</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="decision-trees.html"><a href="decision-trees.html#training-with-caret-1"><i class="fa fa-check"></i><b>4.2.1</b> Training with Caret</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>4.3</b> Bagged Trees</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="decision-trees.html"><a href="decision-trees.html#bagged-classification-tree"><i class="fa fa-check"></i><b>4.3.1</b> Bagged Classification Tree</a></li>
<li class="chapter" data-level="4.3.2" data-path="decision-trees.html"><a href="decision-trees.html#bagging-regression-tree"><i class="fa fa-check"></i><b>4.3.2</b> Bagging Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="decision-trees.html"><a href="decision-trees.html#random-forests"><i class="fa fa-check"></i><b>4.4</b> Random Forests</a></li>
<li class="chapter" data-level="4.5" data-path="decision-trees.html"><a href="decision-trees.html#gradient-boosting"><i class="fa fa-check"></i><b>4.5</b> Gradient Boosting</a></li>
<li class="chapter" data-level="4.6" data-path="decision-trees.html"><a href="decision-trees.html#summary"><i class="fa fa-check"></i><b>4.6</b> Summary</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-trees"><i class="fa fa-check"></i><b>4.6.1</b> Classification Trees</a></li>
<li class="chapter" data-level="4.6.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>4.6.2</b> Regression Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="non-linear-models.html"><a href="non-linear-models.html"><i class="fa fa-check"></i><b>5</b> Non-linear Models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="non-linear-models.html"><a href="non-linear-models.html#splines"><i class="fa fa-check"></i><b>5.1</b> Splines</a></li>
<li class="chapter" data-level="5.2" data-path="non-linear-models.html"><a href="non-linear-models.html#mars"><i class="fa fa-check"></i><b>5.2</b> MARS</a></li>
<li class="chapter" data-level="5.3" data-path="non-linear-models.html"><a href="non-linear-models.html#gam"><i class="fa fa-check"></i><b>5.3</b> GAM</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>6</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="6.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximal-margin-classifier"><i class="fa fa-check"></i><b>6.1</b> Maximal Margin Classifier</a></li>
<li class="chapter" data-level="6.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-classifier"><i class="fa fa-check"></i><b>6.2</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="6.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#support-vector-machines-1"><i class="fa fa-check"></i><b>6.3</b> Support Vector Machines</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="EMMs.html"><a href="EMMs.html"><i class="fa fa-check"></i><b>7</b> Topics: EMMs</a>
<ul>
<li class="chapter" data-level="7.1" data-path="EMMs.html"><a href="EMMs.html#references"><i class="fa fa-check"></i><b>7.1</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="BayesRegression.html"><a href="BayesRegression.html"><i class="fa fa-check"></i><b>8</b> Bayesian Regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="BayesRegression.html"><a href="BayesRegression.html#compared-to-frequentist-regression"><i class="fa fa-check"></i><b>8.1</b> Compared to Frequentist Regression</a></li>
<li class="chapter" data-level="8.2" data-path="BayesRegression.html"><a href="BayesRegression.html#model-evaluation"><i class="fa fa-check"></i><b>8.2</b> Model Evaluation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="BayesRegression.html"><a href="BayesRegression.html#model-comparison"><i class="fa fa-check"></i><b>8.2.1</b> Model Comparison</a></li>
<li class="chapter" data-level="8.2.2" data-path="BayesRegression.html"><a href="BayesRegression.html#visualization"><i class="fa fa-check"></i><b>8.2.2</b> Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supervised Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ordinary-least-squares" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Ordinary Least Squares<a href="ordinary-least-squares.html#ordinary-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="ordinary-least-squares.html#cb1-1" tabindex="-1"></a><span class="fu">library</span>(caret)</span></code></pre></div>
<pre><code>## Warning: package &#39;caret&#39; was built under R version 4.3.1</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="ordinary-least-squares.html#cb3-1" tabindex="-1"></a><span class="fu">library</span>(Metrics)</span></code></pre></div>
<pre><code>## Warning: package &#39;Metrics&#39; was built under R version 4.3.1</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="ordinary-least-squares.html#cb5-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb5-2"><a href="ordinary-least-squares.html#cb5-2" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span></code></pre></div>
<pre><code>## Warning: package &#39;corrplot&#39; was built under R version 4.3.1</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="ordinary-least-squares.html#cb7-1" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span></code></pre></div>
<pre><code>## Warning: package &#39;gridExtra&#39; was built under R version 4.3.1</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="ordinary-least-squares.html#cb9-1" tabindex="-1"></a><span class="fu">library</span>(car)  <span class="co"># for avPlots</span></span></code></pre></div>
<pre><code>## Warning: package &#39;car&#39; was built under R version 4.3.1</code></pre>
<pre><code>## Warning: package &#39;carData&#39; was built under R version 4.3.1</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="ordinary-least-squares.html#cb12-1" tabindex="-1"></a><span class="fu">library</span>(AppliedPredictiveModeling)</span></code></pre></div>
<pre><code>## Warning: package &#39;AppliedPredictiveModeling&#39; was built under R version 4.3.1</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="ordinary-least-squares.html#cb14-1" tabindex="-1"></a><span class="fu">library</span>(e1071)  <span class="co"># for skewness()</span></span></code></pre></div>
<pre><code>## Warning: package &#39;e1071&#39; was built under R version 4.3.1</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="ordinary-least-squares.html#cb16-1" tabindex="-1"></a><span class="fu">library</span>(purrr)  <span class="co"># for map()</span></span>
<span id="cb16-2"><a href="ordinary-least-squares.html#cb16-2" tabindex="-1"></a><span class="fu">library</span>(broom)  <span class="co"># for augment()</span></span></code></pre></div>
<p>These notes cover <em>linear regression</em>.</p>
<div id="linear-regression-model" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Linear Regression Model<a href="ordinary-least-squares.html#linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The population regression model <span class="math inline">\(E(Y) = X \beta\)</span> summarizes the trend between the predictors and the mean responses. The individual responses are assumed to be normally distributed about the population regression, <span class="math inline">\(y_i = X_i \beta + \epsilon_i\)</span> with varying mean, but constant variance, <span class="math inline">\(y_i \sim N(\mu_i, \sigma^2).\)</span> Equivalently, the model presumes a linear relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span> with residuals <span class="math inline">\(\epsilon\)</span> that are independent normal random variables with mean zero and constant variance <span class="math inline">\(\sigma^2\)</span>. Estimate the population regression model coefficients as <span class="math inline">\(\hat{y} = X \hat{\beta}\)</span>, and the population variance as <span class="math inline">\(\hat{\sigma}^2\)</span>. The most common method of estimating the <span class="math inline">\(\beta\)</span> coefficients and <span class="math inline">\(\sigma\)</span> is ordinary least squares (OLS). OLS minimizes the sum of squared residuals from a random sample. The individual predicted values vary about the actual value, <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>, where <span class="math inline">\(\hat{y}_i = X_i \hat{\beta}\)</span>.</p>
<p>The OLS model is the best linear unbiased estimator (BLUE) if the residuals are independent random variables normally distributed with mean zero and constant variance. Recall these conditions with the LINE pneumonic: <strong>L</strong>inear, <strong>I</strong>ndependent, <strong>N</strong>ormal, and <strong>E</strong>qual.</p>
<p><strong>Linearity</strong>. The explanatory variables are each linearly related to the response variable: <span class="math inline">\(E(\epsilon | X_j) = 0\)</span>.</p>
<p><strong>Independence</strong>. The residuals are unrelated to each other. Independence is violated by repeated measurements and temporal regressors.</p>
<p><strong>Normality</strong>. The residuals are normally distributed: <span class="math inline">\(\epsilon|X \sim N(0, \sigma^2I)\)</span>.</p>
<p><strong>Equal Variances</strong>. The variance of the residuals is constant (homoscedasticity): <span class="math inline">\(E(\epsilon \epsilon&#39; | X) = \sigma^2I\)</span></p>
<p>Additionally, you should make sure you model has “little” or no <strong>multicollinearity</strong> among the variables.</p>
</div>
<div id="parameter-estimation" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Parameter Estimation<a href="ordinary-least-squares.html#parameter-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are two model parameters to estimate: <span class="math inline">\(\hat{\beta}\)</span> estimates the coefficient vector <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\hat{\sigma}\)</span> estimates the variance of the residuals along the regression line.</p>
<p>Derive the coefficient estimators by minimizing the sum of squared residuals <span class="math inline">\(SSE = (y - X \hat{\beta})&#39; (y - X \hat{\beta})\)</span>. The result is</p>
<p><span class="math display">\[\hat{\beta} = (X&#39;X)^{-1}X&#39;y.\]</span></p>
<p>The residual standard error (RSE) estimates the sample deviation around the population regression line. <em>(Think of each value of <span class="math inline">\(X\)</span> along the regression line as a subpopulation with mean <span class="math inline">\(y_i\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. This variance is assumed to be the same for all <span class="math inline">\(X\)</span>.)</em></p>
<p><span class="math display">\[\hat{\sigma} = \sqrt{(n-k-1)^{-1} e&#39;e}.\]</span></p>
<p>The standard error for the coefficient estimators is the square root of the error variance divided by <span class="math inline">\((X&#39;X)\)</span>.</p>
<p><span class="math display">\[SE(\hat{\beta}) = \sqrt{\hat{\sigma}^2 (X&#39;X)^{-1}}.\]</span></p>
<div id="example" class="section level4 hasAnchor" number="1.2.0.1">
<h4><span class="header-section-number">1.2.0.1</span> Example<a href="ordinary-least-squares.html#example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Dataset <code>mtcars</code> contains response variable fuel consumption <code>mpg</code> and 10 aspects of automobile design and performance for 32 automobiles. What is the relationship between the response variable and its predictors?</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="ordinary-least-squares.html#cb17-1" tabindex="-1"></a>d <span class="ot">&lt;-</span> mtcars <span class="sc">%&gt;%</span></span>
<span id="cb17-2"><a href="ordinary-least-squares.html#cb17-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">vs =</span> <span class="fu">factor</span>(vs, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;V&quot;</span>, <span class="st">&quot;S&quot;</span>)),</span>
<span id="cb17-3"><a href="ordinary-least-squares.html#cb17-3" tabindex="-1"></a>         <span class="at">am =</span> <span class="fu">factor</span>(am, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;automatic&quot;</span>, <span class="st">&quot;manual&quot;</span>)),</span>
<span id="cb17-4"><a href="ordinary-least-squares.html#cb17-4" tabindex="-1"></a>         <span class="at">cyl =</span> <span class="fu">ordered</span>(cyl),</span>
<span id="cb17-5"><a href="ordinary-least-squares.html#cb17-5" tabindex="-1"></a>         <span class="at">gear =</span> <span class="fu">ordered</span>(gear), </span>
<span id="cb17-6"><a href="ordinary-least-squares.html#cb17-6" tabindex="-1"></a>         <span class="at">carb =</span> <span class="fu">ordered</span>(carb))</span>
<span id="cb17-7"><a href="ordinary-least-squares.html#cb17-7" tabindex="-1"></a><span class="fu">glimpse</span>(d)</span></code></pre></div>
<pre><code>## Rows: 32
## Columns: 11
## $ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…
## $ cyl  &lt;ord&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…
## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…
## $ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…
## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…
## $ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…
## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…
## $ vs   &lt;fct&gt; V, V, S, S, V, S, V, S, S, S, S, V, V, V, V, V, V, S, S, S, S, V,…
## $ am   &lt;fct&gt; manual, manual, manual, automatic, automatic, automatic, automati…
## $ gear &lt;ord&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…
## $ carb &lt;ord&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…</code></pre>
<p>The data consists of 32 observations. A scatterplot matrix of the numeric variables shows the strongest individual association with <code>mpg</code> is from <code>wt</code> (corr = -0.87) followed by <code>disp</code> (corr = -0.85) and <code>hp</code> (corr = -0.78), <code>drat</code> is moderately correlated with <code>mpg</code> (corr = 0.68), and <code>qsec</code> is weakly correlated with <code>mpg</code> (corr = 0.42).</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="ordinary-least-squares.html#cb19-1" tabindex="-1"></a><span class="fu">corrplot</span>(<span class="fu">cor</span>(<span class="fu">subset</span>(d, <span class="at">select =</span> <span class="fu">c</span>(mpg, disp, hp, drat, wt, qsec))), </span>
<span id="cb19-2"><a href="ordinary-least-squares.html#cb19-2" tabindex="-1"></a>         <span class="at">type =</span> <span class="st">&quot;upper&quot;</span>, </span>
<span id="cb19-3"><a href="ordinary-least-squares.html#cb19-3" tabindex="-1"></a>         <span class="at">method =</span> <span class="st">&quot;number&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Many of the Predictor variables are strongly correlated with each other. Boxplots of the categorical variables shows differences in levels, although ordinal variables <code>gear</code> and and <code>carb</code> do not have a monotonic relationshiop with <code>mpg</code>.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="ordinary-least-squares.html#cb20-1" tabindex="-1"></a>p_list <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb20-2"><a href="ordinary-least-squares.html#cb20-2" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">c</span>(<span class="st">&quot;cyl&quot;</span>, <span class="st">&quot;vs&quot;</span>, <span class="st">&quot;am&quot;</span>, <span class="st">&quot;gear&quot;</span>, <span class="st">&quot;carb&quot;</span>)) {</span>
<span id="cb20-3"><a href="ordinary-least-squares.html#cb20-3" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(d, <span class="fu">aes_string</span>(<span class="at">x =</span> i, <span class="at">y =</span> <span class="st">&quot;mpg&quot;</span>)) <span class="sc">+</span> <span class="fu">geom_boxplot</span>()</span>
<span id="cb20-4"><a href="ordinary-least-squares.html#cb20-4" tabindex="-1"></a>  p_list <span class="ot">&lt;-</span> <span class="fu">c</span>(p_list, <span class="fu">list</span>(p))</span>
<span id="cb20-5"><a href="ordinary-least-squares.html#cb20-5" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Warning: `aes_string()` was deprecated in ggplot2 3.0.0.
## ℹ Please use tidy evaluation idioms with `aes()`.
## ℹ See also `vignette(&quot;ggplot2-in-packages&quot;)` for more information.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="ordinary-least-squares.html#cb22-1" tabindex="-1"></a><span class="fu">do.call</span>(<span class="st">&quot;grid.arrange&quot;</span>, <span class="fu">c</span>(p_list, <span class="at">ncol =</span> <span class="dv">2</span>))</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>I’ll drop the <code>gear</code> and <code>carb</code> predictors, and fit a population model to the remaining predictors.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="ordinary-least-squares.html#cb23-1" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> d[,<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>])</span>
<span id="cb23-2"><a href="ordinary-least-squares.html#cb23-2" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ ., data = d[, 1:9])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.9978 -1.3551 -0.3108  1.1992  4.1102 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) 19.540985  14.146419   1.381   0.1810  
## cyl.L        0.342558   2.764833   0.124   0.9025  
## cyl.Q        1.388429   1.112097   1.248   0.2250  
## disp         0.006688   0.013512   0.495   0.6255  
## hp          -0.029141   0.017182  -1.696   0.1040  
## drat         0.588059   1.503111   0.391   0.6994  
## wt          -3.155246   1.420235  -2.222   0.0369 *
## qsec         0.523235   0.690130   0.758   0.4564  
## vsS          1.237800   2.106056   0.588   0.5627  
## ammanual     3.000910   1.853400   1.619   0.1197  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.514 on 22 degrees of freedom
## Multiple R-squared:  0.8765, Adjusted R-squared:  0.826 
## F-statistic: 17.35 on 9 and 22 DF,  p-value: 4.814e-08</code></pre>
<p>The <code>summary()</code> function shows <span class="math inline">\(\hat{\beta}\)</span> as <code>Estimate</code>, <span class="math inline">\(SE({\hat{\beta}})\)</span> as <code>Std. Error</code>, and <span class="math inline">\(\hat{\sigma}\)</span> as <code>Residual standard error</code>. You can verify this by manually peforming these calculations using matrix algebra (see matrix algebra in r notes at <a href="https://www.dummies.com/programming/r/how-to-do-matrix-arithmetic-in-r/">R for Dummies</a>). Here are the coefficient estimators, <span class="math inline">\(\hat{\beta} = (X&#39;X)^{-1}X&#39;y\)</span>.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="ordinary-least-squares.html#cb25-1" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(m)</span>
<span id="cb25-2"><a href="ordinary-least-squares.html#cb25-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> d<span class="sc">$</span>mpg</span>
<span id="cb25-3"><a href="ordinary-least-squares.html#cb25-3" tabindex="-1"></a></span>
<span id="cb25-4"><a href="ordinary-least-squares.html#cb25-4" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb25-5"><a href="ordinary-least-squares.html#cb25-5" tabindex="-1"></a><span class="fu">round</span>(beta_hat, <span class="dv">5</span>)</span></code></pre></div>
<pre><code>##                 [,1]
## (Intercept) 19.54098
## cyl.L        0.34256
## cyl.Q        1.38843
## disp         0.00669
## hp          -0.02914
## drat         0.58806
## wt          -3.15525
## qsec         0.52324
## vsS          1.23780
## ammanual     3.00091</code></pre>
<p>Here is the residual standard error, <span class="math inline">\(\hat{\sigma} = \sqrt{(n-k-1)^{-1} \hat{e}&#39;\hat{e}}\)</span>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="ordinary-least-squares.html#cb27-1" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb27-2"><a href="ordinary-least-squares.html#cb27-2" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X) <span class="sc">-</span> <span class="dv">1</span>  <span class="co"># exclude the intercept term</span></span>
<span id="cb27-3"><a href="ordinary-least-squares.html#cb27-3" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_hat</span>
<span id="cb27-4"><a href="ordinary-least-squares.html#cb27-4" tabindex="-1"></a>sse <span class="ot">&lt;-</span> <span class="fu">sum</span>((y <span class="sc">-</span> y_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb27-5"><a href="ordinary-least-squares.html#cb27-5" tabindex="-1"></a>rse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(sse <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>))</span>
<span id="cb27-6"><a href="ordinary-least-squares.html#cb27-6" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Residual standard error: &quot;</span>, <span class="fu">round</span>(rse, <span class="dv">3</span>), <span class="st">&quot; on &quot;</span>, (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>), <span class="st">&quot; degrees of freedom.&quot;</span>)</span></code></pre></div>
<pre><code>## Residual standard error:  2.514  on  22  degrees of freedom.</code></pre>
<p>Use the residual standard errors to derive the standard errors of the coefficients, <span class="math inline">\(SE(\hat{\beta}) = \sqrt{\hat{\sigma}^2 (X&#39;X)^{-1}}\)</span>.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="ordinary-least-squares.html#cb29-1" tabindex="-1"></a>se_beta_hat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(rse<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)))</span>
<span id="cb29-2"><a href="ordinary-least-squares.html#cb29-2" tabindex="-1"></a><span class="fu">matrix</span>(<span class="fu">round</span>(se_beta_hat, <span class="dv">5</span>), <span class="at">dimnames =</span> <span class="fu">list</span>(<span class="fu">names</span>(se_beta_hat), <span class="st">&quot;Std. Error&quot;</span>))</span></code></pre></div>
<pre><code>##             Std. Error
## (Intercept)   14.14642
## cyl.L          2.76483
## cyl.Q          1.11210
## disp           0.01351
## hp             0.01718
## drat           1.50311
## wt             1.42023
## qsec           0.69013
## vsS            2.10606
## ammanual       1.85340</code></pre>
</div>
</div>
<div id="model-assumptions" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Model Assumptions<a href="ordinary-least-squares.html#model-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The linear regression model assumes the relationship between the predictors and the response is linear and the residuals are independent random variables normally distributed with mean zero and constant variance.</p>
<p>Additionally, you will want to check for multicollinearity in the predictors because it can produce unreliable coefficient estimates and predicted values.</p>
<p>Use a residuals vs fits plot <span class="math inline">\(\left( e \sim \hat{Y} \right)\)</span> to detect non-linearity and unequal error variances, including outliers. The polynomial trend line should show that the residuals vary around <span class="math inline">\(e = 0\)</span> in a straight line (linearity). The variance should be of constant width (especially no fan shape at the low or high ends).</p>
<p>Use a residuals normal probability plot to compares the theoretical percentiles of the normal distribution versus the observed sample percentiles. It should be approximately linear.</p>
<p>A scale-location plot <span class="math inline">\(\sqrt{e / sd(e)} \sim \hat{y}\)</span> checks the homogeneity of variance of the residuals (homoscedasticity). The square root of the absolute value of the residuals should be spread equally along a horizontal line.</p>
<p>A residuals vs leverage plot identifies influential observations. A plot of the standardized residuals vs the leverage should fall within the 95% probability band.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="ordinary-least-squares.html#cb31-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb31-2"><a href="ordinary-least-squares.html#cb31-2" tabindex="-1"></a><span class="fu">plot</span>(m, <span class="at">labels.id =</span> <span class="cn">NULL</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div id="linearity" class="section level3 hasAnchor" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Linearity<a href="ordinary-least-squares.html#linearity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The explanatory variables should each be linearly related to the response variable: <span class="math inline">\(E(\epsilon | X_j) = 0\)</span>. A good way to test this condition is with a residuals vs fitted values plot. A curved pattern in the residuals indicates a curvature in the relationship between the response and the predictor that is not explained by our model. A linear model does not adequately describe the relationship between the predictor and the response.</p>
<p>Test for linearity four ways:</p>
<ul>
<li>Residuals vs fits plot <span class="math inline">\((e \sim \hat{Y})\)</span> should bounce randomly around 0.</li>
<li>Observed vs fits plot <span class="math inline">\((Y \sim \hat{Y})\)</span> should be symmetric along the 45-degree line.<br />
</li>
<li>Each <span class="math inline">\((Y \sim X_j )\)</span> plot should have correlation <span class="math inline">\(\rho \sim 1\)</span>.<br />
</li>
<li>Each <span class="math inline">\((e \sim X_j)\)</span> plot should exhibit no pattern.</li>
</ul>
<p>If the linearity condition fails, change the functional form of the model with non-linear transformations of the explanatory variables. A common way to do this is with Box-Cox transformations.</p>
<p><span class="math display">\[w_t = \begin{cases} \begin{array}{l} log(y_t) \quad \quad \lambda = 0 \\
(y_t^\lambda - 1) / \lambda \quad \text{otherwise} \end{array} \end{cases}\]</span></p>
<p><span class="math inline">\(\lambda\)</span> can take any value, but values near the following yield familiar transformations.</p>
<ul>
<li><span class="math inline">\(\lambda = 1\)</span> yields no substantive transformation.<br />
</li>
<li><span class="math inline">\(\lambda = 0.5\)</span> is a square root plus linear transformation.</li>
<li><span class="math inline">\(\lambda = 0.333\)</span> is a cube root plus linear transformation.</li>
<li><span class="math inline">\(\lambda = 0\)</span> is a natural log transformation.</li>
<li><span class="math inline">\(\lambda = -1\)</span> is an inverse transformation.</li>
</ul>
<p>A common source of non-linearity in a model is skewed response or independent variables (see discussion <a href="https://blog.minitab.com/blog/applying-statistics-in-quality-projects/how-could-you-benefit-from-a-box-cox-transformation">here</a>). <code>mtcars</code> has some skewed variables.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="ordinary-least-squares.html#cb32-1" tabindex="-1"></a>tmp <span class="ot">&lt;-</span> <span class="fu">map</span>(mtcars, skewness) <span class="sc">%&gt;%</span> </span>
<span id="cb32-2"><a href="ordinary-least-squares.html#cb32-2" tabindex="-1"></a>  <span class="fu">unlist</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb32-3"><a href="ordinary-least-squares.html#cb32-3" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb32-4"><a href="ordinary-least-squares.html#cb32-4" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>() </span>
<span id="cb32-5"><a href="ordinary-least-squares.html#cb32-5" tabindex="-1"></a><span class="fu">colnames</span>(tmp) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;IV&quot;</span>, <span class="st">&quot;skew&quot;</span>)</span>
<span id="cb32-6"><a href="ordinary-least-squares.html#cb32-6" tabindex="-1"></a><span class="fu">ggplot</span>(tmp, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">order</span>(IV, skew), <span class="at">y =</span> skew)) <span class="sc">+</span></span>
<span id="cb32-7"><a href="ordinary-least-squares.html#cb32-7" tabindex="-1"></a>  <span class="fu">geom_col</span>()</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="ordinary-least-squares.html#cb33-1" tabindex="-1"></a>df <span class="ot">&lt;-</span> mtcars</span>
<span id="cb33-2"><a href="ordinary-least-squares.html#cb33-2" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp, <span class="at">data =</span> df)</span>
<span id="cb33-3"><a href="ordinary-least-squares.html#cb33-3" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb33-4"><a href="ordinary-least-squares.html#cb33-4" tabindex="-1"></a><span class="fu">plot</span>(m)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="ordinary-least-squares.html#cb34-1" tabindex="-1"></a><span class="fu">plot</span>(m<span class="sc">$</span>model<span class="sc">$</span>mpg, m<span class="sc">$</span>fitted.values)</span>
<span id="cb34-2"><a href="ordinary-least-squares.html#cb34-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb34-3"><a href="ordinary-least-squares.html#cb34-3" tabindex="-1"></a><span class="fu">cor</span>(df<span class="sc">$</span>mpg, df<span class="sc">$</span>hp)</span></code></pre></div>
<pre><code>## [1] -0.7761684</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="ordinary-least-squares.html#cb36-1" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> m<span class="sc">$</span>fitted.values, <span class="at">obs =</span> m<span class="sc">$</span>model<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 3.7402971 0.6024373 2.9074525</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="ordinary-least-squares.html#cb38-1" tabindex="-1"></a>bc <span class="ot">&lt;-</span> <span class="fu">BoxCoxTrans</span>(mtcars<span class="sc">$</span>hp)</span>
<span id="cb38-2"><a href="ordinary-least-squares.html#cb38-2" tabindex="-1"></a>df<span class="sc">$</span>hp_bc <span class="ot">&lt;-</span> <span class="fu">predict</span>(bc, mtcars<span class="sc">$</span>hp)</span>
<span id="cb38-3"><a href="ordinary-least-squares.html#cb38-3" tabindex="-1"></a>m_bc <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp_bc, <span class="at">data =</span> df)</span>
<span id="cb38-4"><a href="ordinary-least-squares.html#cb38-4" tabindex="-1"></a><span class="fu">plot</span>(m_bc)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-11-3.png" width="672" /></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="ordinary-least-squares.html#cb39-1" tabindex="-1"></a><span class="fu">plot</span>(m_bc<span class="sc">$</span>model<span class="sc">$</span>mpg, m_bc<span class="sc">$</span>fitted.values)</span>
<span id="cb39-2"><a href="ordinary-least-squares.html#cb39-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb39-3"><a href="ordinary-least-squares.html#cb39-3" tabindex="-1"></a><span class="fu">cor</span>(df<span class="sc">$</span>mpg, df<span class="sc">$</span>hp_bc)</span></code></pre></div>
<pre><code>## [1] -0.8487707</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="ordinary-least-squares.html#cb41-1" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> m_bc<span class="sc">$</span>fitted.values, <span class="at">obs =</span> m_bc<span class="sc">$</span>model<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 3.1366261 0.7204118 2.4074705</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="ordinary-least-squares.html#cb43-1" tabindex="-1"></a><span class="co"># Which vars are skewed?</span></span>
<span id="cb43-2"><a href="ordinary-least-squares.html#cb43-2" tabindex="-1"></a><span class="fu">map</span>(mtcars, skewness)</span></code></pre></div>
<pre><code>## $mpg
## [1] 0.610655
## 
## $cyl
## [1] -0.1746119
## 
## $disp
## [1] 0.381657
## 
## $hp
## [1] 0.7260237
## 
## $drat
## [1] 0.2659039
## 
## $wt
## [1] 0.4231465
## 
## $qsec
## [1] 0.3690453
## 
## $vs
## [1] 0.2402577
## 
## $am
## [1] 0.3640159
## 
## $gear
## [1] 0.5288545
## 
## $carb
## [1] 1.050874</code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="ordinary-least-squares.html#cb45-1" tabindex="-1"></a><span class="co"># Benchmark model: mpg ~ hp</span></span>
<span id="cb45-2"><a href="ordinary-least-squares.html#cb45-2" tabindex="-1"></a>d0 <span class="ot">&lt;-</span> mtcars</span>
<span id="cb45-3"><a href="ordinary-least-squares.html#cb45-3" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp, <span class="at">data =</span> d0)</span>
<span id="cb45-4"><a href="ordinary-least-squares.html#cb45-4" tabindex="-1"></a>d0 <span class="ot">&lt;-</span> <span class="fu">augment</span>(m0, d0)</span>
<span id="cb45-5"><a href="ordinary-least-squares.html#cb45-5" tabindex="-1"></a>d0.cor <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">cor</span>(d0<span class="sc">$</span>mpg, d0<span class="sc">$</span>hp), <span class="dv">2</span>)</span>
<span id="cb45-6"><a href="ordinary-least-squares.html#cb45-6" tabindex="-1"></a></span>
<span id="cb45-7"><a href="ordinary-least-squares.html#cb45-7" tabindex="-1"></a><span class="co"># Benchmark diagnostics</span></span>
<span id="cb45-8"><a href="ordinary-least-squares.html#cb45-8" tabindex="-1"></a>p0a <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(d0, <span class="fu">aes</span>(<span class="at">x =</span> hp, <span class="at">y =</span> mpg)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> </span>
<span id="cb45-9"><a href="ordinary-least-squares.html#cb45-9" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Response vs IV&quot;</span>,</span>
<span id="cb45-10"><a href="ordinary-least-squares.html#cb45-10" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="fu">paste0</span>(<span class="st">&quot;Correlation ~ 1?  (rho = &quot;</span>, d0.cor, <span class="st">&quot;)&quot;</span>))</span>
<span id="cb45-11"><a href="ordinary-least-squares.html#cb45-11" tabindex="-1"></a>p0b <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(d0, <span class="fu">aes</span>(<span class="at">x =</span> .fitted, <span class="at">y =</span> .resid)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">0</span>) <span class="sc">+</span> </span>
<span id="cb45-12"><a href="ordinary-least-squares.html#cb45-12" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Residuals vs Fits&quot;</span>,</span>
<span id="cb45-13"><a href="ordinary-least-squares.html#cb45-13" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;Random scatter?&quot;</span>)</span>
<span id="cb45-14"><a href="ordinary-least-squares.html#cb45-14" tabindex="-1"></a>p0c <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(d0, <span class="fu">aes</span>(<span class="at">x =</span> .fitted, <span class="at">y =</span> mpg)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb45-15"><a href="ordinary-least-squares.html#cb45-15" tabindex="-1"></a>  <span class="fu">expand_limits</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">35</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">35</span>)) <span class="sc">+</span></span>
<span id="cb45-16"><a href="ordinary-least-squares.html#cb45-16" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Observed vs Fits&quot;</span>,</span>
<span id="cb45-17"><a href="ordinary-least-squares.html#cb45-17" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;Symmetric along 45 degree line?&quot;</span>)</span>
<span id="cb45-18"><a href="ordinary-least-squares.html#cb45-18" tabindex="-1"></a>p0d <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(d0, <span class="fu">aes</span>(<span class="at">x =</span> hp, <span class="at">y =</span> .resid)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">0</span>) <span class="sc">+</span> </span>
<span id="cb45-19"><a href="ordinary-least-squares.html#cb45-19" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Residuals vs IV&quot;</span>,</span>
<span id="cb45-20"><a href="ordinary-least-squares.html#cb45-20" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;Random scatter?&quot;</span>)</span>
<span id="cb45-21"><a href="ordinary-least-squares.html#cb45-21" tabindex="-1"></a><span class="fu">grid.arrange</span>(p0a, p0b, p0c, p0d, <span class="at">nrow =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="ordinary-least-squares.html#cb47-1" tabindex="-1"></a><span class="co"># Benchmark performance</span></span>
<span id="cb47-2"><a href="ordinary-least-squares.html#cb47-2" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> m0<span class="sc">$</span>fitted.values, <span class="at">obs =</span> m0<span class="sc">$</span>model<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 3.7402971 0.6024373 2.9074525</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="ordinary-least-squares.html#cb49-1" tabindex="-1"></a><span class="co"># Box-Cox transform hp</span></span>
<span id="cb49-2"><a href="ordinary-least-squares.html#cb49-2" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> mtcars</span>
<span id="cb49-3"><a href="ordinary-least-squares.html#cb49-3" tabindex="-1"></a>bc <span class="ot">&lt;-</span> <span class="fu">BoxCoxTrans</span>(d1<span class="sc">$</span>hp)</span>
<span id="cb49-4"><a href="ordinary-least-squares.html#cb49-4" tabindex="-1"></a>d1<span class="sc">$</span>hp_bc <span class="ot">&lt;-</span> <span class="fu">predict</span>(bc, d1<span class="sc">$</span>hp)</span>
<span id="cb49-5"><a href="ordinary-least-squares.html#cb49-5" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> hp_bc, <span class="at">data =</span> d1)</span>
<span id="cb49-6"><a href="ordinary-least-squares.html#cb49-6" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> <span class="fu">augment</span>(m1, d1)</span>
<span id="cb49-7"><a href="ordinary-least-squares.html#cb49-7" tabindex="-1"></a>d1.cor <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">cor</span>(d1<span class="sc">$</span>mpg, d1<span class="sc">$</span>hp_bc), <span class="dv">2</span>)</span>
<span id="cb49-8"><a href="ordinary-least-squares.html#cb49-8" tabindex="-1"></a></span>
<span id="cb49-9"><a href="ordinary-least-squares.html#cb49-9" tabindex="-1"></a>p1a <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(d1, <span class="fu">aes</span>(<span class="at">x =</span> hp_bc, <span class="at">y =</span> mpg)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> </span>
<span id="cb49-10"><a href="ordinary-least-squares.html#cb49-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Response vs IV&quot;</span>,</span>
<span id="cb49-11"><a href="ordinary-least-squares.html#cb49-11" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="fu">paste0</span>(<span class="st">&quot;Correlation ~ 1?  (rho = &quot;</span>, d1.cor, <span class="st">&quot;)&quot;</span>))</span>
<span id="cb49-12"><a href="ordinary-least-squares.html#cb49-12" tabindex="-1"></a>p1b <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(d1, <span class="fu">aes</span>(<span class="at">x =</span> .fitted, <span class="at">y =</span> .resid)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">0</span>) <span class="sc">+</span> </span>
<span id="cb49-13"><a href="ordinary-least-squares.html#cb49-13" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Residuals vs Fits&quot;</span>,</span>
<span id="cb49-14"><a href="ordinary-least-squares.html#cb49-14" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;Random scatter?&quot;</span>)</span>
<span id="cb49-15"><a href="ordinary-least-squares.html#cb49-15" tabindex="-1"></a>p1c <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(d1, <span class="fu">aes</span>(<span class="at">x =</span> .fitted, <span class="at">y =</span> mpg)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb49-16"><a href="ordinary-least-squares.html#cb49-16" tabindex="-1"></a>  <span class="fu">expand_limits</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">35</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">35</span>)) <span class="sc">+</span></span>
<span id="cb49-17"><a href="ordinary-least-squares.html#cb49-17" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Observed vs Fits&quot;</span>,</span>
<span id="cb49-18"><a href="ordinary-least-squares.html#cb49-18" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;Symmetric along 45 degree line?&quot;</span>)</span>
<span id="cb49-19"><a href="ordinary-least-squares.html#cb49-19" tabindex="-1"></a>p1d <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(d1, <span class="fu">aes</span>(<span class="at">x =</span> hp, <span class="at">y =</span> .resid)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">0</span>) <span class="sc">+</span> </span>
<span id="cb49-20"><a href="ordinary-least-squares.html#cb49-20" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Residuals vs IV&quot;</span>,</span>
<span id="cb49-21"><a href="ordinary-least-squares.html#cb49-21" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;Random scatter?&quot;</span>)</span>
<span id="cb49-22"><a href="ordinary-least-squares.html#cb49-22" tabindex="-1"></a><span class="fu">grid.arrange</span>(p1a, p1b, p1c, p1d, <span class="at">nrow =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="ordinary-least-squares.html#cb51-1" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> m1<span class="sc">$</span>fitted.values, <span class="at">obs =</span> m1<span class="sc">$</span>model<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 3.1366261 0.7204118 2.4074705</code></pre>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-11-4.png" width="672" /></p>
</div>
<div id="multicollinearity" class="section level3 hasAnchor" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Multicollinearity<a href="ordinary-least-squares.html#multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The multicollinearity condition is violated when two or more of the predictors in a regression model are correlated. Muticollinearity can occur for <em>structural</em> reasons, as when one variable is a transformation of another variable, or for <em>data</em> reasons, as occurs in observational studies. Multicollinearity is a problem because it inflates the variances of the estimated coefficients, resulting in larger confidence intervals.</p>
<p>When predictor variables are correlated, the precision of the estimated regression coefficients decreases with each added correlated predictor variable. The usual interpretation of a slope coefficient as the change in the mean response for each additional unit increase in the predictor when all the other predictors are held constant breaks down because changing one predictor necessarily changes the others.</p>
<p>A residuals vs fits plot <span class="math inline">\((\epsilon \sim \hat{Y})\)</span> should have correlation <span class="math inline">\(\rho \sim 0\)</span>. A correlation matrix is helpful for picking out the correlation strengths. A good rule of thumb is correlation coefficients should be less than 0.80. However, this test may not work when a variable is correlated with a function of other variables. A model with multicollinearity may have a significant F-test with insignificant individual slope estimator t-tests. Another way to detect multicollinearity is by calculating variance inflation factors. The predictor variance <span class="math inline">\(Var(\hat{\beta_k})\)</span> increases by a factor</p>
<p><span class="math display">\[VIF_k = \frac{1}{1 - R_k^2}\]</span></p>
<p>where <span class="math inline">\(R_k^2\)</span> is the <span class="math inline">\(R^2\)</span> of a regression of the <span class="math inline">\(k^{th}\)</span> predictor on the remaining predictors. A <span class="math inline">\(VIF_k\)</span> of <span class="math inline">\(1\)</span> indicates no inflation (no corellation). A <span class="math inline">\(VIF_k &gt;= 4\)</span> warrants investigation. A <span class="math inline">\(VIF_k &gt;= 10\)</span> requires correction.</p>
<div id="example-1" class="section level4 hasAnchor" number="1.3.2.1">
<h4><span class="header-section-number">1.3.2.1</span> Example<a href="ordinary-least-squares.html#example-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Does the model <code>mpg ~ .</code> exhibit multicollinearity?</p>
<p>The correlation matrix above (and presented again below) has several correlated covariates. <code>disp</code> is strongly correlated with <code>wt</code> (r = 0.89) and <code>hp</code> (r = 0.79).</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="ordinary-least-squares.html#cb53-1" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mtcars)</span>
<span id="cb53-2"><a href="ordinary-least-squares.html#cb53-2" tabindex="-1"></a><span class="fu">corrplot</span>(<span class="fu">cor</span>(<span class="fu">subset</span>(d, <span class="at">select =</span> <span class="fu">c</span>(mpg, disp, hp, drat, wt, qsec))), <span class="at">type =</span> <span class="st">&quot;upper&quot;</span>, <span class="at">method =</span> <span class="st">&quot;number&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Calculate the VIFs.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="ordinary-least-squares.html#cb54-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">vif</span>(m), <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb 
## 15.37 21.62  9.83  3.37 15.16  7.53  4.97  4.65  5.36  7.91</code></pre>
<p>There are two predictors with VIFs greater than 10, cyl (GVIF = 21.36) and disp (GVIF = 13.76). One way to address multicollinearity is removing one or more of the violating predictors from the regression model. Try removing <code>cyl</code>.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="ordinary-least-squares.html#cb56-1" tabindex="-1"></a><span class="fu">vif</span>(m <span class="ot">&lt;-</span><span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">-</span> cyl, <span class="at">data =</span> d[,<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>]))</span></code></pre></div>
<pre><code>##     disp       hp     drat       wt     qsec       vs       am 
## 9.865991 5.448912 2.818949 7.598119 5.979588 4.249244 3.450410</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="ordinary-least-squares.html#cb58-1" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ . - cyl, data = d[, 1:9])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4067 -1.4690 -0.2824  1.1415  4.5365 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) 12.49805   12.48039   1.001  0.32662   
## disp         0.01374    0.01136   1.210  0.23821   
## hp          -0.02282    0.01526  -1.496  0.14778   
## drat         0.95533    1.40737   0.679  0.50376   
## wt          -3.94974    1.26261  -3.128  0.00457 **
## qsec         0.87149    0.61331   1.421  0.16819   
## vsS          0.59017    1.83303   0.322  0.75027   
## ammanual     3.02402    1.66840   1.813  0.08244 . 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.495 on 24 degrees of freedom
## Multiple R-squared:  0.8673, Adjusted R-squared:  0.8286 
## F-statistic:  22.4 on 7 and 24 DF,  p-value: 4.532e-09</code></pre>
<p>Removing <code>cyl</code> reduced the VIFs of the other variables below 10. <code>disp</code> is still right up there (VIF = 9.87), so it may be worth dropping it from the model too. The model summary still shows that there is only one significant (at .05 level a significance) variable (<code>wt</code>, p = .00457). What if I drop <code>disp</code> too?</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="ordinary-least-squares.html#cb60-1" tabindex="-1"></a><span class="fu">vif</span>(m <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">-</span> cyl <span class="sc">-</span> disp, <span class="at">data =</span> d[,<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>]))</span></code></pre></div>
<pre><code>##       hp     drat       wt     qsec       vs       am 
## 5.070665 2.709905 5.105979 5.776361 4.120656 3.272177</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="ordinary-least-squares.html#cb62-1" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ . - cyl - disp, data = d[, 1:9])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3686 -1.7207 -0.2528  1.0986  4.6029 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) 16.14103   12.22322   1.321  0.19862   
## hp          -0.01796    0.01486  -1.209  0.23801   
## drat         0.62051    1.39261   0.446  0.65974   
## wt          -3.07506    1.04458  -2.944  0.00691 **
## qsec         0.73472    0.60836   1.208  0.23846   
## vsS          0.20446    1.82173   0.112  0.91153   
## ammanual     2.56534    1.63972   1.565  0.13027   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.518 on 25 degrees of freedom
## Multiple R-squared:  0.8592, Adjusted R-squared:  0.8254 
## F-statistic: 25.42 on 6 and 25 DF,  p-value: 1.688e-09</code></pre>
<p>The model is not improved, so keep <code>disp</code>.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="ordinary-least-squares.html#cb64-1" tabindex="-1"></a>m <span class="ot">&lt;-</span><span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">-</span> cyl, <span class="at">data =</span> d[,<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>])</span></code></pre></div>
<p>If the multicollinearity occurs because you are using a polynomial regression model, <em>center</em> the predictor variables (subtract their means).</p>
</div>
<div id="example-2" class="section level4 hasAnchor" number="1.3.2.2">
<h4><span class="header-section-number">1.3.2.2</span> Example<a href="ordinary-least-squares.html#example-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Data set <code>exerimmun</code> (<a href="https://newonlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/exerimmun/index.txt">exerimun.txt</a>) contains observations of immunoglobin in blood (a measure of immunity) and maximal oxygen uptake (a measure of exercise level) for <span class="math inline">\(n = 30\)</span> individuals.</p>
<ul>
<li><code>igg</code> = amount of immunoglobin in blood (mg)</li>
<li><code>oxygent</code> = maximal oxygen uptake (ml/kg)</li>
</ul>
<p>How does exercise affect the immune system?</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="ordinary-least-squares.html#cb65-1" tabindex="-1"></a><span class="co">#exerimmun &lt;- read_tsv(file = &quot;./Data/exerimmun.txt&quot;)</span></span>
<span id="cb65-2"><a href="ordinary-least-squares.html#cb65-2" tabindex="-1"></a>exerimmun <span class="ot">&lt;-</span> <span class="fu">tribble</span>(</span>
<span id="cb65-3"><a href="ordinary-least-squares.html#cb65-3" tabindex="-1"></a>  <span class="sc">~</span>igg, <span class="sc">~</span>oxygen,</span>
<span id="cb65-4"><a href="ordinary-least-squares.html#cb65-4" tabindex="-1"></a>  <span class="dv">881</span>,  <span class="fl">34.6</span>,</span>
<span id="cb65-5"><a href="ordinary-least-squares.html#cb65-5" tabindex="-1"></a>  <span class="dv">1290</span>, <span class="dv">45</span>,</span>
<span id="cb65-6"><a href="ordinary-least-squares.html#cb65-6" tabindex="-1"></a>  <span class="dv">2147</span>, <span class="fl">62.3</span>,</span>
<span id="cb65-7"><a href="ordinary-least-squares.html#cb65-7" tabindex="-1"></a>  <span class="dv">1909</span>, <span class="fl">58.9</span>,</span>
<span id="cb65-8"><a href="ordinary-least-squares.html#cb65-8" tabindex="-1"></a>  <span class="dv">1282</span>, <span class="fl">42.5</span>,</span>
<span id="cb65-9"><a href="ordinary-least-squares.html#cb65-9" tabindex="-1"></a>  <span class="dv">1530</span>, <span class="fl">44.3</span>,</span>
<span id="cb65-10"><a href="ordinary-least-squares.html#cb65-10" tabindex="-1"></a>  <span class="dv">2067</span>, <span class="fl">67.9</span>,</span>
<span id="cb65-11"><a href="ordinary-least-squares.html#cb65-11" tabindex="-1"></a>  <span class="dv">1982</span>, <span class="fl">58.5</span>,</span>
<span id="cb65-12"><a href="ordinary-least-squares.html#cb65-12" tabindex="-1"></a>  <span class="dv">1019</span>, <span class="fl">35.6</span>,</span>
<span id="cb65-13"><a href="ordinary-least-squares.html#cb65-13" tabindex="-1"></a>  <span class="dv">1651</span>, <span class="fl">49.6</span>,</span>
<span id="cb65-14"><a href="ordinary-least-squares.html#cb65-14" tabindex="-1"></a>  <span class="dv">752</span>,  <span class="dv">33</span>,</span>
<span id="cb65-15"><a href="ordinary-least-squares.html#cb65-15" tabindex="-1"></a>  <span class="dv">1687</span>, <span class="dv">52</span>,</span>
<span id="cb65-16"><a href="ordinary-least-squares.html#cb65-16" tabindex="-1"></a>  <span class="dv">1782</span>, <span class="fl">61.4</span>,</span>
<span id="cb65-17"><a href="ordinary-least-squares.html#cb65-17" tabindex="-1"></a>  <span class="dv">1529</span>, <span class="fl">50.2</span>,</span>
<span id="cb65-18"><a href="ordinary-least-squares.html#cb65-18" tabindex="-1"></a>  <span class="dv">969</span>,  <span class="fl">34.1</span>,</span>
<span id="cb65-19"><a href="ordinary-least-squares.html#cb65-19" tabindex="-1"></a>  <span class="dv">1660</span>, <span class="fl">52.5</span>,</span>
<span id="cb65-20"><a href="ordinary-least-squares.html#cb65-20" tabindex="-1"></a>  <span class="dv">2121</span>, <span class="fl">69.9</span>,</span>
<span id="cb65-21"><a href="ordinary-least-squares.html#cb65-21" tabindex="-1"></a>  <span class="dv">1382</span>, <span class="fl">38.8</span>,</span>
<span id="cb65-22"><a href="ordinary-least-squares.html#cb65-22" tabindex="-1"></a>  <span class="dv">1714</span>, <span class="fl">50.6</span>,</span>
<span id="cb65-23"><a href="ordinary-least-squares.html#cb65-23" tabindex="-1"></a>  <span class="dv">1959</span>, <span class="fl">69.4</span>,</span>
<span id="cb65-24"><a href="ordinary-least-squares.html#cb65-24" tabindex="-1"></a>  <span class="dv">1158</span>, <span class="fl">37.4</span>,</span>
<span id="cb65-25"><a href="ordinary-least-squares.html#cb65-25" tabindex="-1"></a>  <span class="dv">965</span>,  <span class="fl">35.1</span>,</span>
<span id="cb65-26"><a href="ordinary-least-squares.html#cb65-26" tabindex="-1"></a>  <span class="dv">1456</span>, <span class="dv">43</span>,</span>
<span id="cb65-27"><a href="ordinary-least-squares.html#cb65-27" tabindex="-1"></a>  <span class="dv">1273</span>, <span class="fl">44.1</span>,</span>
<span id="cb65-28"><a href="ordinary-least-squares.html#cb65-28" tabindex="-1"></a>  <span class="dv">1418</span>, <span class="fl">49.8</span>,</span>
<span id="cb65-29"><a href="ordinary-least-squares.html#cb65-29" tabindex="-1"></a>  <span class="dv">1743</span>, <span class="fl">54.4</span>,</span>
<span id="cb65-30"><a href="ordinary-least-squares.html#cb65-30" tabindex="-1"></a>  <span class="dv">1997</span>, <span class="fl">68.5</span>,</span>
<span id="cb65-31"><a href="ordinary-least-squares.html#cb65-31" tabindex="-1"></a>  <span class="dv">2177</span>, <span class="fl">69.5</span>,</span>
<span id="cb65-32"><a href="ordinary-least-squares.html#cb65-32" tabindex="-1"></a>  <span class="dv">1965</span>, <span class="dv">63</span>,</span>
<span id="cb65-33"><a href="ordinary-least-squares.html#cb65-33" tabindex="-1"></a>  <span class="dv">1264</span>, <span class="fl">43.2</span></span>
<span id="cb65-34"><a href="ordinary-least-squares.html#cb65-34" tabindex="-1"></a>)</span>
<span id="cb65-35"><a href="ordinary-least-squares.html#cb65-35" tabindex="-1"></a><span class="fu">head</span>(exerimmun)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 2
##     igg oxygen
##   &lt;dbl&gt;  &lt;dbl&gt;
## 1   881   34.6
## 2  1290   45  
## 3  2147   62.3
## 4  1909   58.9
## 5  1282   42.5
## 6  1530   44.3</code></pre>
<p>The scatterplot <code>oxygen ~ igg</code> shows some curvature. Formulate a quadratic polynomial regression function, <span class="math inline">\(igg_i = \beta_0 + \beta_1 oxygen_i + \beta_2 oxygen_i^2 + \epsilon_i\)</span> where the error terms are assumed to be independent, and normally distributed with equal variance.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="ordinary-least-squares.html#cb67-1" tabindex="-1"></a><span class="fu">ggplot</span>(exerimmun, <span class="fu">aes</span>(<span class="at">y =</span> igg, <span class="at">x =</span> oxygen)) <span class="sc">+</span></span>
<span id="cb67-2"><a href="ordinary-least-squares.html#cb67-2" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb67-3"><a href="ordinary-least-squares.html#cb67-3" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">2</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb67-4"><a href="ordinary-least-squares.html#cb67-4" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Immunoglobin in Blood&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>The formulated regression fits the data well (<span class="math inline">\(adj R^2 = .933\)</span>), but the terms <code>oxygen</code> and <code>oxygen^2</code> are strongly correlated.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="ordinary-least-squares.html#cb68-1" tabindex="-1"></a>m_blood <span class="ot">&lt;-</span> <span class="fu">lm</span>(igg <span class="sc">~</span> <span class="fu">poly</span>(oxygen, <span class="dv">2</span>), <span class="at">data =</span> exerimmun)</span>
<span id="cb68-2"><a href="ordinary-least-squares.html#cb68-2" tabindex="-1"></a><span class="fu">summary</span>(m_blood)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = igg ~ poly(oxygen, 2), data = exerimmun)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -185.375  -82.129    1.047   66.007  227.377 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       1557.63      19.43   80.16  &lt; 2e-16 ***
## poly(oxygen, 2)1  2114.72     106.43   19.87  &lt; 2e-16 ***
## poly(oxygen, 2)2  -360.78     106.43   -3.39  0.00217 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 106.4 on 27 degrees of freedom
## Multiple R-squared:  0.9377, Adjusted R-squared:  0.9331 
## F-statistic: 203.2 on 2 and 27 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="ordinary-least-squares.html#cb70-1" tabindex="-1"></a><span class="fu">cor</span>(exerimmun<span class="sc">$</span>oxygen, exerimmun<span class="sc">$</span>oxygen<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.9949846</code></pre>
<p>Remove the structural multicollinearity by centering the predictors. You can scale the predictors with <code>scale()</code>, but be careful to scale new data when predicting new observations with <code>predict(newdata=)</code>! Whenever possible, perform the transformation right in the model.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="ordinary-least-squares.html#cb72-1" tabindex="-1"></a>m_blood <span class="ot">&lt;-</span> <span class="fu">lm</span>(igg <span class="sc">~</span> <span class="fu">I</span>(oxygen <span class="sc">-</span> <span class="fu">mean</span>(exerimmun<span class="sc">$</span>oxygen)) <span class="sc">+</span></span>
<span id="cb72-2"><a href="ordinary-least-squares.html#cb72-2" tabindex="-1"></a>                <span class="fu">I</span>((oxygen <span class="sc">-</span> <span class="fu">mean</span>(exerimmun<span class="sc">$</span>oxygen))<span class="sc">^</span><span class="dv">2</span>), </span>
<span id="cb72-3"><a href="ordinary-least-squares.html#cb72-3" tabindex="-1"></a>              <span class="at">data =</span> exerimmun)</span>
<span id="cb72-4"><a href="ordinary-least-squares.html#cb72-4" tabindex="-1"></a><span class="fu">summary</span>(m_blood)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = igg ~ I(oxygen - mean(exerimmun$oxygen)) + I((oxygen - 
##     mean(exerimmun$oxygen))^2), data = exerimmun)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -185.375  -82.129    1.047   66.007  227.377 
## 
## Coefficients:
##                                         Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                            1632.1962    29.3486   55.61  &lt; 2e-16
## I(oxygen - mean(exerimmun$oxygen))       33.9995     1.6890   20.13  &lt; 2e-16
## I((oxygen - mean(exerimmun$oxygen))^2)   -0.5362     0.1582   -3.39  0.00217
##                                           
## (Intercept)                            ***
## I(oxygen - mean(exerimmun$oxygen))     ***
## I((oxygen - mean(exerimmun$oxygen))^2) ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 106.4 on 27 degrees of freedom
## Multiple R-squared:  0.9377, Adjusted R-squared:  0.9331 
## F-statistic: 203.2 on 2 and 27 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The estimated intercept coefficient <span class="math inline">\(\hat{\beta}_0 = 1632\)</span> means a person whose maximal oxygen uptake is <span class="math inline">\(50.64\)</span> ml/kg (the mean value) is predicted to have <span class="math inline">\(1632\)</span> mg of immunoglobin in his blood. The estimated coefficient <span class="math inline">\(\hat{\beta}_1 = 34.0\)</span> means a person whose maximal oxygen uptake is near <span class="math inline">\(50.64\)</span> ml/kg is predicted to increase by 34.0 mg for every 1 ml/kg increase in maximal oxygen uptake.</p>
<p>By performing all transformations in the model, it is straightforward to perform predictions. Here is the predicted value of immunoglobin when maximal oxygen uptake = 90.00 ml/kg.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="ordinary-least-squares.html#cb74-1" tabindex="-1"></a><span class="fu">predict</span>(m_blood, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">oxygen =</span> <span class="dv">90</span>), <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 2139.632 1639.597 2639.666</code></pre>
</div>
</div>
<div id="normality" class="section level3 hasAnchor" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> Normality<a href="ordinary-least-squares.html#normality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A normal probability plot or a normal quantile plot should have values near the line with no bow-shaped deviations. A histogram should be normally distributed. A residuals vs fits plot <span class="math inline">\((\epsilon \sim \hat{Y})\)</span> should be randomly scattered around 0. Sometimes the normality check fails when linearity assumption does not hold, so check for linearity first. Parameter estimation is not sensitive to this condition, but prediction intervals are.</p>
</div>
<div id="equal-variances" class="section level3 hasAnchor" number="1.3.4">
<h3><span class="header-section-number">1.3.4</span> Equal Variances<a href="ordinary-least-squares.html#equal-variances" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The residuals should be the same size at both low and high values of the response variable. A residuals vs fits plot <span class="math inline">\((\epsilon \sim \hat{Y})\)</span> should have random scatter in a band of constant width around 0, and no fan shape at the low and high ends. All tests and intervals are sensitive to this condition.</p>
</div>
</div>
<div id="prediction" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Prediction<a href="ordinary-least-squares.html#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The standard error in the <strong>expected value</strong> of <span class="math inline">\(\hat{y}\)</span> at some new set of predictors <span class="math inline">\(X_n\)</span> is</p>
<p><span class="math display">\[SE(\mu_\hat{y}) = \sqrt{\hat{\sigma}^2 (X_n (X&#39;X)^{-1} X_n&#39;)}.\]</span></p>
<p>The standard error increases the further <span class="math inline">\(X_n\)</span> is from <span class="math inline">\(\bar{X}\)</span>. If <span class="math inline">\(X_n = \bar{X}\)</span>, the equation reduces to <span class="math inline">\(SE(\mu_\hat{y}) = \sigma / \sqrt{n}\)</span>. If <span class="math inline">\(n\)</span> is large, or the predictor values are spread out, <span class="math inline">\(SE(\mu_\hat{y})\)</span> will be relatively small. The <span class="math inline">\((1 - \alpha)\%\)</span> <strong>confidence interval</strong> is <span class="math inline">\(\hat{y} \pm t_{\alpha / 2} SE(\mu_\hat{y})\)</span>.</p>
<p>The standard error in the <strong>predicted value</strong> of <span class="math inline">\(\hat{y}\)</span> at some <span class="math inline">\(X_{new}\)</span> is</p>
<p><span class="math display">\[SE(\hat{y}) = SE(\mu_\hat{y})^2 + \sqrt{\hat{\sigma}^2}.\]</span></p>
<p>Notice the standard error for a predicted value is always greater than the standard error of the expected value. The <span class="math inline">\((1 - \alpha)\%\)</span> <strong>prediction interval</strong> is <span class="math inline">\(\hat{y} \pm t_{\alpha / 2} SE(\hat{y})\)</span>.</p>
<div id="example-3" class="section level4 hasAnchor" number="1.4.0.1">
<h4><span class="header-section-number">1.4.0.1</span> Example<a href="ordinary-least-squares.html#example-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>What is the expected value of <code>mpg</code> if the predictor values equal their mean values?</p>
<p>R performs this calucation with the <code>predict()</code> function with parameter <code>interval = confidence</code>.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="ordinary-least-squares.html#cb76-1" tabindex="-1"></a>m <span class="ot">&lt;-</span><span class="fu">lm</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> d[,<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>])</span>
<span id="cb76-2"><a href="ordinary-least-squares.html#cb76-2" tabindex="-1"></a>X_new <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Const =</span> <span class="dv">1</span>,</span>
<span id="cb76-3"><a href="ordinary-least-squares.html#cb76-3" tabindex="-1"></a>                    <span class="at">cyl =</span> <span class="fu">factor</span>(<span class="fu">round</span>(<span class="fu">mean</span>(<span class="fu">as.numeric</span>(<span class="fu">as.character</span>(d<span class="sc">$</span>cyl))),<span class="dv">0</span>), <span class="at">levels =</span> <span class="fu">levels</span>(d<span class="sc">$</span>cyl)), </span>
<span id="cb76-4"><a href="ordinary-least-squares.html#cb76-4" tabindex="-1"></a>                    <span class="at">disp =</span> <span class="fu">mean</span>(d<span class="sc">$</span>disp),</span>
<span id="cb76-5"><a href="ordinary-least-squares.html#cb76-5" tabindex="-1"></a>                    <span class="at">hp =</span> <span class="fu">mean</span>(d<span class="sc">$</span>hp),</span>
<span id="cb76-6"><a href="ordinary-least-squares.html#cb76-6" tabindex="-1"></a>                    <span class="at">drat =</span> <span class="fu">mean</span>(d<span class="sc">$</span>drat),</span>
<span id="cb76-7"><a href="ordinary-least-squares.html#cb76-7" tabindex="-1"></a>                    <span class="at">wt =</span> <span class="fu">mean</span>(d<span class="sc">$</span>wt),</span>
<span id="cb76-8"><a href="ordinary-least-squares.html#cb76-8" tabindex="-1"></a>                    <span class="at">qsec =</span> <span class="fu">mean</span>(d<span class="sc">$</span>qsec),</span>
<span id="cb76-9"><a href="ordinary-least-squares.html#cb76-9" tabindex="-1"></a>                    <span class="at">vs =</span> <span class="fu">factor</span>(<span class="st">&quot;S&quot;</span>, <span class="at">levels =</span> <span class="fu">levels</span>(d<span class="sc">$</span>vs)), </span>
<span id="cb76-10"><a href="ordinary-least-squares.html#cb76-10" tabindex="-1"></a>                    <span class="at">am =</span> <span class="fu">factor</span>(<span class="st">&quot;manual&quot;</span>, <span class="at">levels =</span> <span class="fu">levels</span>(d<span class="sc">$</span>am)))</span>
<span id="cb76-11"><a href="ordinary-least-squares.html#cb76-11" tabindex="-1"></a><span class="fu">predict.lm</span>(<span class="at">object =</span> m, </span>
<span id="cb76-12"><a href="ordinary-least-squares.html#cb76-12" tabindex="-1"></a>           <span class="at">newdata =</span> X_new, </span>
<span id="cb76-13"><a href="ordinary-least-squares.html#cb76-13" tabindex="-1"></a>           <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)</span></code></pre></div>
<pre><code>##        fit     lwr      upr
## 1 21.21748 17.4461 24.98886</code></pre>
<p>You can verify this by manually calculating <span class="math inline">\(SE(\mu_\hat{y}) = \sqrt{\hat{\sigma}^2 (X_{new} (X&#39;X)^{-1} X_{new}&#39;)}\)</span> using matrix algebra.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="ordinary-least-squares.html#cb78-1" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="fu">data.frame</span>(<span class="fu">model.matrix</span>(m)), mean) <span class="sc">%&gt;%</span> <span class="fu">unlist</span>() <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb78-2"><a href="ordinary-least-squares.html#cb78-2" tabindex="-1"></a>X2[<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">contr.poly</span>(<span class="dv">3</span>)[<span class="dv">2</span>,<span class="dv">1</span>]  <span class="co"># cyl linear</span></span>
<span id="cb78-3"><a href="ordinary-least-squares.html#cb78-3" tabindex="-1"></a>X2[<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">contr.poly</span>(<span class="dv">3</span>)[<span class="dv">2</span>,<span class="dv">2</span>]  <span class="co"># cyl quadratic</span></span>
<span id="cb78-4"><a href="ordinary-least-squares.html#cb78-4" tabindex="-1"></a>X2[<span class="dv">9</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb78-5"><a href="ordinary-least-squares.html#cb78-5" tabindex="-1"></a>X2[<span class="dv">10</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb78-6"><a href="ordinary-least-squares.html#cb78-6" tabindex="-1"></a></span>
<span id="cb78-7"><a href="ordinary-least-squares.html#cb78-7" tabindex="-1"></a>y_exp <span class="ot">&lt;-</span> <span class="fu">sum</span>(m<span class="sc">$</span>coefficients <span class="sc">*</span> <span class="fu">as.numeric</span>(X2))</span>
<span id="cb78-8"><a href="ordinary-least-squares.html#cb78-8" tabindex="-1"></a>se_y_exp <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">sqrt</span>(rse<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> </span>
<span id="cb78-9"><a href="ordinary-least-squares.html#cb78-9" tabindex="-1"></a>                              X2 <span class="sc">%*%</span> </span>
<span id="cb78-10"><a href="ordinary-least-squares.html#cb78-10" tabindex="-1"></a>                              <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> </span>
<span id="cb78-11"><a href="ordinary-least-squares.html#cb78-11" tabindex="-1"></a>                              <span class="fu">t</span>(X2)))</span>
<span id="cb78-12"><a href="ordinary-least-squares.html#cb78-12" tabindex="-1"></a></span>
<span id="cb78-13"><a href="ordinary-least-squares.html#cb78-13" tabindex="-1"></a>t_crit <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="at">p =</span> .<span class="dv">05</span> <span class="sc">/</span> <span class="dv">2</span>, <span class="at">df =</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb78-14"><a href="ordinary-least-squares.html#cb78-14" tabindex="-1"></a>me <span class="ot">&lt;-</span> t_crit <span class="sc">*</span> se_y_exp</span>
<span id="cb78-15"><a href="ordinary-least-squares.html#cb78-15" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;fit: &quot;</span>, <span class="fu">round</span>(y_exp, <span class="dv">6</span>),</span>
<span id="cb78-16"><a href="ordinary-least-squares.html#cb78-16" tabindex="-1"></a>    <span class="st">&quot;, 95% CI: (&quot;</span>, <span class="fu">round</span>(y_exp <span class="sc">-</span> me, <span class="dv">6</span>), <span class="st">&quot;, &quot;</span>, <span class="fu">round</span>(y_exp <span class="sc">+</span> me, <span class="dv">6</span>), <span class="st">&quot;)&quot;</span>)</span></code></pre></div>
<pre><code>## fit:  21.21748 , 95% CI: ( 17.4461 ,  24.98886 )</code></pre>
</div>
<div id="example-4" class="section level4 hasAnchor" number="1.4.0.2">
<h4><span class="header-section-number">1.4.0.2</span> Example<a href="ordinary-least-squares.html#example-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>What is the predicted value of mpg if the predictor values equal their mean values?</p>
<p>R performs this calucation with the <code>predict()</code> with parameter <code>interval = prediction</code>.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="ordinary-least-squares.html#cb80-1" tabindex="-1"></a><span class="fu">predict.lm</span>(<span class="at">object =</span> m, </span>
<span id="cb80-2"><a href="ordinary-least-squares.html#cb80-2" tabindex="-1"></a>           <span class="at">newdata =</span> X_new, </span>
<span id="cb80-3"><a href="ordinary-least-squares.html#cb80-3" tabindex="-1"></a>           <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 21.21748 14.78304 27.65191</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="ordinary-least-squares.html#cb82-1" tabindex="-1"></a>se_y_hat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(rse<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> se_y_exp<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb82-2"><a href="ordinary-least-squares.html#cb82-2" tabindex="-1"></a>me <span class="ot">&lt;-</span> t_crit <span class="sc">*</span> se_y_hat</span>
<span id="cb82-3"><a href="ordinary-least-squares.html#cb82-3" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;fit: &quot;</span>, <span class="fu">round</span>(y_exp, <span class="dv">6</span>),</span>
<span id="cb82-4"><a href="ordinary-least-squares.html#cb82-4" tabindex="-1"></a>    <span class="st">&quot;, 95% CI: (&quot;</span>, <span class="fu">round</span>(y_exp <span class="sc">-</span> me, <span class="dv">6</span>), <span class="st">&quot;, &quot;</span>, <span class="fu">round</span>(y_exp <span class="sc">+</span> me, <span class="dv">6</span>), <span class="st">&quot;)&quot;</span>)</span></code></pre></div>
<pre><code>## fit:  21.21748 , 95% CI: ( 14.78304 ,  27.65191 )</code></pre>
</div>
</div>
<div id="inference" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Inference<a href="ordinary-least-squares.html#inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Draw conclusions about the significance of the coefficient estimates with the <em>t</em>-test and/or <em>F</em>-test.</p>
<div id="t-test" class="section level3 hasAnchor" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> <em>t</em>-Test<a href="ordinary-least-squares.html#t-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>By assumption, the residuals are normally distributed, so the <em>Z</em>-test statistic could evaluate the parameter estimators,</p>
<p><span class="math display">\[Z = \frac{\hat{\beta} - \beta_0}{\sqrt{\sigma^2 (X&#39;X)^{-1}}}\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is the null-hypothesized value, usually 0. <span class="math inline">\(\sigma\)</span> is unknown, but <span class="math inline">\(\frac{\hat{\sigma}^2 (n - k)}{\sigma^2} \sim \chi^2\)</span>. The ratio of the normal distribution divided by the adjusted chi-square <span class="math inline">\(\sqrt{\chi^2 / (n - k)}\)</span> is t-distributed,</p>
<p><span class="math display">\[t = \frac{\hat{\beta} - \beta_0}{\sqrt{\hat{\sigma}^2 (X&#39;X)^{-1}}} = \frac{\hat{\beta} - \beta_0}{SE(\hat{\beta})}\]</span></p>
<p>The <span class="math inline">\((1 - \alpha)\)</span> confidence intervals are <span class="math inline">\(CI = \hat{\beta} \pm t_{\alpha / 2, df} SE(\hat{\beta})\)</span> with <em>p</em>-value equaling the probability of measuring a <span class="math inline">\(t\)</span> of that extreme, <span class="math inline">\(p = P(t &gt; |t|)\)</span>. For a one-tail test, divide the reported p-value by two. The <span class="math inline">\(SE(\hat{\beta})\)</span> decreases with 1) a better fitting regression line (smaller <span class="math inline">\(\hat{\sigma}^2\)</span>), 2) greater variation in the predictor (larger <span class="math inline">\(X&#39;X\)</span>), and 3) larger sample size (larger n).</p>
<div id="example-5" class="section level4 hasAnchor" number="1.5.1.1">
<h4><span class="header-section-number">1.5.1.1</span> Example<a href="ordinary-least-squares.html#example-5" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Define a 95% confidence interval around the slope parameters.</p>
<p>The <code>summary()</code> output shows the t values and probabilities in the <code>t value</code> and <code>Pr(&gt;|t|)</code> columns. You can verify this manually using matrix algebra for <span class="math inline">\(t = \frac{(\hat{\beta} - \beta_1)}{SE(\hat{\beta})}\)</span> with <span class="math inline">\(\beta_1 = 0\)</span>. The <span class="math inline">\((1 - \alpha)\)</span> confidence interval is <span class="math inline">\(CI = \hat{\beta} \pm t_{\alpha / 2, df} SE(\hat{\beta})\)</span>. The table below gathers the parameter estimators and t-test results.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="ordinary-least-squares.html#cb84-1" tabindex="-1"></a>t <span class="ot">&lt;-</span> beta_hat <span class="sc">/</span> se_beta_hat</span>
<span id="cb84-2"><a href="ordinary-least-squares.html#cb84-2" tabindex="-1"></a>p_value <span class="ot">&lt;-</span> <span class="fu">pt</span>(<span class="at">q =</span> <span class="fu">abs</span>(t), </span>
<span id="cb84-3"><a href="ordinary-least-squares.html#cb84-3" tabindex="-1"></a>              <span class="at">df =</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>, </span>
<span id="cb84-4"><a href="ordinary-least-squares.html#cb84-4" tabindex="-1"></a>              <span class="at">lower.tail =</span> <span class="cn">FALSE</span>) <span class="sc">*</span> <span class="dv">2</span></span>
<span id="cb84-5"><a href="ordinary-least-squares.html#cb84-5" tabindex="-1"></a>t_crit <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="at">p =</span> .<span class="dv">05</span> <span class="sc">/</span> <span class="dv">2</span>, <span class="at">df =</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb84-6"><a href="ordinary-least-squares.html#cb84-6" tabindex="-1"></a>lcl <span class="ot">=</span> beta_hat <span class="sc">-</span> t_crit <span class="sc">*</span> se_beta_hat</span>
<span id="cb84-7"><a href="ordinary-least-squares.html#cb84-7" tabindex="-1"></a>ucl <span class="ot">=</span> beta_hat <span class="sc">+</span> t_crit <span class="sc">*</span> se_beta_hat</span>
<span id="cb84-8"><a href="ordinary-least-squares.html#cb84-8" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">beta =</span> <span class="fu">round</span>(beta_hat, <span class="dv">4</span>), </span>
<span id="cb84-9"><a href="ordinary-least-squares.html#cb84-9" tabindex="-1"></a>           <span class="at">se =</span> <span class="fu">round</span>(se_beta_hat, <span class="dv">4</span>), </span>
<span id="cb84-10"><a href="ordinary-least-squares.html#cb84-10" tabindex="-1"></a>           <span class="at">t =</span> <span class="fu">round</span>(t, <span class="dv">4</span>), </span>
<span id="cb84-11"><a href="ordinary-least-squares.html#cb84-11" tabindex="-1"></a>           <span class="at">p =</span> <span class="fu">round</span>(p_value, <span class="dv">4</span>),</span>
<span id="cb84-12"><a href="ordinary-least-squares.html#cb84-12" tabindex="-1"></a>           <span class="at">lcl =</span> <span class="fu">round</span>(lcl,<span class="dv">4</span>), </span>
<span id="cb84-13"><a href="ordinary-least-squares.html#cb84-13" tabindex="-1"></a>           <span class="at">ucl =</span> <span class="fu">round</span>(ucl, <span class="dv">4</span>))</span></code></pre></div>
<pre><code>##                beta      se       t      p     lcl     ucl
## (Intercept) 19.5410 14.1464  1.3813 0.1810 -9.7969 48.8789
## cyl.L        0.3426  2.7648  0.1239 0.9025 -5.3914  6.0765
## cyl.Q        1.3884  1.1121  1.2485 0.2250 -0.9179  3.6948
## disp         0.0067  0.0135  0.4950 0.6255 -0.0213  0.0347
## hp          -0.0291  0.0172 -1.6960 0.1040 -0.0648  0.0065
## drat         0.5881  1.5031  0.3912 0.6994 -2.5292  3.7053
## wt          -3.1552  1.4202 -2.2216 0.0369 -6.1006 -0.2099
## qsec         0.5232  0.6901  0.7582 0.4564 -0.9080  1.9545
## vsS          1.2378  2.1061  0.5877 0.5627 -3.1299  5.6055
## ammanual     3.0009  1.8534  1.6191 0.1197 -0.8428  6.8446</code></pre>
</div>
</div>
<div id="f-test" class="section level3 hasAnchor" number="1.5.2">
<h3><span class="header-section-number">1.5.2</span> <em>F</em>-Test<a href="ordinary-least-squares.html#f-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <em>F</em>-test for the model is a test of the null hypothesis that none of the independent variables linearly predict the dependent variable, that is, the model parameters are jointly zero: <span class="math inline">\(H_0 : \beta_1 = \ldots = \beta_k = 0\)</span>. The regression mean sum of squares <span class="math inline">\(MSR = \frac{(\hat{y} - \bar{y})&#39;(\hat{y} - \bar{y})}{k-1}\)</span> and the error mean sum of squares <span class="math inline">\(MSE = \frac{\hat{\epsilon}&#39;\hat{\epsilon}}{n-k}\)</span> are each chi-square variables. Their ratio has an <em>F</em> distribution with <span class="math inline">\(k - 1\)</span> numerator degrees of freedom and <span class="math inline">\(n - k\)</span> denominator degrees of freedom. The F statistic can also be expressed in terms of the coefficient of correlation <span class="math inline">\(R^2 = \frac{MSR}{MST}\)</span>.</p>
<p><span class="math display">\[F(k - 1, n - k) = \frac{MSR}{MSE} = \frac{R^2}{1 - R^2} \frac{n-k}{k-1}\]</span></p>
<p><em>MSE</em> is <span class="math inline">\(\sigma^2\)</span>. If <span class="math inline">\(H_0\)</span> is true, that is, there is no relationship between the predictors and the response, then <span class="math inline">\(MSR\)</span> is also equal to <span class="math inline">\(\sigma^2\)</span>, so <span class="math inline">\(F = 1\)</span>. As <span class="math inline">\(R^2 \rightarrow 1\)</span>, <span class="math inline">\(F \rightarrow \infty\)</span>, and as <span class="math inline">\(R^2 \rightarrow 0\)</span>, <span class="math inline">\(F \rightarrow 0\)</span>. F increases with <span class="math inline">\(n\)</span> and decreases with <span class="math inline">\(k\)</span>.</p>
<div id="example-6" class="section level4 hasAnchor" number="1.5.2.1">
<h4><span class="header-section-number">1.5.2.1</span> Example<a href="ordinary-least-squares.html#example-6" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>What is the probability that all parameters are jointly equal to zero?</p>
<p>The F-statistic is presented at the bottom of the <code>summary()</code> function. You can verify this manually.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="ordinary-least-squares.html#cb86-1" tabindex="-1"></a>ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>((m<span class="sc">$</span>fitted.values <span class="sc">-</span> <span class="fu">mean</span>(d<span class="sc">$</span>mpg))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb86-2"><a href="ordinary-least-squares.html#cb86-2" tabindex="-1"></a>sse <span class="ot">&lt;-</span> <span class="fu">sum</span>(m<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb86-3"><a href="ordinary-least-squares.html#cb86-3" tabindex="-1"></a>sst <span class="ot">&lt;-</span> <span class="fu">sum</span>((m<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">mean</span>(d<span class="sc">$</span>mpg))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb86-4"><a href="ordinary-least-squares.html#cb86-4" tabindex="-1"></a>msr <span class="ot">&lt;-</span> ssr <span class="sc">/</span> k</span>
<span id="cb86-5"><a href="ordinary-least-squares.html#cb86-5" tabindex="-1"></a>mse <span class="ot">&lt;-</span> sse <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb86-6"><a href="ordinary-least-squares.html#cb86-6" tabindex="-1"></a>f <span class="ot">=</span> msr <span class="sc">/</span> mse</span>
<span id="cb86-7"><a href="ordinary-least-squares.html#cb86-7" tabindex="-1"></a>p_value <span class="ot">&lt;-</span> <span class="fu">pf</span>(<span class="at">q =</span> f, <span class="at">df1 =</span> k, <span class="at">df2 =</span> n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb86-8"><a href="ordinary-least-squares.html#cb86-8" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;F-statistic: &quot;</span>, <span class="fu">round</span>(f, <span class="dv">4</span>), <span class="st">&quot; on 3 and 65 DF,  p-value: &quot;</span>, p_value)</span></code></pre></div>
<pre><code>## F-statistic:  17.3549  on 3 and 65 DF,  p-value:  4.814183e-08</code></pre>
<p>There is sufficient evidence <span class="math inline">\((F = 17.35, P &lt; .0001)\)</span> to reject <span class="math inline">\(H_0\)</span> that the parameter estimators are jointly equal to zero.</p>
<p>The <code>aov</code> function calculates the sequential sum of squares. The regression sum of squares SSR for <code>mpg ~ cyl</code> is 824.8. Adding <code>disp</code> to the model increases SSR by 57.6. Adding <code>hp</code> to the model increases SSR by 18.5. It would seem that <code>hp</code> does not improve the model.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="ordinary-least-squares.html#cb88-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(m))</span></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## cyl          2  824.8   412.4  65.260 5.62e-10 ***
## disp         1   57.6    57.6   9.122  0.00629 ** 
## hp           1   18.5    18.5   2.928  0.10112    
## drat         1   11.9    11.9   1.885  0.18355    
## wt           1   55.8    55.8   8.828  0.00705 ** 
## qsec         1    1.5     1.5   0.241  0.62816    
## vs           1    0.3     0.3   0.048  0.82894    
## am           1   16.6    16.6   2.622  0.11967    
## Residuals   22  139.0     6.3                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Order matters. Had we started with <code>disp</code>, then added <code>hp</code> we would find both estimators were significant.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="ordinary-least-squares.html#cb90-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> disp <span class="sc">+</span> hp <span class="sc">+</span> drat <span class="sc">+</span> wt <span class="sc">+</span> qsec <span class="sc">+</span> vs <span class="sc">+</span> am <span class="sc">+</span> cyl, <span class="at">data =</span> d)))</span></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## disp         1  808.9   808.9 128.004 1.22e-10 ***
## hp           1   33.7    33.7   5.327  0.03078 *  
## drat         1   30.1    30.1   4.771  0.03989 *  
## wt           1   70.5    70.5  11.158  0.00296 ** 
## qsec         1   12.7    12.7   2.011  0.17017    
## vs           1    0.2     0.2   0.035  0.85231    
## am           1   20.5    20.5   3.237  0.08571 .  
## cyl          2   10.4     5.2   0.825  0.45141    
## Residuals   22  139.0     6.3                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
</div>
</div>
<div id="interpretation" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Interpretation<a href="ordinary-least-squares.html#interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A plot of the standardized coefficients shows the relative importance of each variable. The distance the coefficients are from zero shows how much a change in a standard deviation of the regressor changes the mean of the predicted value. The CI shows the precision. The plot shows not only which variables are significant, but also which are important.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="ordinary-least-squares.html#cb92-1" tabindex="-1"></a>d_sc <span class="ot">&lt;-</span> d <span class="sc">%&gt;%</span> <span class="fu">mutate_at</span>(<span class="fu">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>, <span class="st">&quot;drat&quot;</span>, <span class="st">&quot;wt&quot;</span>, <span class="st">&quot;qsec&quot;</span>), scale)</span>
<span id="cb92-2"><a href="ordinary-least-squares.html#cb92-2" tabindex="-1"></a>m_sc <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., d_sc[,<span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>])</span>
<span id="cb92-3"><a href="ordinary-least-squares.html#cb92-3" tabindex="-1"></a>lm_summary <span class="ot">&lt;-</span> <span class="fu">summary</span>(m_sc)<span class="sc">$</span>coefficients</span>
<span id="cb92-4"><a href="ordinary-least-squares.html#cb92-4" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Features =</span> <span class="fu">rownames</span>(lm_summary),</span>
<span id="cb92-5"><a href="ordinary-least-squares.html#cb92-5" tabindex="-1"></a>                 <span class="at">Estimate =</span> lm_summary[,<span class="st">&#39;Estimate&#39;</span>],</span>
<span id="cb92-6"><a href="ordinary-least-squares.html#cb92-6" tabindex="-1"></a>                 <span class="at">std_error =</span> lm_summary[,<span class="st">&#39;Std. Error&#39;</span>])</span>
<span id="cb92-7"><a href="ordinary-least-squares.html#cb92-7" tabindex="-1"></a>df<span class="sc">$</span>lower <span class="ot">=</span> df<span class="sc">$</span>Estimate <span class="sc">-</span> <span class="fu">qt</span>(.<span class="dv">05</span><span class="sc">/</span><span class="dv">2</span>, m_sc<span class="sc">$</span>df.residual) <span class="sc">*</span> df<span class="sc">$</span>std_error</span>
<span id="cb92-8"><a href="ordinary-least-squares.html#cb92-8" tabindex="-1"></a>df<span class="sc">$</span>upper <span class="ot">=</span> df<span class="sc">$</span>Estimate <span class="sc">+</span> <span class="fu">qt</span>(.<span class="dv">05</span><span class="sc">/</span><span class="dv">2</span>, m_sc<span class="sc">$</span>df.residual) <span class="sc">*</span> df<span class="sc">$</span>std_error</span>
<span id="cb92-9"><a href="ordinary-least-squares.html#cb92-9" tabindex="-1"></a>df <span class="ot">&lt;-</span> df[df<span class="sc">$</span>Features <span class="sc">!=</span> <span class="st">&quot;(Intercept)&quot;</span>,]</span>
<span id="cb92-10"><a href="ordinary-least-squares.html#cb92-10" tabindex="-1"></a></span>
<span id="cb92-11"><a href="ordinary-least-squares.html#cb92-11" tabindex="-1"></a><span class="fu">ggplot</span>(df) <span class="sc">+</span></span>
<span id="cb92-12"><a href="ordinary-least-squares.html#cb92-12" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb92-13"><a href="ordinary-least-squares.html#cb92-13" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> Estimate, <span class="at">y =</span> Features)) <span class="sc">+</span></span>
<span id="cb92-14"><a href="ordinary-least-squares.html#cb92-14" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">y =</span> Features, <span class="at">yend =</span> Features, <span class="at">x=</span>lower, <span class="at">xend=</span>upper), </span>
<span id="cb92-15"><a href="ordinary-least-squares.html#cb92-15" tabindex="-1"></a>               <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">angle=</span><span class="dv">90</span>, <span class="at">ends=</span><span class="st">&#39;both&#39;</span>, <span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.1</span>, <span class="st">&#39;cm&#39;</span>))) <span class="sc">+</span></span>
<span id="cb92-16"><a href="ordinary-least-squares.html#cb92-16" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Standardized Weight&quot;</span>) <span class="sc">+</span></span>
<span id="cb92-17"><a href="ordinary-least-squares.html#cb92-17" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Model Feature Importance&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>The added variable plot shows the bivariate relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_i\)</span> after accounting for the other variables. For example, the partial regression plots of <code>y ~ x1 + x2 + x3</code> would plot the residuals of <code>y ~ x2 + x3</code> vs <code>x1</code>, and so on.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="ordinary-least-squares.html#cb93-1" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb93-2"><a href="ordinary-least-squares.html#cb93-2" tabindex="-1"></a><span class="fu">avPlots</span>(m)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
</div>
<div id="model-validation" class="section level2 hasAnchor" number="1.7">
<h2><span class="header-section-number">1.7</span> Model Validation<a href="ordinary-least-squares.html#model-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Evaluate predictive accuracy by training the model on a training data set and testing on a test data set.</p>
<div id="accuracy-metrics" class="section level3 hasAnchor" number="1.7.1">
<h3><span class="header-section-number">1.7.1</span> Accuracy Metrics<a href="ordinary-least-squares.html#accuracy-metrics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The most common measures of model fit are R-squared, RMSE, RSE, MAE, Adjusted R-squared, AIC, AICc, BIC, and Mallow’s Cp.</p>
<div id="r-squared" class="section level4 hasAnchor" number="1.7.1.1">
<h4><span class="header-section-number">1.7.1.1</span> R-Squared<a href="ordinary-least-squares.html#r-squared" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The coefficient of determination (<strong>R-squared</strong>) is the percent of total variation in the response variable that is explained by the regression line.</p>
<p><span class="math display">\[R^2 = \frac{RSS}{SST} = 1 - \frac{SSE}{SST}\]</span></p>
<p>where <span class="math inline">\(SSE = \sum_{i=1}^n{(y_i - \hat{y}_i)^2}\)</span> is the sum squared differences between the predicted and observed value, <span class="math inline">\(SST = \sum_{i = 1}^n{(y_i - \bar{y})^2}\)</span> is the sum of squared differences between the observed and overall mean value, and <span class="math inline">\(RSS = \sum_{i=1}^n{(\hat{y}_i - \bar{y})^2}\)</span> is the sum of squared differences between the predicted and overall mean “no-relationship line” value. At the extremes, <span class="math inline">\(R^2 = 1\)</span> means all data points fall perfectly on the regression line - the predictors account for <em>all</em> variation in the response; <span class="math inline">\(R^2 = 0\)</span> means the regression line is horizontal at <span class="math inline">\(\bar{y}\)</span> - the predictors account for <em>none</em> of the variation in the response. In the simple case of a single predictor variable, <span class="math inline">\(R^2\)</span> equals the squared correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, <span class="math inline">\(Cor(x,y)\)</span>.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="ordinary-least-squares.html#cb94-1" tabindex="-1"></a>ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>((m<span class="sc">$</span>fitted.values <span class="sc">-</span> <span class="fu">mean</span>(d<span class="sc">$</span>mpg))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb94-2"><a href="ordinary-least-squares.html#cb94-2" tabindex="-1"></a>sse <span class="ot">&lt;-</span> <span class="fu">sum</span>(m<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb94-3"><a href="ordinary-least-squares.html#cb94-3" tabindex="-1"></a>sst <span class="ot">&lt;-</span> <span class="fu">sum</span>((d<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">mean</span>(d<span class="sc">$</span>mpg))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb94-4"><a href="ordinary-least-squares.html#cb94-4" tabindex="-1"></a>(r2 <span class="ot">&lt;-</span> ssr <span class="sc">/</span> sst)</span></code></pre></div>
<pre><code>## [1] 0.8765389</code></pre>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="ordinary-least-squares.html#cb96-1" tabindex="-1"></a>(r2 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> sse <span class="sc">/</span> sst)</span></code></pre></div>
<pre><code>## [1] 0.8765389</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="ordinary-least-squares.html#cb98-1" tabindex="-1"></a>(r2 <span class="ot">&lt;-</span> <span class="fu">summary</span>(m)<span class="sc">$</span>r.squared)</span></code></pre></div>
<pre><code>## [1] 0.8765389</code></pre>
<p>The sums of squares are the same thing as the variances multiplied by the degrees of freedom.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="ordinary-least-squares.html#cb100-1" tabindex="-1"></a>ssr2 <span class="ot">&lt;-</span> <span class="fu">var</span>(<span class="fu">fitted</span>(m)) <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb100-2"><a href="ordinary-least-squares.html#cb100-2" tabindex="-1"></a>sse2 <span class="ot">&lt;-</span> <span class="fu">var</span>(<span class="fu">residuals</span>(m)) <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb100-3"><a href="ordinary-least-squares.html#cb100-3" tabindex="-1"></a>sst2 <span class="ot">&lt;-</span> <span class="fu">var</span>(d<span class="sc">$</span>mpg) <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb100-4"><a href="ordinary-least-squares.html#cb100-4" tabindex="-1"></a>ssr2 <span class="sc">/</span> sst2</span></code></pre></div>
<pre><code>## [1] 0.8765389</code></pre>
<p><span class="math inline">\(R^2\)</span> is also equal to the correlation between the fitted value and observed values, <span class="math inline">\(R^2 = Cor(Y, \hat{Y})^2\)</span>.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="ordinary-least-squares.html#cb102-1" tabindex="-1"></a><span class="fu">cor</span>(m<span class="sc">$</span>fitted.values, d<span class="sc">$</span>mpg)<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.8765389</code></pre>
<p>R-squared is proportional to the the variance in the response, <em>SST</em>. Given a constant percentage error in predictions, a test set with relatively low variation in the reponse will have a lower R-squared. Conversely, test sets with large variation, e.g., housing data with home sale ranging from $60K to $2M may have a large R-squared despite average prediction errors of &gt;$10K.</p>
<p>A close variant of R-squared is the non-parametric Spearman’s rank correlation. This statistic is the correlation of the <em>ranks</em> of the response and the predicted values. It is used when the model goal is ranking.</p>
</div>
<div id="rmse" class="section level4 hasAnchor" number="1.7.1.2">
<h4><span class="header-section-number">1.7.1.2</span> RMSE<a href="ordinary-least-squares.html#rmse" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The root mean squared error (<strong>RMSE</strong>) is the average prediction error (square root of mean squared error).</p>
<p><span class="math display">\[RMSE = \sqrt{\frac{\sum_{i=1}^n{(y_i - \hat{y}_i)^2}}{n}}\]</span></p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="ordinary-least-squares.html#cb104-1" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((d<span class="sc">$</span>mpg <span class="sc">-</span> m<span class="sc">$</span>fitted.values)<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 2.084339</code></pre>
<p>The <code>rmse()</code> function from the <code>Metrics</code> package, and the <code>postResample()</code> function in <code>caret</code> calculate RMSE.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="ordinary-least-squares.html#cb106-1" tabindex="-1"></a><span class="fu">rmse</span>(<span class="at">actual =</span> d<span class="sc">$</span>mpg, <span class="at">predicted =</span> m<span class="sc">$</span>fitted.values)</span></code></pre></div>
<pre><code>## [1] 2.084339</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="ordinary-least-squares.html#cb108-1" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> m<span class="sc">$</span>fitted.values, <span class="at">obs =</span> d<span class="sc">$</span>mpg)[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>##     RMSE 
## 2.084339</code></pre>
<p>The mean squared error of a model with theoretical residual of mean zero and constant variance <span class="math inline">\(\sigma^2\)</span> can be decomposed into the model’s bias and the model’s variance:</p>
<p><span class="math display">\[E[MSE] = \sigma^2 + Bias^2 + Var.\]</span></p>
<p>A model that predicts the response closely will have low bias, but be relatively sensitive to the training data and thus have high variance. A model that predicts the response conservatively (e.g., a simple mean) will have large bias, but be relatively insensitive to nuances in the training data. Here is an example of a simulated sine wave. A model predicting the mean value at the upper and lower levels has low variance, but high bias, and a model of an actual sine wave has low bias and high variance. This is referred to as the variance-bias trade-off.</p>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
</div>
<div id="rse" class="section level4 hasAnchor" number="1.7.1.3">
<h4><span class="header-section-number">1.7.1.3</span> RSE<a href="ordinary-least-squares.html#rse" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The residual standard error (<strong>RSE</strong>, or model sigma <span class="math inline">\(\hat{\sigma}\)</span>) is an estimate of the standard deviation of <span class="math inline">\(\epsilon\)</span>. It is roughly the average amount the response deviates from the true regression line.</p>
<p><span class="math display">\[\sigma = \sqrt{\frac{\sum_{i=1}^n{(y_i - \hat{y}_i)^2}}{n-k-1}}\]</span></p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="ordinary-least-squares.html#cb110-1" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>((d<span class="sc">$</span>mpg <span class="sc">-</span> m<span class="sc">$</span>fitted.values)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 2.513808</code></pre>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="ordinary-least-squares.html#cb112-1" tabindex="-1"></a><span class="co"># sd is sqrt(sse / (n-1)), sigma = sqrt(sse / (n - k - 1))</span></span>
<span id="cb112-2"><a href="ordinary-least-squares.html#cb112-2" tabindex="-1"></a><span class="fu">sd</span>(m<span class="sc">$</span>residuals) <span class="sc">*</span> <span class="fu">sqrt</span>((n <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>))  </span></code></pre></div>
<pre><code>## [1] 2.513808</code></pre>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="ordinary-least-squares.html#cb114-1" tabindex="-1"></a><span class="fu">summary</span>(m)<span class="sc">$</span>sigma </span></code></pre></div>
<pre><code>## [1] 2.513808</code></pre>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="ordinary-least-squares.html#cb116-1" tabindex="-1"></a><span class="fu">sigma</span>(m)</span></code></pre></div>
<pre><code>## [1] 2.513808</code></pre>
</div>
<div id="mae" class="section level4 hasAnchor" number="1.7.1.4">
<h4><span class="header-section-number">1.7.1.4</span> MAE<a href="ordinary-least-squares.html#mae" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The mean absolute error (<strong>MAE</strong>) is the average absolute prediction arror. It is less sensitive to outliers.</p>
<p><span class="math display">\[MAE = \frac{\sum_{i=1}^n{|y_i - \hat{y}_i|}}{n}\]</span></p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="ordinary-least-squares.html#cb118-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">abs</span>(d<span class="sc">$</span>mpg <span class="sc">-</span> m<span class="sc">$</span>fitted.values)) <span class="sc">/</span> n</span></code></pre></div>
<pre><code>## [1] 1.704941</code></pre>
<p>The <code>postResample()</code> function in <code>caret</code> conveniently calculates all three.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="ordinary-least-squares.html#cb120-1" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> m<span class="sc">$</span>fitted.values, <span class="at">obs =</span> d<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 2.0843393 0.8765389 1.7049409</code></pre>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="ordinary-least-squares.html#cb122-1" tabindex="-1"></a><span class="fu">defaultSummary</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">obs =</span> d<span class="sc">$</span>mpg, <span class="at">pred =</span> m<span class="sc">$</span>fitted.values), <span class="at">model =</span> m)</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 2.0843393 0.8765389 1.7049409</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="ordinary-least-squares.html#cb124-1" tabindex="-1"></a><span class="fu">apply</span>(<span class="fu">as.matrix</span>(m<span class="sc">$</span>fitted.values), <span class="dv">2</span>, postResample, <span class="at">obs =</span> d<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>##               [,1]
## RMSE     2.0843393
## Rsquared 0.8765389
## MAE      1.7049409</code></pre>
<p>These metrics are good for evaluating a model, but less useful for comparing models. The problem is that they tend to improve with additional variables added to the model, even if the improvement is not significant. The following metrics aid model comparison by penalizing added variables.</p>
</div>
<div id="adjusted-r-squared" class="section level4 hasAnchor" number="1.7.1.5">
<h4><span class="header-section-number">1.7.1.5</span> Adjusted R-squared<a href="ordinary-least-squares.html#adjusted-r-squared" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The adjusted R-squared (<span class="math inline">\(\bar{R}^2\)</span>) penalizes the R-squared metric for increasing number of predictors.</p>
<p><span class="math display">\[\bar{R}^2 = 1 - \frac{SSE}{SST} \cdot \frac{n-1}{n-k-1}\]</span></p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="ordinary-least-squares.html#cb126-1" tabindex="-1"></a>(adj_r2 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> sse<span class="sc">/</span>sst <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.8260321</code></pre>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="ordinary-least-squares.html#cb128-1" tabindex="-1"></a><span class="fu">summary</span>(m)<span class="sc">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.8260321</code></pre>
</div>
<div id="aic" class="section level4 hasAnchor" number="1.7.1.6">
<h4><span class="header-section-number">1.7.1.6</span> AIC<a href="ordinary-least-squares.html#aic" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Akaike’s Information Criteria (<strong>AIC</strong>) is a penalization metric. The lower the AIC, the better the model.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="ordinary-least-squares.html#cb130-1" tabindex="-1"></a><span class="fu">AIC</span>(m)</span></code></pre></div>
<pre><code>## [1] 159.817</code></pre>
</div>
<div id="aicc" class="section level4 hasAnchor" number="1.7.1.7">
<h4><span class="header-section-number">1.7.1.7</span> AICc<a href="ordinary-least-squares.html#aicc" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>AICc</strong> corrects AIC for small sample sizes.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="ordinary-least-squares.html#cb132-1" tabindex="-1"></a><span class="fu">AIC</span>(m) <span class="sc">+</span> (<span class="dv">2</span> <span class="sc">*</span> k <span class="sc">*</span> (k <span class="sc">+</span> <span class="dv">1</span>)) <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 167.9988</code></pre>
</div>
<div id="bic" class="section level4 hasAnchor" number="1.7.1.8">
<h4><span class="header-section-number">1.7.1.8</span> BIC<a href="ordinary-least-squares.html#bic" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Basiean information criteria (<strong>BIC</strong>) is like AIC, but with a stronger penalty for additional variables.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="ordinary-least-squares.html#cb134-1" tabindex="-1"></a><span class="fu">BIC</span>(m)</span></code></pre></div>
<pre><code>## [1] 175.9401</code></pre>
</div>
<div id="mallows-cp" class="section level4 hasAnchor" number="1.7.1.9">
<h4><span class="header-section-number">1.7.1.9</span> Mallows Cp<a href="ordinary-least-squares.html#mallows-cp" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Mallows Cp is a variant of AIC.</p>
<div id="example-7" class="section level5 hasAnchor" number="1.7.1.9.1">
<h5><span class="header-section-number">1.7.1.9.1</span> Example<a href="ordinary-least-squares.html#example-7" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Compare the full model to a model without <code>cyl</code>.</p>
<p>The <code>glance()</code> function from the <code>broom</code> package calculates many validation metrics. Here are the validation stats for the full model and then the reduced model.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="ordinary-least-squares.html#cb136-1" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb136-2"><a href="ordinary-least-squares.html#cb136-2" tabindex="-1"></a></span>
<span id="cb136-3"><a href="ordinary-least-squares.html#cb136-3" tabindex="-1"></a><span class="fu">glance</span>(m) <span class="sc">%&gt;%</span> <span class="fu">select</span>(adj.r.squared, sigma, AIC, BIC, p.value)</span></code></pre></div>
<pre><code>## # A tibble: 1 × 5
##   adj.r.squared sigma   AIC   BIC      p.value
##           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;
## 1         0.826  2.51  160.  176. 0.0000000481</code></pre>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="ordinary-least-squares.html#cb138-1" tabindex="-1"></a><span class="fu">glance</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">-</span> cyl, d[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>])) <span class="sc">%&gt;%</span> <span class="fu">select</span>(adj.r.squared, sigma, AIC, BIC, p.value)</span></code></pre></div>
<pre><code>## # A tibble: 1 × 5
##   adj.r.squared sigma   AIC   BIC       p.value
##           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;
## 1         0.829  2.50  158.  171. 0.00000000453</code></pre>
<p>The ajusted R2 increased and AIC and BIC decreased, meaning the full model is less efficient at explaining the variability in the response value. The residual standard error <code>sigma</code> is smaller for the reduced model. Finally, the <em>F</em> statistic p-value is smaller for the reduced model, meaning the reduced model is statistically more significant.</p>
<p>Note that these regression metrics are all internal measures, that is they have been computed on the training dataset, not the test dataset.</p>
</div>
</div>
</div>
<div id="cross-validation" class="section level3 hasAnchor" number="1.7.2">
<h3><span class="header-section-number">1.7.2</span> Cross-Validation<a href="ordinary-least-squares.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cross-validation is a set of methods for measuring the performance of a predictive model on a test dataset. The main measures of prediction performance are R2, RMSE and MAE.</p>
<div id="validation-set" class="section level4 hasAnchor" number="1.7.2.1">
<h4><span class="header-section-number">1.7.2.1</span> Validation Set<a href="ordinary-least-squares.html#validation-set" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To perform validation set cross validation, randomly split the data into a training data set and a test data set. Fit models to the training data set, then predict values with the validation set. The model that produces the best prediction performance is the preferred model.</p>
<p>The <code>caret</code> package provides useful methods for cross-validation.</p>
<div id="example-8" class="section level5 hasAnchor" number="1.7.2.1.1">
<h5><span class="header-section-number">1.7.2.1.1</span> Example<a href="ordinary-least-squares.html#example-8" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="ordinary-least-squares.html#cb140-1" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb140-2"><a href="ordinary-least-squares.html#cb140-2" tabindex="-1"></a></span>
<span id="cb140-3"><a href="ordinary-least-squares.html#cb140-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb140-4"><a href="ordinary-least-squares.html#cb140-4" tabindex="-1"></a>train_idx <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(<span class="at">y =</span> d<span class="sc">$</span>mpg, <span class="at">p =</span> <span class="fl">0.80</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb140-5"><a href="ordinary-least-squares.html#cb140-5" tabindex="-1"></a>d.train <span class="ot">&lt;-</span> d[train_idx, ]</span>
<span id="cb140-6"><a href="ordinary-least-squares.html#cb140-6" tabindex="-1"></a>d.test <span class="ot">&lt;-</span> d[<span class="sc">-</span>train_idx, ]</span></code></pre></div>
<p>Build the model using <code>d.train</code>, make predictions, then calculate the R2, RMSE, and MAE. Use the <code>train()</code> function from the <code>caret</code> package. Use <code>method = "none"</code> to simply fit the model to the entire data set.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="ordinary-least-squares.html#cb141-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb141-2"><a href="ordinary-least-squares.html#cb141-2" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> ., </span>
<span id="cb141-3"><a href="ordinary-least-squares.html#cb141-3" tabindex="-1"></a>            <span class="at">data =</span> d.train[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>],</span>
<span id="cb141-4"><a href="ordinary-least-squares.html#cb141-4" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb141-5"><a href="ordinary-least-squares.html#cb141-5" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;none&quot;</span>))</span>
<span id="cb141-6"><a href="ordinary-least-squares.html#cb141-6" tabindex="-1"></a><span class="fu">print</span>(m1)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  8 predictor
## 
## No pre-processing
## Resampling: None</code></pre>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="ordinary-least-squares.html#cb143-1" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(m1, <span class="at">newdata =</span> d.test), </span>
<span id="cb143-2"><a href="ordinary-least-squares.html#cb143-2" tabindex="-1"></a>             <span class="at">obs =</span> d.test<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 3.1026305 0.9567828 2.4543559</code></pre>
<p>The validation set method is only useful when you have a large data set to partition. A second disadvantage is that building a model on a fraction of the data leaves out information. The test error will vary with which observations are included in the training set.</p>
</div>
</div>
<div id="loocv" class="section level4 hasAnchor" number="1.7.2.2">
<h4><span class="header-section-number">1.7.2.2</span> LOOCV<a href="ordinary-least-squares.html#loocv" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Leave one out cross validation (LOOCV) works by successively modeling with training sets leaving out one data point, then averaging the prediction errors.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="ordinary-least-squares.html#cb145-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb145-2"><a href="ordinary-least-squares.html#cb145-2" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> ., </span>
<span id="cb145-3"><a href="ordinary-least-squares.html#cb145-3" tabindex="-1"></a>            <span class="at">data =</span> d.train[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>],</span>
<span id="cb145-4"><a href="ordinary-least-squares.html#cb145-4" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb145-5"><a href="ordinary-least-squares.html#cb145-5" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;LOOCV&quot;</span>))</span>
<span id="cb145-6"><a href="ordinary-least-squares.html#cb145-6" tabindex="-1"></a><span class="fu">print</span>(m2)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  8 predictor
## 
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation 
## Summary of sample sizes: 27, 27, 27, 27, 27, 27, ... 
## Resampling results:
## 
##   RMSE      Rsquared  MAE     
##   2.779283  0.758735  2.317904
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="ordinary-least-squares.html#cb147-1" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(m2, <span class="at">newdata =</span> d.test), </span>
<span id="cb147-2"><a href="ordinary-least-squares.html#cb147-2" tabindex="-1"></a>             <span class="at">obs =</span> d.test<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 3.1026305 0.9567828 2.4543559</code></pre>
<p>This method isn’t perfect either. It repeats as many times as there are data points, so the execution time may be long. LOOCV is also sensitive to outliers.</p>
</div>
<div id="k-fold-cross-validation" class="section level4 hasAnchor" number="1.7.2.3">
<h4><span class="header-section-number">1.7.2.3</span> K-fold Cross-Validation<a href="ordinary-least-squares.html#k-fold-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>K-fold cross-validation splits the dataset into <em>k</em> folds (subsets), then uses <em>k-1</em> of the folds for a training set and the remaining fold for a test set, then repeats for all permutations of k taken k-1 at a time. E.g., 3-fold cross-validation will partition the data into sets A, B, and C, then create train/test splits of [AB, C], [AC, B], and [BC, A].</p>
<p>K-fold cross-validation is less computationally expensive than LOOCV, and often yields more accurate test error rate estimates. What is the right value of k? The lower is <em>k</em> the more biased the estimates; the higher is <em>k</em> the larger the estimate variability. At the extremes <em>k</em> = 2 is the validation set method, and <em>k = n</em> is the LOOCV method. In practice, one typically performs k-fold cross-validation using k = 5 or k = 10 because these values have been empirically shown to balence bias and variance.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="ordinary-least-squares.html#cb149-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb149-2"><a href="ordinary-least-squares.html#cb149-2" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> ., </span>
<span id="cb149-3"><a href="ordinary-least-squares.html#cb149-3" tabindex="-1"></a>            <span class="at">data =</span> d.train[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>],</span>
<span id="cb149-4"><a href="ordinary-least-squares.html#cb149-4" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb149-5"><a href="ordinary-least-squares.html#cb149-5" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>,</span>
<span id="cb149-6"><a href="ordinary-least-squares.html#cb149-6" tabindex="-1"></a>                                     <span class="at">number =</span> <span class="dv">5</span>))</span>
<span id="cb149-7"><a href="ordinary-least-squares.html#cb149-7" tabindex="-1"></a><span class="fu">print</span>(m3)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 22, 22, 23, 22, 23 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   2.956977  0.8523226  2.591746
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="ordinary-least-squares.html#cb151-1" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(m3, <span class="at">newdata =</span> d.test), </span>
<span id="cb151-2"><a href="ordinary-least-squares.html#cb151-2" tabindex="-1"></a>             <span class="at">obs =</span> d.test<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 3.1026305 0.9567828 2.4543559</code></pre>
</div>
<div id="repeated-k-fold-cv" class="section level4 hasAnchor" number="1.7.2.4">
<h4><span class="header-section-number">1.7.2.4</span> Repeated K-fold CV<a href="ordinary-least-squares.html#repeated-k-fold-cv" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>You can also perform k-fold cross-validation multiple times and average the results. Specify <code>method = "repeatedcv"</code> and <code>repeats = 3</code> in the <code>trainControl</code> object for three repeats.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="ordinary-least-squares.html#cb153-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb153-2"><a href="ordinary-least-squares.html#cb153-2" tabindex="-1"></a>m4 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> ., </span>
<span id="cb153-3"><a href="ordinary-least-squares.html#cb153-3" tabindex="-1"></a>            <span class="at">data =</span> d.train[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>],</span>
<span id="cb153-4"><a href="ordinary-least-squares.html#cb153-4" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb153-5"><a href="ordinary-least-squares.html#cb153-5" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb153-6"><a href="ordinary-least-squares.html#cb153-6" tabindex="-1"></a>                                     <span class="at">number =</span> <span class="dv">5</span>,</span>
<span id="cb153-7"><a href="ordinary-least-squares.html#cb153-7" tabindex="-1"></a>                                     <span class="at">repeats =</span> <span class="dv">3</span>))</span>
<span id="cb153-8"><a href="ordinary-least-squares.html#cb153-8" tabindex="-1"></a><span class="fu">print</span>(m4)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 3 times) 
## Summary of sample sizes: 22, 22, 23, 22, 23, 23, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE   
##   3.070563  0.8133672  2.7155
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="ordinary-least-squares.html#cb155-1" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(m4, <span class="at">newdata =</span> d.test), </span>
<span id="cb155-2"><a href="ordinary-least-squares.html#cb155-2" tabindex="-1"></a>             <span class="at">obs =</span> d.test<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 3.1026305 0.9567828 2.4543559</code></pre>
</div>
<div id="bootstrapping" class="section level4 hasAnchor" number="1.7.2.5">
<h4><span class="header-section-number">1.7.2.5</span> Bootstrapping<a href="ordinary-least-squares.html#bootstrapping" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Bootstrapping randomly selects a sample of n observations with replacement from the original dataset to evaluate the model. The procedure is repeated many times.</p>
<p>Specify <code>method = "boot"</code> and <code>number = 100</code> to perform 100 bootstrap samples.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="ordinary-least-squares.html#cb157-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb157-2"><a href="ordinary-least-squares.html#cb157-2" tabindex="-1"></a>m5 <span class="ot">&lt;-</span> <span class="fu">train</span>(mpg <span class="sc">~</span> ., </span>
<span id="cb157-3"><a href="ordinary-least-squares.html#cb157-3" tabindex="-1"></a>            <span class="at">data =</span> d.train[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>],</span>
<span id="cb157-4"><a href="ordinary-least-squares.html#cb157-4" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb157-5"><a href="ordinary-least-squares.html#cb157-5" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;boot&quot;</span>,</span>
<span id="cb157-6"><a href="ordinary-least-squares.html#cb157-6" tabindex="-1"></a>                                     <span class="at">number =</span> <span class="dv">100</span>))</span></code></pre></div>
<pre><code>## Warning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;
## attr(*, &quot;non-estim&quot;) has doubtful cases

## Warning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;
## attr(*, &quot;non-estim&quot;) has doubtful cases

## Warning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;
## attr(*, &quot;non-estim&quot;) has doubtful cases</code></pre>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="ordinary-least-squares.html#cb159-1" tabindex="-1"></a><span class="fu">print</span>(m5)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  8 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (100 reps) 
## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   3.872687  0.6362661  3.235582
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="ordinary-least-squares.html#cb161-1" tabindex="-1"></a><span class="fu">postResample</span>(<span class="at">pred =</span> <span class="fu">predict</span>(m5, <span class="at">newdata =</span> d.test), </span>
<span id="cb161-2"><a href="ordinary-least-squares.html#cb161-2" tabindex="-1"></a>             <span class="at">obs =</span> d.test<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 3.1026305 0.9567828 2.4543559</code></pre>
</div>
</div>
<div id="gain-curve" class="section level3 hasAnchor" number="1.7.3">
<h3><span class="header-section-number">1.7.3</span> Gain Curve<a href="ordinary-least-squares.html#gain-curve" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For supervised learning purposes, a visual way to evaluate a regression model is with the gain curve. This visualization compares a predictive model score to an actual outcome (either binary (0/1) or continuous). The gain curve plot measures how well the model score sorts the data compared to the true outcome value. The x-axis is the fraction of items seen when sorted by score, and the y-axis is the cumulative summed true outcome when sorted by score. For comparison, GainCurvePlot also plots the “wizard curve”: the gain curve when the data is sorted according to its true outcome. A relative Gini score close to 1 means the model sorts responses well.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="ordinary-least-squares.html#cb163-1" tabindex="-1"></a><span class="fu">library</span>(WVPlots)</span>
<span id="cb163-2"><a href="ordinary-least-squares.html#cb163-2" tabindex="-1"></a>d<span class="sc">$</span>fitted <span class="ot">&lt;-</span> m<span class="sc">$</span>fitted.values</span>
<span id="cb163-3"><a href="ordinary-least-squares.html#cb163-3" tabindex="-1"></a><span class="fu">GainCurvePlot</span>(d, <span class="at">xvar =</span> <span class="st">&quot;fitted&quot;</span>, <span class="at">truthVar =</span> <span class="st">&quot;mpg&quot;</span>, <span class="at">title =</span> <span class="st">&quot;Model Gain Curve&quot;</span>)</span></code></pre></div>
<p><img src="supervised-ml_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
</div>
</div>
<div id="ols-reference" class="section level2 hasAnchor" number="1.8">
<h2><span class="header-section-number">1.8</span> OLS Reference<a href="ordinary-least-squares.html#ols-reference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Penn State University, STAT 501, Lesson 12: Multicollinearity &amp; Other Regression Pitfalls. <a href="https://newonlinecourses.science.psu.edu/stat501/lesson/12">https://newonlinecourses.science.psu.edu/stat501/lesson/12</a>.</p>
<p>STHDA. Bootstrap Resampling Essentials in R. <a href="http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/">http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/</a></p>
<p>Molnar, Christoph. “Interpretable machine learning. A Guide for Making Black Box Models Explainable”, 2019. <a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="generalized-linear-models-glm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["supervised-ml.pdf", "supervised-ml.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
