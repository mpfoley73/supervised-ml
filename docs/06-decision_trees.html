<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Decision Trees – Supervised Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./07-support_vector_machines.html" rel="next">
<link href="./05-regularization.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./06-decision_trees.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Decision Trees</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Supervised Machine Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intro</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-generalized_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Generalized Linear Models (GLM)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-linear_mixed_effects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Mixed Effects Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-nonlinear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Non-linear Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-decision_trees.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Decision Trees</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-support_vector_machines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-bayesian_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-emmeans.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Estimated Marginal Means</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#case-studies" id="toc-case-studies" class="nav-link active" data-scroll-target="#case-studies"><span class="header-section-number">6.1</span> Case Studies</a></li>
  <li><a href="#classification-tree" id="toc-classification-tree" class="nav-link" data-scroll-target="#classification-tree"><span class="header-section-number">6.2</span> Classification Tree</a>
  <ul>
  <li><a href="#fit-the-model" id="toc-fit-the-model" class="nav-link" data-scroll-target="#fit-the-model">Fit the Model</a></li>
  <li><a href="#performance" id="toc-performance" class="nav-link" data-scroll-target="#performance">Performance</a>
  <ul class="collapse">
  <li><a href="#confusion-matrix" id="toc-confusion-matrix" class="nav-link" data-scroll-target="#confusion-matrix">Confusion Matrix</a></li>
  <li><a href="#roc-curve" id="toc-roc-curve" class="nav-link" data-scroll-target="#roc-curve">ROC Curve</a></li>
  <li><a href="#gain-curve" id="toc-gain-curve" class="nav-link" data-scroll-target="#gain-curve"><span class="header-section-number">6.2.0.1</span> Gain Curve</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#regression-tree" id="toc-regression-tree" class="nav-link" data-scroll-target="#regression-tree"><span class="header-section-number">6.3</span> Regression Tree</a>
  <ul>
  <li><a href="#measuring-performance" id="toc-measuring-performance" class="nav-link" data-scroll-target="#measuring-performance">Measuring Performance</a></li>
  </ul></li>
  <li><a href="#bagged-trees" id="toc-bagged-trees" class="nav-link" data-scroll-target="#bagged-trees"><span class="header-section-number">6.4</span> Bagged Trees</a>
  <ul>
  <li><a href="#bagged-classification-tree" id="toc-bagged-classification-tree" class="nav-link" data-scroll-target="#bagged-classification-tree"><span class="header-section-number">6.4.1</span> Bagged Classification Tree</a></li>
  <li><a href="#bagging-regression-tree" id="toc-bagging-regression-tree" class="nav-link" data-scroll-target="#bagging-regression-tree"><span class="header-section-number">6.4.2</span> Bagging Regression Tree</a></li>
  </ul></li>
  <li><a href="#random-forests" id="toc-random-forests" class="nav-link" data-scroll-target="#random-forests"><span class="header-section-number">6.5</span> Random Forests</a>
  <ul>
  <li><a href="#random-forest-classification-tree" id="toc-random-forest-classification-tree" class="nav-link" data-scroll-target="#random-forest-classification-tree"><span class="header-section-number">6.5.0.1</span> Random Forest Classification Tree</a></li>
  <li><a href="#random-forest-regression-tree" id="toc-random-forest-regression-tree" class="nav-link" data-scroll-target="#random-forest-regression-tree"><span class="header-section-number">6.5.0.2</span> Random Forest Regression Tree</a></li>
  </ul></li>
  <li><a href="#gradient-boosting" id="toc-gradient-boosting" class="nav-link" data-scroll-target="#gradient-boosting"><span class="header-section-number">6.6</span> Gradient Boosting</a>
  <ul>
  <li><a href="#gradient-boosting-classification-tree" id="toc-gradient-boosting-classification-tree" class="nav-link" data-scroll-target="#gradient-boosting-classification-tree"><span class="header-section-number">6.6.0.1</span> Gradient Boosting Classification Tree</a></li>
  <li><a href="#gradient-boosting-regression-tree" id="toc-gradient-boosting-regression-tree" class="nav-link" data-scroll-target="#gradient-boosting-regression-tree"><span class="header-section-number">6.6.0.2</span> Gradient Boosting Regression Tree</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Decision Trees</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Decision tree models, also known as classification and regression trees (CART), split the dataset into subsets based on the value of input features, creating a tree-like structure of decision rules. At each node, the algorithm chooses the best split. The process continues recursively until a stopping condition is met. The result is a tree that can be used to make predictions for new data points.</p>
<p>CART models define the nodes through a <em>top-down greedy</em> process called <em>recursive binary splitting</em>. <em>Top-down</em> because it begins at the top of the tree with all observations in a single region and successively splits the predictor space; and <em>greedy</em> because at each splitting step, the best split is made at that particular step without consideration to subsequent splits.</p>
<p>The best split is the predictor variable and cut point that minimizes its cost function. For regression trees, that’s the sum of squared residuals. In <a href="#eq-06-rss" class="quarto-xref">Equation&nbsp;<span>6.1</span></a>, <span class="math inline">\(A_k\)</span> is node <span class="math inline">\(k\)</span> of <span class="math inline">\(K\)</span> nodes.</p>
<p><span id="eq-06-rss"><span class="math display">\[RSS = \sum_{k=1}^K\sum_{i \in A_k}{\left(y_i - \hat{y}_{A_k} \right)^2}. \tag{6.1}\]</span></span></p>
<p>For classification trees, it’s the Gini index,</p>
<p><span id="eq-06-gini"><span class="math display">\[G = \sum_{c=1}^C{\hat{p}_{kc}(1 - \hat{p}_{kc})}, \tag{6.2}\]</span></span></p>
<p><span class="math inline">\(\hat{p}_{kc}\)</span> is the proportion of predictions in node <span class="math inline">\(k\)</span> that are class <span class="math inline">\(c\)</span>. A completely <em>pure</em> node in a binary tree would have <span class="math inline">\(\hat{p} \in \{ 0, 1 \}\)</span> for a Gini index of <span class="math inline">\(G = 0\)</span>. A completely <em>impure</em> node would have <span class="math inline">\(\hat{p} = 0.5\)</span> and <span class="math inline">\(G = (0.5)(0.5) = 0.25\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>CART repeats the splitting process for each child node until a <em>stopping criterion</em> is satisfied, usually when no node size surpasses a predefined maximum, or continued splitting does not improve the model significantly. CART may also impose a minimum number of observations in each node.</p>
<p>The resulting tree will over-fit the data and not generalize well, so CART <em>prunes</em> the tree, minimizing the cross-validated prediction error. Rather than cross-validating every possible subtree, CART uses <em>cost-complexity pruning</em>. Cost complexity is the tradeoff between error (cost) and tree size (complexity). The cost complexity of the tree, <span class="math inline">\(R_{c_p}(T)\)</span>, is the sum of its risk (error) plus a cost complexity factor, <span class="math inline">\(c_p\)</span>, multiple of the tree size <span class="math inline">\(|T|\)</span>.</p>
<p><span id="eq-06-cp"><span class="math display">\[R_{c_p}(T) = R(T) + c_p|T| \tag{6.3}\]</span></span></p>
<p><span class="math inline">\(c_p\)</span> can take any value from <span class="math inline">\([0..\infty]\)</span>, but it turns out there is an optimal tree for ranges of <span class="math inline">\(c_p\)</span> values, so there is only a finite set of interesting values for <span class="math inline">\(c_p\)</span> <span class="citation" data-cites="James2013 Therneau2019 Kuhn2016">(<a href="10-references.html#ref-James2013" role="doc-biblioref">James et al. 2013</a>; <a href="10-references.html#ref-Therneau2019" role="doc-biblioref">Therneau and Atkinson 2019</a>; and <a href="10-references.html#ref-Kuhn2016" role="doc-biblioref">Kuhn and Johnson 2016</a>)</span>. A parametric algorithm identifies the interesting <span class="math inline">\(c_p\)</span> values and their associated pruned trees, <span class="math inline">\(T_{c_p}\)</span>.</p>
<section id="case-studies" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="case-studies"><span class="header-section-number">6.1</span> Case Studies</h2>
<p>The sections in this chapter work through two case studies. The first fits classification trees to the <code>ISLR::OJ</code> dataset to predict which of two brands of orange juice customers <code>Purchase</code>. The second fits regression trees to the <code>ISLR::Carseats</code> dataset to predict <code>Sales</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>oj_dat <span class="ot">&lt;-</span> ISLR<span class="sc">::</span>OJ</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(oj_dat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1,070
Columns: 18
$ Purchase       &lt;fct&gt; CH, CH, CH, MM, CH, CH, CH, CH, CH, CH, CH, CH, CH, CH,…
$ WeekofPurchase &lt;dbl&gt; 237, 239, 245, 227, 228, 230, 232, 234, 235, 238, 240, …
$ StoreID        &lt;dbl&gt; 1, 1, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 2, 2…
$ PriceCH        &lt;dbl&gt; 1.75, 1.75, 1.86, 1.69, 1.69, 1.69, 1.69, 1.75, 1.75, 1…
$ PriceMM        &lt;dbl&gt; 1.99, 1.99, 2.09, 1.69, 1.69, 1.99, 1.99, 1.99, 1.99, 1…
$ DiscCH         &lt;dbl&gt; 0.00, 0.00, 0.17, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…
$ DiscMM         &lt;dbl&gt; 0.00, 0.30, 0.00, 0.00, 0.00, 0.00, 0.40, 0.40, 0.40, 0…
$ SpecialCH      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
$ SpecialMM      &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0…
$ LoyalCH        &lt;dbl&gt; 0.500000, 0.600000, 0.680000, 0.400000, 0.956535, 0.965…
$ SalePriceMM    &lt;dbl&gt; 1.99, 1.69, 2.09, 1.69, 1.69, 1.99, 1.59, 1.59, 1.59, 1…
$ SalePriceCH    &lt;dbl&gt; 1.75, 1.75, 1.69, 1.69, 1.69, 1.69, 1.69, 1.75, 1.75, 1…
$ PriceDiff      &lt;dbl&gt; 0.24, -0.06, 0.40, 0.00, 0.00, 0.30, -0.10, -0.16, -0.1…
$ Store7         &lt;fct&gt; No, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes,…
$ PctDiscMM      &lt;dbl&gt; 0.000000, 0.150754, 0.000000, 0.000000, 0.000000, 0.000…
$ PctDiscCH      &lt;dbl&gt; 0.000000, 0.000000, 0.091398, 0.000000, 0.000000, 0.000…
$ ListPriceDiff  &lt;dbl&gt; 0.24, 0.24, 0.23, 0.00, 0.00, 0.30, 0.30, 0.24, 0.24, 0…
$ STORE          &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2…</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>cs_dat <span class="ot">&lt;-</span> ISLR<span class="sc">::</span>Carseats</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(cs_dat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 400
Columns: 11
$ Sales       &lt;dbl&gt; 9.50, 11.22, 10.06, 7.40, 4.15, 10.81, 6.63, 11.85, 6.54, …
$ CompPrice   &lt;dbl&gt; 138, 111, 113, 117, 141, 124, 115, 136, 132, 132, 121, 117…
$ Income      &lt;dbl&gt; 73, 48, 35, 100, 64, 113, 105, 81, 110, 113, 78, 94, 35, 2…
$ Advertising &lt;dbl&gt; 11, 16, 10, 4, 3, 13, 0, 15, 0, 0, 9, 4, 2, 11, 11, 5, 0, …
$ Population  &lt;dbl&gt; 276, 260, 269, 466, 340, 501, 45, 425, 108, 131, 150, 503,…
$ Price       &lt;dbl&gt; 120, 83, 80, 97, 128, 72, 108, 120, 124, 124, 100, 94, 136…
$ ShelveLoc   &lt;fct&gt; Bad, Good, Medium, Medium, Bad, Bad, Medium, Good, Medium,…
$ Age         &lt;dbl&gt; 42, 65, 59, 55, 38, 78, 71, 67, 76, 76, 26, 50, 62, 53, 52…
$ Education   &lt;dbl&gt; 17, 10, 12, 14, 13, 16, 15, 10, 10, 17, 10, 13, 18, 18, 18…
$ Urban       &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, No, No, No, Yes, Ye…
$ US          &lt;fct&gt; Yes, Yes, Yes, Yes, No, Yes, No, Yes, No, Yes, Yes, Yes, N…</code></pre>
</div>
</div>
<p>Partition the data into training and testing datasets. We’ll fit models to the training data and compare their performance using testing data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>(oj_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(oj_dat, <span class="at">prop =</span> <span class="fl">0.8</span>, <span class="at">strata =</span> Purchase))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;Training/Testing/Total&gt;</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;855/215/1070&gt;</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>oj_train <span class="ot">&lt;-</span> <span class="fu">training</span>(oj_split)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>oj_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(oj_split)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>(cs_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(cs_dat, <span class="at">prop =</span> <span class="fl">0.8</span>, <span class="at">strata =</span> Sales))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;Training/Testing/Total&gt;</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;319/81/400&gt;</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>cs_train <span class="ot">&lt;-</span> <span class="fu">training</span>(cs_split)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>cs_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(cs_split)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="classification-tree" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="classification-tree"><span class="header-section-number">6.2</span> Classification Tree</h2>
<p>Function <code>parsnip::decision_tree()</code> defines a decision tree model. Its default engine is <code>rpart</code> and has three hyperparameters.</p>
<ul>
<li><code>tree_depth</code>: maximum number of layers. Larger depths result in more complex models, but they are also more prone to overfitting.</li>
<li><code>min_n</code>: minimal node size required for splitting.</li>
<li><code>cost_complexity</code>: a penalty for adding more nodes to the tree. This is another control regulating the tree complexity.</li>
</ul>
<p>The hyperparameters can either take assigned values, or you can tune them with <code>tune()</code>. We’ll predict <code>Purchase</code> [CH, MM] as a function of all 17 predictors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>oj_cart_mdl <span class="ot">&lt;-</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">decision_tree</span>(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">cost_complexity =</span> <span class="fu">tune</span>(),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="fu">tune</span>(),</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"rpart"</span>) <span class="sc">|&gt;</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"classification"</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>oj_cart_wf <span class="ot">&lt;-</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(oj_cart_mdl) <span class="sc">|&gt;</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_formula</span>(Purchase <span class="sc">~</span> .)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="fit-the-model" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="fit-the-model">Fit the Model</h3>
<p>Tune the hyperparameters with 10-fold CV using a regular grid. With 3 hyperparameters and 5 levels, the grid has 5^3=125 combinations. That means the tuning exercise will fit 125 models for each of the 10 folds - 1,250 fits! We’ll evaluate the fits based on accuracy and AUC. The dataset is somewhat imbalanced (61% “CH”), so AUC will be the more important metric. It takes about a minute and a half to fit the 855 x 18 dataset.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>oj_cart_resamples <span class="ot">&lt;-</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  oj_cart_wf <span class="sc">|&gt;</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(oj_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>      <span class="fu">cost_complexity</span>(), </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>      <span class="fu">tree_depth</span>(), </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">min_n</span>(), </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>      <span class="at">levels =</span> <span class="dv">5</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">metric_set</span>(roc_auc, accuracy)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="fu">toc</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>62.47 sec elapsed</code></pre>
</div>
</div>
<p><code>tune_grid()</code> returns a <code>resamples</code> object with one row per fold.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>oj_cart_resamples</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># Tuning results
# 10-fold cross-validation 
# A tibble: 10 × 4
   splits           id     .metrics           .notes          
   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          
 1 &lt;split [769/86]&gt; Fold01 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;
 2 &lt;split [769/86]&gt; Fold02 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;
 3 &lt;split [769/86]&gt; Fold03 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;
 4 &lt;split [769/86]&gt; Fold04 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;
 5 &lt;split [769/86]&gt; Fold05 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;
 6 &lt;split [770/85]&gt; Fold06 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;
 7 &lt;split [770/85]&gt; Fold07 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;
 8 &lt;split [770/85]&gt; Fold08 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;
 9 &lt;split [770/85]&gt; Fold09 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;
10 &lt;split [770/85]&gt; Fold10 &lt;tibble [250 × 7]&gt; &lt;tibble [0 × 3]&gt;</code></pre>
</div>
</div>
<p>Each row contains a metrics list with values for each hyperparameter combination and performance metric. There were 125 parameter combinations and two metrics, so 250 rows. Here’s what the metrics object from the first fold look like.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>oj_cart_resamples[<span class="dv">1</span>, ]<span class="sc">$</span>.metrics[[<span class="dv">1</span>]] <span class="sc">|&gt;</span> <span class="fu">glimpse</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 250
Columns: 7
$ cost_complexity &lt;dbl&gt; 1.000000e-10, 1.000000e-10, 1.778279e-08, 1.778279e-08…
$ tree_depth      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, …
$ min_n           &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …
$ .metric         &lt;chr&gt; "accuracy", "roc_auc", "accuracy", "roc_auc", "accurac…
$ .estimator      &lt;chr&gt; "binary", "binary", "binary", "binary", "binary", "bin…
$ .estimate       &lt;dbl&gt; 0.7558140, 0.7310950, 0.7558140, 0.7310950, 0.7558140,…
$ .config         &lt;chr&gt; "Preprocessor1_Model001", "Preprocessor1_Model001", "P…</code></pre>
</div>
</div>
<p><code>collect_metrics()</code> unnests the <code>metrics</code> column and averages the values over the folds. <code>show_best()</code> shows the best model hyperparameter combination, but the mean AUC is the same for several combinations. The best performing tree is 8 layers deep.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>oj_cart_resamples <span class="sc">|&gt;</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"roc_auc"</span>) <span class="sc">|&gt;</span> knitr<span class="sc">::</span><span class="fu">kable</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 16%">
<col style="width: 11%">
<col style="width: 6%">
<col style="width: 8%">
<col style="width: 11%">
<col style="width: 10%">
<col style="width: 3%">
<col style="width: 10%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">cost_complexity</th>
<th style="text-align: right;">tree_depth</th>
<th style="text-align: right;">min_n</th>
<th style="text-align: left;">.metric</th>
<th style="text-align: left;">.estimator</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">n</th>
<th style="text-align: right;">std_err</th>
<th style="text-align: left;">.config</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.0000000</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">30</td>
<td style="text-align: left;">roc_auc</td>
<td style="text-align: left;">binary</td>
<td style="text-align: right;">0.8671621</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">0.0142507</td>
<td style="text-align: left;">Preprocessor1_Model086</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.0000000</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">30</td>
<td style="text-align: left;">roc_auc</td>
<td style="text-align: left;">binary</td>
<td style="text-align: right;">0.8671621</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">0.0142507</td>
<td style="text-align: left;">Preprocessor1_Model087</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.0000032</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">30</td>
<td style="text-align: left;">roc_auc</td>
<td style="text-align: left;">binary</td>
<td style="text-align: right;">0.8671621</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">0.0142507</td>
<td style="text-align: left;">Preprocessor1_Model088</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.0005623</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">30</td>
<td style="text-align: left;">roc_auc</td>
<td style="text-align: left;">binary</td>
<td style="text-align: right;">0.8671621</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">0.0142507</td>
<td style="text-align: left;">Preprocessor1_Model089</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.0000000</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">30</td>
<td style="text-align: left;">roc_auc</td>
<td style="text-align: left;">binary</td>
<td style="text-align: right;">0.8671621</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">0.0142507</td>
<td style="text-align: left;">Preprocessor1_Model091</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Plot the sensitivity to the hyperparameter combinations to make sure you’ve explored a range that gives you a peek.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">collect_metrics</span>(oj_cart_resamples)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">cost_complextity =</span> df <span class="sc">|&gt;</span> <span class="fu">rename</span>(<span class="at">x =</span> cost_complexity) <span class="sc">|&gt;</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">.by =</span> <span class="fu">c</span>(x, .metric), <span class="at">mean_auc =</span> <span class="fu">mean</span>(mean)),</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">tree_depth =</span> df <span class="sc">|&gt;</span> <span class="fu">rename</span>(<span class="at">x =</span> tree_depth) <span class="sc">|&gt;</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">.by =</span> <span class="fu">c</span>(x, .metric), <span class="at">mean_auc =</span> <span class="fu">mean</span>(mean)),</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_n =</span> df <span class="sc">|&gt;</span> <span class="fu">rename</span>(<span class="at">x =</span> min_n) <span class="sc">|&gt;</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">.by =</span> <span class="fu">c</span>(x, .metric), <span class="at">mean_auc =</span> <span class="fu">mean</span>(mean)),</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">.id =</span> <span class="st">"hyperparameter"</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>) <span class="sc">|&gt;</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> mean_auc, <span class="at">color =</span> .metric)) <span class="sc">+</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="fu">vars</span>(hyperparameter), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="cn">NULL</span>, <span class="at">y =</span> <span class="st">"Mean AUC"</span>, <span class="at">color =</span> <span class="cn">NULL</span>, <span class="at">title =</span> <span class="st">"Hyperparameter Tuning"</span>) <span class="sc">+</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Select the hyperparameter combination with the highest AUC and finalize the workflow. <code>last_fit()</code> fits the model with the full training set and evaluates it on the testing data.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>oj_cart_hyperparameters <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_cart_resamples, <span class="at">metric =</span> <span class="st">"roc_auc"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="ot">&lt;-</span> </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(oj_cart_wf, oj_cart_hyperparameters) <span class="sc">|&gt;</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Here is the tree. The output starts with the root node. Terminal nodes are labeled with an asterisk (*), so this is just a summary node of everything beneath. Node 1 Contains all 855 observations with 333 misclassifications. The majority class is CH, with a proportion of 0.6105 for CH and 0.3895 for MM. Node 2 Splits on <code>LoyalCH</code> &gt;= 0.48285, containing 534 observations with 92 misclassifications. The majority class is CH, with a higher proportion of CH. The child nodes of node “x” having labeling pattern 2x) and 2x+1), so for node 1) it’s 2) and 3), and for 2) it’s 4) and 5).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">extract_workflow</span>(oj_cart_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>══ Workflow [trained] ══════════════════════════════════════════════════════════
Preprocessor: Formula
Model: decision_tree()

── Preprocessor ────────────────────────────────────────────────────────────────
Purchase ~ .

── Model ───────────────────────────────────────────────────────────────────────
n= 855 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 855 333 CH (0.61052632 0.38947368)  
   2) LoyalCH&gt;=0.48285 534  92 CH (0.82771536 0.17228464)  
     4) LoyalCH&gt;=0.7535455 273  13 CH (0.95238095 0.04761905) *
     5) LoyalCH&lt; 0.7535455 261  79 CH (0.69731801 0.30268199)  
      10) PriceDiff&gt;=-0.165 221  48 CH (0.78280543 0.21719457)  
        20) ListPriceDiff&gt;=0.135 185  30 CH (0.83783784 0.16216216) *
        21) ListPriceDiff&lt; 0.135 36  18 CH (0.50000000 0.50000000)  
          42) PriceDiff&gt;=0.015 17   6 CH (0.64705882 0.35294118) *
          43) PriceDiff&lt; 0.015 19   7 MM (0.36842105 0.63157895) *
      11) PriceDiff&lt; -0.165 40   9 MM (0.22500000 0.77500000) *
   3) LoyalCH&lt; 0.48285 321  80 MM (0.24922118 0.75077882)  
     6) LoyalCH&gt;=0.2761415 148  58 MM (0.39189189 0.60810811)  
      12) SalePriceMM&gt;=2.04 71  31 CH (0.56338028 0.43661972)  
        24) PriceMM&lt; 2.205 58  22 CH (0.62068966 0.37931034)  
          48) SpecialMM&lt; 0.5 48  15 CH (0.68750000 0.31250000) *
          49) SpecialMM&gt;=0.5 10   3 MM (0.30000000 0.70000000) *
        25) PriceMM&gt;=2.205 13   4 MM (0.30769231 0.69230769) *
      13) SalePriceMM&lt; 2.04 77  18 MM (0.23376623 0.76623377)  
        26) SpecialCH&gt;=0.5 14   6 CH (0.57142857 0.42857143) *
        27) SpecialCH&lt; 0.5 63  10 MM (0.15873016 0.84126984) *
     7) LoyalCH&lt; 0.2761415 173  22 MM (0.12716763 0.87283237) *</code></pre>
</div>
</div>
<p>A diagram of the tree can sometimes help if its not too large. The node label indicates the predicted value, error rate, and proportion of observations included. Below the nodes are the splitting criteria.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">|&gt;</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_fit_engine</span>() <span class="sc">|&gt;</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rpart.plot</span>(<span class="at">yesno =</span> <span class="cn">TRUE</span>, <span class="at">roundint =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>A variable’s importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. From the <strong>rpart</strong> <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">vignette</a> (page 12),</p>
<blockquote class="blockquote">
<p>“An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate.”</p>
</blockquote>
<p><code>LoyalCH</code> was the most important variable, followed by <code>PriceDiff</code>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">|&gt;</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_fit_parsnip</span>() <span class="sc">|&gt;</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  vip<span class="sc">::</span><span class="fu">vip</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="performance" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="performance">Performance</h3>
<p><code>collect_metrics()</code> shows the performance metrics on the 215 testing dataset observations.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(oj_cart_fit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 4
  .metric     .estimator .estimate .config             
  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
1 accuracy    binary         0.856 Preprocessor1_Model1
2 roc_auc     binary         0.915 Preprocessor1_Model1
3 brier_class binary         0.109 Preprocessor1_Model1</code></pre>
</div>
</div>
<p>Use the full confusion matrix and ROC curve to explore performance.</p>
<section id="confusion-matrix" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="confusion-matrix">Confusion Matrix</h4>
<p>The confusion matrix calculates the model performance on the testing dataset.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>oj_cart_conf_mtrx <span class="ot">&lt;-</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  oj_cart_fit <span class="sc">|&gt;</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">|&gt;</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> Purchase, <span class="at">estimate =</span> .pred_class)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(oj_cart_conf_mtrx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 13 × 3
   .metric              .estimator .estimate
   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;
 1 accuracy             binary         0.856
 2 kap                  binary         0.695
 3 sens                 binary         0.893
 4 spec                 binary         0.798
 5 ppv                  binary         0.873
 6 npv                  binary         0.827
 7 mcc                  binary         0.696
 8 j_index              binary         0.691
 9 bal_accuracy         binary         0.845
10 detection_prevalence binary         0.623
11 precision            binary         0.873
12 recall               binary         0.893
13 f_meas               binary         0.883</code></pre>
</div>
</div>
<p>These measures come directly from the confusion matrix. The first value (CH) is considered a “positive” result and the second (MM) “negative”. The metrics are usually expressed in terms of true and false positives and negatives.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>oj_cart_conf_mtrx</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          Truth
Prediction  CH  MM
        CH 117  17
        MM  14  67</code></pre>
</div>
</div>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th style="text-align: left;">Desc</th>
<th style="text-align: left;">Formula</th>
<th style="text-align: left;">Calculation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Accuracy</strong></td>
<td style="text-align: left;">Correct predictions percent.</td>
<td style="text-align: left;"><span class="math inline">\(\frac{TP + FP}{total}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{117+67}{215}=.856\)</span></td>
</tr>
<tr class="even">
<td><strong>Sensitivity</strong>, <br><strong>Recall</strong></td>
<td style="text-align: left;">Proportion of positives identified.</td>
<td style="text-align: left;"><span class="math inline">\(\frac{TP}{TP + FN}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{117}{117+114}=.893\)</span></td>
</tr>
<tr class="odd">
<td><strong>Specificity</strong></td>
<td style="text-align: left;">Proportion of negatives identified.</td>
<td style="text-align: left;"><span class="math inline">\(\frac{TN}{TN + FP}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{67}{67 + 17}=.798\)</span></td>
</tr>
<tr class="even">
<td><strong>Positive Predictive Value</strong>, <br><strong>Precision</strong></td>
<td style="text-align: left;">Proportion of positive predictions that are correct.</td>
<td style="text-align: left;"><span class="math inline">\(\frac{TP}{TP + FP}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{117}{117 + 17}=.873\)</span></td>
</tr>
<tr class="odd">
<td><strong>Negative Predictive value</strong></td>
<td style="text-align: left;">Proportion of negative predictions that are correct.</td>
<td style="text-align: left;"><span class="math inline">\(\frac{TN}{TN + FN}\)</span></td>
<td style="text-align: left;">$=.827</td>
</tr>
<tr class="even">
<td><strong>F1-Score</strong></td>
<td style="text-align: left;">Harmonic mean of precision and recall.</td>
<td style="text-align: left;"><span class="math inline">\(2 \times \frac{\text{prec} \times \text{recall}} {\text{prec} + \text{recall}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(2 \times \frac{0.873 \times 0.893} {0.873 + 0.893} = .883\)</span></td>
</tr>
</tbody>
</table>
<p>You can bound the accuracy with a 95% CI using the binomial test.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> oj_cart_conf_mtrx<span class="sc">$</span>table <span class="sc">%&gt;%</span> <span class="fu">diag</span>() <span class="sc">%&gt;%</span> <span class="fu">sum</span>(), </span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> oj_cart_conf_mtrx<span class="sc">$</span>table <span class="sc">%&gt;%</span> <span class="fu">sum</span>()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    Exact binomial test

data:  oj_cart_conf_mtrx$table %&gt;% diag() %&gt;% sum() and oj_cart_conf_mtrx$table %&gt;% sum()
number of successes = 184, number of trials = 215, p-value &lt; 2.2e-16
alternative hypothesis: true probability of success is not equal to 0.5
95 percent confidence interval:
 0.8016244 0.8998833
sample estimates:
probability of success 
              0.855814 </code></pre>
</div>
</div>
<p>The detection prevalence (aka, no information rate (NIR)) statistic is the class rate for the largest class. In this case CH is the largest class, so NIR = 133/213 = 0.6186. The binomial test for NIR is the probability of that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH).</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> oj_cart_conf_mtrx<span class="sc">$</span>table[<span class="st">"CH"</span>, ] <span class="sc">%&gt;%</span> <span class="fu">sum</span>(), </span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> oj_cart_conf_mtrx<span class="sc">$</span>table <span class="sc">%&gt;%</span> <span class="fu">sum</span>(),</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> <span class="fu">sum</span>(oj_cart_conf_mtrx<span class="sc">$</span>table[, <span class="st">"CH"</span>]) <span class="sc">/</span> <span class="fu">sum</span>(oj_cart_conf_mtrx<span class="sc">$</span>table),</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">alternative =</span> <span class="st">"greater"</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    Exact binomial test

data:  oj_cart_conf_mtrx$table["CH", ] %&gt;% sum() and oj_cart_conf_mtrx$table %&gt;% sum()
number of successes = 134, number of trials = 215, p-value = 0.3651
alternative hypothesis: true probability of success is greater than 0.6093023
95 percent confidence interval:
 0.5655367 1.0000000
sample estimates:
probability of success 
             0.6232558 </code></pre>
</div>
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(<span class="at">x =</span> <span class="dv">116</span> <span class="sc">+</span> <span class="dv">67</span>, <span class="at">n =</span> <span class="fu">sum</span>(oj_cart_conf_mtrx<span class="sc">$</span>table), </span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">p =</span> (<span class="dv">116</span><span class="sc">+</span><span class="dv">17</span>)<span class="sc">/</span><span class="fu">sum</span>(oj_cart_conf_mtrx<span class="sc">$</span>table), </span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">alternative =</span> <span class="st">"greater"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    Exact binomial test

data:  116 + 67 and sum(oj_cart_conf_mtrx$table)
number of successes = 183, number of trials = 215, p-value = 5.461e-14
alternative hypothesis: true probability of success is greater than 0.6186047
95 percent confidence interval:
 0.8052886 1.0000000
sample estimates:
probability of success 
             0.8511628 </code></pre>
</div>
</div>
<p>The accuracy statistic indicates the model predicts 85.6% of the observations correctly. That’s good, but less impressive when you consider the prevalence of CH is 62.3% - you could achieve that accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen’s kappa statistic. The kappa statistic is explained <a href="https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/">here</a>. It compares the accuracy to the accuracy of a “random system”. It is defined as</p>
<p><span class="math display">\[\kappa = \frac{\text{Accuracy} - RA}{1-RA}\]</span></p>
<p>where</p>
<p><span class="math display">\[RA = \frac{\text{ActFalse} \times \text{PredFalse} + \text{ActTrue} \times \text{PredTrue}}{\text{Total} \times \text{Total}}\]</span></p>
<p>is the hypothetical probability of a chance agreement.</p>
<p>The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations. In this case, a kappa statistic of 0.7064 is “substantial”. See chart <a href="https://www.statisticshowto.datasciencecentral.com/cohens-kappa-statistic/">here</a>.</p>
<p>You can remind yourself what the other confusion matrix measures are from the documentation. Visuals are almost always helpful. Here is a plot of the confusion matrix.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">%&gt;%</span> </span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Purchase, .pred_class) <span class="sc">%&gt;%</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">"Simple Classification: Predicted vs. Actual"</span>,</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">"Actual"</span>,</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">"Predicted"</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="roc-curve" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="roc-curve">ROC Curve</h4>
<p>The ROC (receiver operating characteristics) curve <span class="citation" data-cites="Fawcett2005">(<a href="10-references.html#ref-Fawcett2005" role="doc-biblioref">Fawcett 2005</a>)</span> is another measure of accuracy. The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold. <code>precrec::evalmod()</code> calculates the confusion matrix values from the model using the holdout data set. The AUC on the holdout set is 0.915. <code>pRoc::plot.roc()</code>, <code>plotROC::geom_roc()</code>, and <code>yardstick::roc_curve()</code> are options for plotting a ROC curve.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">%&gt;%</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ CART ROC Curve"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>A few points on the ROC space are helpful for understanding how to use it.</p>
<ul>
<li>The lower left point (0, 0) is the result of <em>always</em> predicting “negative” or in this case “MM” if “CH” is taken as the default class. No false positives, but no true positives either.</li>
<li>The upper right point (1, 1) is the result of <em>always</em> predicting “positive” (“CH” here). You catch all true positives, but miss all the true negatives.</li>
<li>The upper left point (0, 1) is the result of perfect accuracy.</li>
<li>The lower right point (1, 0) is the result of perfect imbecility. You made the exact wrong prediction every time.</li>
<li>The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time. If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc.</li>
</ul>
<p>The goal is for all nodes to bunch up in the upper left.</p>
<p>Points to the left of the diagonal with a low TPR can be thought of as “conservative” predictors - they only make positive (CH) predictions with strong evidence. Points to the left of the diagonal with a high TPR can be thought of as “liberal” predictors - they make positive (CH) predictions with weak evidence.</p>
</section>
<section id="gain-curve" class="level4" data-number="6.2.0.1">
<h4 data-number="6.2.0.1" class="anchored" data-anchor-id="gain-curve"><span class="header-section-number">6.2.0.1</span> Gain Curve</h4>
<p>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The gray area is the “gain” over a random prediction.</p>
<p>131 of the 215 consumers in the holdout testing set purchased CH.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">%&gt;%</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 11 × 4
      .n .n_events .percent_tested .percent_found
   &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;
 1     0         0             0              0  
 2    82        79            38.1           60.3
 3   121       111            56.3           84.7
 4   129       114            60             87.0
 5   132       115            61.4           87.8
 6   134       117            62.3           89.3
 7   139       119            64.7           90.8
 8   140       120            65.1           91.6
 9   146       123            67.9           93.9
10   165       126            76.7           96.2
11   215       131           100            100  </code></pre>
</div>
</div>
<ul>
<li><p>The gain curve encountered 79 CH purchasers (60.3%) within the first 82 observations (38.1%).</p></li>
<li><p>It encountered all 131 CH purchasers on the 140th observation (100%).</p></li>
<li><p>The bottom of the gray area is the outcome of a random model. Only half the CH purchasers would be observed within 50% of the observations. The top of the gray area is the outcome of the perfect model, the “wizard curve”. Half the CH purchasers would be observed in ~30% of the observations.</p></li>
</ul>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">%&gt;%</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ CART Gain Curve"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="regression-tree" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="regression-tree"><span class="header-section-number">6.3</span> Regression Tree</h2>
<p>A simple regression tree is built in a manner similar to a simple classification tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic.</p>
<p>The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp). The only difference here is the <code>set_Mode("regression")</code> call to produce a regression tree.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>cs_cart <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `decision_tree` has 3 hyperparameters (`cost_complexity`, `tree_depth`, and</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="co"># optimize just `cost_complexity` and `tree_depth`.</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">decision_tree</span>(</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">cost_complexity =</span> <span class="fu">tune</span>(),</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="fu">tune</span>()</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"rpart"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(cs_cart<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_formula</span>(Sales <span class="sc">~</span> .)</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>  cs_cart<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(cs_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">cost_complexity</span>(), <span class="fu">tree_depth</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: rmse and rsq.</span></span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tree_depth =</span> <span class="fu">factor</span>(tree_depth)) <span class="sc">%&gt;%</span></span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> cost_complexity, <span class="at">y =</span> mean, <span class="at">color =</span> tree_depth)) <span class="sc">+</span></span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/fit-cart-reg-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The best models in terms of RMSE was the tree depth of 8 and any cp &lt; 5.6E-04.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"rmse"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 8
  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     
            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       
1    0.0000000001          8 rmse    standard    2.02    10  0.0731 Preprocesso…
2    0.0000000178          8 rmse    standard    2.02    10  0.0731 Preprocesso…
3    0.00000316            8 rmse    standard    2.02    10  0.0731 Preprocesso…
4    0.000562              8 rmse    standard    2.02    10  0.0731 Preprocesso…
5    0.0000000001         11 rmse    standard    2.02    10  0.0731 Preprocesso…</code></pre>
</div>
</div>
<p>Select the best model in terms of rmse and finalize the model.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_cart<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_cart<span class="sc">$</span>workflow, cs_cart<span class="sc">$</span>best_tune)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>  cs_cart<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Here is the tree. The output starts with the root node. The predicted sales at the root is the mean sales in the testing data set is 7.47 (values are $000s). The deviance at the root is the SSE, 2,412. The first split is at <code>ShelveLoc</code> = [Bad, Medium] vs Good.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">extract_workflow</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>══ Workflow [trained] ══════════════════════════════════════════════════════════
Preprocessor: Formula
Model: decision_tree()

── Preprocessor ────────────────────────────────────────────────────────────────
Sales ~ .

── Model ───────────────────────────────────────────────────────────────────────
n= 319 

node), split, n, deviance, yval
      * denotes terminal node

  1) root 319 2411.784000  7.473072  
    2) ShelveLoc=Bad,Medium 250 1564.689000  6.807960  
      4) Price&gt;=105.5 159  791.704800  6.040252  
        8) ShelveLoc=Bad 51  219.627100  4.827059  
         16) Population&lt; 196.5 18   72.973230  3.686111 *
         17) Population&gt;=196.5 33  110.441200  5.449394  
           34) Age&gt;=60 7    9.771743  3.797143 *
           35) Age&lt; 60 26   76.415030  5.894231  
             70) CompPrice&lt; 137.5 18   29.894430  5.313889 *
             71) CompPrice&gt;=137.5 8   26.818000  7.200000 *
        9) ShelveLoc=Medium 108  461.567300  6.613148  
         18) Income&lt; 60.5 41  193.069500  5.589268  
           36) Price&gt;=135.5 12   49.608090  4.014167 *
           37) Price&lt; 135.5 29  101.370900  6.241034  
             74) CompPrice&lt; 124.5 15   24.591290  5.199333 *
             75) CompPrice&gt;=124.5 14   43.062690  7.357143 *
         19) Income&gt;=60.5 67  199.214200  7.239701  
           38) Advertising&lt; 13.5 50  131.275800  6.799600  
             76) Age&gt;=68 12   11.557370  5.348333 *
             77) Age&lt; 68 38   86.463030  7.257895  
              154) Price&gt;=135 11   11.881470  6.124545 *
              155) Price&lt; 135 27   54.695900  7.719630  
                310) CompPrice&lt; 129.5 15   21.461290  6.987333 *
                311) CompPrice&gt;=129.5 12   15.135900  8.635000 *
           39) Advertising&gt;=13.5 17   29.770210  8.534118 *
      5) Price&lt; 105.5 91  515.537400  8.149341  
       10) CompPrice&lt; 123.5 65  339.278900  7.467077  
         20) Price&gt;=92.5 33  110.968900  6.316970  
           40) Income&lt; 96 26   69.841400  5.850000  
             80) Population&gt;=246 14   21.230090  4.899286 *
             81) Population&lt; 246 12   21.194290  6.959167 *
           41) Income&gt;=96 7   14.399490  8.051429 *
         21) Price&lt; 92.5 32  139.644700  8.653125  
           42) ShelveLoc=Bad 12   40.694300  7.025000 *
           43) ShelveLoc=Medium 20   48.055200  9.630000  
             86) Education&gt;=15.5 7    3.955971  8.394286 *
             87) Education&lt; 15.5 13   27.654720 10.295380 *
       11) CompPrice&gt;=123.5 26   70.360850  9.855000  
         22) Advertising&lt; 9 12   26.948700  8.865000 *
         23) Advertising&gt;=9 14   21.569920 10.703570 *
    3) ShelveLoc=Good 69  335.799800  9.882899  
      6) Price&gt;=108.5 49  195.188000  9.127347  
       12) Advertising&lt; 14 42  127.875600  8.696190  
         24) Income&lt; 43 12    9.912025  7.442500 *
         25) Income&gt;=43 30   91.558340  9.197667  

...
and 6 more lines.</code></pre>
</div>
</div>
<p>Here is a diagram of the tree. The node label indicates the predicted value (mean) and the proportion of observations that are in the node (or child nodes). Below the nodes are the splitting criteria.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_fit_engine</span>() <span class="sc">%&gt;%</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rpart.plot</span>(<span class="at">yesno =</span> <span class="cn">TRUE</span>, <span class="at">roundint =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><code>Price</code> and <code>ShelveLoc</code> were the most important variables.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_fit_parsnip</span>() <span class="sc">%&gt;%</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>  vip<span class="sc">::</span><span class="fu">vip</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="measuring-performance" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="measuring-performance">Measuring Performance</h3>
<p><code>collect_metrics()</code> returns the RMSE, <span class="math inline">\(RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})\)</span> and the model <span class="math inline">\(R^2\)</span>. The RMSE of 2.14 in the test data set is pretty good considering the standard deviation of <code>Sales</code> is 3.10.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 4
  .metric .estimator .estimate .config             
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
1 rmse    standard       2.14  Preprocessor1_Model1
2 rsq     standard       0.548 Preprocessor1_Model1</code></pre>
</div>
</div>
<p>Here is a predicted vs actual plot.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">"cadetblue"</span>) <span class="sc">+</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"loess"</span>, <span class="at">formula =</span> <span class="st">"y~x"</span>) <span class="sc">+</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Carseats CART, Predicted vs Actual"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-30-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The tree nodes do a decent job of binning the observations. The predictions vs actuals plot suggests the model over-estimates at the low end and underestimates at the high end. Calculate the test data set RMSE.</p>
</section>
</section>
<section id="bagged-trees" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="bagged-trees"><span class="header-section-number">6.4</span> Bagged Trees</h2>
<p>One drawback of decision trees is that they are high-variance estimators. A small number of additional training observations can dramatically alter the prediction performance of a learned tree.</p>
<p>Bootstrap aggregation, or <em>bagging</em>, is a general-purpose procedure for reducing the variance of a statistical learning method. The algorithm constructs <em>B</em> regression trees using <em>B</em> bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging the <em>B</em> trees reduces the variance. The predicted value for an observation is the mode (classification) or mean (regression) of the trees. <em>B</em> usually equals ~25.</p>
<p>To test the model accuracy, the out-of-bag observations are predicted from the models. For a training set of size <em>n</em>, each tree is composed of <span class="math inline">\(\sim (1 - e^{-1})n = .632n\)</span> unique observations in-bag and <span class="math inline">\(.368n\)</span> out-of-bag. For each tree in the ensemble, bagging makes predictions on the tree’s out-of-bag observations. I think (<em>see</em> page 197 of <span class="citation" data-cites="Kuhn2016">(<a href="10-references.html#ref-Kuhn2016" role="doc-biblioref">Kuhn and Johnson 2016</a>)</span>) bagging measures the performance (RMSE, Accuracy, ROC, etc.) of each tree in the ensemble and averages them to produce an overall performance estimate. (This makes no sense to me. If each tree has poor performance, then the average performance of many trees will still be poor. An ensemble of <em>B</em> trees will produce <span class="math inline">\(\sim .368 B\)</span> predictions per unique observation. Seems like you should take the mean/mode of each observation’s prediction as the final prediction. Then you have <em>n</em> predictions to compare to <em>n</em> actuals, and you assess performance on that.)</p>
<p>The downside to bagging is that there is no single tree with a set of rules to interpret. It becomes unclear which variables are more important than others.</p>
<p>The next section explains how bagged trees are a special case of random forests.</p>
<section id="bagged-classification-tree" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="bagged-classification-tree"><span class="header-section-number">6.4.1</span> Bagged Classification Tree</h3>
<p>Leaning by example, I’ll predict <code>Purchase</code> from the <code>OJ</code> data set again, this time using the bagging with <code>parsnip::bag_tree()</code>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>oj_bag <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `bag_tree` has 4 hyperparameters (`cost_complexity`, `tree_depth`, and</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="co"># optimize just `cost_complexity` and `tree_depth`.</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bag_tree</span>(</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">cost_complexity =</span> <span class="fu">tune</span>(),</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="fu">tune</span>()</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"rpart"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"classification"</span>)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(oj_bag<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_formula</span>(Purchase <span class="sc">~</span> .)</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>  oj_bag<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(oj_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">cost_complexity</span>(), <span class="fu">tree_depth</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tree_depth =</span> <span class="fu">factor</span>(tree_depth)) <span class="sc">%&gt;%</span></span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> cost_complexity, <span class="at">y =</span> mean, <span class="at">color =</span> tree_depth)) <span class="sc">+</span></span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/fit-bag-class-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The best models in terms of accuracy and ROC was the tree depth of 4 and any cp &lt;= 3.16e-06.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"accuracy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 8
  cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    
            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      
1    0.000562              4 accuracy binary     0.825    10  0.0107 Preprocess…
2    0.0000000001          4 accuracy binary     0.823    10  0.0109 Preprocess…
3    0.0000000178          4 accuracy binary     0.822    10  0.0108 Preprocess…
4    0.00000316            4 accuracy binary     0.821    10  0.0126 Preprocess…
5    0.000562              8 accuracy binary     0.813    10  0.0156 Preprocess…</code></pre>
</div>
</div>
<p>Select the best model in terms of accuracy and finalize the model.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_bag<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"accuracy"</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(oj_bag<span class="sc">$</span>workflow, oj_bag<span class="sc">$</span>best_tune)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>  oj_bag<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>I <em>think</em> tidymodels started by splitting the training set into 10 folds, then using 9 of the folds to run the bagging algorithm and collect performance measures on the hold-out fold. After repeating the process for all 10 folds, it averaged the performance measures to produce the resampling results shown below. With hyperparameters, the process is repeated for all combinations and the resampling results above are from the best performing combination.</p>
<p>There is no single tree to visualize, and you can’t even produce a VIP. Let’s look at the performance on the holdout data set. <code>collect_metrics()</code> shows the accuracy and ROC AUC metrics. The accuracy is slightly lower than the single tree, but ROC AUC is higher.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 4
  .metric     .estimator .estimate .config             
  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
1 accuracy    binary         0.842 Preprocessor1_Model1
2 roc_auc     binary         0.925 Preprocessor1_Model1
3 brier_class binary         0.105 Preprocessor1_Model1</code></pre>
</div>
</div>
<p>You can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>confmat <span class="ot">&lt;-</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>  oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> Purchase, <span class="at">estimate =</span> .pred_class)</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>confmat</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="do">##           Truth</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Prediction  CH  MM</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="do">##         CH 115  18</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="do">##         MM  16  66</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(oj_bag<span class="sc">$</span>confmat)</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 13 × 3</span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a><span class="do">##    .metric              .estimator .estimate</span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a><span class="do">##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a><span class="do">##  1 accuracy             binary         0.842</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a><span class="do">##  2 kap                  binary         0.666</span></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a><span class="do">##  3 sens                 binary         0.878</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a><span class="do">##  4 spec                 binary         0.786</span></span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a><span class="do">##  5 ppv                  binary         0.865</span></span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a><span class="do">##  6 npv                  binary         0.805</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a><span class="do">##  7 mcc                  binary         0.667</span></span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a><span class="do">##  8 j_index              binary         0.664</span></span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a><span class="do">##  9 bal_accuracy         binary         0.832</span></span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a><span class="do">## 10 detection_prevalence binary         0.619</span></span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a><span class="do">## 11 precision            binary         0.865</span></span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a><span class="do">## 12 recall               binary         0.878</span></span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a><span class="do">## 13 f_meas               binary         0.871</span></span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Purchase, .pred_class) <span class="sc">%&gt;%</span></span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(</span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">"Bagged Trees: Predicted vs. Actual"</span>,</span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">"Actual"</span>,</span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">"Predicted"</span></span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-34-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is 0.925.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ Bagged Trees ROC Curve"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-35-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ Bagged Trees Gain Curve"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-36-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="bagging-regression-tree" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="bagging-regression-tree"><span class="header-section-number">6.4.2</span> Bagging Regression Tree</h3>
<p>I’ll predict <code>Sales</code> from the <code>Carseats</code> data set again, this time with <code>parsnip::bag_tree()</code>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(baguette)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>cs_bag <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="co"># `bag_tree` has 4 hyperparameters (`cost_complexity`, `tree_depth`, and</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="co"># optimize just `cost_complexity` and `tree_depth`.</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bag_tree</span>(</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">cost_complexity =</span> <span class="fu">tune</span>(),</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="fu">tune</span>()</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"rpart"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>)</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(cs_bag<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_formula</span>(Sales <span class="sc">~</span> .)</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>  cs_bag<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(cs_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">cost_complexity</span>(), <span class="fu">tree_depth</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tree_depth =</span> <span class="fu">factor</span>(tree_depth)) <span class="sc">%&gt;%</span></span>
<span id="cb57-36"><a href="#cb57-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> cost_complexity, <span class="at">y =</span> mean, <span class="at">color =</span> tree_depth)) <span class="sc">+</span></span>
<span id="cb57-37"><a href="#cb57-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb57-38"><a href="#cb57-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb57-39"><a href="#cb57-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/fit-bag-reg-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The best models in terms of RMSE was the tree depth of 8 and any cp &lt; 5.6E-04.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"rmse"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 8
  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     
            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       
1    0.000562             11 rmse    standard    1.58    10  0.0416 Preprocesso…
2    0.0000000178         15 rmse    standard    1.58    10  0.0585 Preprocesso…
3    0.000562             15 rmse    standard    1.59    10  0.0639 Preprocesso…
4    0.0000000001         11 rmse    standard    1.60    10  0.0652 Preprocesso…
5    0.00000316            8 rmse    standard    1.63    10  0.0618 Preprocesso…</code></pre>
</div>
</div>
<p>Select the best model in terms of rmse and finalize the model.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_bag<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_bag<span class="sc">$</span>workflow, cs_bag<span class="sc">$</span>best_tune)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>  cs_bag<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>collect_metrics()</code> returns the RMSE, <span class="math inline">\(RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})\)</span> and the model <span class="math inline">\(R^2\)</span>. The RMSE of <code>cs_bag$fit %&gt;% collect_metrics() %&gt;% filter(.metric == "rmse") %&gt;% pull(.estimate) %&gt;% comma(.01)</code> in the test data set is pretty good considering the standard deviation of <code>Sales</code> is <code>comma(sd(cs_test$Sales), .01)</code>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 4
  .metric .estimator .estimate .config             
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
1 rmse    standard       1.65  Preprocessor1_Model1
2 rsq     standard       0.738 Preprocessor1_Model1</code></pre>
</div>
</div>
<p>Here is a predicted vs actual plot.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">"cadetblue"</span>) <span class="sc">+</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"loess"</span>, <span class="at">formula =</span> <span class="st">"y~x"</span>) <span class="sc">+</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Carseats Bagged Trees, Predicted vs Actual"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-40-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="random-forests" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="random-forests"><span class="header-section-number">6.5</span> Random Forests</h2>
<p>Random forests improve bagged trees by way of a small tweak that de-correlates the trees. As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of <em>mtry</em> predictors is chosen as split candidates from the full set of <em>p</em> predictors. A fresh sample of <em>mtry</em> predictors is taken at each split. Typically <span class="math inline">\(mtry \sim \sqrt{p}\)</span>. Bagged trees are thus a special case of random forests where <em>mtry = p</em>.</p>
<section id="random-forest-classification-tree" class="level4" data-number="6.5.0.1">
<h4 data-number="6.5.0.1" class="anchored" data-anchor-id="random-forest-classification-tree"><span class="header-section-number">6.5.0.1</span> Random Forest Classification Tree</h4>
<p>Leaning by example, I’ll predict <code>Purchase</code> from the <code>OJ</code> data set again, this time using the bagging with <code>parsnip::rand_forest()</code>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>oj_rf <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="co"># optimize just `trees` and `min_n`.</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rand_forest</span>(</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"ranger"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"classification"</span>)</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(oj_rf<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_formula</span>(Purchase <span class="sc">~</span> .)</span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>  oj_rf<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(oj_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">trees</span>(), <span class="fu">min_n</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trees =</span> <span class="fu">factor</span>(trees)) <span class="sc">%&gt;%</span></span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> min_n, <span class="at">y =</span> mean, <span class="at">color =</span> trees)) <span class="sc">+</span></span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/fit-rf-class-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The best models in terms of accuracy and ROC was the trees of 1000 and any minimum node size of 40.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"accuracy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 8
  trees min_n .metric  .estimator  mean     n std_err .config              
  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
1  1500    30 accuracy binary     0.813    10  0.0122 Preprocessor1_Model19
2  1000    40 accuracy binary     0.812    10  0.0111 Preprocessor1_Model23
3   500    40 accuracy binary     0.812    10  0.0125 Preprocessor1_Model22
4  1000    30 accuracy binary     0.812    10  0.0130 Preprocessor1_Model18
5   500    30 accuracy binary     0.812    10  0.0127 Preprocessor1_Model17</code></pre>
</div>
</div>
<p>Select the best model in terms of accuracy and finalize the model.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_rf<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"accuracy"</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(oj_rf<span class="sc">$</span>workflow, oj_rf<span class="sc">$</span>best_tune)</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>  oj_rf<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>There is no single tree to visualize, and you can’t even produce a VIP. Let’s look at the performance on the holdout data set. <code>collect_metrics()</code> shows the accuracy and ROC AUC metrics. The accuracy is slightly lower than the single tree, but ROC AUC is higher than both the single tree and bagged trees.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 4
  .metric     .estimator .estimate .config             
  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
1 accuracy    binary         0.847 Preprocessor1_Model1
2 roc_auc     binary         0.935 Preprocessor1_Model1
3 brier_class binary         0.107 Preprocessor1_Model1</code></pre>
</div>
</div>
<p>You can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>confmat <span class="ot">&lt;-</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>  oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> Purchase, <span class="at">estimate =</span> .pred_class)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>confmat</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a><span class="do">##           Truth</span></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Prediction  CH  MM</span></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a><span class="do">##         CH 116  18</span></span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a><span class="do">##         MM  15  66</span></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(oj_rf<span class="sc">$</span>confmat)</span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 13 × 3</span></span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a><span class="do">##    .metric              .estimator .estimate</span></span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a><span class="do">##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a><span class="do">##  1 accuracy             binary         0.847</span></span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a><span class="do">##  2 kap                  binary         0.676</span></span>
<span id="cb70-18"><a href="#cb70-18" aria-hidden="true" tabindex="-1"></a><span class="do">##  3 sens                 binary         0.885</span></span>
<span id="cb70-19"><a href="#cb70-19" aria-hidden="true" tabindex="-1"></a><span class="do">##  4 spec                 binary         0.786</span></span>
<span id="cb70-20"><a href="#cb70-20" aria-hidden="true" tabindex="-1"></a><span class="do">##  5 ppv                  binary         0.866</span></span>
<span id="cb70-21"><a href="#cb70-21" aria-hidden="true" tabindex="-1"></a><span class="do">##  6 npv                  binary         0.815</span></span>
<span id="cb70-22"><a href="#cb70-22" aria-hidden="true" tabindex="-1"></a><span class="do">##  7 mcc                  binary         0.676</span></span>
<span id="cb70-23"><a href="#cb70-23" aria-hidden="true" tabindex="-1"></a><span class="do">##  8 j_index              binary         0.671</span></span>
<span id="cb70-24"><a href="#cb70-24" aria-hidden="true" tabindex="-1"></a><span class="do">##  9 bal_accuracy         binary         0.836</span></span>
<span id="cb70-25"><a href="#cb70-25" aria-hidden="true" tabindex="-1"></a><span class="do">## 10 detection_prevalence binary         0.623</span></span>
<span id="cb70-26"><a href="#cb70-26" aria-hidden="true" tabindex="-1"></a><span class="do">## 11 precision            binary         0.866</span></span>
<span id="cb70-27"><a href="#cb70-27" aria-hidden="true" tabindex="-1"></a><span class="do">## 12 recall               binary         0.885</span></span>
<span id="cb70-28"><a href="#cb70-28" aria-hidden="true" tabindex="-1"></a><span class="do">## 13 f_meas               binary         0.875</span></span>
<span id="cb70-29"><a href="#cb70-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-30"><a href="#cb70-30" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb70-31"><a href="#cb70-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb70-32"><a href="#cb70-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Purchase, .pred_class) <span class="sc">%&gt;%</span></span>
<span id="cb70-33"><a href="#cb70-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(</span>
<span id="cb70-34"><a href="#cb70-34" aria-hidden="true" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">"Random Forest: Predicted vs. Actual"</span>,</span>
<span id="cb70-35"><a href="#cb70-35" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">"Actual"</span>,</span>
<span id="cb70-36"><a href="#cb70-36" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">"Predicted"</span></span>
<span id="cb70-37"><a href="#cb70-37" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-44-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is 0.935.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ Random Forest ROC Curve"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-45-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ Random Forest Gain Curve"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-46-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="random-forest-regression-tree" class="level4" data-number="6.5.0.2">
<h4 data-number="6.5.0.2" class="anchored" data-anchor-id="random-forest-regression-tree"><span class="header-section-number">6.5.0.2</span> Random Forest Regression Tree</h4>
<p>I’ll predict <code>Sales</code> from the <code>Carseats</code> data set again, this time with <code>parsnip::rand_forest()</code>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>cs_rf <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="co"># optimize just `trees` and `min_n`.</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rand_forest</span>(</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"ranger"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>)</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(cs_rf<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_formula</span>(Sales <span class="sc">~</span> .)</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>  cs_rf<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(cs_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">trees</span>(), <span class="fu">min_n</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-30"><a href="#cb73-30" aria-hidden="true" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb73-31"><a href="#cb73-31" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb73-32"><a href="#cb73-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb73-33"><a href="#cb73-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trees =</span> <span class="fu">factor</span>(trees)) <span class="sc">%&gt;%</span></span>
<span id="cb73-34"><a href="#cb73-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> min_n, <span class="at">y =</span> mean, <span class="at">color =</span> trees)) <span class="sc">+</span></span>
<span id="cb73-35"><a href="#cb73-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb73-36"><a href="#cb73-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb73-37"><a href="#cb73-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/fit-rf-reg-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The best models in terms of RMSE was 2000 tree with at least 2 data points per node.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"rmse"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 8
  trees min_n .metric .estimator  mean     n std_err .config              
  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
1  1000     2 rmse    standard    1.72    10  0.0707 Preprocessor1_Model03
2   500     2 rmse    standard    1.73    10  0.0724 Preprocessor1_Model02
3  1500     2 rmse    standard    1.73    10  0.0696 Preprocessor1_Model04
4  2000     2 rmse    standard    1.73    10  0.0687 Preprocessor1_Model05
5  1500    11 rmse    standard    1.77    10  0.0681 Preprocessor1_Model09</code></pre>
</div>
</div>
<p>Select the best model in terms of rmse and finalize the model.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_rf<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_rf<span class="sc">$</span>workflow, cs_rf<span class="sc">$</span>best_tune)</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>  cs_rf<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>collect_metrics()</code> returns the RMSE, <span class="math inline">\(RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})\)</span> and the model <span class="math inline">\(R^2\)</span>. The RMSE of 1.90 in the test data set is pretty good considering the standard deviation of <code>Sales</code> is 3.10.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 4
  .metric .estimator .estimate .config             
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
1 rmse    standard       1.90  Preprocessor1_Model1
2 rsq     standard       0.732 Preprocessor1_Model1</code></pre>
</div>
</div>
<p>Here is a predicted vs actual plot.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">"cadetblue"</span>) <span class="sc">+</span></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"loess"</span>, <span class="at">formula =</span> <span class="st">"y~x"</span>) <span class="sc">+</span></span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Carseats Random Forest, Predicted vs Actual"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-50-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="gradient-boosting" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="gradient-boosting"><span class="header-section-number">6.6</span> Gradient Boosting</h2>
<p><strong>Note</strong>: I learned gradient boosting from <a href="https://explained.ai/gradient-boosting/L2-loss.html">explained.ai</a>.</p>
<p>Gradient boosting machine (GBM) is an additive modeling algorithm that gradually builds a composite model by iteratively adding <em>M</em> weak sub-models based on the performance of the prior iteration’s composite,</p>
<p><span class="math display">\[F_M(x) = \sum_m^M f_m(x).\]</span></p>
<p>The idea is to fit a weak model, then replace the response values with the residuals from that model, and fit another model. Adding the residual prediction model to the original response prediction model produces a more accurate model. GBM repeats this process over and over, running new models to predict the residuals of the previous composite models, and adding the results to produce new composites. With each iteration, the model becomes stronger and stronger. The successive trees are usually weighted to slow down the learning rate. “Shrinkage” reduces the influence of each individual tree and leaves space for future trees to improve the model.</p>
<p><span class="math display">\[F_M(x) = f_0 + \eta\sum_{m = 1}^M f_m(x).\]</span></p>
<p>The smaller the learning rate, <span class="math inline">\(\eta\)</span>, the larger the number of trees, <span class="math inline">\(M\)</span>. <span class="math inline">\(\eta\)</span> and <span class="math inline">\(M\)</span> are hyperparameters. Other constraints to the trees are usually applied as additional hyperparameters, including, tree depth, number of nodes, minimum observations per split, and minimum improvement to loss.</p>
<p>The name “gradient boosting” refers to the <em>boosting</em> of a model with a <em>gradient</em>. Each round of training builds a <em>weak learner</em> and uses the residuals to calculate a gradient, the partial derivative of the loss function. Gradient boosting “descends the gradient” to adjust the model parameters to reduce the error in the next round of training.</p>
<p>In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error. GBM continues until it reaches maximum number of trees or an acceptable error level.</p>
<section id="gradient-boosting-classification-tree" class="level4" data-number="6.6.0.1">
<h4 data-number="6.6.0.1" class="anchored" data-anchor-id="gradient-boosting-classification-tree"><span class="header-section-number">6.6.0.1</span> Gradient Boosting Classification Tree</h4>
<p>Leaning by example, I’ll predict <code>Purchase</code> from the <code>OJ</code> data set again, this time using the bagging with <code>parsnip::boost_tree()</code></p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>oj_xgb <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `boost_tree` has 8 hyperparameters. Let's optimize just `trees` and `min_n`.</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">boost_tree</span>(</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"xgboost"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"classification"</span>)</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(oj_xgb<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_formula</span>(Purchase <span class="sc">~</span> .)</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>  oj_xgb<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(oj_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb80-25"><a href="#cb80-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">trees</span>(), <span class="fu">min_n</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb80-26"><a href="#cb80-26" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb80-27"><a href="#cb80-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-28"><a href="#cb80-28" aria-hidden="true" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb80-29"><a href="#cb80-29" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb80-30"><a href="#cb80-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb80-31"><a href="#cb80-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trees =</span> <span class="fu">factor</span>(trees)) <span class="sc">%&gt;%</span></span>
<span id="cb80-32"><a href="#cb80-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> min_n, <span class="at">y =</span> mean, <span class="at">color =</span> trees)) <span class="sc">+</span></span>
<span id="cb80-33"><a href="#cb80-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb80-34"><a href="#cb80-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb80-35"><a href="#cb80-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/fit-xgboost-class-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The best models in terms of accuracy and ROC was the trees of 1000 and any minimum node size of 40.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"accuracy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 8
  trees min_n .metric  .estimator  mean     n std_err .config              
  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
1   500    30 accuracy binary     0.814    10 0.0146  Preprocessor1_Model17
2  1500    30 accuracy binary     0.814    10 0.0121  Preprocessor1_Model19
3  2000    30 accuracy binary     0.814    10 0.0121  Preprocessor1_Model20
4     1     2 accuracy binary     0.814    10 0.00999 Preprocessor1_Model01
5  1000    30 accuracy binary     0.811    10 0.0149  Preprocessor1_Model18</code></pre>
</div>
</div>
<p>Select the best model in terms of accuracy and finalize the model.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_xgb<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"accuracy"</span>)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(oj_xgb<span class="sc">$</span>workflow, oj_xgb<span class="sc">$</span>best_tune)</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>  oj_xgb<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>There is no single tree to visualize, and you can’t even produce a VIP. Let’s look at the performance on the holdout data set. <code>collect_metrics()</code> shows the accuracy and ROC AUC metrics.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 4
  .metric     .estimator .estimate .config             
  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
1 accuracy    binary         0.842 Preprocessor1_Model1
2 roc_auc     binary         0.932 Preprocessor1_Model1
3 brier_class binary         0.103 Preprocessor1_Model1</code></pre>
</div>
</div>
<p>You can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>confmat <span class="ot">&lt;-</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>  oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> Purchase, <span class="at">estimate =</span> .pred_class)</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>confmat</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a><span class="do">##           Truth</span></span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Prediction  CH  MM</span></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a><span class="do">##         CH 114  17</span></span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a><span class="do">##         MM  17  67</span></span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(oj_xgb<span class="sc">$</span>confmat)</span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a><span class="do">## # A tibble: 13 × 3</span></span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a><span class="do">##    .metric              .estimator .estimate</span></span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a><span class="do">##    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a><span class="do">##  1 accuracy             binary         0.842</span></span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a><span class="do">##  2 kap                  binary         0.668</span></span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a><span class="do">##  3 sens                 binary         0.870</span></span>
<span id="cb86-19"><a href="#cb86-19" aria-hidden="true" tabindex="-1"></a><span class="do">##  4 spec                 binary         0.798</span></span>
<span id="cb86-20"><a href="#cb86-20" aria-hidden="true" tabindex="-1"></a><span class="do">##  5 ppv                  binary         0.870</span></span>
<span id="cb86-21"><a href="#cb86-21" aria-hidden="true" tabindex="-1"></a><span class="do">##  6 npv                  binary         0.798</span></span>
<span id="cb86-22"><a href="#cb86-22" aria-hidden="true" tabindex="-1"></a><span class="do">##  7 mcc                  binary         0.668</span></span>
<span id="cb86-23"><a href="#cb86-23" aria-hidden="true" tabindex="-1"></a><span class="do">##  8 j_index              binary         0.668</span></span>
<span id="cb86-24"><a href="#cb86-24" aria-hidden="true" tabindex="-1"></a><span class="do">##  9 bal_accuracy         binary         0.834</span></span>
<span id="cb86-25"><a href="#cb86-25" aria-hidden="true" tabindex="-1"></a><span class="do">## 10 detection_prevalence binary         0.609</span></span>
<span id="cb86-26"><a href="#cb86-26" aria-hidden="true" tabindex="-1"></a><span class="do">## 11 precision            binary         0.870</span></span>
<span id="cb86-27"><a href="#cb86-27" aria-hidden="true" tabindex="-1"></a><span class="do">## 12 recall               binary         0.870</span></span>
<span id="cb86-28"><a href="#cb86-28" aria-hidden="true" tabindex="-1"></a><span class="do">## 13 f_meas               binary         0.870</span></span>
<span id="cb86-29"><a href="#cb86-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-30"><a href="#cb86-30" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb86-31"><a href="#cb86-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb86-32"><a href="#cb86-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Purchase, .pred_class) <span class="sc">%&gt;%</span></span>
<span id="cb86-33"><a href="#cb86-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(</span>
<span id="cb86-34"><a href="#cb86-34" aria-hidden="true" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">"XGBoost: Predicted vs. Actual"</span>,</span>
<span id="cb86-35"><a href="#cb86-35" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">"Actual"</span>,</span>
<span id="cb86-36"><a href="#cb86-36" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">"Predicted"</span></span>
<span id="cb86-37"><a href="#cb86-37" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-54-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is 0.932.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ XGBoost ROC Curve"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-55-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ XGBoost Gain Curve"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-56-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="gradient-boosting-regression-tree" class="level4" data-number="6.6.0.2">
<h4 data-number="6.6.0.2" class="anchored" data-anchor-id="gradient-boosting-regression-tree"><span class="header-section-number">6.6.0.2</span> Gradient Boosting Regression Tree</h4>
<p>I’ll predict <code>Sales</code> from the <code>Carseats</code> data set again, this time with <code>parsnip::rand_forest()</code>.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>cs_xgb <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a><span class="co"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a><span class="co"># optimize just `trees` and `min_n`.</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>model <span class="ot">&lt;-</span></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">boost_tree</span>(</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">trees =</span> <span class="fu">tune</span>(),</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"xgboost"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>)</span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune a model using the workflow framework.</span></span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>workflow <span class="ot">&lt;-</span></span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(cs_xgb<span class="sc">$</span>model) <span class="sc">%&gt;%</span></span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_formula</span>(Sales <span class="sc">~</span> .)</span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a><span class="co"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a><span class="co"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>tune_grid <span class="ot">&lt;-</span></span>
<span id="cb89-24"><a href="#cb89-24" aria-hidden="true" tabindex="-1"></a>  cs_xgb<span class="sc">$</span>workflow <span class="sc">%&gt;%</span></span>
<span id="cb89-25"><a href="#cb89-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb89-26"><a href="#cb89-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(cs_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb89-27"><a href="#cb89-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(<span class="fu">trees</span>(), <span class="fu">min_n</span>(), <span class="at">levels =</span> <span class="dv">5</span>)</span>
<span id="cb89-28"><a href="#cb89-28" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb89-29"><a href="#cb89-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-30"><a href="#cb89-30" aria-hidden="true" tabindex="-1"></a><span class="co"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb89-31"><a href="#cb89-31" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>tune_grid <span class="sc">%&gt;%</span> </span>
<span id="cb89-32"><a href="#cb89-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_metrics</span>() <span class="sc">%&gt;%</span></span>
<span id="cb89-33"><a href="#cb89-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trees =</span> <span class="fu">factor</span>(trees)) <span class="sc">%&gt;%</span></span>
<span id="cb89-34"><a href="#cb89-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> min_n, <span class="at">y =</span> mean, <span class="at">color =</span> trees)) <span class="sc">+</span></span>
<span id="cb89-35"><a href="#cb89-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb89-36"><a href="#cb89-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(.metric), <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb89-37"><a href="#cb89-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/fit-xgboost-reg-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The best models in terms of RMSE was 500 trees with at least 21 data points per node.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"rmse"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 8
  trees min_n .metric .estimator  mean     n std_err .config              
  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
1   500    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model07
2  1000    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model08
3  1500    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model09
4  2000    11 rmse    standard    1.36    10  0.0674 Preprocessor1_Model10
5   500    40 rmse    standard    1.38    10  0.0759 Preprocessor1_Model22</code></pre>
</div>
</div>
<p>Select the best model in terms of rmse and finalize the model.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_xgb<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_xgb<span class="sc">$</span>workflow, cs_xgb<span class="sc">$</span>best_tune)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>  cs_rf<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><code>collect_metrics()</code> returns the RMSE, <span class="math inline">\(RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})\)</span> and the model <span class="math inline">\(R^2\)</span>. The RMSE of 1.89 in the test data set is pretty good considering the standard deviation of <code>Sales</code> is 3.10.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 4
  .metric .estimator .estimate .config             
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
1 rmse    standard       1.89  Preprocessor1_Model1
2 rsq     standard       0.740 Preprocessor1_Model1</code></pre>
</div>
</div>
<p>Here is a predicted vs actual plot.</p>
<div class="cell">
<details open="" class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">"cadetblue"</span>) <span class="sc">+</span></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"loess"</span>, <span class="at">formula =</span> <span class="st">"y~x"</span>) <span class="sc">+</span></span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Carseats Random Forest, Predicted vs Actual"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="06-decision_trees_files/figure-html/unnamed-chunk-60-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Fawcett2005" class="csl-entry" role="listitem">
Fawcett, Tom. 2005. <em>An Introduction to ROC Analysis</em>. ELSEVIER. <a href="https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf">https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf</a>.
</div>
<div id="ref-James2013" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning: With Applications in r</em>. 1st ed. New York, NY: Springer. <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/book.html">http://faculty.marshall.usc.edu/gareth-james/ISL/book.html</a>.
</div>
<div id="ref-Kuhn2016" class="csl-entry" role="listitem">
Kuhn, Max, and Kjell Johnson. 2016. <em>Applied Predictive Modeling</em>. 1st ed. New York, NY: Springer. <a href="http://appliedpredictivemodeling.com/">http://appliedpredictivemodeling.com/</a>.
</div>
<div id="ref-Therneau2019" class="csl-entry" role="listitem">
Therneau, Terry, and Elizabeth Atkinson. 2019. <em>An Introduction to Recursive Partitioning Using the RPART Routines</em>. Boca Raton, Florida: Chapman; Hall/CRC. <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>You might come across a variation of the Gini index called entropy, the <em>information statistic</em>, <span class="math inline">\(D = - \sum_{c=1}^C{\hat{p}_{kc} \log \hat{p}_{kc}}\)</span>. Whereas Gini can range from 0 to .25, entropy ranges from 0 to <span class="math inline">\(-(.5 \log(.5)) \cdot 2 = 0.69\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./05-regularization.html" class="pagination-link" aria-label="Regularization">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regularization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./07-support_vector_machines.html" class="pagination-link" aria-label="Support Vector Machines">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb96" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Decision Trees</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r include=FALSE}</span></span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a><span class="in">library(tidyverse)</span></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a><span class="in">library(tidymodels)</span></span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a><span class="in">library(tictoc)</span></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a><span class="in">library(rpart.plot)  # better formatted plots than the ones in rpart</span></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a><span class="in">library(baguette)</span></span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a><span class="in">tidymodels_prefer()</span></span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a><span class="in">theme_set(</span></span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_light() +</span></span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a><span class="in">    theme(</span></span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a><span class="in">      plot.caption = element_text(hjust = 0),</span></span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a><span class="in">      strip.background = element_rect(fill = "gray90", color = "gray60"),</span></span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a><span class="in">      strip.text = element_text(color = "gray20")</span></span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a><span class="in">    )</span></span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a><span class="in">scoreboard &lt;- function(dat){</span></span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a><span class="in">   dat %&gt;%</span></span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a><span class="in">      flextable::flextable() %&gt;%</span></span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a><span class="in">      flextable::colformat_num(j = 2, digits = 4) %&gt;%</span></span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a><span class="in">      flextable::autofit()</span></span>
<span id="cb96-26"><a href="#cb96-26" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb96-27"><a href="#cb96-27" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-28"><a href="#cb96-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-29"><a href="#cb96-29" aria-hidden="true" tabindex="-1"></a>Decision tree models, also known as classification and regression trees (CART), split the dataset into subsets based on the value of input features, creating a tree-like structure of decision rules. At each node, the algorithm chooses the best split. The process continues recursively until a stopping condition is met. The result is a tree that can be used to make predictions for new data points.</span>
<span id="cb96-30"><a href="#cb96-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-31"><a href="#cb96-31" aria-hidden="true" tabindex="-1"></a>CART models define the nodes through a *top-down greedy* process called *recursive binary splitting*. *Top-down* because it begins at the top of the tree with all observations in a single region and successively splits the predictor space; and *greedy* because at each splitting step, the best split is made at that particular step without consideration to subsequent splits.</span>
<span id="cb96-32"><a href="#cb96-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-33"><a href="#cb96-33" aria-hidden="true" tabindex="-1"></a>The best split is the predictor variable and cut point that minimizes its cost function. For regression trees, that's the sum of squared residuals. In @eq-06-rss, $A_k$ is node $k$ of $K$ nodes.</span>
<span id="cb96-34"><a href="#cb96-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-35"><a href="#cb96-35" aria-hidden="true" tabindex="-1"></a>$$RSS = \sum_{k=1}^K\sum_{i \in A_k}{\left(y_i - \hat{y}_{A_k} \right)^2}.$$ {#eq-06-rss}</span>
<span id="cb96-36"><a href="#cb96-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-37"><a href="#cb96-37" aria-hidden="true" tabindex="-1"></a>For classification trees, it's the Gini index,</span>
<span id="cb96-38"><a href="#cb96-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-39"><a href="#cb96-39" aria-hidden="true" tabindex="-1"></a>$$G = \sum_{c=1}^C{\hat{p}_{kc}(1 - \hat{p}_{kc})},$$ {#eq-06-gini}</span>
<span id="cb96-40"><a href="#cb96-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-41"><a href="#cb96-41" aria-hidden="true" tabindex="-1"></a>$\hat{p}_{kc}$ is the proportion of predictions in node $k$ that are class $c$. A completely *pure* node in a binary tree would have $\hat{p} \in \{ 0, 1 \}$ for a Gini index of $G = 0$. A completely *impure* node would have $\hat{p} = 0.5$ and $G = (0.5)(0.5) = 0.25$.<span class="ot">[^fn-06-entropy]</span></span>
<span id="cb96-42"><a href="#cb96-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-43"><a href="#cb96-43" aria-hidden="true" tabindex="-1"></a><span class="ot">[^fn-06-entropy]: </span>You might come across a variation of the Gini index called entropy, the *information statistic*, $D = - \sum_{c=1}^C{\hat{p}_{kc} \log \hat{p}_{kc}}$. Whereas Gini can range from 0 to .25, entropy ranges from 0 to $-(.5 \log(.5)) \cdot 2 = 0.69$.</span>
<span id="cb96-44"><a href="#cb96-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-45"><a href="#cb96-45" aria-hidden="true" tabindex="-1"></a>CART repeats the splitting process for each child node until a *stopping criterion* is satisfied, usually when no node size surpasses a predefined maximum, or continued splitting does not improve the model significantly. CART may also impose a minimum number of observations in each node.</span>
<span id="cb96-46"><a href="#cb96-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-47"><a href="#cb96-47" aria-hidden="true" tabindex="-1"></a>The resulting tree will over-fit the data and not generalize well, so CART *prunes* the tree, minimizing the cross-validated prediction error. Rather than cross-validating every possible subtree, CART uses *cost-complexity pruning*. Cost complexity is the tradeoff between error (cost) and tree size (complexity). The cost complexity of the tree, $R_{c_p}(T)$, is the sum of its risk (error) plus a cost complexity factor, $c_p$, multiple of the tree size $|T|$.</span>
<span id="cb96-48"><a href="#cb96-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-49"><a href="#cb96-49" aria-hidden="true" tabindex="-1"></a>$$R_{c_p}(T) = R(T) + c_p|T|$$ {#eq-06-cp}</span>
<span id="cb96-50"><a href="#cb96-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-51"><a href="#cb96-51" aria-hidden="true" tabindex="-1"></a>$c_p$ can take any value from $<span class="co">[</span><span class="ot">0..\infty</span><span class="co">]</span>$, but it turns out there is an optimal tree for ranges of $c_p$ values, so there is only a finite set of interesting values for $c_p$ <span class="co">[</span><span class="ot">@James2013; @Therneau2019; and @Kuhn2016</span><span class="co">]</span>. A parametric algorithm identifies the interesting $c_p$ values and their associated pruned trees, $T_{c_p}$.</span>
<span id="cb96-52"><a href="#cb96-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-53"><a href="#cb96-53" aria-hidden="true" tabindex="-1"></a><span class="fu">## Case Studies</span></span>
<span id="cb96-54"><a href="#cb96-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-55"><a href="#cb96-55" aria-hidden="true" tabindex="-1"></a>The sections in this chapter work through two case studies. The first fits classification trees to the <span class="in">`ISLR::OJ`</span> dataset to predict which of two brands of orange juice customers <span class="in">`Purchase`</span>. The second fits regression trees to the <span class="in">`ISLR::Carseats`</span> dataset to predict <span class="in">`Sales`</span>.</span>
<span id="cb96-56"><a href="#cb96-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-59"><a href="#cb96-59" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-60"><a href="#cb96-60" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb96-61"><a href="#cb96-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-62"><a href="#cb96-62" aria-hidden="true" tabindex="-1"></a>oj_dat <span class="ot">&lt;-</span> ISLR<span class="sc">::</span>OJ</span>
<span id="cb96-63"><a href="#cb96-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-64"><a href="#cb96-64" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(oj_dat)</span>
<span id="cb96-65"><a href="#cb96-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-66"><a href="#cb96-66" aria-hidden="true" tabindex="-1"></a>cs_dat <span class="ot">&lt;-</span> ISLR<span class="sc">::</span>Carseats</span>
<span id="cb96-67"><a href="#cb96-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-68"><a href="#cb96-68" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(cs_dat)</span>
<span id="cb96-69"><a href="#cb96-69" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-70"><a href="#cb96-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-71"><a href="#cb96-71" aria-hidden="true" tabindex="-1"></a>Partition the data into training and testing datasets. We'll fit models to the training data and compare their performance using testing data.</span>
<span id="cb96-72"><a href="#cb96-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-73"><a href="#cb96-73" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb96-74"><a href="#cb96-74" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: false</span></span>
<span id="cb96-75"><a href="#cb96-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-76"><a href="#cb96-76" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(12345)</span></span>
<span id="cb96-77"><a href="#cb96-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-78"><a href="#cb96-78" aria-hidden="true" tabindex="-1"></a><span class="in">(oj_split &lt;- initial_split(oj_dat, prop = 0.8, strata = Purchase))</span></span>
<span id="cb96-79"><a href="#cb96-79" aria-hidden="true" tabindex="-1"></a><span class="in">oj_train &lt;- training(oj_split)</span></span>
<span id="cb96-80"><a href="#cb96-80" aria-hidden="true" tabindex="-1"></a><span class="in">oj_test &lt;- testing(oj_split)</span></span>
<span id="cb96-81"><a href="#cb96-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-82"><a href="#cb96-82" aria-hidden="true" tabindex="-1"></a><span class="in">(cs_split &lt;- initial_split(cs_dat, prop = 0.8, strata = Sales))</span></span>
<span id="cb96-83"><a href="#cb96-83" aria-hidden="true" tabindex="-1"></a><span class="in">cs_train &lt;- training(cs_split)</span></span>
<span id="cb96-84"><a href="#cb96-84" aria-hidden="true" tabindex="-1"></a><span class="in">cs_test &lt;- testing(cs_split)</span></span>
<span id="cb96-85"><a href="#cb96-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-86"><a href="#cb96-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-87"><a href="#cb96-87" aria-hidden="true" tabindex="-1"></a><span class="fu">## Classification Tree</span></span>
<span id="cb96-88"><a href="#cb96-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-89"><a href="#cb96-89" aria-hidden="true" tabindex="-1"></a>Function <span class="in">`parsnip::decision_tree()`</span> defines a decision tree model. Its default engine is <span class="in">`rpart`</span> and has three hyperparameters.</span>
<span id="cb96-90"><a href="#cb96-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-91"><a href="#cb96-91" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`tree_depth`</span>: maximum number of layers. Larger depths result in more complex models, but they are also more prone to overfitting.</span>
<span id="cb96-92"><a href="#cb96-92" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`min_n`</span>: minimal node size required for splitting.</span>
<span id="cb96-93"><a href="#cb96-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`cost_complexity`</span>: a penalty for adding more nodes to the tree. This is another control regulating the tree complexity.</span>
<span id="cb96-94"><a href="#cb96-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-95"><a href="#cb96-95" aria-hidden="true" tabindex="-1"></a>The hyperparameters can either take assigned values, or you can tune them with <span class="in">`tune()`</span>. We'll predict <span class="in">`Purchase`</span> <span class="co">[</span><span class="ot">CH, MM</span><span class="co">]</span> as a function of all 17 predictors.</span>
<span id="cb96-96"><a href="#cb96-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-99"><a href="#cb96-99" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-100"><a href="#cb96-100" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb96-101"><a href="#cb96-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-102"><a href="#cb96-102" aria-hidden="true" tabindex="-1"></a>oj_cart_mdl <span class="ot">&lt;-</span></span>
<span id="cb96-103"><a href="#cb96-103" aria-hidden="true" tabindex="-1"></a>  <span class="fu">decision_tree</span>(</span>
<span id="cb96-104"><a href="#cb96-104" aria-hidden="true" tabindex="-1"></a>    <span class="at">cost_complexity =</span> <span class="fu">tune</span>(),</span>
<span id="cb96-105"><a href="#cb96-105" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree_depth =</span> <span class="fu">tune</span>(),</span>
<span id="cb96-106"><a href="#cb96-106" aria-hidden="true" tabindex="-1"></a>    <span class="at">min_n =</span> <span class="fu">tune</span>()</span>
<span id="cb96-107"><a href="#cb96-107" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb96-108"><a href="#cb96-108" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"rpart"</span>) <span class="sc">|&gt;</span></span>
<span id="cb96-109"><a href="#cb96-109" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">"classification"</span>)</span>
<span id="cb96-110"><a href="#cb96-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-111"><a href="#cb96-111" aria-hidden="true" tabindex="-1"></a>oj_cart_wf <span class="ot">&lt;-</span></span>
<span id="cb96-112"><a href="#cb96-112" aria-hidden="true" tabindex="-1"></a>  <span class="fu">workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb96-113"><a href="#cb96-113" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_model</span>(oj_cart_mdl) <span class="sc">|&gt;</span></span>
<span id="cb96-114"><a href="#cb96-114" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_formula</span>(Purchase <span class="sc">~</span> .)</span>
<span id="cb96-115"><a href="#cb96-115" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-116"><a href="#cb96-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-117"><a href="#cb96-117" aria-hidden="true" tabindex="-1"></a><span class="fu">### Fit the Model {.unnumbered}</span></span>
<span id="cb96-118"><a href="#cb96-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-119"><a href="#cb96-119" aria-hidden="true" tabindex="-1"></a>Tune the hyperparameters with 10-fold CV using a regular grid. With 3 hyperparameters and 5 levels, the grid has 5\^3=125 combinations. That means the tuning exercise will fit 125 models for each of the 10 folds - 1,250 fits! We'll evaluate the fits based on accuracy and AUC. The dataset is somewhat imbalanced (<span class="in">`r percent(mean(oj_dat$Purchase == "CH"), 1)`</span> "CH"), so AUC will be the more important metric. It takes about a minute and a half to fit the <span class="in">`r nrow(oj_train)`</span> x <span class="in">`r ncol(oj_train)`</span> dataset.</span>
<span id="cb96-120"><a href="#cb96-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-123"><a href="#cb96-123" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-124"><a href="#cb96-124" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fit-cart-class</span></span>
<span id="cb96-125"><a href="#cb96-125" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb96-126"><a href="#cb96-126" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb96-127"><a href="#cb96-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-128"><a href="#cb96-128" aria-hidden="true" tabindex="-1"></a><span class="fu">tic</span>()</span>
<span id="cb96-129"><a href="#cb96-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-130"><a href="#cb96-130" aria-hidden="true" tabindex="-1"></a>oj_cart_resamples <span class="ot">&lt;-</span></span>
<span id="cb96-131"><a href="#cb96-131" aria-hidden="true" tabindex="-1"></a>  oj_cart_wf <span class="sc">|&gt;</span></span>
<span id="cb96-132"><a href="#cb96-132" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tune_grid</span>(</span>
<span id="cb96-133"><a href="#cb96-133" aria-hidden="true" tabindex="-1"></a>    <span class="at">resamples =</span> <span class="fu">vfold_cv</span>(oj_train, <span class="at">v =</span> <span class="dv">10</span>), </span>
<span id="cb96-134"><a href="#cb96-134" aria-hidden="true" tabindex="-1"></a>    <span class="at">grid =</span> <span class="fu">grid_regular</span>(</span>
<span id="cb96-135"><a href="#cb96-135" aria-hidden="true" tabindex="-1"></a>      <span class="fu">cost_complexity</span>(), </span>
<span id="cb96-136"><a href="#cb96-136" aria-hidden="true" tabindex="-1"></a>      <span class="fu">tree_depth</span>(), </span>
<span id="cb96-137"><a href="#cb96-137" aria-hidden="true" tabindex="-1"></a>      <span class="fu">min_n</span>(), </span>
<span id="cb96-138"><a href="#cb96-138" aria-hidden="true" tabindex="-1"></a>      <span class="at">levels =</span> <span class="dv">5</span></span>
<span id="cb96-139"><a href="#cb96-139" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb96-140"><a href="#cb96-140" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">metric_set</span>(roc_auc, accuracy)</span>
<span id="cb96-141"><a href="#cb96-141" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb96-142"><a href="#cb96-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-143"><a href="#cb96-143" aria-hidden="true" tabindex="-1"></a><span class="fu">toc</span>()</span>
<span id="cb96-144"><a href="#cb96-144" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-145"><a href="#cb96-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-146"><a href="#cb96-146" aria-hidden="true" tabindex="-1"></a><span class="in">`tune_grid()`</span> returns a <span class="in">`resamples`</span> object with one row per fold. </span>
<span id="cb96-147"><a href="#cb96-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-150"><a href="#cb96-150" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-151"><a href="#cb96-151" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb96-152"><a href="#cb96-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-153"><a href="#cb96-153" aria-hidden="true" tabindex="-1"></a>oj_cart_resamples</span>
<span id="cb96-154"><a href="#cb96-154" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-155"><a href="#cb96-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-156"><a href="#cb96-156" aria-hidden="true" tabindex="-1"></a>Each row contains a metrics list with values for each hyperparameter combination and performance metric. There were 125 parameter combinations and two metrics, so 250 rows. Here's what the metrics object from the first fold look like.</span>
<span id="cb96-157"><a href="#cb96-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-160"><a href="#cb96-160" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-161"><a href="#cb96-161" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb96-162"><a href="#cb96-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-163"><a href="#cb96-163" aria-hidden="true" tabindex="-1"></a>oj_cart_resamples[<span class="dv">1</span>, ]<span class="sc">$</span>.metrics[[<span class="dv">1</span>]] <span class="sc">|&gt;</span> <span class="fu">glimpse</span>()</span>
<span id="cb96-164"><a href="#cb96-164" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-165"><a href="#cb96-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-166"><a href="#cb96-166" aria-hidden="true" tabindex="-1"></a><span class="in">`collect_metrics()`</span> unnests the <span class="in">`metrics`</span> column and averages the values over the folds. <span class="in">`show_best()`</span> shows the best model hyperparameter combination, but the mean AUC is the same for several combinations. The best performing tree is 8 layers deep.</span>
<span id="cb96-167"><a href="#cb96-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-170"><a href="#cb96-170" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-171"><a href="#cb96-171" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb96-172"><a href="#cb96-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-173"><a href="#cb96-173" aria-hidden="true" tabindex="-1"></a>oj_cart_resamples <span class="sc">|&gt;</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"roc_auc"</span>) <span class="sc">|&gt;</span> knitr<span class="sc">::</span><span class="fu">kable</span>()</span>
<span id="cb96-174"><a href="#cb96-174" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-175"><a href="#cb96-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-176"><a href="#cb96-176" aria-hidden="true" tabindex="-1"></a>Plot the sensitivity to the hyperparameter combinations to make sure you've explored a range that gives you a peek.</span>
<span id="cb96-177"><a href="#cb96-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-180"><a href="#cb96-180" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-181"><a href="#cb96-181" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb96-182"><a href="#cb96-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-183"><a href="#cb96-183" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">collect_metrics</span>(oj_cart_resamples)</span>
<span id="cb96-184"><a href="#cb96-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-185"><a href="#cb96-185" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb96-186"><a href="#cb96-186" aria-hidden="true" tabindex="-1"></a>  <span class="at">cost_complextity =</span> df <span class="sc">|&gt;</span> <span class="fu">rename</span>(<span class="at">x =</span> cost_complexity) <span class="sc">|&gt;</span></span>
<span id="cb96-187"><a href="#cb96-187" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">.by =</span> <span class="fu">c</span>(x, .metric), <span class="at">mean_auc =</span> <span class="fu">mean</span>(mean)),</span>
<span id="cb96-188"><a href="#cb96-188" aria-hidden="true" tabindex="-1"></a>  <span class="at">tree_depth =</span> df <span class="sc">|&gt;</span> <span class="fu">rename</span>(<span class="at">x =</span> tree_depth) <span class="sc">|&gt;</span></span>
<span id="cb96-189"><a href="#cb96-189" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">.by =</span> <span class="fu">c</span>(x, .metric), <span class="at">mean_auc =</span> <span class="fu">mean</span>(mean)),</span>
<span id="cb96-190"><a href="#cb96-190" aria-hidden="true" tabindex="-1"></a>  <span class="at">min_n =</span> df <span class="sc">|&gt;</span> <span class="fu">rename</span>(<span class="at">x =</span> min_n) <span class="sc">|&gt;</span></span>
<span id="cb96-191"><a href="#cb96-191" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">.by =</span> <span class="fu">c</span>(x, .metric), <span class="at">mean_auc =</span> <span class="fu">mean</span>(mean)),</span>
<span id="cb96-192"><a href="#cb96-192" aria-hidden="true" tabindex="-1"></a>  <span class="at">.id =</span> <span class="st">"hyperparameter"</span></span>
<span id="cb96-193"><a href="#cb96-193" aria-hidden="true" tabindex="-1"></a>) <span class="sc">|&gt;</span></span>
<span id="cb96-194"><a href="#cb96-194" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> mean_auc, <span class="at">color =</span> .metric)) <span class="sc">+</span></span>
<span id="cb96-195"><a href="#cb96-195" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> .<span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb96-196"><a href="#cb96-196" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="fu">vars</span>(hyperparameter), <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb96-197"><a href="#cb96-197" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="cn">NULL</span>, <span class="at">y =</span> <span class="st">"Mean AUC"</span>, <span class="at">color =</span> <span class="cn">NULL</span>, <span class="at">title =</span> <span class="st">"Hyperparameter Tuning"</span>) <span class="sc">+</span></span>
<span id="cb96-198"><a href="#cb96-198" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>)</span>
<span id="cb96-199"><a href="#cb96-199" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-200"><a href="#cb96-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-201"><a href="#cb96-201" aria-hidden="true" tabindex="-1"></a>Select the hyperparameter combination with the highest AUC and finalize the workflow. <span class="in">`last_fit()`</span> fits the model with the full training set and evaluates it on the testing data.</span>
<span id="cb96-202"><a href="#cb96-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-205"><a href="#cb96-205" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-206"><a href="#cb96-206" aria-hidden="true" tabindex="-1"></a>oj_cart_hyperparameters <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_cart_resamples, <span class="at">metric =</span> <span class="st">"roc_auc"</span>)</span>
<span id="cb96-207"><a href="#cb96-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-208"><a href="#cb96-208" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="ot">&lt;-</span> </span>
<span id="cb96-209"><a href="#cb96-209" aria-hidden="true" tabindex="-1"></a>  <span class="fu">finalize_workflow</span>(oj_cart_wf, oj_cart_hyperparameters) <span class="sc">|&gt;</span></span>
<span id="cb96-210"><a href="#cb96-210" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span>
<span id="cb96-211"><a href="#cb96-211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-212"><a href="#cb96-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-213"><a href="#cb96-213" aria-hidden="true" tabindex="-1"></a>Here is the tree. The output starts with the root node. Terminal nodes are labeled with an asterisk (<span class="sc">\*</span>), so this is just a summary node of everything beneath. Node 1 Contains all 855 observations with 333 misclassifications. The majority class is CH, with a proportion of 0.6105 for CH and 0.3895 for MM. Node 2 Splits on <span class="in">`LoyalCH`</span> &gt;= 0.48285, containing 534 observations with 92 misclassifications. The majority class is CH, with a higher proportion of CH. The child nodes of node "x" having labeling pattern 2x) and 2x+1), so for node 1) it's 2) and 3), and for 2) it's 4) and 5).</span>
<span id="cb96-214"><a href="#cb96-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-217"><a href="#cb96-217" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-218"><a href="#cb96-218" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb96-219"><a href="#cb96-219" aria-hidden="true" tabindex="-1"></a><span class="fu">extract_workflow</span>(oj_cart_fit)</span>
<span id="cb96-220"><a href="#cb96-220" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-221"><a href="#cb96-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-222"><a href="#cb96-222" aria-hidden="true" tabindex="-1"></a>A diagram of the tree can sometimes help if its not too large. The node label indicates the predicted value, error rate, and proportion of observations included. Below the nodes are the splitting criteria.</span>
<span id="cb96-223"><a href="#cb96-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-226"><a href="#cb96-226" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-227"><a href="#cb96-227" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">|&gt;</span></span>
<span id="cb96-228"><a href="#cb96-228" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb96-229"><a href="#cb96-229" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_fit_engine</span>() <span class="sc">|&gt;</span></span>
<span id="cb96-230"><a href="#cb96-230" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rpart.plot</span>(<span class="at">yesno =</span> <span class="cn">TRUE</span>, <span class="at">roundint =</span> <span class="cn">FALSE</span>)</span>
<span id="cb96-231"><a href="#cb96-231" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-232"><a href="#cb96-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-233"><a href="#cb96-233" aria-hidden="true" tabindex="-1"></a>A variable's importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. From the **rpart** <span class="co">[</span><span class="ot">vignette</span><span class="co">](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)</span> (page 12),</span>
<span id="cb96-234"><a href="#cb96-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-235"><a href="#cb96-235" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate."</span></span>
<span id="cb96-236"><a href="#cb96-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-237"><a href="#cb96-237" aria-hidden="true" tabindex="-1"></a><span class="in">`LoyalCH`</span> was the most important variable, followed by <span class="in">`PriceDiff`</span>.</span>
<span id="cb96-238"><a href="#cb96-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-241"><a href="#cb96-241" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-242"><a href="#cb96-242" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">|&gt;</span></span>
<span id="cb96-243"><a href="#cb96-243" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb96-244"><a href="#cb96-244" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_fit_parsnip</span>() <span class="sc">|&gt;</span></span>
<span id="cb96-245"><a href="#cb96-245" aria-hidden="true" tabindex="-1"></a>  vip<span class="sc">::</span><span class="fu">vip</span>()</span>
<span id="cb96-246"><a href="#cb96-246" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-247"><a href="#cb96-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-248"><a href="#cb96-248" aria-hidden="true" tabindex="-1"></a><span class="fu">### Performance {.unnumbered}</span></span>
<span id="cb96-249"><a href="#cb96-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-250"><a href="#cb96-250" aria-hidden="true" tabindex="-1"></a><span class="in">`collect_metrics()`</span> shows the performance metrics on the <span class="in">`r nrow(oj_test)`</span> testing dataset observations.</span>
<span id="cb96-251"><a href="#cb96-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-254"><a href="#cb96-254" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-255"><a href="#cb96-255" aria-hidden="true" tabindex="-1"></a><span class="fu">collect_metrics</span>(oj_cart_fit)</span>
<span id="cb96-256"><a href="#cb96-256" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-257"><a href="#cb96-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-258"><a href="#cb96-258" aria-hidden="true" tabindex="-1"></a>Use the full confusion matrix and ROC curve to explore performance.</span>
<span id="cb96-259"><a href="#cb96-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-260"><a href="#cb96-260" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Confusion Matrix {.unnumbered}</span></span>
<span id="cb96-261"><a href="#cb96-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-262"><a href="#cb96-262" aria-hidden="true" tabindex="-1"></a>The confusion matrix calculates the model performance on the testing dataset.</span>
<span id="cb96-263"><a href="#cb96-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-266"><a href="#cb96-266" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-267"><a href="#cb96-267" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb96-268"><a href="#cb96-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-269"><a href="#cb96-269" aria-hidden="true" tabindex="-1"></a>oj_cart_conf_mtrx <span class="ot">&lt;-</span></span>
<span id="cb96-270"><a href="#cb96-270" aria-hidden="true" tabindex="-1"></a>  oj_cart_fit <span class="sc">|&gt;</span></span>
<span id="cb96-271"><a href="#cb96-271" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">|&gt;</span></span>
<span id="cb96-272"><a href="#cb96-272" aria-hidden="true" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> Purchase, <span class="at">estimate =</span> .pred_class)</span>
<span id="cb96-273"><a href="#cb96-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-274"><a href="#cb96-274" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(oj_cart_conf_mtrx)</span>
<span id="cb96-275"><a href="#cb96-275" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-276"><a href="#cb96-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-277"><a href="#cb96-277" aria-hidden="true" tabindex="-1"></a>These measures come directly from the confusion matrix. The first value (CH) is considered a "positive" result and the second (MM) "negative". The metrics are usually expressed in terms of true and false positives and negatives.</span>
<span id="cb96-278"><a href="#cb96-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-281"><a href="#cb96-281" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-282"><a href="#cb96-282" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb96-283"><a href="#cb96-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-284"><a href="#cb96-284" aria-hidden="true" tabindex="-1"></a>oj_cart_conf_mtrx</span>
<span id="cb96-285"><a href="#cb96-285" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-286"><a href="#cb96-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-287"><a href="#cb96-287" aria-hidden="true" tabindex="-1"></a>| Metric | Desc   | Formula | Calculation |</span>
<span id="cb96-288"><a href="#cb96-288" aria-hidden="true" tabindex="-1"></a>|--------|:-------|:------- |:------------|</span>
<span id="cb96-289"><a href="#cb96-289" aria-hidden="true" tabindex="-1"></a>| **Accuracy** | Correct predictions percent. | $\frac{TP + FP}{total}$ | $\frac{117+67}{215}=.856$ |</span>
<span id="cb96-290"><a href="#cb96-290" aria-hidden="true" tabindex="-1"></a>| **Sensitivity**, &lt;br&gt;**Recall** | Proportion of positives identified. | $\frac{TP}{TP + FN}$ | $\frac{117}{117+114}=.893$ |</span>
<span id="cb96-291"><a href="#cb96-291" aria-hidden="true" tabindex="-1"></a>| **Specificity** | Proportion of negatives identified.   | $\frac{TN}{TN + FP}$ | $\frac{67}{67 + 17}=.798$ |</span>
<span id="cb96-292"><a href="#cb96-292" aria-hidden="true" tabindex="-1"></a>| **Positive Predictive Value**, &lt;br&gt;**Precision** | Proportion of positive predictions that are correct. | $\frac{TP}{TP + FP}$ | $\frac{117}{117 + 17}=.873$ |</span>
<span id="cb96-293"><a href="#cb96-293" aria-hidden="true" tabindex="-1"></a>| **Negative Predictive value** | Proportion of negative predictions that are correct.   | $\frac{TN}{TN + FN}$ | $\frac{67}{67 + 14}=.827 |</span>
<span id="cb96-294"><a href="#cb96-294" aria-hidden="true" tabindex="-1"></a>| **F1-Score** | Harmonic mean of precision and recall.   | $2 \times \frac{\text{prec} \times \text{recall}} {\text{prec} + \text{recall}}$| $2 \times \frac{0.873 \times 0.893} {0.873 + 0.893} = .883$ |</span>
<span id="cb96-295"><a href="#cb96-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-296"><a href="#cb96-296" aria-hidden="true" tabindex="-1"></a>You can bound the accuracy with a 95% CI using the binomial test.</span>
<span id="cb96-297"><a href="#cb96-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-300"><a href="#cb96-300" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-301"><a href="#cb96-301" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(</span>
<span id="cb96-302"><a href="#cb96-302" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> oj_cart_conf_mtrx<span class="sc">$</span>table <span class="sc">%&gt;%</span> <span class="fu">diag</span>() <span class="sc">%&gt;%</span> <span class="fu">sum</span>(), </span>
<span id="cb96-303"><a href="#cb96-303" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> oj_cart_conf_mtrx<span class="sc">$</span>table <span class="sc">%&gt;%</span> <span class="fu">sum</span>()</span>
<span id="cb96-304"><a href="#cb96-304" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb96-305"><a href="#cb96-305" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-306"><a href="#cb96-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-307"><a href="#cb96-307" aria-hidden="true" tabindex="-1"></a>The detection prevalence (aka, no information rate (NIR)) statistic is the class rate for the largest class. In this case CH is the largest class, so NIR = 133/213 = 0.6186. The binomial test for NIR is the probability of that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH).</span>
<span id="cb96-308"><a href="#cb96-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-311"><a href="#cb96-311" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-312"><a href="#cb96-312" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(</span>
<span id="cb96-313"><a href="#cb96-313" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> oj_cart_conf_mtrx<span class="sc">$</span>table[<span class="st">"CH"</span>, ] <span class="sc">%&gt;%</span> <span class="fu">sum</span>(), </span>
<span id="cb96-314"><a href="#cb96-314" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> oj_cart_conf_mtrx<span class="sc">$</span>table <span class="sc">%&gt;%</span> <span class="fu">sum</span>(),</span>
<span id="cb96-315"><a href="#cb96-315" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> <span class="fu">sum</span>(oj_cart_conf_mtrx<span class="sc">$</span>table[, <span class="st">"CH"</span>]) <span class="sc">/</span> <span class="fu">sum</span>(oj_cart_conf_mtrx<span class="sc">$</span>table),</span>
<span id="cb96-316"><a href="#cb96-316" aria-hidden="true" tabindex="-1"></a>  <span class="at">alternative =</span> <span class="st">"greater"</span></span>
<span id="cb96-317"><a href="#cb96-317" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb96-318"><a href="#cb96-318" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(<span class="at">x =</span> <span class="dv">116</span> <span class="sc">+</span> <span class="dv">67</span>, <span class="at">n =</span> <span class="fu">sum</span>(oj_cart_conf_mtrx<span class="sc">$</span>table), </span>
<span id="cb96-319"><a href="#cb96-319" aria-hidden="true" tabindex="-1"></a>           <span class="at">p =</span> (<span class="dv">116</span><span class="sc">+</span><span class="dv">17</span>)<span class="sc">/</span><span class="fu">sum</span>(oj_cart_conf_mtrx<span class="sc">$</span>table), </span>
<span id="cb96-320"><a href="#cb96-320" aria-hidden="true" tabindex="-1"></a>           <span class="at">alternative =</span> <span class="st">"greater"</span>)</span>
<span id="cb96-321"><a href="#cb96-321" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-322"><a href="#cb96-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-323"><a href="#cb96-323" aria-hidden="true" tabindex="-1"></a>The accuracy statistic indicates the model predicts <span class="in">`r summary(oj_cart_conf_mtrx) %&gt;% filter(.metric == "accuracy") %&gt;% pull(.estimate) %&gt;% percent(.1)`</span> of the observations correctly. That's good, but less impressive when you consider the prevalence of CH is <span class="in">`r (sum(oj_cart_conf_mtrx$table[1, ]) / sum(oj_cart_conf_mtrx$table)) %&gt;% percent(.1)`</span> - you could achieve that accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen's kappa statistic. The kappa statistic is explained <span class="co">[</span><span class="ot">here</span><span class="co">](https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/)</span>. It compares the accuracy to the accuracy of a "random system". It is defined as</span>
<span id="cb96-324"><a href="#cb96-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-325"><a href="#cb96-325" aria-hidden="true" tabindex="-1"></a>$$\kappa = \frac{\text{Accuracy} - RA}{1-RA}$$</span>
<span id="cb96-326"><a href="#cb96-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-327"><a href="#cb96-327" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb96-328"><a href="#cb96-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-329"><a href="#cb96-329" aria-hidden="true" tabindex="-1"></a>$$RA = \frac{\text{ActFalse} \times \text{PredFalse} + \text{ActTrue} \times \text{PredTrue}}{\text{Total} \times \text{Total}}$$</span>
<span id="cb96-330"><a href="#cb96-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-331"><a href="#cb96-331" aria-hidden="true" tabindex="-1"></a>is the hypothetical probability of a chance agreement.</span>
<span id="cb96-332"><a href="#cb96-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-333"><a href="#cb96-333" aria-hidden="true" tabindex="-1"></a>The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations. In this case, a kappa statistic of 0.7064 is "substantial". See chart <span class="co">[</span><span class="ot">here</span><span class="co">](https://www.statisticshowto.datasciencecentral.com/cohens-kappa-statistic/)</span>.</span>
<span id="cb96-334"><a href="#cb96-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-335"><a href="#cb96-335" aria-hidden="true" tabindex="-1"></a>You can remind yourself what the other confusion matrix measures are from the documentation. Visuals are almost always helpful. Here is a plot of the confusion matrix.</span>
<span id="cb96-336"><a href="#cb96-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-339"><a href="#cb96-339" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-340"><a href="#cb96-340" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">%&gt;%</span> </span>
<span id="cb96-341"><a href="#cb96-341" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb96-342"><a href="#cb96-342" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Purchase, .pred_class) <span class="sc">%&gt;%</span></span>
<span id="cb96-343"><a href="#cb96-343" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(</span>
<span id="cb96-344"><a href="#cb96-344" aria-hidden="true" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">"Simple Classification: Predicted vs. Actual"</span>,</span>
<span id="cb96-345"><a href="#cb96-345" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="st">"Actual"</span>,</span>
<span id="cb96-346"><a href="#cb96-346" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">"Predicted"</span></span>
<span id="cb96-347"><a href="#cb96-347" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb96-348"><a href="#cb96-348" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-349"><a href="#cb96-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-350"><a href="#cb96-350" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ROC Curve {.unnumbered}</span></span>
<span id="cb96-351"><a href="#cb96-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-352"><a href="#cb96-352" aria-hidden="true" tabindex="-1"></a>The ROC (receiver operating characteristics) curve <span class="co">[</span><span class="ot">@Fawcett2005</span><span class="co">]</span> is another measure of accuracy. The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold. <span class="in">`precrec::evalmod()`</span> calculates the confusion matrix values from the model using the holdout data set. The AUC on the holdout set is <span class="in">`r oj_cart_fit %&gt;% collect_predictions() %&gt;% yardstick::roc_auc(Purchase, .pred_CH) %&gt;% pull(.estimate) %&gt;% comma(.001)`</span>. <span class="in">`pRoc::plot.roc()`</span>, <span class="in">`plotROC::geom_roc()`</span>, and <span class="in">`yardstick::roc_curve()`</span> are options for plotting a ROC curve.</span>
<span id="cb96-353"><a href="#cb96-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-356"><a href="#cb96-356" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-357"><a href="#cb96-357" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">%&gt;%</span></span>
<span id="cb96-358"><a href="#cb96-358" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-359"><a href="#cb96-359" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb96-360"><a href="#cb96-360" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb96-361"><a href="#cb96-361" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ CART ROC Curve"</span>)</span>
<span id="cb96-362"><a href="#cb96-362" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-363"><a href="#cb96-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-364"><a href="#cb96-364" aria-hidden="true" tabindex="-1"></a>A few points on the ROC space are helpful for understanding how to use it.</span>
<span id="cb96-365"><a href="#cb96-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-366"><a href="#cb96-366" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The lower left point (0, 0) is the result of *always* predicting "negative" or in this case "MM" if "CH" is taken as the default class. No false positives, but no true positives either.</span>
<span id="cb96-367"><a href="#cb96-367" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The upper right point (1, 1) is the result of *always* predicting "positive" ("CH" here). You catch all true positives, but miss all the true negatives.</span>
<span id="cb96-368"><a href="#cb96-368" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The upper left point (0, 1) is the result of perfect accuracy.</span>
<span id="cb96-369"><a href="#cb96-369" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The lower right point (1, 0) is the result of perfect imbecility. You made the exact wrong prediction every time.</span>
<span id="cb96-370"><a href="#cb96-370" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time. If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc.</span>
<span id="cb96-371"><a href="#cb96-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-372"><a href="#cb96-372" aria-hidden="true" tabindex="-1"></a>The goal is for all nodes to bunch up in the upper left.</span>
<span id="cb96-373"><a href="#cb96-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-374"><a href="#cb96-374" aria-hidden="true" tabindex="-1"></a>Points to the left of the diagonal with a low TPR can be thought of as "conservative" predictors - they only make positive (CH) predictions with strong evidence. Points to the left of the diagonal with a high TPR can be thought of as "liberal" predictors - they make positive (CH) predictions with weak evidence.</span>
<span id="cb96-375"><a href="#cb96-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-376"><a href="#cb96-376" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Gain Curve</span></span>
<span id="cb96-377"><a href="#cb96-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-378"><a href="#cb96-378" aria-hidden="true" tabindex="-1"></a>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The gray area is the “gain” over a random prediction.</span>
<span id="cb96-379"><a href="#cb96-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-380"><a href="#cb96-380" aria-hidden="true" tabindex="-1"></a><span class="in">```{r include=FALSE}</span></span>
<span id="cb96-381"><a href="#cb96-381" aria-hidden="true" tabindex="-1"></a><span class="in">x &lt;- oj_cart_fit %&gt;% collect_predictions() %&gt;% yardstick::gain_curve(Purchase, .pred_CH)</span></span>
<span id="cb96-382"><a href="#cb96-382" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-383"><a href="#cb96-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-384"><a href="#cb96-384" aria-hidden="true" tabindex="-1"></a><span class="in">`r sum(oj_test$Purchase == "CH")`</span> of the <span class="in">`r nrow(oj_test)`</span> consumers in the holdout testing set purchased CH.</span>
<span id="cb96-385"><a href="#cb96-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-388"><a href="#cb96-388" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-389"><a href="#cb96-389" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">%&gt;%</span></span>
<span id="cb96-390"><a href="#cb96-390" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-391"><a href="#cb96-391" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH)</span>
<span id="cb96-392"><a href="#cb96-392" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-393"><a href="#cb96-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-394"><a href="#cb96-394" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The gain curve encountered <span class="in">`r x[2, ]$.n_events`</span> CH purchasers (<span class="in">`r x[2, ]$.percent_found %&gt;% comma(.1)`</span>%) within the first <span class="in">`r x[2, ]$.n`</span> observations (<span class="in">`r x[2, ]$.percent_tested %&gt;% comma(.1)`</span>%).</span>
<span id="cb96-395"><a href="#cb96-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-396"><a href="#cb96-396" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>It encountered all <span class="in">`r sum(oj_test$Purchase == "CH")`</span> CH purchasers on the <span class="in">`r x[8, ]$.n`</span>th observation (100%).</span>
<span id="cb96-397"><a href="#cb96-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-398"><a href="#cb96-398" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The bottom of the gray area is the outcome of a random model. Only half the CH purchasers would be observed within 50% of the observations. The top of the gray area is the outcome of the perfect model, the “wizard curve”. Half the CH purchasers would be observed in \~30% of the observations.</span>
<span id="cb96-399"><a href="#cb96-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-402"><a href="#cb96-402" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-403"><a href="#cb96-403" aria-hidden="true" tabindex="-1"></a>oj_cart_fit <span class="sc">%&gt;%</span></span>
<span id="cb96-404"><a href="#cb96-404" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-405"><a href="#cb96-405" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb96-406"><a href="#cb96-406" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb96-407"><a href="#cb96-407" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ CART Gain Curve"</span>)</span>
<span id="cb96-408"><a href="#cb96-408" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-409"><a href="#cb96-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-410"><a href="#cb96-410" aria-hidden="true" tabindex="-1"></a><span class="fu">## Regression Tree</span></span>
<span id="cb96-411"><a href="#cb96-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-412"><a href="#cb96-412" aria-hidden="true" tabindex="-1"></a>A simple regression tree is built in a manner similar to a simple classification tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic.</span>
<span id="cb96-413"><a href="#cb96-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-414"><a href="#cb96-414" aria-hidden="true" tabindex="-1"></a>The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp). The only difference here is the <span class="in">`set_Mode("regression")`</span> call to produce a regression tree.</span>
<span id="cb96-415"><a href="#cb96-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-416"><a href="#cb96-416" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fit-cart-reg, cache=TRUE}</span></span>
<span id="cb96-417"><a href="#cb96-417" aria-hidden="true" tabindex="-1"></a><span class="in">cs_cart &lt;- list()</span></span>
<span id="cb96-418"><a href="#cb96-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-419"><a href="#cb96-419" aria-hidden="true" tabindex="-1"></a><span class="in"># `decision_tree` has 3 hyperparameters (`cost_complexity`, `tree_depth`, and</span></span>
<span id="cb96-420"><a href="#cb96-420" aria-hidden="true" tabindex="-1"></a><span class="in"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb96-421"><a href="#cb96-421" aria-hidden="true" tabindex="-1"></a><span class="in"># optimize just `cost_complexity` and `tree_depth`.</span></span>
<span id="cb96-422"><a href="#cb96-422" aria-hidden="true" tabindex="-1"></a><span class="in">cs_cart$model &lt;-</span></span>
<span id="cb96-423"><a href="#cb96-423" aria-hidden="true" tabindex="-1"></a><span class="in">  decision_tree(</span></span>
<span id="cb96-424"><a href="#cb96-424" aria-hidden="true" tabindex="-1"></a><span class="in">    cost_complexity = tune(),</span></span>
<span id="cb96-425"><a href="#cb96-425" aria-hidden="true" tabindex="-1"></a><span class="in">    tree_depth = tune()</span></span>
<span id="cb96-426"><a href="#cb96-426" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb96-427"><a href="#cb96-427" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb96-428"><a href="#cb96-428" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb96-429"><a href="#cb96-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-430"><a href="#cb96-430" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune a model using the workflow framework.</span></span>
<span id="cb96-431"><a href="#cb96-431" aria-hidden="true" tabindex="-1"></a><span class="in">cs_cart$workflow &lt;-</span></span>
<span id="cb96-432"><a href="#cb96-432" aria-hidden="true" tabindex="-1"></a><span class="in">  workflow() %&gt;%</span></span>
<span id="cb96-433"><a href="#cb96-433" aria-hidden="true" tabindex="-1"></a><span class="in">  add_model(cs_cart$model) %&gt;%</span></span>
<span id="cb96-434"><a href="#cb96-434" aria-hidden="true" tabindex="-1"></a><span class="in">  add_formula(Sales ~ .)</span></span>
<span id="cb96-435"><a href="#cb96-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-436"><a href="#cb96-436" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb96-437"><a href="#cb96-437" aria-hidden="true" tabindex="-1"></a><span class="in"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb96-438"><a href="#cb96-438" aria-hidden="true" tabindex="-1"></a><span class="in"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb96-439"><a href="#cb96-439" aria-hidden="true" tabindex="-1"></a><span class="in">cs_cart$tune_grid &lt;-</span></span>
<span id="cb96-440"><a href="#cb96-440" aria-hidden="true" tabindex="-1"></a><span class="in">  cs_cart$workflow %&gt;%</span></span>
<span id="cb96-441"><a href="#cb96-441" aria-hidden="true" tabindex="-1"></a><span class="in">  tune_grid(</span></span>
<span id="cb96-442"><a href="#cb96-442" aria-hidden="true" tabindex="-1"></a><span class="in">    resamples = vfold_cv(cs_train, v = 10), </span></span>
<span id="cb96-443"><a href="#cb96-443" aria-hidden="true" tabindex="-1"></a><span class="in">    grid = grid_regular(cost_complexity(), tree_depth(), levels = 5)</span></span>
<span id="cb96-444"><a href="#cb96-444" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb96-445"><a href="#cb96-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-446"><a href="#cb96-446" aria-hidden="true" tabindex="-1"></a><span class="in"># `collect_metrics()` returns two metrics: rmse and rsq.</span></span>
<span id="cb96-447"><a href="#cb96-447" aria-hidden="true" tabindex="-1"></a><span class="in">cs_cart$tune_grid %&gt;% </span></span>
<span id="cb96-448"><a href="#cb96-448" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_metrics() %&gt;%</span></span>
<span id="cb96-449"><a href="#cb96-449" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(tree_depth = factor(tree_depth)) %&gt;%</span></span>
<span id="cb96-450"><a href="#cb96-450" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(aes(x = cost_complexity, y = mean, color = tree_depth)) +</span></span>
<span id="cb96-451"><a href="#cb96-451" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line(linewidth = 1.5, alpha = .6) +</span></span>
<span id="cb96-452"><a href="#cb96-452" aria-hidden="true" tabindex="-1"></a><span class="in">  facet_wrap(facets = vars(.metric), scales = "free") +</span></span>
<span id="cb96-453"><a href="#cb96-453" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_x_log10()</span></span>
<span id="cb96-454"><a href="#cb96-454" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-455"><a href="#cb96-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-456"><a href="#cb96-456" aria-hidden="true" tabindex="-1"></a>The best models in terms of RMSE was the tree depth of 8 and any cp <span class="sc">\&lt;</span> 5.6E-04.</span>
<span id="cb96-457"><a href="#cb96-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-460"><a href="#cb96-460" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-461"><a href="#cb96-461" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb96-462"><a href="#cb96-462" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-463"><a href="#cb96-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-464"><a href="#cb96-464" aria-hidden="true" tabindex="-1"></a>Select the best model in terms of rmse and finalize the model.</span>
<span id="cb96-465"><a href="#cb96-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-468"><a href="#cb96-468" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-469"><a href="#cb96-469" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_cart<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb96-470"><a href="#cb96-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-471"><a href="#cb96-471" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb96-472"><a href="#cb96-472" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_cart<span class="sc">$</span>workflow, cs_cart<span class="sc">$</span>best_tune)</span>
<span id="cb96-473"><a href="#cb96-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-474"><a href="#cb96-474" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb96-475"><a href="#cb96-475" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb96-476"><a href="#cb96-476" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb96-477"><a href="#cb96-477" aria-hidden="true" tabindex="-1"></a>  cs_cart<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb96-478"><a href="#cb96-478" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span>
<span id="cb96-479"><a href="#cb96-479" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-480"><a href="#cb96-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-481"><a href="#cb96-481" aria-hidden="true" tabindex="-1"></a>Here is the tree. The output starts with the root node. The predicted sales at the root is the mean sales in the testing data set is <span class="in">`r cs_cart$fit %&gt;% extract_workflow() %&gt;% pluck("fit", "fit", "fit", "frame") %&gt;% filter(var == "ShelveLoc") %&gt;% head(1) %&gt;% pull(yval) %&gt;% comma(.01)`</span> (values are \$000s). The deviance at the root is the SSE, <span class="in">`r cs_cart$fit %&gt;% extract_workflow() %&gt;% pluck("fit", "fit", "fit", "frame") %&gt;% filter(var == "ShelveLoc") %&gt;% head(1) %&gt;% pull(dev) %&gt;% comma(1)`</span>. The first split is at <span class="in">`ShelveLoc`</span> = <span class="sc">\[</span>Bad, Medium<span class="sc">\]</span> vs Good.</span>
<span id="cb96-482"><a href="#cb96-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-485"><a href="#cb96-485" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-486"><a href="#cb96-486" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">extract_workflow</span>()</span>
<span id="cb96-487"><a href="#cb96-487" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-488"><a href="#cb96-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-489"><a href="#cb96-489" aria-hidden="true" tabindex="-1"></a>Here is a diagram of the tree. The node label indicates the predicted value (mean) and the proportion of observations that are in the node (or child nodes). Below the nodes are the splitting criteria.</span>
<span id="cb96-490"><a href="#cb96-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-493"><a href="#cb96-493" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-494"><a href="#cb96-494" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb96-495"><a href="#cb96-495" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-496"><a href="#cb96-496" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_fit_engine</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-497"><a href="#cb96-497" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rpart.plot</span>(<span class="at">yesno =</span> <span class="cn">TRUE</span>, <span class="at">roundint =</span> <span class="cn">FALSE</span>)</span>
<span id="cb96-498"><a href="#cb96-498" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-499"><a href="#cb96-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-500"><a href="#cb96-500" aria-hidden="true" tabindex="-1"></a><span class="in">`Price`</span> and <span class="in">`ShelveLoc`</span> were the most important variables.</span>
<span id="cb96-501"><a href="#cb96-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-504"><a href="#cb96-504" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-505"><a href="#cb96-505" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb96-506"><a href="#cb96-506" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_workflow</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-507"><a href="#cb96-507" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extract_fit_parsnip</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-508"><a href="#cb96-508" aria-hidden="true" tabindex="-1"></a>  vip<span class="sc">::</span><span class="fu">vip</span>()</span>
<span id="cb96-509"><a href="#cb96-509" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-510"><a href="#cb96-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-511"><a href="#cb96-511" aria-hidden="true" tabindex="-1"></a><span class="fu">### Measuring Performance {.unnumbered}</span></span>
<span id="cb96-512"><a href="#cb96-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-513"><a href="#cb96-513" aria-hidden="true" tabindex="-1"></a><span class="in">`collect_metrics()`</span> returns the RMSE, $RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})$ and the model $R^2$. The RMSE of <span class="in">`r cs_cart$fit %&gt;% collect_metrics() %&gt;% filter(.metric == "rmse") %&gt;% pull(.estimate) %&gt;% comma(.01)`</span> in the test data set is pretty good considering the standard deviation of <span class="in">`Sales`</span> is <span class="in">`r comma(sd(cs_test$Sales), .01)`</span>.</span>
<span id="cb96-514"><a href="#cb96-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-517"><a href="#cb96-517" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-518"><a href="#cb96-518" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb96-519"><a href="#cb96-519" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-520"><a href="#cb96-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-521"><a href="#cb96-521" aria-hidden="true" tabindex="-1"></a>Here is a predicted vs actual plot.</span>
<span id="cb96-522"><a href="#cb96-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-525"><a href="#cb96-525" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-526"><a href="#cb96-526" aria-hidden="true" tabindex="-1"></a>cs_cart<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb96-527"><a href="#cb96-527" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-528"><a href="#cb96-528" aria-hidden="true" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb96-529"><a href="#cb96-529" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">"cadetblue"</span>) <span class="sc">+</span></span>
<span id="cb96-530"><a href="#cb96-530" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"loess"</span>, <span class="at">formula =</span> <span class="st">"y~x"</span>) <span class="sc">+</span></span>
<span id="cb96-531"><a href="#cb96-531" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb96-532"><a href="#cb96-532" aria-hidden="true" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Carseats CART, Predicted vs Actual"</span>)</span>
<span id="cb96-533"><a href="#cb96-533" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-534"><a href="#cb96-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-535"><a href="#cb96-535" aria-hidden="true" tabindex="-1"></a>The tree nodes do a decent job of binning the observations. The predictions vs actuals plot suggests the model over-estimates at the low end and underestimates at the high end. Calculate the test data set RMSE.</span>
<span id="cb96-536"><a href="#cb96-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-537"><a href="#cb96-537" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bagged Trees</span></span>
<span id="cb96-538"><a href="#cb96-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-539"><a href="#cb96-539" aria-hidden="true" tabindex="-1"></a>One drawback of decision trees is that they are high-variance estimators. A small number of additional training observations can dramatically alter the prediction performance of a learned tree.</span>
<span id="cb96-540"><a href="#cb96-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-541"><a href="#cb96-541" aria-hidden="true" tabindex="-1"></a>Bootstrap aggregation, or *bagging*, is a general-purpose procedure for reducing the variance of a statistical learning method. The algorithm constructs *B* regression trees using *B* bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging the *B* trees reduces the variance. The predicted value for an observation is the mode (classification) or mean (regression) of the trees. *B* usually equals \~25.</span>
<span id="cb96-542"><a href="#cb96-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-543"><a href="#cb96-543" aria-hidden="true" tabindex="-1"></a>To test the model accuracy, the out-of-bag observations are predicted from the models. For a training set of size *n*, each tree is composed of $\sim (1 - e^{-1})n = .632n$ unique observations in-bag and $.368n$ out-of-bag. For each tree in the ensemble, bagging makes predictions on the tree's out-of-bag observations. I think (*see* page 197 of [@Kuhn2016]) bagging measures the performance (RMSE, Accuracy, ROC, etc.) of each tree in the ensemble and averages them to produce an overall performance estimate. (This makes no sense to me. If each tree has poor performance, then the average performance of many trees will still be poor. An ensemble of *B* trees will produce $\sim .368 B$ predictions per unique observation. Seems like you should take the mean/mode of each observation's prediction as the final prediction. Then you have *n* predictions to compare to *n* actuals, and you assess performance on that.)</span>
<span id="cb96-544"><a href="#cb96-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-545"><a href="#cb96-545" aria-hidden="true" tabindex="-1"></a>The downside to bagging is that there is no single tree with a set of rules to interpret. It becomes unclear which variables are more important than others.</span>
<span id="cb96-546"><a href="#cb96-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-547"><a href="#cb96-547" aria-hidden="true" tabindex="-1"></a>The next section explains how bagged trees are a special case of random forests.</span>
<span id="cb96-548"><a href="#cb96-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-549"><a href="#cb96-549" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bagged Classification Tree</span></span>
<span id="cb96-550"><a href="#cb96-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-551"><a href="#cb96-551" aria-hidden="true" tabindex="-1"></a>Leaning by example, I'll predict <span class="in">`Purchase`</span> from the <span class="in">`OJ`</span> data set again, this time using the bagging with <span class="in">`parsnip::bag_tree()`</span>.</span>
<span id="cb96-552"><a href="#cb96-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-553"><a href="#cb96-553" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fit-bag-class, cache=TRUE}</span></span>
<span id="cb96-554"><a href="#cb96-554" aria-hidden="true" tabindex="-1"></a><span class="in">oj_bag &lt;- list()</span></span>
<span id="cb96-555"><a href="#cb96-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-556"><a href="#cb96-556" aria-hidden="true" tabindex="-1"></a><span class="in"># `bag_tree` has 4 hyperparameters (`cost_complexity`, `tree_depth`, and</span></span>
<span id="cb96-557"><a href="#cb96-557" aria-hidden="true" tabindex="-1"></a><span class="in"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb96-558"><a href="#cb96-558" aria-hidden="true" tabindex="-1"></a><span class="in"># optimize just `cost_complexity` and `tree_depth`.</span></span>
<span id="cb96-559"><a href="#cb96-559" aria-hidden="true" tabindex="-1"></a><span class="in">oj_bag$model &lt;-</span></span>
<span id="cb96-560"><a href="#cb96-560" aria-hidden="true" tabindex="-1"></a><span class="in">  bag_tree(</span></span>
<span id="cb96-561"><a href="#cb96-561" aria-hidden="true" tabindex="-1"></a><span class="in">    cost_complexity = tune(),</span></span>
<span id="cb96-562"><a href="#cb96-562" aria-hidden="true" tabindex="-1"></a><span class="in">    tree_depth = tune()</span></span>
<span id="cb96-563"><a href="#cb96-563" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb96-564"><a href="#cb96-564" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb96-565"><a href="#cb96-565" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("classification")</span></span>
<span id="cb96-566"><a href="#cb96-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-567"><a href="#cb96-567" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune a model using the workflow framework.</span></span>
<span id="cb96-568"><a href="#cb96-568" aria-hidden="true" tabindex="-1"></a><span class="in">oj_bag$workflow &lt;-</span></span>
<span id="cb96-569"><a href="#cb96-569" aria-hidden="true" tabindex="-1"></a><span class="in">  workflow() %&gt;%</span></span>
<span id="cb96-570"><a href="#cb96-570" aria-hidden="true" tabindex="-1"></a><span class="in">  add_model(oj_bag$model) %&gt;%</span></span>
<span id="cb96-571"><a href="#cb96-571" aria-hidden="true" tabindex="-1"></a><span class="in">  add_formula(Purchase ~ .)</span></span>
<span id="cb96-572"><a href="#cb96-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-573"><a href="#cb96-573" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb96-574"><a href="#cb96-574" aria-hidden="true" tabindex="-1"></a><span class="in"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb96-575"><a href="#cb96-575" aria-hidden="true" tabindex="-1"></a><span class="in"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb96-576"><a href="#cb96-576" aria-hidden="true" tabindex="-1"></a><span class="in">oj_bag$tune_grid &lt;-</span></span>
<span id="cb96-577"><a href="#cb96-577" aria-hidden="true" tabindex="-1"></a><span class="in">  oj_bag$workflow %&gt;%</span></span>
<span id="cb96-578"><a href="#cb96-578" aria-hidden="true" tabindex="-1"></a><span class="in">  tune_grid(</span></span>
<span id="cb96-579"><a href="#cb96-579" aria-hidden="true" tabindex="-1"></a><span class="in">    resamples = vfold_cv(oj_train, v = 10), </span></span>
<span id="cb96-580"><a href="#cb96-580" aria-hidden="true" tabindex="-1"></a><span class="in">    grid = grid_regular(cost_complexity(), tree_depth(), levels = 5)</span></span>
<span id="cb96-581"><a href="#cb96-581" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb96-582"><a href="#cb96-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-583"><a href="#cb96-583" aria-hidden="true" tabindex="-1"></a><span class="in"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb96-584"><a href="#cb96-584" aria-hidden="true" tabindex="-1"></a><span class="in">oj_bag$tune_grid %&gt;% </span></span>
<span id="cb96-585"><a href="#cb96-585" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_metrics() %&gt;%</span></span>
<span id="cb96-586"><a href="#cb96-586" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(tree_depth = factor(tree_depth)) %&gt;%</span></span>
<span id="cb96-587"><a href="#cb96-587" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(aes(x = cost_complexity, y = mean, color = tree_depth)) +</span></span>
<span id="cb96-588"><a href="#cb96-588" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line(linewidth = 1.5, alpha = .6) +</span></span>
<span id="cb96-589"><a href="#cb96-589" aria-hidden="true" tabindex="-1"></a><span class="in">  facet_wrap(facets = vars(.metric), scales = "free") +</span></span>
<span id="cb96-590"><a href="#cb96-590" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_x_log10()</span></span>
<span id="cb96-591"><a href="#cb96-591" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-592"><a href="#cb96-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-593"><a href="#cb96-593" aria-hidden="true" tabindex="-1"></a>The best models in terms of accuracy and ROC was the tree depth of 4 and any cp <span class="sc">\&lt;</span>= 3.16e-06.</span>
<span id="cb96-594"><a href="#cb96-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-597"><a href="#cb96-597" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-598"><a href="#cb96-598" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"accuracy"</span>)</span>
<span id="cb96-599"><a href="#cb96-599" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-600"><a href="#cb96-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-601"><a href="#cb96-601" aria-hidden="true" tabindex="-1"></a>Select the best model in terms of accuracy and finalize the model.</span>
<span id="cb96-602"><a href="#cb96-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-605"><a href="#cb96-605" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-606"><a href="#cb96-606" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_bag<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"accuracy"</span>)</span>
<span id="cb96-607"><a href="#cb96-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-608"><a href="#cb96-608" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb96-609"><a href="#cb96-609" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(oj_bag<span class="sc">$</span>workflow, oj_bag<span class="sc">$</span>best_tune)</span>
<span id="cb96-610"><a href="#cb96-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-611"><a href="#cb96-611" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb96-612"><a href="#cb96-612" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb96-613"><a href="#cb96-613" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb96-614"><a href="#cb96-614" aria-hidden="true" tabindex="-1"></a>  oj_bag<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb96-615"><a href="#cb96-615" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span>
<span id="cb96-616"><a href="#cb96-616" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-617"><a href="#cb96-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-618"><a href="#cb96-618" aria-hidden="true" tabindex="-1"></a>I *think* tidymodels started by splitting the training set into 10 folds, then using 9 of the folds to run the bagging algorithm and collect performance measures on the hold-out fold. After repeating the process for all 10 folds, it averaged the performance measures to produce the resampling results shown below. With hyperparameters, the process is repeated for all combinations and the resampling results above are from the best performing combination.</span>
<span id="cb96-619"><a href="#cb96-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-620"><a href="#cb96-620" aria-hidden="true" tabindex="-1"></a>There is no single tree to visualize, and you can't even produce a VIP. Let's look at the performance on the holdout data set. <span class="in">`collect_metrics()`</span> shows the accuracy and ROC AUC metrics. The accuracy is slightly lower than the single tree, but ROC AUC is higher.</span>
<span id="cb96-621"><a href="#cb96-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-624"><a href="#cb96-624" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-625"><a href="#cb96-625" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb96-626"><a href="#cb96-626" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-627"><a href="#cb96-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-628"><a href="#cb96-628" aria-hidden="true" tabindex="-1"></a>You can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.</span>
<span id="cb96-629"><a href="#cb96-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-630"><a href="#cb96-630" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb96-631"><a href="#cb96-631" aria-hidden="true" tabindex="-1"></a><span class="in">oj_bag$confmat &lt;-</span></span>
<span id="cb96-632"><a href="#cb96-632" aria-hidden="true" tabindex="-1"></a><span class="in">  oj_bag$fit %&gt;% </span></span>
<span id="cb96-633"><a href="#cb96-633" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_predictions() %&gt;% </span></span>
<span id="cb96-634"><a href="#cb96-634" aria-hidden="true" tabindex="-1"></a><span class="in">  conf_mat(truth = Purchase, estimate = .pred_class)</span></span>
<span id="cb96-635"><a href="#cb96-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-636"><a href="#cb96-636" aria-hidden="true" tabindex="-1"></a><span class="in">oj_bag$confmat</span></span>
<span id="cb96-637"><a href="#cb96-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-638"><a href="#cb96-638" aria-hidden="true" tabindex="-1"></a><span class="in">summary(oj_bag$confmat)</span></span>
<span id="cb96-639"><a href="#cb96-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-640"><a href="#cb96-640" aria-hidden="true" tabindex="-1"></a><span class="in">oj_bag$fit %&gt;% </span></span>
<span id="cb96-641"><a href="#cb96-641" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_predictions() %&gt;% </span></span>
<span id="cb96-642"><a href="#cb96-642" aria-hidden="true" tabindex="-1"></a><span class="in">  select(Purchase, .pred_class) %&gt;%</span></span>
<span id="cb96-643"><a href="#cb96-643" aria-hidden="true" tabindex="-1"></a><span class="in">  plot(</span></span>
<span id="cb96-644"><a href="#cb96-644" aria-hidden="true" tabindex="-1"></a><span class="in">    main = "Bagged Trees: Predicted vs. Actual",</span></span>
<span id="cb96-645"><a href="#cb96-645" aria-hidden="true" tabindex="-1"></a><span class="in">    xlab = "Actual",</span></span>
<span id="cb96-646"><a href="#cb96-646" aria-hidden="true" tabindex="-1"></a><span class="in">    ylab = "Predicted"</span></span>
<span id="cb96-647"><a href="#cb96-647" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb96-648"><a href="#cb96-648" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-649"><a href="#cb96-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-650"><a href="#cb96-650" aria-hidden="true" tabindex="-1"></a>The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is <span class="in">`r oj_bag$fit %&gt;% collect_predictions() %&gt;% yardstick::roc_auc(Purchase, .pred_CH) %&gt;% pull(.estimate) %&gt;% comma(.001)`</span>.</span>
<span id="cb96-651"><a href="#cb96-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-654"><a href="#cb96-654" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-655"><a href="#cb96-655" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb96-656"><a href="#cb96-656" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-657"><a href="#cb96-657" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb96-658"><a href="#cb96-658" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb96-659"><a href="#cb96-659" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ Bagged Trees ROC Curve"</span>)</span>
<span id="cb96-660"><a href="#cb96-660" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-661"><a href="#cb96-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-662"><a href="#cb96-662" aria-hidden="true" tabindex="-1"></a>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.</span>
<span id="cb96-663"><a href="#cb96-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-666"><a href="#cb96-666" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-667"><a href="#cb96-667" aria-hidden="true" tabindex="-1"></a>oj_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb96-668"><a href="#cb96-668" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-669"><a href="#cb96-669" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb96-670"><a href="#cb96-670" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb96-671"><a href="#cb96-671" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ Bagged Trees Gain Curve"</span>)</span>
<span id="cb96-672"><a href="#cb96-672" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-673"><a href="#cb96-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-674"><a href="#cb96-674" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bagging Regression Tree</span></span>
<span id="cb96-675"><a href="#cb96-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-676"><a href="#cb96-676" aria-hidden="true" tabindex="-1"></a>I'll predict <span class="in">`Sales`</span> from the <span class="in">`Carseats`</span> data set again, this time with <span class="in">`parsnip::bag_tree()`</span>.</span>
<span id="cb96-677"><a href="#cb96-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-678"><a href="#cb96-678" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fit-bag-reg, cache=TRUE}</span></span>
<span id="cb96-679"><a href="#cb96-679" aria-hidden="true" tabindex="-1"></a><span class="in">library(baguette)</span></span>
<span id="cb96-680"><a href="#cb96-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-681"><a href="#cb96-681" aria-hidden="true" tabindex="-1"></a><span class="in">cs_bag &lt;- list()</span></span>
<span id="cb96-682"><a href="#cb96-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-683"><a href="#cb96-683" aria-hidden="true" tabindex="-1"></a><span class="in"># `bag_tree` has 4 hyperparameters (`cost_complexity`, `tree_depth`, and</span></span>
<span id="cb96-684"><a href="#cb96-684" aria-hidden="true" tabindex="-1"></a><span class="in"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb96-685"><a href="#cb96-685" aria-hidden="true" tabindex="-1"></a><span class="in"># optimize just `cost_complexity` and `tree_depth`.</span></span>
<span id="cb96-686"><a href="#cb96-686" aria-hidden="true" tabindex="-1"></a><span class="in">cs_bag$model &lt;-</span></span>
<span id="cb96-687"><a href="#cb96-687" aria-hidden="true" tabindex="-1"></a><span class="in">  bag_tree(</span></span>
<span id="cb96-688"><a href="#cb96-688" aria-hidden="true" tabindex="-1"></a><span class="in">    cost_complexity = tune(),</span></span>
<span id="cb96-689"><a href="#cb96-689" aria-hidden="true" tabindex="-1"></a><span class="in">    tree_depth = tune()</span></span>
<span id="cb96-690"><a href="#cb96-690" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb96-691"><a href="#cb96-691" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("rpart") %&gt;%</span></span>
<span id="cb96-692"><a href="#cb96-692" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb96-693"><a href="#cb96-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-694"><a href="#cb96-694" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune a model using the workflow framework.</span></span>
<span id="cb96-695"><a href="#cb96-695" aria-hidden="true" tabindex="-1"></a><span class="in">cs_bag$workflow &lt;-</span></span>
<span id="cb96-696"><a href="#cb96-696" aria-hidden="true" tabindex="-1"></a><span class="in">  workflow() %&gt;%</span></span>
<span id="cb96-697"><a href="#cb96-697" aria-hidden="true" tabindex="-1"></a><span class="in">  add_model(cs_bag$model) %&gt;%</span></span>
<span id="cb96-698"><a href="#cb96-698" aria-hidden="true" tabindex="-1"></a><span class="in">  add_formula(Sales ~ .)</span></span>
<span id="cb96-699"><a href="#cb96-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-700"><a href="#cb96-700" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb96-701"><a href="#cb96-701" aria-hidden="true" tabindex="-1"></a><span class="in"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb96-702"><a href="#cb96-702" aria-hidden="true" tabindex="-1"></a><span class="in"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb96-703"><a href="#cb96-703" aria-hidden="true" tabindex="-1"></a><span class="in">cs_bag$tune_grid &lt;-</span></span>
<span id="cb96-704"><a href="#cb96-704" aria-hidden="true" tabindex="-1"></a><span class="in">  cs_bag$workflow %&gt;%</span></span>
<span id="cb96-705"><a href="#cb96-705" aria-hidden="true" tabindex="-1"></a><span class="in">  tune_grid(</span></span>
<span id="cb96-706"><a href="#cb96-706" aria-hidden="true" tabindex="-1"></a><span class="in">    resamples = vfold_cv(cs_train, v = 10), </span></span>
<span id="cb96-707"><a href="#cb96-707" aria-hidden="true" tabindex="-1"></a><span class="in">    grid = grid_regular(cost_complexity(), tree_depth(), levels = 5)</span></span>
<span id="cb96-708"><a href="#cb96-708" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb96-709"><a href="#cb96-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-710"><a href="#cb96-710" aria-hidden="true" tabindex="-1"></a><span class="in"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb96-711"><a href="#cb96-711" aria-hidden="true" tabindex="-1"></a><span class="in">cs_bag$tune_grid %&gt;% </span></span>
<span id="cb96-712"><a href="#cb96-712" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_metrics() %&gt;%</span></span>
<span id="cb96-713"><a href="#cb96-713" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(tree_depth = factor(tree_depth)) %&gt;%</span></span>
<span id="cb96-714"><a href="#cb96-714" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(aes(x = cost_complexity, y = mean, color = tree_depth)) +</span></span>
<span id="cb96-715"><a href="#cb96-715" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line(linewidth = 1.5, alpha = .6) +</span></span>
<span id="cb96-716"><a href="#cb96-716" aria-hidden="true" tabindex="-1"></a><span class="in">  facet_wrap(facets = vars(.metric), scales = "free") +</span></span>
<span id="cb96-717"><a href="#cb96-717" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_x_log10()</span></span>
<span id="cb96-718"><a href="#cb96-718" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-719"><a href="#cb96-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-720"><a href="#cb96-720" aria-hidden="true" tabindex="-1"></a>The best models in terms of RMSE was the tree depth of 8 and any cp <span class="sc">\&lt;</span> 5.6E-04.</span>
<span id="cb96-721"><a href="#cb96-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-724"><a href="#cb96-724" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-725"><a href="#cb96-725" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb96-726"><a href="#cb96-726" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-727"><a href="#cb96-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-728"><a href="#cb96-728" aria-hidden="true" tabindex="-1"></a>Select the best model in terms of rmse and finalize the model.</span>
<span id="cb96-729"><a href="#cb96-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-732"><a href="#cb96-732" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-733"><a href="#cb96-733" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_bag<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb96-734"><a href="#cb96-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-735"><a href="#cb96-735" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb96-736"><a href="#cb96-736" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_bag<span class="sc">$</span>workflow, cs_bag<span class="sc">$</span>best_tune)</span>
<span id="cb96-737"><a href="#cb96-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-738"><a href="#cb96-738" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb96-739"><a href="#cb96-739" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb96-740"><a href="#cb96-740" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb96-741"><a href="#cb96-741" aria-hidden="true" tabindex="-1"></a>  cs_bag<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb96-742"><a href="#cb96-742" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span>
<span id="cb96-743"><a href="#cb96-743" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-744"><a href="#cb96-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-745"><a href="#cb96-745" aria-hidden="true" tabindex="-1"></a><span class="in">`collect_metrics()`</span> returns the RMSE, $RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})$ and the model $R^2$. The RMSE of <span class="in">`cs_bag$fit %&gt;% collect_metrics() %&gt;% filter(.metric == "rmse") %&gt;% pull(.estimate) %&gt;% comma(.01)`</span> in the test data set is pretty good considering the standard deviation of <span class="in">`Sales`</span> is <span class="in">`comma(sd(cs_test$Sales), .01)`</span>.</span>
<span id="cb96-746"><a href="#cb96-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-749"><a href="#cb96-749" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-750"><a href="#cb96-750" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb96-751"><a href="#cb96-751" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-752"><a href="#cb96-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-753"><a href="#cb96-753" aria-hidden="true" tabindex="-1"></a>Here is a predicted vs actual plot.</span>
<span id="cb96-754"><a href="#cb96-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-757"><a href="#cb96-757" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-758"><a href="#cb96-758" aria-hidden="true" tabindex="-1"></a>cs_bag<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb96-759"><a href="#cb96-759" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-760"><a href="#cb96-760" aria-hidden="true" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb96-761"><a href="#cb96-761" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">"cadetblue"</span>) <span class="sc">+</span></span>
<span id="cb96-762"><a href="#cb96-762" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"loess"</span>, <span class="at">formula =</span> <span class="st">"y~x"</span>) <span class="sc">+</span></span>
<span id="cb96-763"><a href="#cb96-763" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb96-764"><a href="#cb96-764" aria-hidden="true" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Carseats Bagged Trees, Predicted vs Actual"</span>)</span>
<span id="cb96-765"><a href="#cb96-765" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-766"><a href="#cb96-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-767"><a href="#cb96-767" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Forests</span></span>
<span id="cb96-768"><a href="#cb96-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-769"><a href="#cb96-769" aria-hidden="true" tabindex="-1"></a>Random forests improve bagged trees by way of a small tweak that de-correlates the trees. As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of *mtry* predictors is chosen as split candidates from the full set of *p* predictors. A fresh sample of *mtry* predictors is taken at each split. Typically $mtry \sim \sqrt{p}$. Bagged trees are thus a special case of random forests where *mtry = p*.</span>
<span id="cb96-770"><a href="#cb96-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-771"><a href="#cb96-771" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Random Forest Classification Tree</span></span>
<span id="cb96-772"><a href="#cb96-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-773"><a href="#cb96-773" aria-hidden="true" tabindex="-1"></a>Leaning by example, I'll predict <span class="in">`Purchase`</span> from the <span class="in">`OJ`</span> data set again, this time using the bagging with <span class="in">`parsnip::rand_forest()`</span>.</span>
<span id="cb96-774"><a href="#cb96-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-775"><a href="#cb96-775" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fit-rf-class, cache=TRUE}</span></span>
<span id="cb96-776"><a href="#cb96-776" aria-hidden="true" tabindex="-1"></a><span class="in">oj_rf &lt;- list()</span></span>
<span id="cb96-777"><a href="#cb96-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-778"><a href="#cb96-778" aria-hidden="true" tabindex="-1"></a><span class="in"># `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and</span></span>
<span id="cb96-779"><a href="#cb96-779" aria-hidden="true" tabindex="-1"></a><span class="in"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb96-780"><a href="#cb96-780" aria-hidden="true" tabindex="-1"></a><span class="in"># optimize just `trees` and `min_n`.</span></span>
<span id="cb96-781"><a href="#cb96-781" aria-hidden="true" tabindex="-1"></a><span class="in">oj_rf$model &lt;-</span></span>
<span id="cb96-782"><a href="#cb96-782" aria-hidden="true" tabindex="-1"></a><span class="in">  rand_forest(</span></span>
<span id="cb96-783"><a href="#cb96-783" aria-hidden="true" tabindex="-1"></a><span class="in">    trees = tune(),</span></span>
<span id="cb96-784"><a href="#cb96-784" aria-hidden="true" tabindex="-1"></a><span class="in">    min_n = tune()</span></span>
<span id="cb96-785"><a href="#cb96-785" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb96-786"><a href="#cb96-786" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("ranger") %&gt;%</span></span>
<span id="cb96-787"><a href="#cb96-787" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("classification")</span></span>
<span id="cb96-788"><a href="#cb96-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-789"><a href="#cb96-789" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune a model using the workflow framework.</span></span>
<span id="cb96-790"><a href="#cb96-790" aria-hidden="true" tabindex="-1"></a><span class="in">oj_rf$workflow &lt;-</span></span>
<span id="cb96-791"><a href="#cb96-791" aria-hidden="true" tabindex="-1"></a><span class="in">  workflow() %&gt;%</span></span>
<span id="cb96-792"><a href="#cb96-792" aria-hidden="true" tabindex="-1"></a><span class="in">  add_model(oj_rf$model) %&gt;%</span></span>
<span id="cb96-793"><a href="#cb96-793" aria-hidden="true" tabindex="-1"></a><span class="in">  add_formula(Purchase ~ .)</span></span>
<span id="cb96-794"><a href="#cb96-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-795"><a href="#cb96-795" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb96-796"><a href="#cb96-796" aria-hidden="true" tabindex="-1"></a><span class="in"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb96-797"><a href="#cb96-797" aria-hidden="true" tabindex="-1"></a><span class="in"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb96-798"><a href="#cb96-798" aria-hidden="true" tabindex="-1"></a><span class="in">oj_rf$tune_grid &lt;-</span></span>
<span id="cb96-799"><a href="#cb96-799" aria-hidden="true" tabindex="-1"></a><span class="in">  oj_rf$workflow %&gt;%</span></span>
<span id="cb96-800"><a href="#cb96-800" aria-hidden="true" tabindex="-1"></a><span class="in">  tune_grid(</span></span>
<span id="cb96-801"><a href="#cb96-801" aria-hidden="true" tabindex="-1"></a><span class="in">    resamples = vfold_cv(oj_train, v = 10), </span></span>
<span id="cb96-802"><a href="#cb96-802" aria-hidden="true" tabindex="-1"></a><span class="in">    grid = grid_regular(trees(), min_n(), levels = 5)</span></span>
<span id="cb96-803"><a href="#cb96-803" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb96-804"><a href="#cb96-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-805"><a href="#cb96-805" aria-hidden="true" tabindex="-1"></a><span class="in"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb96-806"><a href="#cb96-806" aria-hidden="true" tabindex="-1"></a><span class="in">oj_rf$tune_grid %&gt;% </span></span>
<span id="cb96-807"><a href="#cb96-807" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_metrics() %&gt;%</span></span>
<span id="cb96-808"><a href="#cb96-808" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(trees = factor(trees)) %&gt;%</span></span>
<span id="cb96-809"><a href="#cb96-809" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(aes(x = min_n, y = mean, color = trees)) +</span></span>
<span id="cb96-810"><a href="#cb96-810" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line(linewidth = 1.5, alpha = .6) +</span></span>
<span id="cb96-811"><a href="#cb96-811" aria-hidden="true" tabindex="-1"></a><span class="in">  facet_wrap(facets = vars(.metric), scales = "free") +</span></span>
<span id="cb96-812"><a href="#cb96-812" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_x_log10()</span></span>
<span id="cb96-813"><a href="#cb96-813" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-814"><a href="#cb96-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-815"><a href="#cb96-815" aria-hidden="true" tabindex="-1"></a>The best models in terms of accuracy and ROC was the trees of 1000 and any minimum node size of 40.</span>
<span id="cb96-816"><a href="#cb96-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-819"><a href="#cb96-819" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-820"><a href="#cb96-820" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"accuracy"</span>)</span>
<span id="cb96-821"><a href="#cb96-821" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-822"><a href="#cb96-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-823"><a href="#cb96-823" aria-hidden="true" tabindex="-1"></a>Select the best model in terms of accuracy and finalize the model.</span>
<span id="cb96-824"><a href="#cb96-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-827"><a href="#cb96-827" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-828"><a href="#cb96-828" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_rf<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"accuracy"</span>)</span>
<span id="cb96-829"><a href="#cb96-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-830"><a href="#cb96-830" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb96-831"><a href="#cb96-831" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(oj_rf<span class="sc">$</span>workflow, oj_rf<span class="sc">$</span>best_tune)</span>
<span id="cb96-832"><a href="#cb96-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-833"><a href="#cb96-833" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb96-834"><a href="#cb96-834" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb96-835"><a href="#cb96-835" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb96-836"><a href="#cb96-836" aria-hidden="true" tabindex="-1"></a>  oj_rf<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb96-837"><a href="#cb96-837" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span>
<span id="cb96-838"><a href="#cb96-838" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-839"><a href="#cb96-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-840"><a href="#cb96-840" aria-hidden="true" tabindex="-1"></a>There is no single tree to visualize, and you can't even produce a VIP. Let's look at the performance on the holdout data set. <span class="in">`collect_metrics()`</span> shows the accuracy and ROC AUC metrics. The accuracy is slightly lower than the single tree, but ROC AUC is higher than both the single tree and bagged trees.</span>
<span id="cb96-841"><a href="#cb96-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-844"><a href="#cb96-844" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-845"><a href="#cb96-845" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb96-846"><a href="#cb96-846" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-847"><a href="#cb96-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-848"><a href="#cb96-848" aria-hidden="true" tabindex="-1"></a>You can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.</span>
<span id="cb96-849"><a href="#cb96-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-850"><a href="#cb96-850" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb96-851"><a href="#cb96-851" aria-hidden="true" tabindex="-1"></a><span class="in">oj_rf$confmat &lt;-</span></span>
<span id="cb96-852"><a href="#cb96-852" aria-hidden="true" tabindex="-1"></a><span class="in">  oj_rf$fit %&gt;% </span></span>
<span id="cb96-853"><a href="#cb96-853" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_predictions() %&gt;% </span></span>
<span id="cb96-854"><a href="#cb96-854" aria-hidden="true" tabindex="-1"></a><span class="in">  conf_mat(truth = Purchase, estimate = .pred_class)</span></span>
<span id="cb96-855"><a href="#cb96-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-856"><a href="#cb96-856" aria-hidden="true" tabindex="-1"></a><span class="in">oj_rf$confmat</span></span>
<span id="cb96-857"><a href="#cb96-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-858"><a href="#cb96-858" aria-hidden="true" tabindex="-1"></a><span class="in">summary(oj_rf$confmat)</span></span>
<span id="cb96-859"><a href="#cb96-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-860"><a href="#cb96-860" aria-hidden="true" tabindex="-1"></a><span class="in">oj_rf$fit %&gt;% </span></span>
<span id="cb96-861"><a href="#cb96-861" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_predictions() %&gt;% </span></span>
<span id="cb96-862"><a href="#cb96-862" aria-hidden="true" tabindex="-1"></a><span class="in">  select(Purchase, .pred_class) %&gt;%</span></span>
<span id="cb96-863"><a href="#cb96-863" aria-hidden="true" tabindex="-1"></a><span class="in">  plot(</span></span>
<span id="cb96-864"><a href="#cb96-864" aria-hidden="true" tabindex="-1"></a><span class="in">    main = "Random Forest: Predicted vs. Actual",</span></span>
<span id="cb96-865"><a href="#cb96-865" aria-hidden="true" tabindex="-1"></a><span class="in">    xlab = "Actual",</span></span>
<span id="cb96-866"><a href="#cb96-866" aria-hidden="true" tabindex="-1"></a><span class="in">    ylab = "Predicted"</span></span>
<span id="cb96-867"><a href="#cb96-867" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb96-868"><a href="#cb96-868" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-869"><a href="#cb96-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-870"><a href="#cb96-870" aria-hidden="true" tabindex="-1"></a>The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is <span class="in">`r oj_rf$fit %&gt;% collect_predictions() %&gt;% yardstick::roc_auc(Purchase, .pred_CH) %&gt;% pull(.estimate) %&gt;% comma(.001)`</span>.</span>
<span id="cb96-871"><a href="#cb96-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-874"><a href="#cb96-874" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-875"><a href="#cb96-875" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb96-876"><a href="#cb96-876" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-877"><a href="#cb96-877" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb96-878"><a href="#cb96-878" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb96-879"><a href="#cb96-879" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ Random Forest ROC Curve"</span>)</span>
<span id="cb96-880"><a href="#cb96-880" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-881"><a href="#cb96-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-882"><a href="#cb96-882" aria-hidden="true" tabindex="-1"></a>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.</span>
<span id="cb96-883"><a href="#cb96-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-886"><a href="#cb96-886" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-887"><a href="#cb96-887" aria-hidden="true" tabindex="-1"></a>oj_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb96-888"><a href="#cb96-888" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-889"><a href="#cb96-889" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb96-890"><a href="#cb96-890" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb96-891"><a href="#cb96-891" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ Random Forest Gain Curve"</span>)</span>
<span id="cb96-892"><a href="#cb96-892" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-893"><a href="#cb96-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-894"><a href="#cb96-894" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Random Forest Regression Tree</span></span>
<span id="cb96-895"><a href="#cb96-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-896"><a href="#cb96-896" aria-hidden="true" tabindex="-1"></a>I'll predict <span class="in">`Sales`</span> from the <span class="in">`Carseats`</span> data set again, this time with <span class="in">`parsnip::rand_forest()`</span>.</span>
<span id="cb96-897"><a href="#cb96-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-898"><a href="#cb96-898" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fit-rf-reg, cache=TRUE}</span></span>
<span id="cb96-899"><a href="#cb96-899" aria-hidden="true" tabindex="-1"></a><span class="in">cs_rf &lt;- list()</span></span>
<span id="cb96-900"><a href="#cb96-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-901"><a href="#cb96-901" aria-hidden="true" tabindex="-1"></a><span class="in"># `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and</span></span>
<span id="cb96-902"><a href="#cb96-902" aria-hidden="true" tabindex="-1"></a><span class="in"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb96-903"><a href="#cb96-903" aria-hidden="true" tabindex="-1"></a><span class="in"># optimize just `trees` and `min_n`.</span></span>
<span id="cb96-904"><a href="#cb96-904" aria-hidden="true" tabindex="-1"></a><span class="in">cs_rf$model &lt;-</span></span>
<span id="cb96-905"><a href="#cb96-905" aria-hidden="true" tabindex="-1"></a><span class="in">  rand_forest(</span></span>
<span id="cb96-906"><a href="#cb96-906" aria-hidden="true" tabindex="-1"></a><span class="in">    trees = tune(),</span></span>
<span id="cb96-907"><a href="#cb96-907" aria-hidden="true" tabindex="-1"></a><span class="in">    min_n = tune()</span></span>
<span id="cb96-908"><a href="#cb96-908" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb96-909"><a href="#cb96-909" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("ranger") %&gt;%</span></span>
<span id="cb96-910"><a href="#cb96-910" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb96-911"><a href="#cb96-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-912"><a href="#cb96-912" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune a model using the workflow framework.</span></span>
<span id="cb96-913"><a href="#cb96-913" aria-hidden="true" tabindex="-1"></a><span class="in">cs_rf$workflow &lt;-</span></span>
<span id="cb96-914"><a href="#cb96-914" aria-hidden="true" tabindex="-1"></a><span class="in">  workflow() %&gt;%</span></span>
<span id="cb96-915"><a href="#cb96-915" aria-hidden="true" tabindex="-1"></a><span class="in">  add_model(cs_rf$model) %&gt;%</span></span>
<span id="cb96-916"><a href="#cb96-916" aria-hidden="true" tabindex="-1"></a><span class="in">  add_formula(Sales ~ .)</span></span>
<span id="cb96-917"><a href="#cb96-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-918"><a href="#cb96-918" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb96-919"><a href="#cb96-919" aria-hidden="true" tabindex="-1"></a><span class="in"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb96-920"><a href="#cb96-920" aria-hidden="true" tabindex="-1"></a><span class="in"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb96-921"><a href="#cb96-921" aria-hidden="true" tabindex="-1"></a><span class="in">cs_rf$tune_grid &lt;-</span></span>
<span id="cb96-922"><a href="#cb96-922" aria-hidden="true" tabindex="-1"></a><span class="in">  cs_rf$workflow %&gt;%</span></span>
<span id="cb96-923"><a href="#cb96-923" aria-hidden="true" tabindex="-1"></a><span class="in">  tune_grid(</span></span>
<span id="cb96-924"><a href="#cb96-924" aria-hidden="true" tabindex="-1"></a><span class="in">    resamples = vfold_cv(cs_train, v = 10), </span></span>
<span id="cb96-925"><a href="#cb96-925" aria-hidden="true" tabindex="-1"></a><span class="in">    grid = grid_regular(trees(), min_n(), levels = 5)</span></span>
<span id="cb96-926"><a href="#cb96-926" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb96-927"><a href="#cb96-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-928"><a href="#cb96-928" aria-hidden="true" tabindex="-1"></a><span class="in"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb96-929"><a href="#cb96-929" aria-hidden="true" tabindex="-1"></a><span class="in">cs_rf$tune_grid %&gt;% </span></span>
<span id="cb96-930"><a href="#cb96-930" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_metrics() %&gt;%</span></span>
<span id="cb96-931"><a href="#cb96-931" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(trees = factor(trees)) %&gt;%</span></span>
<span id="cb96-932"><a href="#cb96-932" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(aes(x = min_n, y = mean, color = trees)) +</span></span>
<span id="cb96-933"><a href="#cb96-933" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line(linewidth = 1.5, alpha = .6) +</span></span>
<span id="cb96-934"><a href="#cb96-934" aria-hidden="true" tabindex="-1"></a><span class="in">  facet_wrap(facets = vars(.metric), scales = "free") +</span></span>
<span id="cb96-935"><a href="#cb96-935" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_x_log10()</span></span>
<span id="cb96-936"><a href="#cb96-936" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-937"><a href="#cb96-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-938"><a href="#cb96-938" aria-hidden="true" tabindex="-1"></a>The best models in terms of RMSE was 2000 tree with at least 2 data points per node.</span>
<span id="cb96-939"><a href="#cb96-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-942"><a href="#cb96-942" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-943"><a href="#cb96-943" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb96-944"><a href="#cb96-944" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-945"><a href="#cb96-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-946"><a href="#cb96-946" aria-hidden="true" tabindex="-1"></a>Select the best model in terms of rmse and finalize the model.</span>
<span id="cb96-947"><a href="#cb96-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-950"><a href="#cb96-950" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-951"><a href="#cb96-951" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_rf<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb96-952"><a href="#cb96-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-953"><a href="#cb96-953" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb96-954"><a href="#cb96-954" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_rf<span class="sc">$</span>workflow, cs_rf<span class="sc">$</span>best_tune)</span>
<span id="cb96-955"><a href="#cb96-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-956"><a href="#cb96-956" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb96-957"><a href="#cb96-957" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb96-958"><a href="#cb96-958" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb96-959"><a href="#cb96-959" aria-hidden="true" tabindex="-1"></a>  cs_rf<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb96-960"><a href="#cb96-960" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span>
<span id="cb96-961"><a href="#cb96-961" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-962"><a href="#cb96-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-963"><a href="#cb96-963" aria-hidden="true" tabindex="-1"></a><span class="in">`collect_metrics()`</span> returns the RMSE, $RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})$ and the model $R^2$. The RMSE of <span class="in">`r cs_rf$fit %&gt;% collect_metrics() %&gt;% filter(.metric == "rmse") %&gt;% pull(.estimate) %&gt;% comma(.01)`</span> in the test data set is pretty good considering the standard deviation of <span class="in">`Sales`</span> is <span class="in">`r comma(sd(cs_test$Sales), .01)`</span>.</span>
<span id="cb96-964"><a href="#cb96-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-967"><a href="#cb96-967" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-968"><a href="#cb96-968" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb96-969"><a href="#cb96-969" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-970"><a href="#cb96-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-971"><a href="#cb96-971" aria-hidden="true" tabindex="-1"></a>Here is a predicted vs actual plot.</span>
<span id="cb96-972"><a href="#cb96-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-975"><a href="#cb96-975" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-976"><a href="#cb96-976" aria-hidden="true" tabindex="-1"></a>cs_rf<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb96-977"><a href="#cb96-977" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-978"><a href="#cb96-978" aria-hidden="true" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb96-979"><a href="#cb96-979" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">"cadetblue"</span>) <span class="sc">+</span></span>
<span id="cb96-980"><a href="#cb96-980" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"loess"</span>, <span class="at">formula =</span> <span class="st">"y~x"</span>) <span class="sc">+</span></span>
<span id="cb96-981"><a href="#cb96-981" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb96-982"><a href="#cb96-982" aria-hidden="true" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Carseats Random Forest, Predicted vs Actual"</span>)</span>
<span id="cb96-983"><a href="#cb96-983" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-984"><a href="#cb96-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-985"><a href="#cb96-985" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gradient Boosting</span></span>
<span id="cb96-986"><a href="#cb96-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-987"><a href="#cb96-987" aria-hidden="true" tabindex="-1"></a>**Note**: I learned gradient boosting from <span class="co">[</span><span class="ot">explained.ai</span><span class="co">](https://explained.ai/gradient-boosting/L2-loss.html)</span>.</span>
<span id="cb96-988"><a href="#cb96-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-989"><a href="#cb96-989" aria-hidden="true" tabindex="-1"></a>Gradient boosting machine (GBM) is an additive modeling algorithm that gradually builds a composite model by iteratively adding *M* weak sub-models based on the performance of the prior iteration's composite,</span>
<span id="cb96-990"><a href="#cb96-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-991"><a href="#cb96-991" aria-hidden="true" tabindex="-1"></a>$$F_M(x) = \sum_m^M f_m(x).$$</span>
<span id="cb96-992"><a href="#cb96-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-993"><a href="#cb96-993" aria-hidden="true" tabindex="-1"></a>The idea is to fit a weak model, then replace the response values with the residuals from that model, and fit another model. Adding the residual prediction model to the original response prediction model produces a more accurate model. GBM repeats this process over and over, running new models to predict the residuals of the previous composite models, and adding the results to produce new composites. With each iteration, the model becomes stronger and stronger. The successive trees are usually weighted to slow down the learning rate. "Shrinkage" reduces the influence of each individual tree and leaves space for future trees to improve the model.</span>
<span id="cb96-994"><a href="#cb96-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-995"><a href="#cb96-995" aria-hidden="true" tabindex="-1"></a>$$F_M(x) = f_0 + \eta\sum_{m = 1}^M f_m(x).$$</span>
<span id="cb96-996"><a href="#cb96-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-997"><a href="#cb96-997" aria-hidden="true" tabindex="-1"></a>The smaller the learning rate, $\eta$, the larger the number of trees, $M$. $\eta$ and $M$ are hyperparameters. Other constraints to the trees are usually applied as additional hyperparameters, including, tree depth, number of nodes, minimum observations per split, and minimum improvement to loss.</span>
<span id="cb96-998"><a href="#cb96-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-999"><a href="#cb96-999" aria-hidden="true" tabindex="-1"></a>The name "gradient boosting" refers to the *boosting* of a model with a *gradient*. Each round of training builds a *weak learner* and uses the residuals to calculate a gradient, the partial derivative of the loss function. Gradient boosting "descends the gradient" to adjust the model parameters to reduce the error in the next round of training.</span>
<span id="cb96-1000"><a href="#cb96-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1001"><a href="#cb96-1001" aria-hidden="true" tabindex="-1"></a>In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error. GBM continues until it reaches maximum number of trees or an acceptable error level.</span>
<span id="cb96-1002"><a href="#cb96-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1003"><a href="#cb96-1003" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Gradient Boosting Classification Tree</span></span>
<span id="cb96-1004"><a href="#cb96-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1005"><a href="#cb96-1005" aria-hidden="true" tabindex="-1"></a>Leaning by example, I'll predict <span class="in">`Purchase`</span> from the <span class="in">`OJ`</span> data set again, this time using the bagging with <span class="in">`parsnip::boost_tree()`</span></span>
<span id="cb96-1006"><a href="#cb96-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1007"><a href="#cb96-1007" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fit-xgboost-class, cache=TRUE}</span></span>
<span id="cb96-1008"><a href="#cb96-1008" aria-hidden="true" tabindex="-1"></a><span class="in">oj_xgb &lt;- list()</span></span>
<span id="cb96-1009"><a href="#cb96-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1010"><a href="#cb96-1010" aria-hidden="true" tabindex="-1"></a><span class="in"># `boost_tree` has 8 hyperparameters. Let's optimize just `trees` and `min_n`.</span></span>
<span id="cb96-1011"><a href="#cb96-1011" aria-hidden="true" tabindex="-1"></a><span class="in">oj_xgb$model &lt;-</span></span>
<span id="cb96-1012"><a href="#cb96-1012" aria-hidden="true" tabindex="-1"></a><span class="in">  boost_tree(</span></span>
<span id="cb96-1013"><a href="#cb96-1013" aria-hidden="true" tabindex="-1"></a><span class="in">    trees = tune(),</span></span>
<span id="cb96-1014"><a href="#cb96-1014" aria-hidden="true" tabindex="-1"></a><span class="in">    min_n = tune()</span></span>
<span id="cb96-1015"><a href="#cb96-1015" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb96-1016"><a href="#cb96-1016" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("xgboost") %&gt;%</span></span>
<span id="cb96-1017"><a href="#cb96-1017" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("classification")</span></span>
<span id="cb96-1018"><a href="#cb96-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1019"><a href="#cb96-1019" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune a model using the workflow framework.</span></span>
<span id="cb96-1020"><a href="#cb96-1020" aria-hidden="true" tabindex="-1"></a><span class="in">oj_xgb$workflow &lt;-</span></span>
<span id="cb96-1021"><a href="#cb96-1021" aria-hidden="true" tabindex="-1"></a><span class="in">  workflow() %&gt;%</span></span>
<span id="cb96-1022"><a href="#cb96-1022" aria-hidden="true" tabindex="-1"></a><span class="in">  add_model(oj_xgb$model) %&gt;%</span></span>
<span id="cb96-1023"><a href="#cb96-1023" aria-hidden="true" tabindex="-1"></a><span class="in">  add_formula(Purchase ~ .)</span></span>
<span id="cb96-1024"><a href="#cb96-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1025"><a href="#cb96-1025" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb96-1026"><a href="#cb96-1026" aria-hidden="true" tabindex="-1"></a><span class="in"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb96-1027"><a href="#cb96-1027" aria-hidden="true" tabindex="-1"></a><span class="in"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb96-1028"><a href="#cb96-1028" aria-hidden="true" tabindex="-1"></a><span class="in">oj_xgb$tune_grid &lt;-</span></span>
<span id="cb96-1029"><a href="#cb96-1029" aria-hidden="true" tabindex="-1"></a><span class="in">  oj_xgb$workflow %&gt;%</span></span>
<span id="cb96-1030"><a href="#cb96-1030" aria-hidden="true" tabindex="-1"></a><span class="in">  tune_grid(</span></span>
<span id="cb96-1031"><a href="#cb96-1031" aria-hidden="true" tabindex="-1"></a><span class="in">    resamples = vfold_cv(oj_train, v = 10), </span></span>
<span id="cb96-1032"><a href="#cb96-1032" aria-hidden="true" tabindex="-1"></a><span class="in">    grid = grid_regular(trees(), min_n(), levels = 5)</span></span>
<span id="cb96-1033"><a href="#cb96-1033" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb96-1034"><a href="#cb96-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1035"><a href="#cb96-1035" aria-hidden="true" tabindex="-1"></a><span class="in"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb96-1036"><a href="#cb96-1036" aria-hidden="true" tabindex="-1"></a><span class="in">oj_xgb$tune_grid %&gt;% </span></span>
<span id="cb96-1037"><a href="#cb96-1037" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_metrics() %&gt;%</span></span>
<span id="cb96-1038"><a href="#cb96-1038" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(trees = factor(trees)) %&gt;%</span></span>
<span id="cb96-1039"><a href="#cb96-1039" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(aes(x = min_n, y = mean, color = trees)) +</span></span>
<span id="cb96-1040"><a href="#cb96-1040" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line(linewidth = 1.5, alpha = .6) +</span></span>
<span id="cb96-1041"><a href="#cb96-1041" aria-hidden="true" tabindex="-1"></a><span class="in">  facet_wrap(facets = vars(.metric), scales = "free") +</span></span>
<span id="cb96-1042"><a href="#cb96-1042" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_x_log10()</span></span>
<span id="cb96-1043"><a href="#cb96-1043" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-1044"><a href="#cb96-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1045"><a href="#cb96-1045" aria-hidden="true" tabindex="-1"></a>The best models in terms of accuracy and ROC was the trees of 1000 and any minimum node size of 40.</span>
<span id="cb96-1046"><a href="#cb96-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1049"><a href="#cb96-1049" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-1050"><a href="#cb96-1050" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"accuracy"</span>)</span>
<span id="cb96-1051"><a href="#cb96-1051" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-1052"><a href="#cb96-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1053"><a href="#cb96-1053" aria-hidden="true" tabindex="-1"></a>Select the best model in terms of accuracy and finalize the model.</span>
<span id="cb96-1054"><a href="#cb96-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1057"><a href="#cb96-1057" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-1058"><a href="#cb96-1058" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(oj_xgb<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"accuracy"</span>)</span>
<span id="cb96-1059"><a href="#cb96-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1060"><a href="#cb96-1060" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb96-1061"><a href="#cb96-1061" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(oj_xgb<span class="sc">$</span>workflow, oj_xgb<span class="sc">$</span>best_tune)</span>
<span id="cb96-1062"><a href="#cb96-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1063"><a href="#cb96-1063" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb96-1064"><a href="#cb96-1064" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb96-1065"><a href="#cb96-1065" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb96-1066"><a href="#cb96-1066" aria-hidden="true" tabindex="-1"></a>  oj_xgb<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb96-1067"><a href="#cb96-1067" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(oj_split)</span>
<span id="cb96-1068"><a href="#cb96-1068" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-1069"><a href="#cb96-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1070"><a href="#cb96-1070" aria-hidden="true" tabindex="-1"></a>There is no single tree to visualize, and you can't even produce a VIP. Let's look at the performance on the holdout data set. <span class="in">`collect_metrics()`</span> shows the accuracy and ROC AUC metrics.</span>
<span id="cb96-1071"><a href="#cb96-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1074"><a href="#cb96-1074" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-1075"><a href="#cb96-1075" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb96-1076"><a href="#cb96-1076" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-1077"><a href="#cb96-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1078"><a href="#cb96-1078" aria-hidden="true" tabindex="-1"></a>You can explore the performance by calculating the full confusion matrix and visualizing the ROC curve. The confusion matrix calculates the model performance predicting on the holdout testing data set.</span>
<span id="cb96-1079"><a href="#cb96-1079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1080"><a href="#cb96-1080" aria-hidden="true" tabindex="-1"></a><span class="in">```{r collapse=TRUE}</span></span>
<span id="cb96-1081"><a href="#cb96-1081" aria-hidden="true" tabindex="-1"></a><span class="in">oj_xgb$confmat &lt;-</span></span>
<span id="cb96-1082"><a href="#cb96-1082" aria-hidden="true" tabindex="-1"></a><span class="in">  oj_xgb$fit %&gt;% </span></span>
<span id="cb96-1083"><a href="#cb96-1083" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_predictions() %&gt;% </span></span>
<span id="cb96-1084"><a href="#cb96-1084" aria-hidden="true" tabindex="-1"></a><span class="in">  conf_mat(truth = Purchase, estimate = .pred_class)</span></span>
<span id="cb96-1085"><a href="#cb96-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1086"><a href="#cb96-1086" aria-hidden="true" tabindex="-1"></a><span class="in">oj_xgb$confmat</span></span>
<span id="cb96-1087"><a href="#cb96-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1088"><a href="#cb96-1088" aria-hidden="true" tabindex="-1"></a><span class="in">summary(oj_xgb$confmat)</span></span>
<span id="cb96-1089"><a href="#cb96-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1090"><a href="#cb96-1090" aria-hidden="true" tabindex="-1"></a><span class="in">oj_xgb$fit %&gt;% </span></span>
<span id="cb96-1091"><a href="#cb96-1091" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_predictions() %&gt;% </span></span>
<span id="cb96-1092"><a href="#cb96-1092" aria-hidden="true" tabindex="-1"></a><span class="in">  select(Purchase, .pred_class) %&gt;%</span></span>
<span id="cb96-1093"><a href="#cb96-1093" aria-hidden="true" tabindex="-1"></a><span class="in">  plot(</span></span>
<span id="cb96-1094"><a href="#cb96-1094" aria-hidden="true" tabindex="-1"></a><span class="in">    main = "XGBoost: Predicted vs. Actual",</span></span>
<span id="cb96-1095"><a href="#cb96-1095" aria-hidden="true" tabindex="-1"></a><span class="in">    xlab = "Actual",</span></span>
<span id="cb96-1096"><a href="#cb96-1096" aria-hidden="true" tabindex="-1"></a><span class="in">    ylab = "Predicted"</span></span>
<span id="cb96-1097"><a href="#cb96-1097" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb96-1098"><a href="#cb96-1098" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-1099"><a href="#cb96-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1100"><a href="#cb96-1100" aria-hidden="true" tabindex="-1"></a>The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. The AUC on the holdout set is <span class="in">`r oj_xgb$fit %&gt;% collect_predictions() %&gt;% yardstick::roc_auc(Purchase, .pred_CH) %&gt;% pull(.estimate) %&gt;% comma(.001)`</span>.</span>
<span id="cb96-1101"><a href="#cb96-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1104"><a href="#cb96-1104" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-1105"><a href="#cb96-1105" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb96-1106"><a href="#cb96-1106" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-1107"><a href="#cb96-1107" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">roc_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb96-1108"><a href="#cb96-1108" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb96-1109"><a href="#cb96-1109" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ XGBoost ROC Curve"</span>)</span>
<span id="cb96-1110"><a href="#cb96-1110" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-1111"><a href="#cb96-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1112"><a href="#cb96-1112" aria-hidden="true" tabindex="-1"></a>The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value.</span>
<span id="cb96-1113"><a href="#cb96-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1116"><a href="#cb96-1116" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-1117"><a href="#cb96-1117" aria-hidden="true" tabindex="-1"></a>oj_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span></span>
<span id="cb96-1118"><a href="#cb96-1118" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-1119"><a href="#cb96-1119" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">gain_curve</span>(Purchase, .pred_CH) <span class="sc">%&gt;%</span></span>
<span id="cb96-1120"><a href="#cb96-1120" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>() <span class="sc">+</span></span>
<span id="cb96-1121"><a href="#cb96-1121" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"OJ XGBoost Gain Curve"</span>)</span>
<span id="cb96-1122"><a href="#cb96-1122" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-1123"><a href="#cb96-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1124"><a href="#cb96-1124" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Gradient Boosting Regression Tree</span></span>
<span id="cb96-1125"><a href="#cb96-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1126"><a href="#cb96-1126" aria-hidden="true" tabindex="-1"></a>I'll predict <span class="in">`Sales`</span> from the <span class="in">`Carseats`</span> data set again, this time with <span class="in">`parsnip::rand_forest()`</span>.</span>
<span id="cb96-1127"><a href="#cb96-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1128"><a href="#cb96-1128" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fit-xgboost-reg, cache=TRUE}</span></span>
<span id="cb96-1129"><a href="#cb96-1129" aria-hidden="true" tabindex="-1"></a><span class="in">cs_xgb &lt;- list()</span></span>
<span id="cb96-1130"><a href="#cb96-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1131"><a href="#cb96-1131" aria-hidden="true" tabindex="-1"></a><span class="in"># `rand_forest` has 3 hyperparameters (`mtry`, `trees`, and</span></span>
<span id="cb96-1132"><a href="#cb96-1132" aria-hidden="true" tabindex="-1"></a><span class="in"># `min_n`). Set their value to `tune()` if you want to optimize any one. Let's</span></span>
<span id="cb96-1133"><a href="#cb96-1133" aria-hidden="true" tabindex="-1"></a><span class="in"># optimize just `trees` and `min_n`.</span></span>
<span id="cb96-1134"><a href="#cb96-1134" aria-hidden="true" tabindex="-1"></a><span class="in">cs_xgb$model &lt;-</span></span>
<span id="cb96-1135"><a href="#cb96-1135" aria-hidden="true" tabindex="-1"></a><span class="in">  boost_tree(</span></span>
<span id="cb96-1136"><a href="#cb96-1136" aria-hidden="true" tabindex="-1"></a><span class="in">    trees = tune(),</span></span>
<span id="cb96-1137"><a href="#cb96-1137" aria-hidden="true" tabindex="-1"></a><span class="in">    min_n = tune()</span></span>
<span id="cb96-1138"><a href="#cb96-1138" aria-hidden="true" tabindex="-1"></a><span class="in">  ) %&gt;%</span></span>
<span id="cb96-1139"><a href="#cb96-1139" aria-hidden="true" tabindex="-1"></a><span class="in">  set_engine("xgboost") %&gt;%</span></span>
<span id="cb96-1140"><a href="#cb96-1140" aria-hidden="true" tabindex="-1"></a><span class="in">  set_mode("regression")</span></span>
<span id="cb96-1141"><a href="#cb96-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1142"><a href="#cb96-1142" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune a model using the workflow framework.</span></span>
<span id="cb96-1143"><a href="#cb96-1143" aria-hidden="true" tabindex="-1"></a><span class="in">cs_xgb$workflow &lt;-</span></span>
<span id="cb96-1144"><a href="#cb96-1144" aria-hidden="true" tabindex="-1"></a><span class="in">  workflow() %&gt;%</span></span>
<span id="cb96-1145"><a href="#cb96-1145" aria-hidden="true" tabindex="-1"></a><span class="in">  add_model(cs_xgb$model) %&gt;%</span></span>
<span id="cb96-1146"><a href="#cb96-1146" aria-hidden="true" tabindex="-1"></a><span class="in">  add_formula(Sales ~ .)</span></span>
<span id="cb96-1147"><a href="#cb96-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1148"><a href="#cb96-1148" aria-hidden="true" tabindex="-1"></a><span class="in"># Tune the model with 10-fold CV using a regular grid of cost complexity values.</span></span>
<span id="cb96-1149"><a href="#cb96-1149" aria-hidden="true" tabindex="-1"></a><span class="in"># With 2 hyperparameters and 5 levels, the grid has 5^2=25 combinations. That</span></span>
<span id="cb96-1150"><a href="#cb96-1150" aria-hidden="true" tabindex="-1"></a><span class="in"># means the tuning exercise will fit 25 models to each of 10 folds = 250 fits.</span></span>
<span id="cb96-1151"><a href="#cb96-1151" aria-hidden="true" tabindex="-1"></a><span class="in">cs_xgb$tune_grid &lt;-</span></span>
<span id="cb96-1152"><a href="#cb96-1152" aria-hidden="true" tabindex="-1"></a><span class="in">  cs_xgb$workflow %&gt;%</span></span>
<span id="cb96-1153"><a href="#cb96-1153" aria-hidden="true" tabindex="-1"></a><span class="in">  tune_grid(</span></span>
<span id="cb96-1154"><a href="#cb96-1154" aria-hidden="true" tabindex="-1"></a><span class="in">    resamples = vfold_cv(cs_train, v = 10), </span></span>
<span id="cb96-1155"><a href="#cb96-1155" aria-hidden="true" tabindex="-1"></a><span class="in">    grid = grid_regular(trees(), min_n(), levels = 5)</span></span>
<span id="cb96-1156"><a href="#cb96-1156" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb96-1157"><a href="#cb96-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1158"><a href="#cb96-1158" aria-hidden="true" tabindex="-1"></a><span class="in"># `collect_metrics()` returns two metrics: accuracy and ROC-AUC.</span></span>
<span id="cb96-1159"><a href="#cb96-1159" aria-hidden="true" tabindex="-1"></a><span class="in">cs_xgb$tune_grid %&gt;% </span></span>
<span id="cb96-1160"><a href="#cb96-1160" aria-hidden="true" tabindex="-1"></a><span class="in">  collect_metrics() %&gt;%</span></span>
<span id="cb96-1161"><a href="#cb96-1161" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(trees = factor(trees)) %&gt;%</span></span>
<span id="cb96-1162"><a href="#cb96-1162" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(aes(x = min_n, y = mean, color = trees)) +</span></span>
<span id="cb96-1163"><a href="#cb96-1163" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_line(linewidth = 1.5, alpha = .6) +</span></span>
<span id="cb96-1164"><a href="#cb96-1164" aria-hidden="true" tabindex="-1"></a><span class="in">  facet_wrap(facets = vars(.metric), scales = "free") +</span></span>
<span id="cb96-1165"><a href="#cb96-1165" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_x_log10()</span></span>
<span id="cb96-1166"><a href="#cb96-1166" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-1167"><a href="#cb96-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1168"><a href="#cb96-1168" aria-hidden="true" tabindex="-1"></a>The best models in terms of RMSE was 500 trees with at least 21 data points per node.</span>
<span id="cb96-1169"><a href="#cb96-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1172"><a href="#cb96-1172" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-1173"><a href="#cb96-1173" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>tune <span class="sc">%&gt;%</span> <span class="fu">show_best</span>(<span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb96-1174"><a href="#cb96-1174" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-1175"><a href="#cb96-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1176"><a href="#cb96-1176" aria-hidden="true" tabindex="-1"></a>Select the best model in terms of rmse and finalize the model.</span>
<span id="cb96-1177"><a href="#cb96-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1180"><a href="#cb96-1180" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-1181"><a href="#cb96-1181" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>best_tune <span class="ot">&lt;-</span> <span class="fu">select_best</span>(cs_xgb<span class="sc">$</span>tune_grid, <span class="at">metric =</span> <span class="st">"rmse"</span>)</span>
<span id="cb96-1182"><a href="#cb96-1182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1183"><a href="#cb96-1183" aria-hidden="true" tabindex="-1"></a><span class="co"># `finalize_workflow()` applies the tuning parameters to the workflow.</span></span>
<span id="cb96-1184"><a href="#cb96-1184" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>final_workflow <span class="ot">&lt;-</span> <span class="fu">finalize_workflow</span>(cs_xgb<span class="sc">$</span>workflow, cs_xgb<span class="sc">$</span>best_tune)</span>
<span id="cb96-1185"><a href="#cb96-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1186"><a href="#cb96-1186" aria-hidden="true" tabindex="-1"></a><span class="co"># last_fit() fits the model with the full training set and evaluates it on the </span></span>
<span id="cb96-1187"><a href="#cb96-1187" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data.</span></span>
<span id="cb96-1188"><a href="#cb96-1188" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>fit <span class="ot">&lt;-</span></span>
<span id="cb96-1189"><a href="#cb96-1189" aria-hidden="true" tabindex="-1"></a>  cs_rf<span class="sc">$</span>final_workflow <span class="sc">%&gt;%</span></span>
<span id="cb96-1190"><a href="#cb96-1190" aria-hidden="true" tabindex="-1"></a>  <span class="fu">last_fit</span>(cs_split)</span>
<span id="cb96-1191"><a href="#cb96-1191" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-1192"><a href="#cb96-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1193"><a href="#cb96-1193" aria-hidden="true" tabindex="-1"></a><span class="in">`collect_metrics()`</span> returns the RMSE, $RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})$ and the model $R^2$. The RMSE of <span class="in">`r cs_xgb$fit %&gt;% collect_metrics() %&gt;% filter(.metric == "rmse") %&gt;% pull(.estimate) %&gt;% comma(.01)`</span> in the test data set is pretty good considering the standard deviation of <span class="in">`Sales`</span> is <span class="in">`r comma(sd(cs_test$Sales), .01)`</span>.</span>
<span id="cb96-1194"><a href="#cb96-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1197"><a href="#cb96-1197" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-1198"><a href="#cb96-1198" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb96-1199"><a href="#cb96-1199" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb96-1200"><a href="#cb96-1200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1201"><a href="#cb96-1201" aria-hidden="true" tabindex="-1"></a>Here is a predicted vs actual plot.</span>
<span id="cb96-1202"><a href="#cb96-1202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-1205"><a href="#cb96-1205" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb96-1206"><a href="#cb96-1206" aria-hidden="true" tabindex="-1"></a>cs_xgb<span class="sc">$</span>fit <span class="sc">%&gt;%</span> </span>
<span id="cb96-1207"><a href="#cb96-1207" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect_predictions</span>() <span class="sc">%&gt;%</span></span>
<span id="cb96-1208"><a href="#cb96-1208" aria-hidden="true" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Sales, <span class="at">y =</span> .pred)) <span class="sc">+</span></span>
<span id="cb96-1209"><a href="#cb96-1209" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> <span class="st">"cadetblue"</span>) <span class="sc">+</span></span>
<span id="cb96-1210"><a href="#cb96-1210" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"loess"</span>, <span class="at">formula =</span> <span class="st">"y~x"</span>) <span class="sc">+</span></span>
<span id="cb96-1211"><a href="#cb96-1211" aria-hidden="true" tabindex="-1"></a>   <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb96-1212"><a href="#cb96-1212" aria-hidden="true" tabindex="-1"></a>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Carseats Random Forest, Predicted vs Actual"</span>)</span>
<span id="cb96-1213"><a href="#cb96-1213" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>