[["index.html", "Supervised Machine Learning My handbook Intro", " Supervised Machine Learning My handbook Michael Foley 2023-07-24 Intro Machine learning (ML) develops algorithms to identify patterns in data (unsupervised ML) or make predictions and inferences (supervised ML). Supervised ML trains the machine to learn from prior examples to predict either a categorical outcome (classification) or a numeric outcome (regression), or to infer the relationships between the outcome and its explanatory variables. Two early forms of supervised ML are linear regression (OLS) and generalized linear models (GLM) (Poisson and logistic regression). These methods have been improved with advanced linear methods, including stepwise selection, regularization (ridge, lasso, elastic net), principal components regression, and partial least squares. With greater computing capacity, non-linear models are now in use, including polynomial regression, step functions, splines, and generalized additive models (GAM). Decision trees (bagging, random forests, and, boosting) are additional options for regression and classification, and support vector machines is an additional option for classification. "],["ordinary-least-squares.html", "Chapter 1 Ordinary Least Squares 1.1 Linear Regression Model 1.2 Parameter Estimation 1.3 Model Assumptions 1.4 Prediction 1.5 Inference 1.6 Interpretation 1.7 Model Validation 1.8 OLS Reference", " Chapter 1 Ordinary Least Squares library(caret) ## Warning: package &#39;caret&#39; was built under R version 4.3.1 library(Metrics) ## Warning: package &#39;Metrics&#39; was built under R version 4.3.1 library(tidyverse) library(corrplot) ## Warning: package &#39;corrplot&#39; was built under R version 4.3.1 library(gridExtra) ## Warning: package &#39;gridExtra&#39; was built under R version 4.3.1 library(car) # for avPlots ## Warning: package &#39;car&#39; was built under R version 4.3.1 ## Warning: package &#39;carData&#39; was built under R version 4.3.1 library(AppliedPredictiveModeling) ## Warning: package &#39;AppliedPredictiveModeling&#39; was built under R version 4.3.1 library(e1071) # for skewness() ## Warning: package &#39;e1071&#39; was built under R version 4.3.1 library(purrr) # for map() library(broom) # for augment() These notes cover linear regression. 1.1 Linear Regression Model The population regression model \\(E(Y) = X \\beta\\) summarizes the trend between the predictors and the mean responses. The individual responses are assumed to be normally distributed about the population regression, \\(y_i = X_i \\beta + \\epsilon_i\\) with varying mean, but constant variance, \\(y_i \\sim N(\\mu_i, \\sigma^2).\\) Equivalently, the model presumes a linear relationship between \\(y\\) and \\(X\\) with residuals \\(\\epsilon\\) that are independent normal random variables with mean zero and constant variance \\(\\sigma^2\\). Estimate the population regression model coefficients as \\(\\hat{y} = X \\hat{\\beta}\\), and the population variance as \\(\\hat{\\sigma}^2\\). The most common method of estimating the \\(\\beta\\) coefficients and \\(\\sigma\\) is ordinary least squares (OLS). OLS minimizes the sum of squared residuals from a random sample. The individual predicted values vary about the actual value, \\(e_i = y_i - \\hat{y}_i\\), where \\(\\hat{y}_i = X_i \\hat{\\beta}\\). The OLS model is the best linear unbiased estimator (BLUE) if the residuals are independent random variables normally distributed with mean zero and constant variance. Recall these conditions with the LINE pneumonic: Linear, Independent, Normal, and Equal. Linearity. The explanatory variables are each linearly related to the response variable: \\(E(\\epsilon | X_j) = 0\\). Independence. The residuals are unrelated to each other. Independence is violated by repeated measurements and temporal regressors. Normality. The residuals are normally distributed: \\(\\epsilon|X \\sim N(0, \\sigma^2I)\\). Equal Variances. The variance of the residuals is constant (homoscedasticity): \\(E(\\epsilon \\epsilon&#39; | X) = \\sigma^2I\\) Additionally, you should make sure you model has “little” or no multicollinearity among the variables. 1.2 Parameter Estimation There are two model parameters to estimate: \\(\\hat{\\beta}\\) estimates the coefficient vector \\(\\beta\\), and \\(\\hat{\\sigma}\\) estimates the variance of the residuals along the regression line. Derive the coefficient estimators by minimizing the sum of squared residuals \\(SSE = (y - X \\hat{\\beta})&#39; (y - X \\hat{\\beta})\\). The result is \\[\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y.\\] The residual standard error (RSE) estimates the sample deviation around the population regression line. (Think of each value of \\(X\\) along the regression line as a subpopulation with mean \\(y_i\\) and variance \\(\\sigma^2\\). This variance is assumed to be the same for all \\(X\\).) \\[\\hat{\\sigma} = \\sqrt{(n-k-1)^{-1} e&#39;e}.\\] The standard error for the coefficient estimators is the square root of the error variance divided by \\((X&#39;X)\\). \\[SE(\\hat{\\beta}) = \\sqrt{\\hat{\\sigma}^2 (X&#39;X)^{-1}}.\\] 1.2.0.1 Example Dataset mtcars contains response variable fuel consumption mpg and 10 aspects of automobile design and performance for 32 automobiles. What is the relationship between the response variable and its predictors? d &lt;- mtcars %&gt;% mutate(vs = factor(vs, labels = c(&quot;V&quot;, &quot;S&quot;)), am = factor(am, labels = c(&quot;automatic&quot;, &quot;manual&quot;)), cyl = ordered(cyl), gear = ordered(gear), carb = ordered(carb)) glimpse(d) ## Rows: 32 ## Columns: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,… ## $ cyl &lt;ord&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,… ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16… ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180… ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,… ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.… ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18… ## $ vs &lt;fct&gt; V, V, S, S, V, S, V, S, S, S, S, V, V, V, V, V, V, S, S, S, S, V,… ## $ am &lt;fct&gt; manual, manual, manual, automatic, automatic, automatic, automati… ## $ gear &lt;ord&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,… ## $ carb &lt;ord&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,… The data consists of 32 observations. A scatterplot matrix of the numeric variables shows the strongest individual association with mpg is from wt (corr = -0.87) followed by disp (corr = -0.85) and hp (corr = -0.78), drat is moderately correlated with mpg (corr = 0.68), and qsec is weakly correlated with mpg (corr = 0.42). corrplot(cor(subset(d, select = c(mpg, disp, hp, drat, wt, qsec))), type = &quot;upper&quot;, method = &quot;number&quot;) Many of the Predictor variables are strongly correlated with each other. Boxplots of the categorical variables shows differences in levels, although ordinal variables gear and and carb do not have a monotonic relationshiop with mpg. p_list &lt;- list() for(i in c(&quot;cyl&quot;, &quot;vs&quot;, &quot;am&quot;, &quot;gear&quot;, &quot;carb&quot;)) { p &lt;- ggplot(d, aes_string(x = i, y = &quot;mpg&quot;)) + geom_boxplot() p_list &lt;- c(p_list, list(p)) } ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation idioms with `aes()`. ## ℹ See also `vignette(&quot;ggplot2-in-packages&quot;)` for more information. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. do.call(&quot;grid.arrange&quot;, c(p_list, ncol = 2)) I’ll drop the gear and carb predictors, and fit a population model to the remaining predictors. m &lt;- lm(mpg ~ ., data = d[,1:9]) summary(m) ## ## Call: ## lm(formula = mpg ~ ., data = d[, 1:9]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9978 -1.3551 -0.3108 1.1992 4.1102 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.540985 14.146419 1.381 0.1810 ## cyl.L 0.342558 2.764833 0.124 0.9025 ## cyl.Q 1.388429 1.112097 1.248 0.2250 ## disp 0.006688 0.013512 0.495 0.6255 ## hp -0.029141 0.017182 -1.696 0.1040 ## drat 0.588059 1.503111 0.391 0.6994 ## wt -3.155246 1.420235 -2.222 0.0369 * ## qsec 0.523235 0.690130 0.758 0.4564 ## vsS 1.237800 2.106056 0.588 0.5627 ## ammanual 3.000910 1.853400 1.619 0.1197 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.514 on 22 degrees of freedom ## Multiple R-squared: 0.8765, Adjusted R-squared: 0.826 ## F-statistic: 17.35 on 9 and 22 DF, p-value: 4.814e-08 The summary() function shows \\(\\hat{\\beta}\\) as Estimate, \\(SE({\\hat{\\beta}})\\) as Std. Error, and \\(\\hat{\\sigma}\\) as Residual standard error. You can verify this by manually peforming these calculations using matrix algebra (see matrix algebra in r notes at R for Dummies). Here are the coefficient estimators, \\(\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y\\). X &lt;- model.matrix(m) y &lt;- d$mpg beta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y round(beta_hat, 5) ## [,1] ## (Intercept) 19.54098 ## cyl.L 0.34256 ## cyl.Q 1.38843 ## disp 0.00669 ## hp -0.02914 ## drat 0.58806 ## wt -3.15525 ## qsec 0.52324 ## vsS 1.23780 ## ammanual 3.00091 Here is the residual standard error, \\(\\hat{\\sigma} = \\sqrt{(n-k-1)^{-1} \\hat{e}&#39;\\hat{e}}\\). n &lt;- nrow(X) k &lt;- ncol(X) - 1 # exclude the intercept term y_hat &lt;- X %*% beta_hat sse &lt;- sum((y - y_hat)^2) rse &lt;- sqrt(sse / (n - k - 1)) cat(&quot;Residual standard error: &quot;, round(rse, 3), &quot; on &quot;, (n - k - 1), &quot; degrees of freedom.&quot;) ## Residual standard error: 2.514 on 22 degrees of freedom. Use the residual standard errors to derive the standard errors of the coefficients, \\(SE(\\hat{\\beta}) = \\sqrt{\\hat{\\sigma}^2 (X&#39;X)^{-1}}\\). se_beta_hat &lt;- sqrt(diag(rse^2 * solve(t(X) %*% X))) matrix(round(se_beta_hat, 5), dimnames = list(names(se_beta_hat), &quot;Std. Error&quot;)) ## Std. Error ## (Intercept) 14.14642 ## cyl.L 2.76483 ## cyl.Q 1.11210 ## disp 0.01351 ## hp 0.01718 ## drat 1.50311 ## wt 1.42023 ## qsec 0.69013 ## vsS 2.10606 ## ammanual 1.85340 1.3 Model Assumptions The linear regression model assumes the relationship between the predictors and the response is linear and the residuals are independent random variables normally distributed with mean zero and constant variance. Additionally, you will want to check for multicollinearity in the predictors because it can produce unreliable coefficient estimates and predicted values. Use a residuals vs fits plot \\(\\left( e \\sim \\hat{Y} \\right)\\) to detect non-linearity and unequal error variances, including outliers. The polynomial trend line should show that the residuals vary around \\(e = 0\\) in a straight line (linearity). The variance should be of constant width (especially no fan shape at the low or high ends). Use a residuals normal probability plot to compares the theoretical percentiles of the normal distribution versus the observed sample percentiles. It should be approximately linear. A scale-location plot \\(\\sqrt{e / sd(e)} \\sim \\hat{y}\\) checks the homogeneity of variance of the residuals (homoscedasticity). The square root of the absolute value of the residuals should be spread equally along a horizontal line. A residuals vs leverage plot identifies influential observations. A plot of the standardized residuals vs the leverage should fall within the 95% probability band. par(mfrow = c(2, 2)) plot(m, labels.id = NULL) 1.3.1 Linearity The explanatory variables should each be linearly related to the response variable: \\(E(\\epsilon | X_j) = 0\\). A good way to test this condition is with a residuals vs fitted values plot. A curved pattern in the residuals indicates a curvature in the relationship between the response and the predictor that is not explained by our model. A linear model does not adequately describe the relationship between the predictor and the response. Test for linearity four ways: Residuals vs fits plot \\((e \\sim \\hat{Y})\\) should bounce randomly around 0. Observed vs fits plot \\((Y \\sim \\hat{Y})\\) should be symmetric along the 45-degree line. Each \\((Y \\sim X_j )\\) plot should have correlation \\(\\rho \\sim 1\\). Each \\((e \\sim X_j)\\) plot should exhibit no pattern. If the linearity condition fails, change the functional form of the model with non-linear transformations of the explanatory variables. A common way to do this is with Box-Cox transformations. \\[w_t = \\begin{cases} \\begin{array}{l} log(y_t) \\quad \\quad \\lambda = 0 \\\\ (y_t^\\lambda - 1) / \\lambda \\quad \\text{otherwise} \\end{array} \\end{cases}\\] \\(\\lambda\\) can take any value, but values near the following yield familiar transformations. \\(\\lambda = 1\\) yields no substantive transformation. \\(\\lambda = 0.5\\) is a square root plus linear transformation. \\(\\lambda = 0.333\\) is a cube root plus linear transformation. \\(\\lambda = 0\\) is a natural log transformation. \\(\\lambda = -1\\) is an inverse transformation. A common source of non-linearity in a model is skewed response or independent variables (see discussion here). mtcars has some skewed variables. tmp &lt;- map(mtcars, skewness) %&gt;% unlist() %&gt;% as.data.frame() %&gt;% rownames_to_column() colnames(tmp) &lt;- c(&quot;IV&quot;, &quot;skew&quot;) ggplot(tmp, aes(x = order(IV, skew), y = skew)) + geom_col() df &lt;- mtcars m &lt;- lm(mpg ~ hp, data = df) par(mfrow = c(2, 2)) plot(m) plot(m$model$mpg, m$fitted.values) abline(0, 1) cor(df$mpg, df$hp) ## [1] -0.7761684 postResample(pred = m$fitted.values, obs = m$model$mpg) ## RMSE Rsquared MAE ## 3.7402971 0.6024373 2.9074525 bc &lt;- BoxCoxTrans(mtcars$hp) df$hp_bc &lt;- predict(bc, mtcars$hp) m_bc &lt;- lm(mpg ~ hp_bc, data = df) plot(m_bc) plot(m_bc$model$mpg, m_bc$fitted.values) abline(0, 1) cor(df$mpg, df$hp_bc) ## [1] -0.8487707 postResample(pred = m_bc$fitted.values, obs = m_bc$model$mpg) ## RMSE Rsquared MAE ## 3.1366261 0.7204118 2.4074705 # Which vars are skewed? map(mtcars, skewness) ## $mpg ## [1] 0.610655 ## ## $cyl ## [1] -0.1746119 ## ## $disp ## [1] 0.381657 ## ## $hp ## [1] 0.7260237 ## ## $drat ## [1] 0.2659039 ## ## $wt ## [1] 0.4231465 ## ## $qsec ## [1] 0.3690453 ## ## $vs ## [1] 0.2402577 ## ## $am ## [1] 0.3640159 ## ## $gear ## [1] 0.5288545 ## ## $carb ## [1] 1.050874 # Benchmark model: mpg ~ hp d0 &lt;- mtcars m0 &lt;- lm(mpg ~ hp, data = d0) d0 &lt;- augment(m0, d0) d0.cor &lt;- round(cor(d0$mpg, d0$hp), 2) # Benchmark diagnostics p0a &lt;- ggplot(d0, aes(x = hp, y = mpg)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Response vs IV&quot;, subtitle = paste0(&quot;Correlation ~ 1? (rho = &quot;, d0.cor, &quot;)&quot;)) p0b &lt;- ggplot(d0, aes(x = .fitted, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs Fits&quot;, subtitle = &quot;Random scatter?&quot;) p0c &lt;- ggplot(d0, aes(x = .fitted, y = mpg)) + geom_point() + geom_abline(intercept = 0, slope = 1) + expand_limits(x = c(0, 35), y = c(0, 35)) + labs(title = &quot;Observed vs Fits&quot;, subtitle = &quot;Symmetric along 45 degree line?&quot;) p0d &lt;- ggplot(d0, aes(x = hp, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs IV&quot;, subtitle = &quot;Random scatter?&quot;) grid.arrange(p0a, p0b, p0c, p0d, nrow = 2) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Benchmark performance postResample(pred = m0$fitted.values, obs = m0$model$mpg) ## RMSE Rsquared MAE ## 3.7402971 0.6024373 2.9074525 # Box-Cox transform hp d1 &lt;- mtcars bc &lt;- BoxCoxTrans(d1$hp) d1$hp_bc &lt;- predict(bc, d1$hp) m1 &lt;- lm(mpg ~ hp_bc, data = d1) d1 &lt;- augment(m1, d1) d1.cor &lt;- round(cor(d1$mpg, d1$hp_bc), 2) p1a &lt;- ggplot(d1, aes(x = hp_bc, y = mpg)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Response vs IV&quot;, subtitle = paste0(&quot;Correlation ~ 1? (rho = &quot;, d1.cor, &quot;)&quot;)) p1b &lt;- ggplot(d1, aes(x = .fitted, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs Fits&quot;, subtitle = &quot;Random scatter?&quot;) p1c &lt;- ggplot(d1, aes(x = .fitted, y = mpg)) + geom_point() + geom_abline(intercept = 0, slope = 1) + expand_limits(x = c(0, 35), y = c(0, 35)) + labs(title = &quot;Observed vs Fits&quot;, subtitle = &quot;Symmetric along 45 degree line?&quot;) p1d &lt;- ggplot(d1, aes(x = hp, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs IV&quot;, subtitle = &quot;Random scatter?&quot;) grid.arrange(p1a, p1b, p1c, p1d, nrow = 2) ## `geom_smooth()` using formula = &#39;y ~ x&#39; postResample(pred = m1$fitted.values, obs = m1$model$mpg) ## RMSE Rsquared MAE ## 3.1366261 0.7204118 2.4074705 1.3.2 Multicollinearity The multicollinearity condition is violated when two or more of the predictors in a regression model are correlated. Muticollinearity can occur for structural reasons, as when one variable is a transformation of another variable, or for data reasons, as occurs in observational studies. Multicollinearity is a problem because it inflates the variances of the estimated coefficients, resulting in larger confidence intervals. When predictor variables are correlated, the precision of the estimated regression coefficients decreases with each added correlated predictor variable. The usual interpretation of a slope coefficient as the change in the mean response for each additional unit increase in the predictor when all the other predictors are held constant breaks down because changing one predictor necessarily changes the others. A residuals vs fits plot \\((\\epsilon \\sim \\hat{Y})\\) should have correlation \\(\\rho \\sim 0\\). A correlation matrix is helpful for picking out the correlation strengths. A good rule of thumb is correlation coefficients should be less than 0.80. However, this test may not work when a variable is correlated with a function of other variables. A model with multicollinearity may have a significant F-test with insignificant individual slope estimator t-tests. Another way to detect multicollinearity is by calculating variance inflation factors. The predictor variance \\(Var(\\hat{\\beta_k})\\) increases by a factor \\[VIF_k = \\frac{1}{1 - R_k^2}\\] where \\(R_k^2\\) is the \\(R^2\\) of a regression of the \\(k^{th}\\) predictor on the remaining predictors. A \\(VIF_k\\) of \\(1\\) indicates no inflation (no corellation). A \\(VIF_k &gt;= 4\\) warrants investigation. A \\(VIF_k &gt;= 10\\) requires correction. 1.3.2.1 Example Does the model mpg ~ . exhibit multicollinearity? The correlation matrix above (and presented again below) has several correlated covariates. disp is strongly correlated with wt (r = 0.89) and hp (r = 0.79). m &lt;- lm(mpg ~ ., data = mtcars) corrplot(cor(subset(d, select = c(mpg, disp, hp, drat, wt, qsec))), type = &quot;upper&quot;, method = &quot;number&quot;) Calculate the VIFs. round(vif(m), 2) ## cyl disp hp drat wt qsec vs am gear carb ## 15.37 21.62 9.83 3.37 15.16 7.53 4.97 4.65 5.36 7.91 There are two predictors with VIFs greater than 10, cyl (GVIF = 21.36) and disp (GVIF = 13.76). One way to address multicollinearity is removing one or more of the violating predictors from the regression model. Try removing cyl. vif(m &lt;-lm(mpg ~ . - cyl, data = d[,1:9])) ## disp hp drat wt qsec vs am ## 9.865991 5.448912 2.818949 7.598119 5.979588 4.249244 3.450410 summary(m) ## ## Call: ## lm(formula = mpg ~ . - cyl, data = d[, 1:9]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4067 -1.4690 -0.2824 1.1415 4.5365 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.49805 12.48039 1.001 0.32662 ## disp 0.01374 0.01136 1.210 0.23821 ## hp -0.02282 0.01526 -1.496 0.14778 ## drat 0.95533 1.40737 0.679 0.50376 ## wt -3.94974 1.26261 -3.128 0.00457 ** ## qsec 0.87149 0.61331 1.421 0.16819 ## vsS 0.59017 1.83303 0.322 0.75027 ## ammanual 3.02402 1.66840 1.813 0.08244 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.495 on 24 degrees of freedom ## Multiple R-squared: 0.8673, Adjusted R-squared: 0.8286 ## F-statistic: 22.4 on 7 and 24 DF, p-value: 4.532e-09 Removing cyl reduced the VIFs of the other variables below 10. disp is still right up there (VIF = 9.87), so it may be worth dropping it from the model too. The model summary still shows that there is only one significant (at .05 level a significance) variable (wt, p = .00457). What if I drop disp too? vif(m &lt;- lm(mpg ~ . - cyl - disp, data = d[,1:9])) ## hp drat wt qsec vs am ## 5.070665 2.709905 5.105979 5.776361 4.120656 3.272177 summary(m) ## ## Call: ## lm(formula = mpg ~ . - cyl - disp, data = d[, 1:9]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3686 -1.7207 -0.2528 1.0986 4.6029 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.14103 12.22322 1.321 0.19862 ## hp -0.01796 0.01486 -1.209 0.23801 ## drat 0.62051 1.39261 0.446 0.65974 ## wt -3.07506 1.04458 -2.944 0.00691 ** ## qsec 0.73472 0.60836 1.208 0.23846 ## vsS 0.20446 1.82173 0.112 0.91153 ## ammanual 2.56534 1.63972 1.565 0.13027 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.518 on 25 degrees of freedom ## Multiple R-squared: 0.8592, Adjusted R-squared: 0.8254 ## F-statistic: 25.42 on 6 and 25 DF, p-value: 1.688e-09 The model is not improved, so keep disp. m &lt;-lm(mpg ~ . - cyl, data = d[,1:9]) If the multicollinearity occurs because you are using a polynomial regression model, center the predictor variables (subtract their means). 1.3.2.2 Example Data set exerimmun (exerimun.txt) contains observations of immunoglobin in blood (a measure of immunity) and maximal oxygen uptake (a measure of exercise level) for \\(n = 30\\) individuals. igg = amount of immunoglobin in blood (mg) oxygent = maximal oxygen uptake (ml/kg) How does exercise affect the immune system? #exerimmun &lt;- read_tsv(file = &quot;./Data/exerimmun.txt&quot;) exerimmun &lt;- tribble( ~igg, ~oxygen, 881, 34.6, 1290, 45, 2147, 62.3, 1909, 58.9, 1282, 42.5, 1530, 44.3, 2067, 67.9, 1982, 58.5, 1019, 35.6, 1651, 49.6, 752, 33, 1687, 52, 1782, 61.4, 1529, 50.2, 969, 34.1, 1660, 52.5, 2121, 69.9, 1382, 38.8, 1714, 50.6, 1959, 69.4, 1158, 37.4, 965, 35.1, 1456, 43, 1273, 44.1, 1418, 49.8, 1743, 54.4, 1997, 68.5, 2177, 69.5, 1965, 63, 1264, 43.2 ) head(exerimmun) ## # A tibble: 6 × 2 ## igg oxygen ## &lt;dbl&gt; &lt;dbl&gt; ## 1 881 34.6 ## 2 1290 45 ## 3 2147 62.3 ## 4 1909 58.9 ## 5 1282 42.5 ## 6 1530 44.3 The scatterplot oxygen ~ igg shows some curvature. Formulate a quadratic polynomial regression function, \\(igg_i = \\beta_0 + \\beta_1 oxygen_i + \\beta_2 oxygen_i^2 + \\epsilon_i\\) where the error terms are assumed to be independent, and normally distributed with equal variance. ggplot(exerimmun, aes(y = igg, x = oxygen)) + geom_point() + geom_smooth(method = lm, formula = y ~ poly(x, 2), se = FALSE) + labs(title = &quot;Immunoglobin in Blood&quot;) The formulated regression fits the data well (\\(adj R^2 = .933\\)), but the terms oxygen and oxygen^2 are strongly correlated. m_blood &lt;- lm(igg ~ poly(oxygen, 2), data = exerimmun) summary(m_blood) ## ## Call: ## lm(formula = igg ~ poly(oxygen, 2), data = exerimmun) ## ## Residuals: ## Min 1Q Median 3Q Max ## -185.375 -82.129 1.047 66.007 227.377 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1557.63 19.43 80.16 &lt; 2e-16 *** ## poly(oxygen, 2)1 2114.72 106.43 19.87 &lt; 2e-16 *** ## poly(oxygen, 2)2 -360.78 106.43 -3.39 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 106.4 on 27 degrees of freedom ## Multiple R-squared: 0.9377, Adjusted R-squared: 0.9331 ## F-statistic: 203.2 on 2 and 27 DF, p-value: &lt; 2.2e-16 cor(exerimmun$oxygen, exerimmun$oxygen^2) ## [1] 0.9949846 Remove the structural multicollinearity by centering the predictors. You can scale the predictors with scale(), but be careful to scale new data when predicting new observations with predict(newdata=)! Whenever possible, perform the transformation right in the model. m_blood &lt;- lm(igg ~ I(oxygen - mean(exerimmun$oxygen)) + I((oxygen - mean(exerimmun$oxygen))^2), data = exerimmun) summary(m_blood) ## ## Call: ## lm(formula = igg ~ I(oxygen - mean(exerimmun$oxygen)) + I((oxygen - ## mean(exerimmun$oxygen))^2), data = exerimmun) ## ## Residuals: ## Min 1Q Median 3Q Max ## -185.375 -82.129 1.047 66.007 227.377 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1632.1962 29.3486 55.61 &lt; 2e-16 ## I(oxygen - mean(exerimmun$oxygen)) 33.9995 1.6890 20.13 &lt; 2e-16 ## I((oxygen - mean(exerimmun$oxygen))^2) -0.5362 0.1582 -3.39 0.00217 ## ## (Intercept) *** ## I(oxygen - mean(exerimmun$oxygen)) *** ## I((oxygen - mean(exerimmun$oxygen))^2) ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 106.4 on 27 degrees of freedom ## Multiple R-squared: 0.9377, Adjusted R-squared: 0.9331 ## F-statistic: 203.2 on 2 and 27 DF, p-value: &lt; 2.2e-16 The estimated intercept coefficient \\(\\hat{\\beta}_0 = 1632\\) means a person whose maximal oxygen uptake is \\(50.64\\) ml/kg (the mean value) is predicted to have \\(1632\\) mg of immunoglobin in his blood. The estimated coefficient \\(\\hat{\\beta}_1 = 34.0\\) means a person whose maximal oxygen uptake is near \\(50.64\\) ml/kg is predicted to increase by 34.0 mg for every 1 ml/kg increase in maximal oxygen uptake. By performing all transformations in the model, it is straightforward to perform predictions. Here is the predicted value of immunoglobin when maximal oxygen uptake = 90.00 ml/kg. predict(m_blood, newdata = data.frame(oxygen = 90), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 2139.632 1639.597 2639.666 1.3.3 Normality A normal probability plot or a normal quantile plot should have values near the line with no bow-shaped deviations. A histogram should be normally distributed. A residuals vs fits plot \\((\\epsilon \\sim \\hat{Y})\\) should be randomly scattered around 0. Sometimes the normality check fails when linearity assumption does not hold, so check for linearity first. Parameter estimation is not sensitive to this condition, but prediction intervals are. 1.3.4 Equal Variances The residuals should be the same size at both low and high values of the response variable. A residuals vs fits plot \\((\\epsilon \\sim \\hat{Y})\\) should have random scatter in a band of constant width around 0, and no fan shape at the low and high ends. All tests and intervals are sensitive to this condition. 1.4 Prediction The standard error in the expected value of \\(\\hat{y}\\) at some new set of predictors \\(X_n\\) is \\[SE(\\mu_\\hat{y}) = \\sqrt{\\hat{\\sigma}^2 (X_n (X&#39;X)^{-1} X_n&#39;)}.\\] The standard error increases the further \\(X_n\\) is from \\(\\bar{X}\\). If \\(X_n = \\bar{X}\\), the equation reduces to \\(SE(\\mu_\\hat{y}) = \\sigma / \\sqrt{n}\\). If \\(n\\) is large, or the predictor values are spread out, \\(SE(\\mu_\\hat{y})\\) will be relatively small. The \\((1 - \\alpha)\\%\\) confidence interval is \\(\\hat{y} \\pm t_{\\alpha / 2} SE(\\mu_\\hat{y})\\). The standard error in the predicted value of \\(\\hat{y}\\) at some \\(X_{new}\\) is \\[SE(\\hat{y}) = SE(\\mu_\\hat{y})^2 + \\sqrt{\\hat{\\sigma}^2}.\\] Notice the standard error for a predicted value is always greater than the standard error of the expected value. The \\((1 - \\alpha)\\%\\) prediction interval is \\(\\hat{y} \\pm t_{\\alpha / 2} SE(\\hat{y})\\). 1.4.0.1 Example What is the expected value of mpg if the predictor values equal their mean values? R performs this calucation with the predict() function with parameter interval = confidence. m &lt;-lm(mpg ~ ., data = d[,1:9]) X_new &lt;- data.frame(Const = 1, cyl = factor(round(mean(as.numeric(as.character(d$cyl))),0), levels = levels(d$cyl)), disp = mean(d$disp), hp = mean(d$hp), drat = mean(d$drat), wt = mean(d$wt), qsec = mean(d$qsec), vs = factor(&quot;S&quot;, levels = levels(d$vs)), am = factor(&quot;manual&quot;, levels = levels(d$am))) predict.lm(object = m, newdata = X_new, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 21.21748 17.4461 24.98886 You can verify this by manually calculating \\(SE(\\mu_\\hat{y}) = \\sqrt{\\hat{\\sigma}^2 (X_{new} (X&#39;X)^{-1} X_{new}&#39;)}\\) using matrix algebra. X2 &lt;- lapply(data.frame(model.matrix(m)), mean) %&gt;% unlist() %&gt;% t() X2[2] &lt;- contr.poly(3)[2,1] # cyl linear X2[3] &lt;- contr.poly(3)[2,2] # cyl quadratic X2[9] &lt;- 1 X2[10] &lt;- 1 y_exp &lt;- sum(m$coefficients * as.numeric(X2)) se_y_exp &lt;- as.numeric(sqrt(rse^2 * X2 %*% solve(t(X) %*% X) %*% t(X2))) t_crit &lt;- qt(p = .05 / 2, df = n - k - 1, lower.tail = FALSE) me &lt;- t_crit * se_y_exp cat(&quot;fit: &quot;, round(y_exp, 6), &quot;, 95% CI: (&quot;, round(y_exp - me, 6), &quot;, &quot;, round(y_exp + me, 6), &quot;)&quot;) ## fit: 21.21748 , 95% CI: ( 17.4461 , 24.98886 ) 1.4.0.2 Example What is the predicted value of mpg if the predictor values equal their mean values? R performs this calucation with the predict() with parameter interval = prediction. predict.lm(object = m, newdata = X_new, interval = &quot;prediction&quot;) ## fit lwr upr ## 1 21.21748 14.78304 27.65191 se_y_hat &lt;- sqrt(rse^2 + se_y_exp^2) me &lt;- t_crit * se_y_hat cat(&quot;fit: &quot;, round(y_exp, 6), &quot;, 95% CI: (&quot;, round(y_exp - me, 6), &quot;, &quot;, round(y_exp + me, 6), &quot;)&quot;) ## fit: 21.21748 , 95% CI: ( 14.78304 , 27.65191 ) 1.5 Inference Draw conclusions about the significance of the coefficient estimates with the t-test and/or F-test. 1.5.1 t-Test By assumption, the residuals are normally distributed, so the Z-test statistic could evaluate the parameter estimators, \\[Z = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\sigma^2 (X&#39;X)^{-1}}}\\] where \\(\\beta_0\\) is the null-hypothesized value, usually 0. \\(\\sigma\\) is unknown, but \\(\\frac{\\hat{\\sigma}^2 (n - k)}{\\sigma^2} \\sim \\chi^2\\). The ratio of the normal distribution divided by the adjusted chi-square \\(\\sqrt{\\chi^2 / (n - k)}\\) is t-distributed, \\[t = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\hat{\\sigma}^2 (X&#39;X)^{-1}}} = \\frac{\\hat{\\beta} - \\beta_0}{SE(\\hat{\\beta})}\\] The \\((1 - \\alpha)\\) confidence intervals are \\(CI = \\hat{\\beta} \\pm t_{\\alpha / 2, df} SE(\\hat{\\beta})\\) with p-value equaling the probability of measuring a \\(t\\) of that extreme, \\(p = P(t &gt; |t|)\\). For a one-tail test, divide the reported p-value by two. The \\(SE(\\hat{\\beta})\\) decreases with 1) a better fitting regression line (smaller \\(\\hat{\\sigma}^2\\)), 2) greater variation in the predictor (larger \\(X&#39;X\\)), and 3) larger sample size (larger n). 1.5.1.1 Example Define a 95% confidence interval around the slope parameters. The summary() output shows the t values and probabilities in the t value and Pr(&gt;|t|) columns. You can verify this manually using matrix algebra for \\(t = \\frac{(\\hat{\\beta} - \\beta_1)}{SE(\\hat{\\beta})}\\) with \\(\\beta_1 = 0\\). The \\((1 - \\alpha)\\) confidence interval is \\(CI = \\hat{\\beta} \\pm t_{\\alpha / 2, df} SE(\\hat{\\beta})\\). The table below gathers the parameter estimators and t-test results. t &lt;- beta_hat / se_beta_hat p_value &lt;- pt(q = abs(t), df = n - k - 1, lower.tail = FALSE) * 2 t_crit &lt;- qt(p = .05 / 2, df = n - k - 1, lower.tail = FALSE) lcl = beta_hat - t_crit * se_beta_hat ucl = beta_hat + t_crit * se_beta_hat data.frame(beta = round(beta_hat, 4), se = round(se_beta_hat, 4), t = round(t, 4), p = round(p_value, 4), lcl = round(lcl,4), ucl = round(ucl, 4)) ## beta se t p lcl ucl ## (Intercept) 19.5410 14.1464 1.3813 0.1810 -9.7969 48.8789 ## cyl.L 0.3426 2.7648 0.1239 0.9025 -5.3914 6.0765 ## cyl.Q 1.3884 1.1121 1.2485 0.2250 -0.9179 3.6948 ## disp 0.0067 0.0135 0.4950 0.6255 -0.0213 0.0347 ## hp -0.0291 0.0172 -1.6960 0.1040 -0.0648 0.0065 ## drat 0.5881 1.5031 0.3912 0.6994 -2.5292 3.7053 ## wt -3.1552 1.4202 -2.2216 0.0369 -6.1006 -0.2099 ## qsec 0.5232 0.6901 0.7582 0.4564 -0.9080 1.9545 ## vsS 1.2378 2.1061 0.5877 0.5627 -3.1299 5.6055 ## ammanual 3.0009 1.8534 1.6191 0.1197 -0.8428 6.8446 1.5.2 F-Test The F-test for the model is a test of the null hypothesis that none of the independent variables linearly predict the dependent variable, that is, the model parameters are jointly zero: \\(H_0 : \\beta_1 = \\ldots = \\beta_k = 0\\). The regression mean sum of squares \\(MSR = \\frac{(\\hat{y} - \\bar{y})&#39;(\\hat{y} - \\bar{y})}{k-1}\\) and the error mean sum of squares \\(MSE = \\frac{\\hat{\\epsilon}&#39;\\hat{\\epsilon}}{n-k}\\) are each chi-square variables. Their ratio has an F distribution with \\(k - 1\\) numerator degrees of freedom and \\(n - k\\) denominator degrees of freedom. The F statistic can also be expressed in terms of the coefficient of correlation \\(R^2 = \\frac{MSR}{MST}\\). \\[F(k - 1, n - k) = \\frac{MSR}{MSE} = \\frac{R^2}{1 - R^2} \\frac{n-k}{k-1}\\] MSE is \\(\\sigma^2\\). If \\(H_0\\) is true, that is, there is no relationship between the predictors and the response, then \\(MSR\\) is also equal to \\(\\sigma^2\\), so \\(F = 1\\). As \\(R^2 \\rightarrow 1\\), \\(F \\rightarrow \\infty\\), and as \\(R^2 \\rightarrow 0\\), \\(F \\rightarrow 0\\). F increases with \\(n\\) and decreases with \\(k\\). 1.5.2.1 Example What is the probability that all parameters are jointly equal to zero? The F-statistic is presented at the bottom of the summary() function. You can verify this manually. ssr &lt;- sum((m$fitted.values - mean(d$mpg))^2) sse &lt;- sum(m$residuals^2) sst &lt;- sum((m$mpg - mean(d$mpg))^2) msr &lt;- ssr / k mse &lt;- sse / (n - k - 1) f = msr / mse p_value &lt;- pf(q = f, df1 = k, df2 = n - k - 1, lower.tail = FALSE) cat(&quot;F-statistic: &quot;, round(f, 4), &quot; on 3 and 65 DF, p-value: &quot;, p_value) ## F-statistic: 17.3549 on 3 and 65 DF, p-value: 4.814183e-08 There is sufficient evidence \\((F = 17.35, P &lt; .0001)\\) to reject \\(H_0\\) that the parameter estimators are jointly equal to zero. The aov function calculates the sequential sum of squares. The regression sum of squares SSR for mpg ~ cyl is 824.8. Adding disp to the model increases SSR by 57.6. Adding hp to the model increases SSR by 18.5. It would seem that hp does not improve the model. summary(aov(m)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl 2 824.8 412.4 65.260 5.62e-10 *** ## disp 1 57.6 57.6 9.122 0.00629 ** ## hp 1 18.5 18.5 2.928 0.10112 ## drat 1 11.9 11.9 1.885 0.18355 ## wt 1 55.8 55.8 8.828 0.00705 ** ## qsec 1 1.5 1.5 0.241 0.62816 ## vs 1 0.3 0.3 0.048 0.82894 ## am 1 16.6 16.6 2.622 0.11967 ## Residuals 22 139.0 6.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Order matters. Had we started with disp, then added hp we would find both estimators were significant. summary(aov(lm(mpg ~ disp + hp + drat + wt + qsec + vs + am + cyl, data = d))) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## disp 1 808.9 808.9 128.004 1.22e-10 *** ## hp 1 33.7 33.7 5.327 0.03078 * ## drat 1 30.1 30.1 4.771 0.03989 * ## wt 1 70.5 70.5 11.158 0.00296 ** ## qsec 1 12.7 12.7 2.011 0.17017 ## vs 1 0.2 0.2 0.035 0.85231 ## am 1 20.5 20.5 3.237 0.08571 . ## cyl 2 10.4 5.2 0.825 0.45141 ## Residuals 22 139.0 6.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 1.6 Interpretation A plot of the standardized coefficients shows the relative importance of each variable. The distance the coefficients are from zero shows how much a change in a standard deviation of the regressor changes the mean of the predicted value. The CI shows the precision. The plot shows not only which variables are significant, but also which are important. d_sc &lt;- d %&gt;% mutate_at(c(&quot;mpg&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;drat&quot;, &quot;wt&quot;, &quot;qsec&quot;), scale) m_sc &lt;- lm(mpg ~ ., d_sc[,1:9]) lm_summary &lt;- summary(m_sc)$coefficients df &lt;- data.frame(Features = rownames(lm_summary), Estimate = lm_summary[,&#39;Estimate&#39;], std_error = lm_summary[,&#39;Std. Error&#39;]) df$lower = df$Estimate - qt(.05/2, m_sc$df.residual) * df$std_error df$upper = df$Estimate + qt(.05/2, m_sc$df.residual) * df$std_error df &lt;- df[df$Features != &quot;(Intercept)&quot;,] ggplot(df) + geom_vline(xintercept = 0, linetype = 4) + geom_point(aes(x = Estimate, y = Features)) + geom_segment(aes(y = Features, yend = Features, x=lower, xend=upper), arrow = arrow(angle=90, ends=&#39;both&#39;, length = unit(0.1, &#39;cm&#39;))) + scale_x_continuous(&quot;Standardized Weight&quot;) + labs(title = &quot;Model Feature Importance&quot;) The added variable plot shows the bivariate relationship between \\(Y\\) and \\(X_i\\) after accounting for the other variables. For example, the partial regression plots of y ~ x1 + x2 + x3 would plot the residuals of y ~ x2 + x3 vs x1, and so on. library(car) avPlots(m) 1.7 Model Validation Evaluate predictive accuracy by training the model on a training data set and testing on a test data set. 1.7.1 Accuracy Metrics The most common measures of model fit are R-squared, RMSE, RSE, MAE, Adjusted R-squared, AIC, AICc, BIC, and Mallow’s Cp. 1.7.1.1 R-Squared The coefficient of determination (R-squared) is the percent of total variation in the response variable that is explained by the regression line. \\[R^2 = \\frac{RSS}{SST} = 1 - \\frac{SSE}{SST}\\] where \\(SSE = \\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}\\) is the sum squared differences between the predicted and observed value, \\(SST = \\sum_{i = 1}^n{(y_i - \\bar{y})^2}\\) is the sum of squared differences between the observed and overall mean value, and \\(RSS = \\sum_{i=1}^n{(\\hat{y}_i - \\bar{y})^2}\\) is the sum of squared differences between the predicted and overall mean “no-relationship line” value. At the extremes, \\(R^2 = 1\\) means all data points fall perfectly on the regression line - the predictors account for all variation in the response; \\(R^2 = 0\\) means the regression line is horizontal at \\(\\bar{y}\\) - the predictors account for none of the variation in the response. In the simple case of a single predictor variable, \\(R^2\\) equals the squared correlation between \\(x\\) and \\(y\\), \\(Cor(x,y)\\). ssr &lt;- sum((m$fitted.values - mean(d$mpg))^2) sse &lt;- sum(m$residuals^2) sst &lt;- sum((d$mpg - mean(d$mpg))^2) (r2 &lt;- ssr / sst) ## [1] 0.8765389 (r2 &lt;- 1 - sse / sst) ## [1] 0.8765389 (r2 &lt;- summary(m)$r.squared) ## [1] 0.8765389 The sums of squares are the same thing as the variances multiplied by the degrees of freedom. ssr2 &lt;- var(fitted(m)) * (n - 1) sse2 &lt;- var(residuals(m)) * (n - 1) sst2 &lt;- var(d$mpg) * (n - 1) ssr2 / sst2 ## [1] 0.8765389 \\(R^2\\) is also equal to the correlation between the fitted value and observed values, \\(R^2 = Cor(Y, \\hat{Y})^2\\). cor(m$fitted.values, d$mpg)^2 ## [1] 0.8765389 R-squared is proportional to the the variance in the response, SST. Given a constant percentage error in predictions, a test set with relatively low variation in the reponse will have a lower R-squared. Conversely, test sets with large variation, e.g., housing data with home sale ranging from $60K to $2M may have a large R-squared despite average prediction errors of &gt;$10K. A close variant of R-squared is the non-parametric Spearman’s rank correlation. This statistic is the correlation of the ranks of the response and the predicted values. It is used when the model goal is ranking. 1.7.1.2 RMSE The root mean squared error (RMSE) is the average prediction error (square root of mean squared error). \\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}}{n}}\\] sqrt(mean((d$mpg - m$fitted.values)^2)) ## [1] 2.084339 The rmse() function from the Metrics package, and the postResample() function in caret calculate RMSE. rmse(actual = d$mpg, predicted = m$fitted.values) ## [1] 2.084339 postResample(pred = m$fitted.values, obs = d$mpg)[1] ## RMSE ## 2.084339 The mean squared error of a model with theoretical residual of mean zero and constant variance \\(\\sigma^2\\) can be decomposed into the model’s bias and the model’s variance: \\[E[MSE] = \\sigma^2 + Bias^2 + Var.\\] A model that predicts the response closely will have low bias, but be relatively sensitive to the training data and thus have high variance. A model that predicts the response conservatively (e.g., a simple mean) will have large bias, but be relatively insensitive to nuances in the training data. Here is an example of a simulated sine wave. A model predicting the mean value at the upper and lower levels has low variance, but high bias, and a model of an actual sine wave has low bias and high variance. This is referred to as the variance-bias trade-off. 1.7.1.3 RSE The residual standard error (RSE, or model sigma \\(\\hat{\\sigma}\\)) is an estimate of the standard deviation of \\(\\epsilon\\). It is roughly the average amount the response deviates from the true regression line. \\[\\sigma = \\sqrt{\\frac{\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}}{n-k-1}}\\] sqrt(sum((d$mpg - m$fitted.values)^2) / (n - k - 1)) ## [1] 2.513808 # sd is sqrt(sse / (n-1)), sigma = sqrt(sse / (n - k - 1)) sd(m$residuals) * sqrt((n - 1) / (n - k - 1)) ## [1] 2.513808 summary(m)$sigma ## [1] 2.513808 sigma(m) ## [1] 2.513808 1.7.1.4 MAE The mean absolute error (MAE) is the average absolute prediction arror. It is less sensitive to outliers. \\[MAE = \\frac{\\sum_{i=1}^n{|y_i - \\hat{y}_i|}}{n}\\] sum(abs(d$mpg - m$fitted.values)) / n ## [1] 1.704941 The postResample() function in caret conveniently calculates all three. postResample(pred = m$fitted.values, obs = d$mpg) ## RMSE Rsquared MAE ## 2.0843393 0.8765389 1.7049409 defaultSummary(data = data.frame(obs = d$mpg, pred = m$fitted.values), model = m) ## RMSE Rsquared MAE ## 2.0843393 0.8765389 1.7049409 apply(as.matrix(m$fitted.values), 2, postResample, obs = d$mpg) ## [,1] ## RMSE 2.0843393 ## Rsquared 0.8765389 ## MAE 1.7049409 These metrics are good for evaluating a model, but less useful for comparing models. The problem is that they tend to improve with additional variables added to the model, even if the improvement is not significant. The following metrics aid model comparison by penalizing added variables. 1.7.1.5 Adjusted R-squared The adjusted R-squared (\\(\\bar{R}^2\\)) penalizes the R-squared metric for increasing number of predictors. \\[\\bar{R}^2 = 1 - \\frac{SSE}{SST} \\cdot \\frac{n-1}{n-k-1}\\] (adj_r2 &lt;- 1 - sse/sst * (n - 1) / (n - k - 1)) ## [1] 0.8260321 summary(m)$adj.r.squared ## [1] 0.8260321 1.7.1.6 AIC Akaike’s Information Criteria (AIC) is a penalization metric. The lower the AIC, the better the model. AIC(m) ## [1] 159.817 1.7.1.7 AICc AICc corrects AIC for small sample sizes. AIC(m) + (2 * k * (k + 1)) / (n - k - 1) ## [1] 167.9988 1.7.1.8 BIC The Basiean information criteria (BIC) is like AIC, but with a stronger penalty for additional variables. BIC(m) ## [1] 175.9401 1.7.1.9 Mallows Cp Mallows Cp is a variant of AIC. 1.7.1.9.1 Example Compare the full model to a model without cyl. The glance() function from the broom package calculates many validation metrics. Here are the validation stats for the full model and then the reduced model. library(broom) glance(m) %&gt;% select(adj.r.squared, sigma, AIC, BIC, p.value) ## # A tibble: 1 × 5 ## adj.r.squared sigma AIC BIC p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.826 2.51 160. 176. 0.0000000481 glance(lm(mpg ~ . - cyl, d[, 1:9])) %&gt;% select(adj.r.squared, sigma, AIC, BIC, p.value) ## # A tibble: 1 × 5 ## adj.r.squared sigma AIC BIC p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.829 2.50 158. 171. 0.00000000453 The ajusted R2 increased and AIC and BIC decreased, meaning the full model is less efficient at explaining the variability in the response value. The residual standard error sigma is smaller for the reduced model. Finally, the F statistic p-value is smaller for the reduced model, meaning the reduced model is statistically more significant. Note that these regression metrics are all internal measures, that is they have been computed on the training dataset, not the test dataset. 1.7.2 Cross-Validation Cross-validation is a set of methods for measuring the performance of a predictive model on a test dataset. The main measures of prediction performance are R2, RMSE and MAE. 1.7.2.1 Validation Set To perform validation set cross validation, randomly split the data into a training data set and a test data set. Fit models to the training data set, then predict values with the validation set. The model that produces the best prediction performance is the preferred model. The caret package provides useful methods for cross-validation. 1.7.2.1.1 Example library(caret) set.seed(123) train_idx &lt;- createDataPartition(y = d$mpg, p = 0.80, list = FALSE) d.train &lt;- d[train_idx, ] d.test &lt;- d[-train_idx, ] Build the model using d.train, make predictions, then calculate the R2, RMSE, and MAE. Use the train() function from the caret package. Use method = \"none\" to simply fit the model to the entire data set. set.seed(123) m1 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;none&quot;)) print(m1) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: None postResample(pred = predict(m1, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.1026305 0.9567828 2.4543559 The validation set method is only useful when you have a large data set to partition. A second disadvantage is that building a model on a fraction of the data leaves out information. The test error will vary with which observations are included in the training set. 1.7.2.2 LOOCV Leave one out cross validation (LOOCV) works by successively modeling with training sets leaving out one data point, then averaging the prediction errors. set.seed(123) m2 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;LOOCV&quot;)) print(m2) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 27, 27, 27, 27, 27, 27, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 2.779283 0.758735 2.317904 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m2, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.1026305 0.9567828 2.4543559 This method isn’t perfect either. It repeats as many times as there are data points, so the execution time may be long. LOOCV is also sensitive to outliers. 1.7.2.3 K-fold Cross-Validation K-fold cross-validation splits the dataset into k folds (subsets), then uses k-1 of the folds for a training set and the remaining fold for a test set, then repeats for all permutations of k taken k-1 at a time. E.g., 3-fold cross-validation will partition the data into sets A, B, and C, then create train/test splits of [AB, C], [AC, B], and [BC, A]. K-fold cross-validation is less computationally expensive than LOOCV, and often yields more accurate test error rate estimates. What is the right value of k? The lower is k the more biased the estimates; the higher is k the larger the estimate variability. At the extremes k = 2 is the validation set method, and k = n is the LOOCV method. In practice, one typically performs k-fold cross-validation using k = 5 or k = 10 because these values have been empirically shown to balence bias and variance. set.seed(123) m3 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5)) print(m3) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 22, 22, 23, 22, 23 ## Resampling results: ## ## RMSE Rsquared MAE ## 2.956977 0.8523226 2.591746 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m3, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.1026305 0.9567828 2.4543559 1.7.2.4 Repeated K-fold CV You can also perform k-fold cross-validation multiple times and average the results. Specify method = \"repeatedcv\" and repeats = 3 in the trainControl object for three repeats. set.seed(123) m4 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3)) print(m4) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 3 times) ## Summary of sample sizes: 22, 22, 23, 22, 23, 23, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 3.070563 0.8133672 2.7155 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m4, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.1026305 0.9567828 2.4543559 1.7.2.5 Bootstrapping Bootstrapping randomly selects a sample of n observations with replacement from the original dataset to evaluate the model. The procedure is repeated many times. Specify method = \"boot\" and number = 100 to perform 100 bootstrap samples. set.seed(123) m5 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;boot&quot;, number = 100)) ## Warning in predict.lm(modelFit, newdata): prediction from rank-deficient fit; ## attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(modelFit, newdata): prediction from rank-deficient fit; ## attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(modelFit, newdata): prediction from rank-deficient fit; ## attr(*, &quot;non-estim&quot;) has doubtful cases print(m5) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Bootstrapped (100 reps) ## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 3.872687 0.6362661 3.235582 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m5, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.1026305 0.9567828 2.4543559 1.7.3 Gain Curve For supervised learning purposes, a visual way to evaluate a regression model is with the gain curve. This visualization compares a predictive model score to an actual outcome (either binary (0/1) or continuous). The gain curve plot measures how well the model score sorts the data compared to the true outcome value. The x-axis is the fraction of items seen when sorted by score, and the y-axis is the cumulative summed true outcome when sorted by score. For comparison, GainCurvePlot also plots the “wizard curve”: the gain curve when the data is sorted according to its true outcome. A relative Gini score close to 1 means the model sorts responses well. library(WVPlots) d$fitted &lt;- m$fitted.values GainCurvePlot(d, xvar = &quot;fitted&quot;, truthVar = &quot;mpg&quot;, title = &quot;Model Gain Curve&quot;) 1.8 OLS Reference Penn State University, STAT 501, Lesson 12: Multicollinearity &amp; Other Regression Pitfalls. https://newonlinecourses.science.psu.edu/stat501/lesson/12. STHDA. Bootstrap Resampling Essentials in R. http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/ Molnar, Christoph. “Interpretable machine learning. A Guide for Making Black Box Models Explainable”, 2019. https://christophm.github.io/interpretable-ml-book/. "],["generalized-linear-models-glm.html", "Chapter 2 Generalized Linear Models (GLM) 2.1 Binomial Logistic Regression Case Study 1 2.2 Multinomial Logistic Regression Case Study 2 2.3 Ordinal Logistic Regression Case Study 3 2.4 Poisson Regression Case Study 4", " Chapter 2 Generalized Linear Models (GLM) The linear regression model, \\(E(y|X) = X \\beta\\), structured as \\(y_i = X_i \\beta + \\epsilon_i\\) where \\(X_i \\beta = \\mu_i\\), assumes the response is a linear function of the predictors and the residuals are independent random variables normally distributed with zero mean and constant variance, \\(\\epsilon \\sim N \\left(0, \\sigma^2 \\right)\\). This implies that given a set of predictors, the response is normally distributed about its expected value, \\(y_i \\sim N \\left(\\mu_i, \\sigma^2 \\right)\\). However, there are many situations where this normality assumption does not hold. Generalized linear models (GLMs) are a generalization of the linear regression model that work with non-normal response distributions.1 The response will not have a normal distribution if the underlying data-generating process is binomial (Section 2.1) or multinomial (Section 2.2), ordinal (Section 2.3), Poisson (counts, Section 2.4), or exponential (time-to-event). In these situations the linear regression model can predict probabilities and proportions outside [0, 1], or negative counts or times. GLMs solve this problem by modeling a function of the expected value of \\(y\\), \\(f(E(y|X)) = X \\beta\\). There are three components to a GLM: a random component specified by the response variable’s probability distribution (normal, binomial, etc.); a systematic component in the form \\(X\\beta\\); and a link function, \\(\\eta\\), that specifies the link between the random and systematic components and converts the response to a range of \\([-\\infty, +\\infty]\\). Linear regression is a special case of GLM where the link function is the identity function, \\(f(E(y|X)) = E(y|X)\\). For binary logistic regression, the link function is the logit, \\[f(E(y|X)) = \\log \\left( \\frac{E(y|X)}{1 - E(y|X)} \\right) = \\log \\left( \\frac{\\pi}{1 - \\pi} \\right) = \\mathrm{logit}(\\pi)\\] where \\(\\pi\\) is the response probability.2 For multinomial logistic regression, the link function is the logit again, but with respect to the baseline level, and there is set of logits (one for each non-baseline level), \\[f(E(y|X)) = \\log \\left( \\frac{E(y_j|X)}{E(y_{j^*}|X)} \\right) = \\log \\left( \\frac{\\pi_j}{\\pi_{j^*}} \\right) = \\mathrm{logit}(\\pi_j)\\] Where \\(j\\) is a level in the dependent variable and \\(j^*\\) is the baseline level. For Poisson regression, the link function is \\[f(E(y|X)) = \\ln (E(y|X)) = \\ln(\\lambda)\\] where \\(\\lambda\\) is the expected event rate. For exponential regression, the link function is \\[f(E(y|X) = -E(y|X) = -\\lambda\\] where \\(\\lambda\\) is the expected time to event. GLM uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations. Fit a GLM just like an linear model, but with the glm() function, specifying the distribution with the family = c(\"gaussian\", \"binomial\", \"poisson\") parameter. Fit a mulinomial logistic regression model with nnet::multinom(). 2.1 Binomial Logistic Regression Logistic regression estimates the probability that a categorical dependent variable is a particular level. The dependent variable levels can be binomial, multinomial, or ordinal. The binary logistic regression model is \\[y_i = \\mathrm{logit}(\\pi_i) = \\log \\left( \\frac{\\pi_i}{1 - \\pi_i} \\right) = X_i \\beta\\] where \\(\\pi_i\\) is the response \\(i\\)’s binary level probability. The model predicts the log odds of the level. Why do this? The range of outcomes need to be between 0 and 1, and a sigmoid function, \\(f(y) = 1 / \\left(1 + e^y \\right)\\), does that. If the log odds of the level equals \\(X_i\\beta\\), then the odds of the level equals \\(e^{X\\beta}\\). You can solve for \\(\\pi_i\\) to get \\(\\pi = \\mathrm{odds} / (\\mathrm{odds} + 1)\\). Substituting, \\[\\pi_i = \\frac{\\exp(y_i)}{1 + \\exp(y_i)} = \\frac{e^{X_i\\beta}}{1 + e^{X_i\\beta}}\\] which you can simplify to \\(\\pi_i = 1 / (1 + e^{-X_i\\beta})\\), a sigmoid function. The upshot is \\(X\\beta\\) is the functional relationship between the independent variables and a function of the response, not the response itself. The model parameters are estimated either by iteratively reweighted least squares optimization or by maximum likelihood estimation (MLE). MLE maximizes the probability produced by a set of parameters \\(\\beta\\) given a data set and probability distribution.3 In logistic regression the probability distribution is the binomial and each observation is the outcome of a single Bernoulli trial. \\[L(\\beta; y, X) = \\prod_{i=1}^n \\pi_i^{y_i}(1 - \\pi_i)^{(1-y_i)} = \\prod_{i=1}^n\\frac{\\exp(y_i X_i \\beta)}{1 + \\exp(X_i \\beta)}.\\] In practice, multiplying many small probabilities can be unstable, so MLE optimizes the log likelihood instead. \\[\\begin{align} l(\\beta; y, X) &amp;= \\sum_{i = 1}^n \\left(y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i) \\right) \\\\ &amp;= \\sum_{i = 1}^n \\left(y_i X_i \\beta - \\log(1 + e^{X_i\\beta}) \\right) \\end{align}\\] Sometimes you will see the cost function optimized. The cost function is the negative of of the log likelihood function. Assumptions The binomial logistic regression model requires a dichotomous dependent variable and independent observations. The sample size should be large, at least 10 observations per dependent variable level and independent variable. There are three conditions related to the data distribution: i) the logit transformation must be linearly related to any continuous independent variables, ii) there must be no multicollinearity, and iii) there must be no influential outliers. Case Study 1 This case study uses the Laerd Statistics article on binomial logistic regression data set. cs1 &lt;- list() cs1$dat &lt;- read.spss(&quot;./input/logistic-regression.sav&quot;, to.data.frame = TRUE) An analysis investigates the relationship between the incidence of heart disease (Yes|No) and age, weight, gender, and maximal aerobic capacity (maxVO2) using data from \\(n\\) = 100 participants. cs1$dat %&gt;% tbl_summary( by = heart_disease, include = -caseno, percent = &quot;row&quot;, statistic = list(all_continuous() ~ &quot;{mean}, {sd}&quot;) ) #ubrqnaoijg table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #ubrqnaoijg thead, #ubrqnaoijg tbody, #ubrqnaoijg tfoot, #ubrqnaoijg tr, #ubrqnaoijg td, #ubrqnaoijg th { border-style: none; } #ubrqnaoijg p { margin: 0; padding: 0; } #ubrqnaoijg .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ubrqnaoijg .gt_caption { padding-top: 4px; padding-bottom: 4px; } #ubrqnaoijg .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ubrqnaoijg .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #ubrqnaoijg .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ubrqnaoijg .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ubrqnaoijg .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ubrqnaoijg .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ubrqnaoijg .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ubrqnaoijg .gt_column_spanner_outer:first-child { padding-left: 0; } #ubrqnaoijg .gt_column_spanner_outer:last-child { padding-right: 0; } #ubrqnaoijg .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ubrqnaoijg .gt_spanner_row { border-bottom-style: hidden; } #ubrqnaoijg .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #ubrqnaoijg .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ubrqnaoijg .gt_from_md > :first-child { margin-top: 0; } #ubrqnaoijg .gt_from_md > :last-child { margin-bottom: 0; } #ubrqnaoijg .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ubrqnaoijg .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #ubrqnaoijg .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #ubrqnaoijg .gt_row_group_first td { border-top-width: 2px; } #ubrqnaoijg .gt_row_group_first th { border-top-width: 2px; } #ubrqnaoijg .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ubrqnaoijg .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #ubrqnaoijg .gt_first_summary_row.thick { border-top-width: 2px; } #ubrqnaoijg .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ubrqnaoijg .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ubrqnaoijg .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ubrqnaoijg .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #ubrqnaoijg .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ubrqnaoijg .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ubrqnaoijg .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ubrqnaoijg .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ubrqnaoijg .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ubrqnaoijg .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ubrqnaoijg .gt_left { text-align: left; } #ubrqnaoijg .gt_center { text-align: center; } #ubrqnaoijg .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ubrqnaoijg .gt_font_normal { font-weight: normal; } #ubrqnaoijg .gt_font_bold { font-weight: bold; } #ubrqnaoijg .gt_font_italic { font-style: italic; } #ubrqnaoijg .gt_super { font-size: 65%; } #ubrqnaoijg .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #ubrqnaoijg .gt_asterisk { font-size: 100%; vertical-align: 0; } #ubrqnaoijg .gt_indent_1 { text-indent: 5px; } #ubrqnaoijg .gt_indent_2 { text-indent: 10px; } #ubrqnaoijg .gt_indent_3 { text-indent: 15px; } #ubrqnaoijg .gt_indent_4 { text-indent: 20px; } #ubrqnaoijg .gt_indent_5 { text-indent: 25px; } Characteristic No, N = 651 Yes, N = 351 age 39, 8 45, 9 weight 77, 14 85, 15 gender     Female 29 (78%) 8 (22%)     Male 36 (57%) 27 (43%) VO2max 45, 9 41, 6 1 Mean, SD; n (%) A brief exploratory analysis helps set your expectations. Men are twice as likely to have heart disease. The odds ratio is (.43/.57) / (.22/.78) = 2.7. cs1$dat %&gt;% tbl_cross(row = heart_disease, col = gender, percent = &quot;col&quot;) #oyyjzosuql table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #oyyjzosuql thead, #oyyjzosuql tbody, #oyyjzosuql tfoot, #oyyjzosuql tr, #oyyjzosuql td, #oyyjzosuql th { border-style: none; } #oyyjzosuql p { margin: 0; padding: 0; } #oyyjzosuql .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #oyyjzosuql .gt_caption { padding-top: 4px; padding-bottom: 4px; } #oyyjzosuql .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #oyyjzosuql .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #oyyjzosuql .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #oyyjzosuql .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oyyjzosuql .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #oyyjzosuql .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #oyyjzosuql .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #oyyjzosuql .gt_column_spanner_outer:first-child { padding-left: 0; } #oyyjzosuql .gt_column_spanner_outer:last-child { padding-right: 0; } #oyyjzosuql .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #oyyjzosuql .gt_spanner_row { border-bottom-style: hidden; } #oyyjzosuql .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #oyyjzosuql .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #oyyjzosuql .gt_from_md > :first-child { margin-top: 0; } #oyyjzosuql .gt_from_md > :last-child { margin-bottom: 0; } #oyyjzosuql .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #oyyjzosuql .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #oyyjzosuql .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #oyyjzosuql .gt_row_group_first td { border-top-width: 2px; } #oyyjzosuql .gt_row_group_first th { border-top-width: 2px; } #oyyjzosuql .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #oyyjzosuql .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #oyyjzosuql .gt_first_summary_row.thick { border-top-width: 2px; } #oyyjzosuql .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oyyjzosuql .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #oyyjzosuql .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #oyyjzosuql .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #oyyjzosuql .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #oyyjzosuql .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oyyjzosuql .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #oyyjzosuql .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #oyyjzosuql .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #oyyjzosuql .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #oyyjzosuql .gt_left { text-align: left; } #oyyjzosuql .gt_center { text-align: center; } #oyyjzosuql .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #oyyjzosuql .gt_font_normal { font-weight: normal; } #oyyjzosuql .gt_font_bold { font-weight: bold; } #oyyjzosuql .gt_font_italic { font-style: italic; } #oyyjzosuql .gt_super { font-size: 65%; } #oyyjzosuql .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #oyyjzosuql .gt_asterisk { font-size: 100%; vertical-align: 0; } #oyyjzosuql .gt_indent_1 { text-indent: 5px; } #oyyjzosuql .gt_indent_2 { text-indent: 10px; } #oyyjzosuql .gt_indent_3 { text-indent: 15px; } #oyyjzosuql .gt_indent_4 { text-indent: 20px; } #oyyjzosuql .gt_indent_5 { text-indent: 25px; } gender Total Female Male heart_disease     No 29 (78%) 36 (57%) 65 (65%)     Yes 8 (22%) 27 (43%) 35 (35%) Total 37 (100%) 63 (100%) 100 (100%) Age, weight, and poor max aerobic capacity are positively associated with hear disease. cs1$dat %&gt;% pivot_longer(cols = c(age, weight, VO2max)) %&gt;% ggplot(aes(x = heart_disease, y = value)) + geom_boxplot(outlier.shape = NA) + geom_jitter(aes(color = name)) + facet_wrap(facets = vars(name), scales = &quot;free_y&quot;) To facilitate model interpretation, I’ll center the continuous variables around their means. Then the intercept term in the fitted model will represent a reasonable condition, not a zero-aged, zero-weighted person with no aerobic capacity. If you want to present your findings in the framework of a baseline probability (or odds) and the incremental effects of the independent variable, this is the way to go. You might even standardize the continuous vars to get a more meaningful increment. On the other hand, if you want to use your model for predicting outcomes, you’ll have to back out of the centering when you predict values. cs1$dat &lt;- cs1$dat %&gt;% mutate( age_c = as.numeric(scale(age, scale = FALSE)), weight_c = as.numeric(scale(weight, scale = FALSE)), VO2max_c = as.numeric(scale(VO2max, scale = FALSE)), gender = factor(gender, levels = c(&quot;Male&quot;, &quot;Female&quot;)) ) cs1$centers &lt;- list(age = mean(cs1$dat$age), weight = mean(cs1$dat$weight), VO2max = mean(cs1$dat$VO2max)) The Model Jump in and fit the model. cs1$fmla &lt;- formula(heart_disease ~ age_c + weight_c + VO2max_c + gender) cs1$fit &lt;- glm(cs1$fmla, data = cs1$dat, family = &quot;binomial&quot;) summary(cs1$fit) ## ## Call: ## glm(formula = cs1$fmla, family = &quot;binomial&quot;, data = cs1$dat) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.093483 0.356753 -0.262 0.79329 ## age_c 0.085098 0.028160 3.022 0.00251 ** ## weight_c 0.005727 0.022442 0.255 0.79858 ## VO2max_c -0.099024 0.047944 -2.065 0.03889 * ## genderFemale -1.949639 0.842413 -2.314 0.02065 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 129.49 on 99 degrees of freedom ## Residual deviance: 102.09 on 95 degrees of freedom ## AIC: 112.09 ## ## Number of Fisher Scoring iterations: 5 Let’s take a tour of the summary object. The top section presents the deviance residuals.4 Deviance residuals are one of several residuals you can calculate from a binary logistic regression. One is the raw residual, \\(\\epsilon_i = y_i - \\hat{p}_i\\), where \\(\\hat{p}_i\\) is the predicted probability. Another is the Pearson residual, \\(r_i = \\frac{\\epsilon_i}{\\sqrt{\\hat{p}_i(1 - \\hat{p}_i)}}\\), the raw residual rescaled by dividing by the estimated standard deviation of a binomial distribution with 1 trial5. A third is the standardized Pearson residual, \\(rs_i = r_i / \\sqrt{1 - \\hat{h}_i}\\), the Pearson residual adjusted for the leverage of the predictors using the hat-values. Hat-values measure the predictor distances from the mean. This residual is especially useful to evaluate model fit because if the model fits well, these residuals have a standard normal distribution. Finally, there are the deviance residuals, \\(d_i = \\mathrm{sign}(\\epsilon_i) \\left[ -2(y_i \\log \\hat{p}_i + (1 - y_i) \\log (1 - \\hat{p}_i)) \\right]^{.5}\\). Deviance Residuals measure how much the estimated probabilities differ from the observed proportions of success. You want deviance residuals to be evenly distributed (in absolute values, 1Q \\(\\approx\\) 3Q, min \\(\\approx\\) max). You also want the min and max to be &lt;3 because deviance residuals are roughly approximated by a standard normal distribution. # Raw residuals(cs1$fit, type = &quot;response&quot;) %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.7955 -0.2500 -0.1062 0.0000 0.3533 0.9181 # Pearson residuals(cs1$fit, type = &quot;pearson&quot;) %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.9721 -0.5774 -0.3447 -0.0187 0.7392 3.3486 # Standardized Pearson rstandard(cs1$fit, type = &quot;pearson&quot;) %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.17942 -0.59189 -0.35211 -0.02113 0.75930 3.39975 # Deviance residuals(cs1$fit, type = &quot;deviance&quot;) %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.78156 -0.75854 -0.47381 -0.08019 0.93363 2.23719 The next section is the coefficients table. summary(cs1$fit)$coefficients ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.093483020 0.35675295 -0.2620385 0.793291735 ## age_c 0.085098077 0.02816013 3.0219351 0.002511644 ## weight_c 0.005726835 0.02244169 0.2551874 0.798578389 ## VO2max_c -0.099023726 0.04794428 -2.0653917 0.038885956 ## genderFemale -1.949638795 0.84241271 -2.3143511 0.020648470 The “z value” is the Wald \\(z\\) statistic, \\(z = \\hat{\\beta} / SE(\\hat{\\beta})\\). Its square is the Wald chi-squared statistic. The p-value is the area to the right of \\(z^2\\) in the \\(\\chi_1^2\\) density curve: pchisq(summary(cs1$fit)$coefficients[, &quot;z value&quot;]^2, df = 1, lower.tail = FALSE) ## (Intercept) age_c weight_c VO2max_c genderFemale ## 0.793291735 0.002511644 0.798578389 0.038885956 0.020648470 Below the coefficients table, the “dispersion parameter” line refers to over-dispersion, a common issue with GLM. For a logistic regression, the response variable should be distributed \\(y_i \\sim \\mathrm{Bin}(n_i, \\pi_i)\\) with \\(\\mu_i = n_i \\pi_i\\) and \\(\\sigma^2 = \\pi (1 - \\pi)\\). Over-dispersion means the data shows evidence of variance greater than \\(\\sigma^2\\). Skipping to the last line, Fisher scoring is a method for ML estimation. Logistic regression uses interatively reweighted least squares to fit the model, so this line indicates whether the algorithm converged. Above Fisher, the null deviance is the likelihood ratio of the intercept-only model, \\(G^2\\) = 129.49. The residual deviance is the likelihood ratio after including the model covariates, \\(G^2\\) = 102.09. It is the sum of the squared deviance residuals discussed above. The residual deviance is distributed chi-squared and can be used to test whether the model differs from the saturated model (model with as many coefficients as observations, \\(G^2\\) = 0, df = 0) where \\(H_0\\) = no difference. The deviance test for lack of fit fails to reject the null hypothesis. # G^2 calculations residuals(cs1$fit, type = &quot;deviance&quot;) %&gt;% .^2 %&gt;% sum() ## [1] 102.0878 deviance(cs1$fit) ## [1] 102.0878 # df df.residual(cs1$fit) ## [1] 95 # G^2 is distributed chi-squared with df degrees of freedom pchisq(deviance(cs1$fit), df = df.residual(cs1$fit), lower.tail = FALSE) ## [1] 0.2911469 vcdExtra::LRstats(cs1$fit) ## Likelihood summary table: ## AIC BIC LR Chisq Df Pr(&gt;Chisq) ## cs1$fit 112.09 125.11 102.09 95 0.2911 These two deviances (null, residual) are shown in the ANOVA summary. An ANOVA table shows the change in deviance from successively adding each variable to the model. anova(cs1$fit) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: heart_disease ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev ## NULL 99 129.49 ## age_c 1 11.9074 98 117.58 ## weight_c 1 9.1820 97 108.40 ## VO2max_c 1 0.5045 96 107.89 ## gender 1 5.8076 95 102.09 Interpretation Start with the dependent variable. Plug in values for the predictor variables to get predictions. \\(\\hat{y}\\) is the log odds of a man of average age, height, and aerobic capacity having heart disease. newdata &lt;- list(age_c = 0, weight_c = 0, VO2max_c = 0, gender = &quot;Male&quot;) (my_log_odds &lt;- predict(cs1$fit, newdata = newdata) ) ## 1 ## -0.09348302 Exponentiate to get the odds, \\(\\exp (\\hat{y}) = \\frac{\\pi}{1 - \\pi}\\). (my_odds &lt;- exp(my_log_odds)) ## 1 ## 0.9107535 Solve for \\(\\pi = \\frac{\\exp (\\hat{y})}{1 + \\exp (\\hat{y})}\\) to get the probability. Do the math, or use predict(type = \"response\"). (my_prob &lt;- my_odds / (1 + my_odds)) ## 1 ## 0.4766462 predict(cs1$fit, newdata = newdata, type = &quot;response&quot;) ## 1 ## 0.4766462 I may already be dead. Move on to the coefficients. Interpret the coefficient estimates as the change in the log odds of \\(y\\) due to a one unit change in \\(x\\). If \\(\\delta = x_a - x_b\\), then a \\(\\delta\\) change in \\(x\\) is associated with a \\(\\delta \\hat{\\beta}\\) change in the log odds of \\(y\\). \\(\\beta\\) is the log odds ratio of \\(x_a\\) vs \\(x_b\\). \\[\\log \\left(\\pi / (1 - \\pi) |_{x = x_a} \\right) - \\log \\left(\\pi / (1 - \\pi) |_{x = x_b} \\right) = \\log \\left( \\frac{\\pi / (1 - \\pi) |_{x = x_a}}{\\pi / (1 - \\pi) |_{x = x_b}} \\right) = \\delta \\hat{\\beta}\\] The exponential of the coefficient estimates is the change in the odds of \\(y\\) due to a \\(\\delta\\) change in \\(x\\). \\(\\exp \\beta\\) is the odds ratio of \\(x_a\\) vs \\(x_b\\). \\[\\mathrm{odds}(y) = e^{\\delta \\hat{\\beta}}\\] coef(cs1$fit) ## (Intercept) age_c weight_c VO2max_c genderFemale ## -0.093483020 0.085098077 0.005726835 -0.099023726 -1.949638795 exp(coef(cs1$fit)) ## (Intercept) age_c weight_c VO2max_c genderFemale ## 0.9107535 1.0888239 1.0057433 0.9057212 0.1423255 The reference case in the model is gender = “Male”, age centered at 41.1 (age_c = 0), weight centered at 79.7 (weight_c = 0), and VO2max centered at 43.6 (VO2max_c = 0). The intercept term is the log-odds of heart disease at the reference case. Because the data is centered, the reference case is actually meaningful (i.e., not age 0, weight 0, and VO2max 0). # The intercept equals the predicted value of the reference case. (my_log_odds &lt;- coef(cs1$fit)[&quot;(Intercept)&quot;]) ## (Intercept) ## -0.09348302 predict(cs1$fit, newdata = newdata, type = &quot;link&quot;) ## 1 ## -0.09348302 # From log-odds to odds. (my_odds &lt;- exp(my_log_odds)) ## (Intercept) ## 0.9107535 # Probability. (my_prob &lt;- my_odds / (1 + my_odds)) ## (Intercept) ## 0.4766462 predict(cs1$fit, newdata = newdata, type = &quot;response&quot;) ## 1 ## 0.4766462 The log odds of heart disease for a 41.1 year old male weighing 79.7 kg and with 43.6 VO2max are \\(\\hat{y} = \\mathrm{Intercept}\\) = -0.09. The odds are \\(\\exp(\\hat{y})\\) = 0.91. The expected probability is \\(\\frac{\\exp(\\hat{y})}{1 + \\exp(\\hat{y})}\\) = 47.7%. What about females? A female’s log odds of heart disease are coef(cs1$fit)[\"genderFemale\"] = -1.95 times that of a male’s (log(OR) = -1.95). A female’s odds are exp(coef(cs1$fit)[\"genderFemale\"]) = 0.14 times that of a male’s (OR = 0.14). A female’s expected probability is predict(cs1$fit, newdata = list(age_c = 0, weight_c = 0, VO2max_c = 0, gender = \"Female\"), type = \"response\") = 11.5%. A one-unit increase in any of the continuous independent variables is interpreted similarly. The reference level is unimportant since the change is constant across the range of values. A one year increase in age increases the log-odds of heart disease by a factor of 0.085, and the odds by a factor of exp(0.085) = 1.089. To calculate the effect of a decade increase in age, multiply \\(\\beta\\) by 10 before exponentiating, or raise the exponentiated coeficient by 10. The effect of a 10-year increase in age is to increase the odds of heart disease by exp(coef(cs1$fit)[\"age_c\"] * 10) = 2.342. The odds more than double every ten years. oddsratio::or_glm() is a handy way to calculate odds ratios from arbitrary increments to the predictors. Here are the ORs of a 10-year age change, 10 kg weight change, and VO2max change of 5. oddsratio::or_glm(cs1$dat, cs1$fit, incr = list(age_c = 10, weight_c = 10, VO2max_c = 5)) ## predictor oddsratio ci_low (2.5) ci_high (97.5) increment ## 1 age_c 2.342 1.391 4.270 10 ## 2 weight_c 1.059 0.680 1.664 10 ## 3 VO2max_c 0.609 0.368 0.952 5 ## 4 genderFemale 0.142 0.025 0.700 Indicator variable Notice how the predicted probabilities have the sigmoidal shape of the binary relationship. augment(cs1$fit, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = age_c, color = gender)) + geom_point(aes(y = as.numeric(heart_disease == &quot;Yes&quot;))) + geom_point(aes(y = .fitted), shape = 4) + geom_smooth(aes(y = .fitted)) + labs(x = &quot;Age&quot;, y = NULL, title = &quot;Binary Fitted Line Plot&quot;) + scale_y_continuous(breaks = c(0,1), labels = c(&quot;Heart Disease&quot;, &quot;Healthy&quot;)) + theme_light() + theme(legend.position = &quot;right&quot;) Assumptions Four assumptions relate to the study design: (1) the dependent variable is dichotomous; (2) the observations are independent; (3) the categories of all nominal variables are mutually exclusive and exhaustive; and (4) there are at least 10 observations per dependent variable level and independent variable. These assumptions are all valid. Three more assumptions related to the data distribution: There is a linear relationship between the logit transformation and the continuous independent variables. Test with a plot and with Box-Tidwell. There is no independent variable multicollinearity. Test with correlation coefficients and variance inflation factors (VIF). There are no influential outliers. Test with Cook’s distance. Test the linearity assumption first. There are two ways to do this (do both). First, fit your model, then plot the fitted values against the continuous predictors. This is the GLM analog to OLS bivariate analysis, except now the dependent variable is the logit transformation. These plotted relationships look pretty linear. glm(heart_disease ~ age + weight + VO2max, data = cs1$dat, family = &quot;binomial&quot;) %&gt;% augment() %&gt;% pivot_longer(c(age, weight, VO2max)) %&gt;% ggplot(aes(x = value, y = .fitted)) + geom_point() + facet_wrap(facets = vars(name), scales = &quot;free_x&quot;) + geom_smooth(method = &quot;lm&quot;, formula = &quot;y~x&quot;) + labs(title = &quot;Linearity Test: predicted vs continuous predictors&quot;, x = NULL) The second test for linearity is the Box-Tidwell approach. Add transformations of the continuous independent variables to the model, \\(x_{Tx} = x \\log x\\), then test their significance level in the fit. # Using non-centered vars to avoid log(0) errors. cs1$boxtidwell &lt;- cs1$dat %&gt;% mutate( age_tx = log(age) * age, weight_tx = log(weight) * weight, VO2max_tx = log(VO2max) * VO2max ) %&gt;% glm(heart_disease ~ age + weight + VO2max + gender + age_tx + weight_tx + VO2max_tx, data = ., family = &quot;binomial&quot;) tidy(cs1$boxtidwell) ## # A tibble: 8 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -38.7 21.8 -1.78 0.0756 ## 2 age 2.73 1.10 2.47 0.0135 ## 3 weight 0.144 0.783 0.184 0.854 ## 4 VO2max 1.32 1.82 0.724 0.469 ## 5 genderFemale -1.85 0.922 -2.01 0.0443 ## 6 age_tx -0.543 0.227 -2.40 0.0164 ## 7 weight_tx -0.0266 0.146 -0.182 0.855 ## 8 VO2max_tx -0.301 0.382 -0.788 0.431 Focus on the three transformed variables. age_tx is the only one with a p-value &lt;.05, but it is customary to apply a Bonferroni adjustment when testing for linearity. There are eight predictors in the model (including the intercept term), so the Bonferroni adjusted p-value for age_tx is 0.0164 x 8 = 0.132, so do not reject the null hypothesis of linearity. If the relationship was nonlinear, you could try transforming the variable by raising it to \\(\\lambda = 1 + b / \\gamma\\) where \\(b\\) is the estimated coefficient of the model without the interaction terms, and \\(\\gamma\\) is the estimated coefficient of the interaction term of the model with interactions. In the case of age, \\(b\\) is 0.085 and \\(\\gamma\\) is -0.543, so \\(\\lambda\\) = 1 + 0.085 / -0.543 = 0.843. This is approximately 1 (no transformation). It seems customary to apply general transformations like .5 (square root), 1/3 (cube root), ln, and the reciprocal. Now check for multicollinearity. Variance inflation factors (VIF) estimate how much the variance of a regression coefficient is inflated due to multicollinearity. When independent variables are correlated, it is difficult to say which variable really influences the dependent variable. The VIF for variable \\(i\\) is \\[ \\mathrm{VIF}_i = \\frac{1}{1 - R_i^2} \\] where \\(R_i^2\\) is the coefficient of determination (i.e., the proportion of dependent variance explained by the model) of a regression of \\(X_i\\) against all of the other predictors, \\(X_i = X_{j \\ne i} \\beta + \\epsilon\\). If \\(X_i\\) is totally unrelated to its covariates, then \\(R_i^2\\) will be zero and \\(\\mathrm{VIF}_i\\) will be 1. If \\(R_i^2\\) is .8, \\(\\mathrm{VIF}_i\\) will be 5. The rule of thumb is that \\(R_i^2 \\le 5\\) is tolerable, and \\(R_i^2 &gt; 5\\) is “highly correlated” and you have to do something about it. These are excellent. car::vif(cs1$fit) ## age_c weight_c VO2max_c gender ## 1.035274 1.900575 2.167067 2.502538 Try calculating the \\(\\mathrm{VIF}\\) for age_c. r2 &lt;- lm(age_c ~ weight_c + VO2max_c + gender, data = cs1$dat) %&gt;% summary() %&gt;% pluck(&quot;r.squared&quot;) (vif &lt;- 1 / (1 - r2)) ## [1] 1.05715 (I don’t know why this doesn’t work. It would work if the underlying model was OLS instead of GLM. The answer seems to be related to GVIF vs VIF, but I didn’t figure it out.) Now check for influential outliers. Predict the response probabilities and filter for the predictions more than two standard deviations from the actual value and a Cook’s Distance greater than 4/N = 0.04.6 Cook’s distance measures how much predicted values change when observation i is removed from the data. Only two fitted values were both an outlier and influential, row ids 59 and 70. An index plot of Cook’s Distance shows the two points at the far left. You might examine the observations for validity. Otherwise, proceed and explain that there were two standardized residuals with value of 2.01 and 2.27 standard deviations which were kept in the analysis. augment(cs1$fit, type.predict = &quot;response&quot;) %&gt;% mutate( id = row_number(), outlier = if_else(abs(.std.resid) &gt;= 2, &quot;Outlier&quot;, &quot;Other&quot;), influential = if_else(.cooksd &gt; 4 / nrow(cs1$dat), &quot;Influential&quot;, &quot;Other&quot;), status = case_when( outlier == &quot;Outlier&quot; &amp; influential == &quot;Influential&quot; ~ &quot;Influential Outlier&quot;, outlier == &quot;Outlier&quot; ~ &quot;Outlier&quot;, influential == &quot;Influential&quot; ~ &quot;Influential&quot;, TRUE ~ &quot;Other&quot; ) ) %&gt;% ggplot(aes(x = .fitted, y = .cooksd)) + geom_point(aes(color = status)) + geom_text(aes(label = if_else(influential == &quot;Influential&quot;, id, NA_integer_)), check_overlap = TRUE, size = 3, nudge_x = .025) + geom_hline(yintercept = 4 / nrow(cs1$dat), linetype = 2, color = &quot;goldenrod&quot;) + scale_color_manual(values = c(&quot;Influential Outlier&quot; = &quot;firebrick&quot;, &quot;Influential&quot; = &quot;goldenrod&quot;, &quot;Outlier&quot; = &quot;slategray&quot;, &quot;Other&quot; = &quot;black&quot;)) + theme(legend.position = &quot;right&quot;) + labs(title = &quot;Index Plot of Cook&#39;s Distance.&quot;, subtitle = &quot;Row id labeled for values &gt; 4 / N.&quot;) Model Fit There are several ways to evaluate the model fit. The likelihood ratio test Pseudo R-squared7. Accuracy measures Gain and ROC curves The likelihood ratio test compares the log likelihood of the fitted model to an intercept-only model. (cs1$lrtest &lt;- lmtest::lrtest(cs1$fit)) ## Likelihood ratio test ## ## Model 1: heart_disease ~ age_c + weight_c + VO2max_c + gender ## Model 2: heart_disease ~ 1 ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 5 -51.044 ## 2 1 -64.745 -4 27.402 1.649e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The fitted model is significant, \\(\\chi^2\\)(4) = 27.4, p &lt; .001. Calculate the pseuedo R2 with DescTools::PseudoR2(). (cs1$pseudo_r2 &lt;- DescTools::PseudoR2(cs1$fit, which = c(&quot;CoxSnell&quot;, &quot;Nagelkerke&quot;, &quot;McFadden&quot;))) ## CoxSnell Nagelkerke McFadden ## 0.2396799 0.3301044 0.2116126 Laerd interprets this as the model explained 33.0% (Nagelkerke R2) of the variance in heart disease. This is how your would interpret R2 in an OLS model. UCLA points out that the various pseudo R-squareds measure other aspects of the model and are unique to the measured quantity. A pseudo R-squared is not very informative on its own; it is useful for comparing models. Accuracy measures formed by the cross-tabulation of observed and predicted classes is the better way to go. (cs1$confusion_matrix &lt;- table( pred = factor(if_else(predict(cs1$fit, type = &quot;response&quot;) &gt; .5, &quot;Yes&quot;, &quot;No&quot;), levels = c(&quot;Yes&quot;, &quot;No&quot;)), truth = factor(cs1$dat$heart_disease, levels = c(&quot;Yes&quot;, &quot;No&quot;)) ) %&gt;% caret::confusionMatrix()) ## Confusion Matrix and Statistics ## ## truth ## pred Yes No ## Yes 16 10 ## No 19 55 ## ## Accuracy : 0.71 ## 95% CI : (0.6107, 0.7964) ## No Information Rate : 0.65 ## P-Value [Acc &gt; NIR] : 0.1236 ## ## Kappa : 0.3224 ## ## Mcnemar&#39;s Test P-Value : 0.1374 ## ## Sensitivity : 0.4571 ## Specificity : 0.8462 ## Pos Pred Value : 0.6154 ## Neg Pred Value : 0.7432 ## Prevalence : 0.3500 ## Detection Rate : 0.1600 ## Detection Prevalence : 0.2600 ## Balanced Accuracy : 0.6516 ## ## &#39;Positive&#39; Class : Yes ## The model accuracy, 71.0%, is the percent of observations correctly classified. The sensitivity, 45.7%, is the accuracy with regard to predicting positive cases. The specificity, 84.6%, is the accuracy with regard to predicting negative cases. The prevalence, 35.0% is the proportion of cases that were positive. Finally, plot the gain curve or ROC curve. In the gain curve, the x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulatively summed true outcome. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicts the response well. The gray area is the “gain” over a random prediction. augment(cs1$fit, type.predict = &quot;response&quot;) %&gt;% # event_level = &quot;second&quot; sets the second level as success yardstick::gain_curve(heart_disease, .fitted, event_level = &quot;second&quot;) %&gt;% autoplot() + labs(title = &quot;Gain Curve&quot;) 35 of the 100 participants had heart disease. The gain curve encountered 18 heart disease cases (51%) within the first 28 observations (28%). It encountered all 35 heart disease cases on the 89th observation. The bottom of the grey area is the outcome of a random model. Only half the heart disease cases would be observed within 50% of the observations. The top of the grey area is the outcome of the perfect model, the “wizard curve”. Half the heart disease cases would be observed in 18/100=18% of the observations. The ROC (Receiver Operating Characteristics) curve plots sensitivity vs specificity at varying cut-off values for the probability ranging from 0 to 1. Ideally, you want very little trade-off between sensitivity and specificity, with a curve hugging the left and top axes. augment(cs1$fit, type.predict = &quot;response&quot;) %&gt;% # event_level = &quot;second&quot; sets the second level as success yardstick::roc_curve(heart_disease, .fitted, event_level = &quot;second&quot;) %&gt;% autoplot() + labs(title = &quot;ROC Curve&quot;) Reporting A binomial logistic regression was performed to ascertain the effects of age, weight, gender and VO2max on the likelihood that participants have heart disease. Linearity of the continuous variables with respect to the logit of the dependent variable was assessed via the Box-Tidwell (1962) procedure. A Bonferroni correction was applied using all eight terms in the model resulting in statistical significance being accepted when p &lt; 0.00625 (Tabachnick &amp; Fidell, 2014). Based on this assessment, all continuous independent variables were found to be linearly related to the logit of the dependent variable. There were two standardized residuals with value of 2.01 and 2.27 standard deviations, which were kept in the analysis. The logistic regression model was statistically significant, χ2(4) = 27.40, p &lt; .001. The model explained 33.0% (Nagelkerke R2) of the variance in heart disease and correctly classified 71.0% of cases. Sensitivity was 45.7%, specificity was 84.6%, positive predictive value was 61.5% and negative predictive value was 74.3%. Of the five predictor variables only three were statistically significant: age, gender and VO2max (as shown in Table 1). Females had 0.14 times lower odds to exhibit heart disease than males. Increasing age was associated with an increased likelihood of exhibiting heart disease, but increasing VO2max was associated with a reduction in the likelihood of exhibiting heart disease. gtsummary::tbl_regression( cs1$fit, exponentiate = TRUE ) #rjgfxbpqve table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #rjgfxbpqve thead, #rjgfxbpqve tbody, #rjgfxbpqve tfoot, #rjgfxbpqve tr, #rjgfxbpqve td, #rjgfxbpqve th { border-style: none; } #rjgfxbpqve p { margin: 0; padding: 0; } #rjgfxbpqve .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #rjgfxbpqve .gt_caption { padding-top: 4px; padding-bottom: 4px; } #rjgfxbpqve .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #rjgfxbpqve .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #rjgfxbpqve .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rjgfxbpqve .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rjgfxbpqve .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rjgfxbpqve .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #rjgfxbpqve .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #rjgfxbpqve .gt_column_spanner_outer:first-child { padding-left: 0; } #rjgfxbpqve .gt_column_spanner_outer:last-child { padding-right: 0; } #rjgfxbpqve .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #rjgfxbpqve .gt_spanner_row { border-bottom-style: hidden; } #rjgfxbpqve .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #rjgfxbpqve .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #rjgfxbpqve .gt_from_md > :first-child { margin-top: 0; } #rjgfxbpqve .gt_from_md > :last-child { margin-bottom: 0; } #rjgfxbpqve .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #rjgfxbpqve .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #rjgfxbpqve .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #rjgfxbpqve .gt_row_group_first td { border-top-width: 2px; } #rjgfxbpqve .gt_row_group_first th { border-top-width: 2px; } #rjgfxbpqve .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rjgfxbpqve .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #rjgfxbpqve .gt_first_summary_row.thick { border-top-width: 2px; } #rjgfxbpqve .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rjgfxbpqve .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rjgfxbpqve .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #rjgfxbpqve .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #rjgfxbpqve .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #rjgfxbpqve .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rjgfxbpqve .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rjgfxbpqve .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #rjgfxbpqve .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rjgfxbpqve .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #rjgfxbpqve .gt_left { text-align: left; } #rjgfxbpqve .gt_center { text-align: center; } #rjgfxbpqve .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rjgfxbpqve .gt_font_normal { font-weight: normal; } #rjgfxbpqve .gt_font_bold { font-weight: bold; } #rjgfxbpqve .gt_font_italic { font-style: italic; } #rjgfxbpqve .gt_super { font-size: 65%; } #rjgfxbpqve .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #rjgfxbpqve .gt_asterisk { font-size: 100%; vertical-align: 0; } #rjgfxbpqve .gt_indent_1 { text-indent: 5px; } #rjgfxbpqve .gt_indent_2 { text-indent: 10px; } #rjgfxbpqve .gt_indent_3 { text-indent: 15px; } #rjgfxbpqve .gt_indent_4 { text-indent: 20px; } #rjgfxbpqve .gt_indent_5 { text-indent: 25px; } Characteristic OR1 95% CI1 p-value age_c 1.09 1.03, 1.16 0.003 weight_c 1.01 0.96, 1.05 0.8 VO2max_c 0.91 0.82, 0.99 0.039 gender     Male — —     Female 0.14 0.02, 0.70 0.021 1 OR = Odds Ratio, CI = Confidence Interval 2.2 Multinomial Logistic Regression The multinomial logistic regression model is \\(J - 1\\) baseline logits, \\[y_i = \\log \\left( \\frac{\\pi_{ij}}{\\pi_{ij^*}} \\right) = X_i \\beta_j, \\hspace{5mm} j \\ne j^*\\] where \\(j\\) is a level of the multinomial response variable. Whereas binomial logistic regression models the log odds of the response level, multinomial logistic regression models the log relative risk, the probability relative to the baseline \\(j^*\\) level.8 As with binary logistic regression, interpret the \\(k^{th}\\) element of \\(\\beta_j\\) as the increase in log odds (log relative risk) of \\(Y_i = j\\) relative to \\(Y_i = j^*\\) given a one-unit increase in the \\(k^{th}\\) element of \\(X\\), holding the other terms constant. The individual probabilities, \\(\\pi_{ij}\\), are \\[\\pi_{ij} = \\frac{\\exp(y_{ij})}{1 + \\sum_{j \\ne j^*} \\exp(y_{ij})} = \\frac{e^{X_i\\beta_j}}{1 + \\sum_{j \\ne j^*} e^{X_i\\beta_j}}\\] and for the baseline category, \\[\\pi_{ij^*} = \\frac{1}{1 + \\sum_{j \\ne j^*} \\exp(y_{ij})} = \\frac{1}{1 + \\sum_{j \\ne j^*} e^{X_i\\beta_j}}\\] Assumptions Multinomial logistic regression applies when the dependent variable is categorical. It presumes a linear relationship between the log odds of the dependent variable and \\(X\\) with residuals \\(\\epsilon\\) that are independent. It also assumes there is no severe multicollinearity in the predictors, and there is independence of irrelevant alternatives (IIA). IIA means the relative likelihood of being in one category compared to the base category would not change if you added other categories. Case Study 2 This case study uses the data set from this UCLA tutorial. download.file( &quot;https://stats.idre.ucla.edu/stat/data/hsbdemo.dta&quot;, &quot;./input/hsbdemo.dta&quot;, mode = &quot;wb&quot; ) cs2 &lt;- list() cs2$dat &lt;- foreign::read.dta(&quot;./input/hsbdemo.dta&quot;) A study measures the association between students’ academic program (academic, general, and vocational) and their socioeconomic status (SES) (low, middle, high) and writing score. cs2$dat %&gt;% tbl_summary( by = prog, include = c(prog, ses, write), statistic = list(all_continuous() ~ &quot;{mean}, {sd}&quot;) ) %&gt;% gtsummary::add_overall() #itcrxqkwxa table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #itcrxqkwxa thead, #itcrxqkwxa tbody, #itcrxqkwxa tfoot, #itcrxqkwxa tr, #itcrxqkwxa td, #itcrxqkwxa th { border-style: none; } #itcrxqkwxa p { margin: 0; padding: 0; } #itcrxqkwxa .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #itcrxqkwxa .gt_caption { padding-top: 4px; padding-bottom: 4px; } #itcrxqkwxa .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #itcrxqkwxa .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #itcrxqkwxa .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #itcrxqkwxa .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #itcrxqkwxa .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #itcrxqkwxa .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #itcrxqkwxa .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #itcrxqkwxa .gt_column_spanner_outer:first-child { padding-left: 0; } #itcrxqkwxa .gt_column_spanner_outer:last-child { padding-right: 0; } #itcrxqkwxa .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #itcrxqkwxa .gt_spanner_row { border-bottom-style: hidden; } #itcrxqkwxa .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #itcrxqkwxa .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #itcrxqkwxa .gt_from_md > :first-child { margin-top: 0; } #itcrxqkwxa .gt_from_md > :last-child { margin-bottom: 0; } #itcrxqkwxa .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #itcrxqkwxa .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #itcrxqkwxa .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #itcrxqkwxa .gt_row_group_first td { border-top-width: 2px; } #itcrxqkwxa .gt_row_group_first th { border-top-width: 2px; } #itcrxqkwxa .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #itcrxqkwxa .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #itcrxqkwxa .gt_first_summary_row.thick { border-top-width: 2px; } #itcrxqkwxa .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #itcrxqkwxa .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #itcrxqkwxa .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #itcrxqkwxa .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #itcrxqkwxa .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #itcrxqkwxa .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #itcrxqkwxa .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #itcrxqkwxa .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #itcrxqkwxa .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #itcrxqkwxa .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #itcrxqkwxa .gt_left { text-align: left; } #itcrxqkwxa .gt_center { text-align: center; } #itcrxqkwxa .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #itcrxqkwxa .gt_font_normal { font-weight: normal; } #itcrxqkwxa .gt_font_bold { font-weight: bold; } #itcrxqkwxa .gt_font_italic { font-style: italic; } #itcrxqkwxa .gt_super { font-size: 65%; } #itcrxqkwxa .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #itcrxqkwxa .gt_asterisk { font-size: 100%; vertical-align: 0; } #itcrxqkwxa .gt_indent_1 { text-indent: 5px; } #itcrxqkwxa .gt_indent_2 { text-indent: 10px; } #itcrxqkwxa .gt_indent_3 { text-indent: 15px; } #itcrxqkwxa .gt_indent_4 { text-indent: 20px; } #itcrxqkwxa .gt_indent_5 { text-indent: 25px; } Characteristic Overall, N = 2001 general, N = 451 academic, N = 1051 vocation, N = 501 ses     low 47 (24%) 16 (36%) 19 (18%) 12 (24%)     middle 95 (48%) 20 (44%) 44 (42%) 31 (62%)     high 58 (29%) 9 (20%) 42 (40%) 7 (14%) write 53, 9 51, 9 56, 8 47, 9 1 n (%); Mean, SD Establish your expectations with a brief exploratory analysis. Academic programs are associated with higher writing scores and SES. General and vocational programs are the opposite, although SES has opposing effects for general (increased probability) and vocational (decreased probability). cs2$dat %&gt;% mutate(write_bin = cut(write, breaks = 5, dig.lab = 1, right = FALSE)) %&gt;% count(prog, ses, write_bin) %&gt;% group_by(ses, write_bin) %&gt;% mutate(prob = n / sum(n)) %&gt;% ungroup() %&gt;% ggplot(aes(x = write_bin, y = prob, color = ses)) + geom_point() + geom_line(aes(group = ses)) + facet_wrap(facets = vars(prog)) + theme(legend.position = &quot;top&quot;) + labs(title = &quot;Program Proportions by SES&quot;) Standardize write to facilitate model interpretation. The intercept will represent a reasonable condition (average writing score) and a one-unit incrase in write_c will represent a 1SD increase in writing. Set academic as the reference level for the dependent variable. cs2$dat &lt;- cs2$dat %&gt;% mutate( write_c = as.numeric(scale(write, scale = TRUE)), prog = fct_relevel(prog, &quot;academic&quot;, after = 0) ) The Model Start by fitting the model with nnet::multinom() or VGAM::vglm(). nnet::multinom() seems to be more popular, but it doesn’t work with some diagnostic functions. VGAM::vglm() also doesn’t work with all diagnostic functions, including tidy() and augment(). I haven’t decided which is preferable. I’ll fit both and move back and forth as convenient. cs2$fmla &lt;- formula(prog ~ ses + write_c) cs2$fit_VGAM &lt;- cs2$dat %&gt;% # `vglm()` uses the _last_ level of the dependent var for the reference. mutate(prog = fct_relevel(prog, &quot;academic&quot;, after = Inf)) %&gt;% # `model = TRUE` enables pseudo-r2 calculation. vglm(cs2$fmla, data = ., family = &quot;multinomial&quot;, model = TRUE) cs2$fit_nnet &lt;- multinom(cs2$fmla, data = cs2$dat) The models produce a set of coefficient estimates for each non-reference level of the dependent variable. The summary objects differ in how the present the coefficients. VGAM::vglm() produces the familiar coefficient table, but it labels the levels with integers. Good luck if you forget that VGAM::vglm() use the last level as the reference!. nnet::multinom() identifies the levels by name, but it presents the coefficient estimates as columns, their standard errors as a second section, and does not present the z-statistic or p-values! That may be foregiveable because you can still get these values from tidy() and gtsummary::tbl_regression() (VGAM is incompatible with both!). summary(cs2$fit_VGAM) ## ## Call: ## vglm(formula = cs2$fmla, family = &quot;multinomial&quot;, data = ., model = TRUE) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept):1 -0.2050 0.3491 -0.587 0.55711 ## (Intercept):2 -0.7772 0.4111 -1.890 0.05869 . ## sesmiddle:1 -0.5333 0.4437 -1.202 0.22943 ## sesmiddle:2 0.2914 0.4764 0.612 0.54074 ## seshigh:1 -1.1628 0.5142 -2.261 0.02374 * ## seshigh:2 -0.9827 0.5955 -1.650 0.09893 . ## write_c:1 -0.5491 0.2029 -2.706 0.00682 ** ## write_c:2 -1.0768 0.2106 -5.113 3.17e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Names of linear predictors: log(mu[,1]/mu[,3]), log(mu[,2]/mu[,3]) ## ## Residual deviance: 359.9635 on 392 degrees of freedom ## ## Log-likelihood: -179.9817 on 392 degrees of freedom ## ## Number of Fisher scoring iterations: 4 ## ## No Hauck-Donner effect found in any of the estimates ## ## ## Reference group is level 3 of the response summary(cs2$fit_nnet) ## Call: ## multinom(formula = cs2$fmla, data = cs2$dat) ## ## Coefficients: ## (Intercept) sesmiddle seshigh write_c ## general -0.2049851 -0.5332857 -1.1628363 -0.5490814 ## vocation -0.7771705 0.2913906 -0.9826773 -1.0767939 ## ## Std. Errors: ## (Intercept) sesmiddle seshigh write_c ## general 0.3491296 0.4437324 0.5142201 0.2029457 ## vocation 0.4111072 0.4763734 0.5955665 0.2106132 ## ## Residual Deviance: 359.9635 ## AIC: 375.9635 The Wald z-statistic is \\(z = \\hat{\\beta} / SE(\\hat{\\beta})\\). Its square is the Wald chi-squared statistic. The p-value is the area to the right of \\(z^2\\) in the \\(\\chi_1^2\\) density curve. The residual deviance, \\(G^2 = 2 \\sum_{i,j}y_{ij} \\log \\frac{y_{ij}}{\\hat{\\pi}_{ij}}\\), and log-likelihood, \\(-G^2 / 2\\) are model diagnostics. More on these in the Model Fit section. deviance(cs2$fit_VGAM) ## [1] 359.9635 df.residual(cs2$fit_VGAM) ## [1] 392 logLik(cs2$fit_VGAM, sum = TRUE) ## [1] -179.9817 Fisher scoring is the method used for the ML estimation. Interpretation Start with interpreting the dependent variable. The model predicts the log relative risk of belonging to a program \\(j \\in\\) [vocation, general] vs. \\(j^*\\) = academic. Plug in values for the predictor variables to get predictions. Below newdata is set to the reference level of ses and the centered value of write. newdata &lt;- list(ses = &quot;low&quot;, write_c = 0) # VGAM predicts risk (`response`) or log RR (`link`). my_log_rr &lt;- predict(cs2$fit_VGAM, newdata = newdata, type = &quot;link&quot;) %&gt;% .[1, ] names(my_log_rr) &lt;- levels(cs2$dat$prog)[2:3] my_log_rr ## general vocation ## -0.2049856 -0.7771791 # nnet predicts risk (`probs`) or class (class), but not log RR. my_probs &lt;- predict(cs2$fit_nnet, newdata = newdata, type = &quot;probs&quot;) log(my_probs[2:3] / my_probs[1]) ## general vocation ## -0.2049851 -0.7771705 Exponentiate to get the relative risk, \\(RR = \\exp (\\hat{y}_j) = \\pi_j / \\pi_{j^*}\\). A student of low SES and mean writing score is less likely to be in a general or vocation program than an academic program. (my_rr &lt;- exp(my_log_rr)) ## general vocation ## 0.8146591 0.4597009 Use predict() to get the individual risks. predict(cs2$fit_VGAM, newdata = newdata, type = &quot;response&quot;) %&gt;% .[1, levels(cs2$dat$prog)] ## academic general vocation ## 0.4396841 0.3581927 0.2021232 (my_pred &lt;- predict(cs2$fit_nnet, newdata = newdata, type = &quot;probs&quot;)) ## academic general vocation ## 0.4396833 0.3581922 0.2021246 Move on to the coefficients. Interpret \\(\\hat{\\beta}\\) as the change in the log relative risk of \\(y_j\\) relative to \\(y_{j^*}\\) due to a \\(\\delta\\) = one unit change in \\(x\\). A \\(\\delta = x_a - x_b\\) change in \\(x\\) is associated with a \\(\\delta \\hat{\\beta}\\) change. \\(\\delta\\beta\\) is the log relative risk ratio. \\[\\log \\left(\\pi_j / \\pi_{j^*} |_{x = x_a} \\right) - \\log \\left(\\pi_j / \\pi_{j^*} |_{x = x_b} \\right) = \\log \\left( \\frac{\\pi_j / \\pi_{j^*} |_{x = x_a}}{\\pi_j / \\pi_{j^*} |_{x = x_b}} \\right) = \\delta \\hat{\\beta}\\] The exponential of \\(\\hat{\\beta}\\) is the change in the relative risk of \\(y_j\\) relative to \\(y_{j^*}\\) due to a \\(\\delta\\) = one unit change in \\(x\\). \\(\\exp \\delta \\beta\\) is the relative risk ratio. \\[\\pi_j / \\pi_{j^*} |_{x = x_a} = \\exp{\\delta \\hat{\\beta}}\\] The reference case in the model is ses = “low” and write centered at 52.8 (write_c = 0). The intercept term is the log-relative risk of \\(y_j\\) relative to \\(y_{j^*}\\) for the reference case. Notice how the intercept matches the predicted values above. # Using VGAM. ref_log_rr &lt;- coef(cs2$fit_VGAM)[c(&quot;(Intercept):1&quot;, &quot;(Intercept):2&quot;)] names(ref_log_rr) &lt;- levels(cs2$dat$prog)[2:3] ref_log_rr ## general vocation ## -0.2049856 -0.7771791 # Using nnet. coef(cs2$fit_nnet)[,&quot;(Intercept)&quot;] ## general vocation ## -0.2049851 -0.7771705 # From log-relative risk to relative risk. (ref_rr &lt;- exp(ref_log_rr)) ## general vocation ## 0.8146591 0.4597009 # Derived from predicted values my_pred[-1] / my_pred[1] ## general vocation ## 0.8146595 0.4597049 The log relative risks of a low SES student with a 52.8 writing score being in program general vs academic are \\(\\hat{y} = \\mathrm{Intercept}_1\\) = -0.205 and \\(\\hat{y} = \\mathrm{Intercept}_2\\) = -0.777 for vocation vs academic. The corresponding relative risks are \\(\\exp(\\hat{y}_j)\\) = 0.815 and 0.460. The expected probabilities are 44.0%, 35.8%, and 20.2% for academic, general, and vocation respectively. What if SES is high instead of low? The log RR of a high SES student being in program general vs academic are coef(cs2$fit_VGAM)[\"seshigh:1\"] = -1.16 times that of a low SES student. For program vocation vs academic, it is coef(cs2$fit_VGAM)[\"seshigh:2\"] = -0.98. The RRs are 0.31 and 0.37. The expected probabilities are 17.8%, 12.1%, and 70.1% for academic, general, and vocation respectively. What if the writing score increases by 1 SD (one unit)? The log RR of being in program general vs academic change by a factor of coef(cs2$fit_VGAM)[\"write_c:1\"] = -0.549, RR = 0.577. A 2 SD increase is associated with a log RR = -1.098, RR = 0.333. One way to visualize the parameter estimates is to plot the predicted values. newdata &lt;- expand.grid( ses = cs2$dat$ses, write_c = seq(from = round(min(cs2$dat$write_c)), to = round(max(cs2$dat$write_c)), by = 1) ) bind_cols( newdata, predict(cs2$fit_VGAM, newdata = newdata, type = &quot;response&quot;) ) %&gt;% pivot_longer(cols = -c(ses, write_c), names_to = &quot;prog&quot;, values_to = &quot;probability&quot;) %&gt;% ggplot(aes(x = write_c, y = probability, color = ses)) + geom_line() + facet_wrap(facets = vars(prog)) Assumptions Four assumptions relate to the study design: (1) the dependent variable is multinomial; (2) the observations are independent; (3) the categories of all nominal variables are mutually exclusive and exhaustive; and (4) there are at least 10 observations per dependent variable level and independent variable. These assumptions are all valid. Three more assumptions related to the data distribution: There is a linear relationship between the logit transformation and the continuous independent variables. Test with a plot and with Box-Tidwell. There is no independent variable multicollinearity. Test with correlation coefficients and variance inflation factors (VIF). There are no influential outliers. Test with Cook’s distance. There are two ways to test for linearity (do both). First, plot the fitted values against the continuous predictors. This is the GLM analog to OLS bivariate analysis, except now the dependent variable is the logit transformation. These plotted relationships look good, except that in the prog = general level, writing score appears to interact with SES. .fitted = predict(cs2$fit_VGAM, type = &quot;response&quot;) %&gt;% as.data.frame() bind_cols(cs2$dat, .fitted) %&gt;% pivot_longer(cols = c(general:academic), values_to = &quot;.fitted&quot;) %&gt;% filter(prog == name) %&gt;% ggplot(aes(x = write_c, y = .fitted, color = ses)) + geom_point() + facet_wrap(facets = vars(name), scales = &quot;free_x&quot;) + geom_smooth(method = &quot;lm&quot;, formula = &quot;y~x&quot;) + labs(title = &quot;Linearity Test: predicted vs continuous predictors&quot;, x = NULL) The second test for linearity is the Box-Tidwell approach. Add transformations of the continuous independent variables to the model, \\(x_{Tx} = x \\log x\\), then test their significance level in the fit. # Using non-centered vars to avoid log(0) errors. cs2$boxtidwell &lt;- cs2$dat %&gt;% mutate(write_tx = log(write) * write) %&gt;% multinom(prog ~ ses + write + write_tx, data = .) tidy(cs2$boxtidwell) ## # A tibble: 10 × 6 ## y.level term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 general (Intercept) 6.38 0.00980 651. 0 ## 2 general sesmiddle -0.529 0.444 -1.19 0.233 ## 3 general seshigh -1.18 0.513 -2.30 0.0215 ## 4 general write -0.409 0.0936 -4.37 0.0000126 ## 5 general write_tx 0.0713 0.0231 3.09 0.00198 ## 6 vocation (Intercept) 5.91 0.0102 580. 0 ## 7 vocation sesmiddle 0.296 0.476 0.621 0.534 ## 8 vocation seshigh -0.983 0.594 -1.65 0.0981 ## 9 vocation write -0.176 0.0963 -1.83 0.0668 ## 10 vocation write_tx 0.0125 0.0239 0.522 0.601 Focus on the transformed variable. write_tx has a p-value &lt;.05 for prog = general. It is customary to apply a Bonferroni adjustment when testing for linearity. There are ten predictors in the model (including the intercept terms), so the Bonferroni adjusted p-values for write_tx are multiplied by 10. We should reject the null hypothesis of linearity. tidy(cs2$boxtidwell) %&gt;% filter(term == &quot;write_tx&quot;) %&gt;% mutate(adj_p = p.value * 10) ## # A tibble: 2 × 7 ## y.level term estimate std.error statistic p.value adj_p ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 general write_tx 0.0713 0.0231 3.09 0.00198 0.0198 ## 2 vocation write_tx 0.0125 0.0239 0.522 0.601 6.01 If the relationship is nonlinear, you can try transforming the variable by raising it to \\(\\lambda = 1 + b / \\gamma\\) where \\(b\\) is the estimated coefficient of the model without the interaction terms, and \\(\\gamma\\) is the estimated coefficient of the interaction term of the model with interactions. For write, \\(b\\) is -0.549 for general and \\(\\gamma\\) is 0.071, so \\(\\lambda\\) = 1 + -0.549 / 0.071 = -6.696. It seems customary to apply general transformations like .5 (square root), 1/3 (cube root), ln, and the reciprocal, so maybe try raising write_c to -7. It seems in this case that the better solution is to add an interaction between write_c and ses to the model. I’m not going to pursue this further here. Check for multicollinearity using variance inflation factors (VIF). VIFs estimate how much the variance of a regression coefficient is inflated due to multicollinearity. When independent variables are correlated, it is difficult to say which variable really influences the dependent variable. The VIF for variable \\(i\\) is \\[ \\mathrm{VIF}_i = \\frac{1}{1 - R_i^2} \\] where \\(R_i^2\\) is the coefficient of determination (i.e., the proportion of dependent variance explained by the model) of a regression of \\(X_i\\) against all of the other predictors, \\(X_i = X_{j \\ne i} \\beta + \\epsilon\\). If \\(X_i\\) is totally unrelated to its covariates, then \\(R_i^2\\) will be zero and \\(\\mathrm{VIF}_i\\) will be 1. If \\(R_i^2\\) is .8, \\(\\mathrm{VIF}_i\\) will be 5. The rule of thumb is that \\(R_i^2 \\le 5\\) is tolerable, and \\(R_i^2 &gt; 5\\) is “highly correlated” and you have to do something about it. car::vif() doesn’t seem to work for multinomial logistic regression. The model type is not actually important here - we’re concerned about the covariate relationships. Below, I successively collapse the dependent variable into two-levels, then fit a binomial logistic regression and pipe that into car::vif(). tmp_fit_general &lt;- cs2$dat %&gt;% mutate(prog = fct_collapse(prog, vocation = &quot;academic&quot;)) %&gt;% glm(cs2$fmla, data = ., family = &quot;binomial&quot;) car::vif(tmp_fit_general) ## GVIF Df GVIF^(1/(2*Df)) ## ses 1.037207 2 1.009175 ## write_c 1.037207 1 1.018433 tmp_fit_vocation &lt;- cs2$dat %&gt;% mutate(prog = fct_collapse(prog, general = &quot;academic&quot;)) %&gt;% glm(cs2$fmla, data = ., family = &quot;binomial&quot;) car::vif(tmp_fit_vocation) ## GVIF Df GVIF^(1/(2*Df)) ## ses 1.020017 2 1.004967 ## write_c 1.020017 1 1.009959 cs2$outliers_general &lt;- augment(tmp_fit_general, type.predict = &quot;response&quot;) %&gt;% filter(abs(.std.resid) &gt;= 2) %&gt;% pull(.std.resid) cs2$outliers_vocation &lt;- augment(tmp_fit_vocation, type.predict = &quot;response&quot;) %&gt;% filter(abs(.std.resid) &gt;= 2) %&gt;% pull(.std.resid) Check for influential outliers. Predict the response probabilities and filter for the predictions more than two standard deviations from the actual value and a Cook’s Distance greater than 4/N = 0.02.9 Cook’s distance measures how much predicted values change when observation i is removed from the data. There is no simple way to do this for the multinomial regression because neither VGAM nor nnet support the augment() generic. Instead, I will calculate the influential outliers for the two binomial logistic regressions I used for the VIF diagnostic. No fitted values were influential outliers in the first fit, and only two were influential outliers in the second fit, row ids 259 and 374. An index plot of Cook’s Distance shows the two points at the far left. You might examine the observations for validity. Otherwise, proceed and explain that there were two standardized residuals with value of 2.20 and 2.59 standard deviations which were kept in the analysis. bind_rows( general = augment(tmp_fit_general, type.predict = &quot;response&quot;), vocation = augment(tmp_fit_vocation, type.predict = &quot;response&quot;), .id = &quot;logit&quot; ) %&gt;% mutate( id = row_number(), outlier = if_else(abs(.std.resid) &gt;= 2, &quot;Outlier&quot;, &quot;Other&quot;), influential = if_else(.cooksd &gt; 4 / nrow(cs1$dat), &quot;Influential&quot;, &quot;Other&quot;), status = case_when( outlier == &quot;Outlier&quot; &amp; influential == &quot;Influential&quot; ~ &quot;Influential Outlier&quot;, outlier == &quot;Outlier&quot; ~ &quot;Outlier&quot;, influential == &quot;Influential&quot; ~ &quot;Influential&quot;, TRUE ~ &quot;Other&quot; ) ) %&gt;% ggplot(aes(x = .fitted, y = .cooksd)) + geom_point(aes(color = status)) + geom_text(aes(label = if_else(influential == &quot;Influential&quot;, id, NA_integer_)), check_overlap = TRUE, size = 3, nudge_x = .025) + geom_hline(yintercept = 4 / nrow(cs1$dat), linetype = 2, color = &quot;goldenrod&quot;) + scale_color_manual(values = c(&quot;Influential Outlier&quot; = &quot;firebrick&quot;, &quot;Influential&quot; = &quot;goldenrod&quot;, &quot;Outlier&quot; = &quot;slategray&quot;, &quot;Other&quot; = &quot;black&quot;)) + theme(legend.position = &quot;right&quot;) + labs(title = &quot;Index Plot of Cook&#39;s Distance.&quot;, subtitle = &quot;Row id labeled for values &gt; 4 / N.&quot;) + facet_wrap(facets = vars(logit), ncol = 1) Model Fit There are several ways to evaluate the model fit. Deviance and chi-squared tests for lack of fit The likelihood ratio test Pseudo R-squared10. Accuracy measures Gain and ROC curves Lack of Fit The deviance test for lack of fit and the likelihood ratio test are residuals tests. The deviance residual is defined as \\(d_i = \\mathrm{sign}(\\epsilon_i) \\left[ -2(y_i \\log \\hat{\\pi}_i + (1 - y_i) \\log (1 - \\hat{\\pi}_i)) \\right]^{.5}\\). You can calculate them by hand. # Actual values (1s and 0s for three response levels) y &lt;- cs2$dat %&gt;% mutate(val = 1) %&gt;% pivot_wider(names_from = prog, values_from = val, values_fill = 0) %&gt;% select(levels(cs2$dat$prog)) %&gt;% as.matrix() # Predicted values (probabilities for three response levels) pi &lt;- predict(cs2$fit_nnet, type = &quot;probs&quot;) * 1 # Raw residuals, by hand or by formula e &lt;- y - pi e &lt;- residuals(cs2$fit_VGAM, type = &quot;response&quot;) # be careful about column order! e &lt;- residuals(cs2$fit_nnet, type = &quot;response&quot;) # Deviance residuals d &lt;- sign(e) * sqrt(-2 * y * log(pi) + (1 - y) * log(1 - pi)) The model deviance, \\(G^2\\), is the sum of the squared deviance residuals. It also equals \\(G^2 = 2 \\sum_{i,j}y_{ij} \\log \\frac{y_{ij}}{\\hat{\\pi}_{ij}}\\). (g2 &lt;- sum(d^2, na.rm = TRUE)) ## [1] 359.9635 (g2 &lt;- 2 * sum(y * log(y / pi), na.rm = TRUE)) ## [1] 359.9635 (g2 &lt;- deviance(cs2$fit_VGAM)) ## [1] 359.9635 (g2 &lt;- deviance(cs2$fit_nnet)) ## [1] 359.9635 The related Pearson statistic, \\(X\\), is the sum of the squared Pearson residuals, \\(pr_i = \\epsilon_i / \\sqrt{\\hat{\\pi}_i}\\), the raw residual scaled by dividing by the estimated standard deviation of a binomial distribution with 1 trial. I don’t see this calculated in the residual() functions. You can do it yourself. (x2 &lt;- sum((e / sqrt(pi))^2)) ## [1] 406.0656 The deviance and Pearson statistic are distributed chi-squared with \\((N - p)(r - 1)\\) degrees of freedom where \\(p\\) = 4 predictor variables, and \\(r\\) = 3 levels of the dependent variable for 392 degrees of freedom. The deviance and Pearson tests for lack of fit calculate the probability of the test statistic. The null hypothesis is that the model is correct. Neither test rejects the null hypothesis. # Deviance test for lack of fit (N &lt;- nrow(cs2$dat)) ## [1] 200 (r &lt;- length(levels(cs2$dat$prog))) ## [1] 3 (p &lt;- length(coef(cs2$fit_VGAM)) / (r - 1)) # coefficients for each level, so divide by # levels ## [1] 4 (df &lt;- (N - p) * (r - 1)) ## [1] 392 # Or just use the function (df &lt;- df.residual(cs2$fit_VGAM)) ## [1] 392 pchisq(g2, df, lower.tail = FALSE) ## [1] 0.8755302 pchisq(x2, df, lower.tail = FALSE) ## [1] 0.3014625 You can do the same calculations for the intercept-only model. io_nnet &lt;- multinom(prog ~ 1, data = cs2$dat) ## # weights: 6 (2 variable) ## initial value 219.722458 ## final value 204.096674 ## converged io_VGAM &lt;- vglm(prog ~ 1, data = cs2$dat, family = &quot;multinomial&quot;) (io_g2 &lt;- deviance(io_nnet)) ## [1] 408.1933 (io_g2 &lt;- deviance(io_VGAM)) ## [1] 408.1933 (io_df &lt;- ((nrow(cs2$dat) - length(coef(io_nnet)) / (r - 1)) * (r - 1))) ## [1] 398 (io_df &lt;- df.residual(io_VGAM)) ## [1] 398 Likelihood Ratio Test The log-likelihood measures the unexplained variability in the model. The likelihood ratio test compares the log likelihood of the fitted model to the intercept-only model. You can use lmtest::lrtest() to test. anova() does the same thing using the residual deviance, \\(G2 = -2 \\times \\mathrm{log likelihood}\\). (cs2$lrtest &lt;- lmtest::lrtest(io_nnet, cs2$fit_nnet)) ## Likelihood ratio test ## ## Model 1: prog ~ 1 ## Model 2: prog ~ ses + write_c ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 2 -204.10 ## 2 8 -179.98 6 48.23 1.063e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (cs2$lrtest &lt;- anova(io_VGAM, cs2$fit_VGAM, type = &quot;I&quot;, test = &quot;LR&quot;)) ## Analysis of Deviance Table ## ## Model 1: prog ~ 1 ## Model 2: prog ~ ses + write_c ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 398 408.19 ## 2 392 359.96 6 48.23 1.063e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The difference in deviances is \\(LR\\) = 48.23 with 6 degrees of freedom. This is distributed chi-squared, with p-value = 1.063e-08. The deviance test for lack of fit concludes that the model fits significantly better than an empty (intercept-only) model. You can use lmtest::lrtest() to perform likelihood ratio tests on the significance of the predictors too. The likelihood ratio test compares the log likelihood with and without the predictor. (cs2$lrtest_ses &lt;- lmtest::lrtest(cs2$fit_nnet, &quot;ses&quot;)) ## # weights: 9 (4 variable) ## initial value 219.722458 ## final value 185.510837 ## converged ## Likelihood ratio test ## ## Model 1: prog ~ ses + write_c ## Model 2: prog ~ write_c ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 8 -179.98 ## 2 4 -185.51 -4 11.058 0.02592 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (cs2$lrtest_write &lt;- lmtest::lrtest(cs2$fit_nnet, &quot;write_c&quot;)) ## # weights: 12 (6 variable) ## initial value 219.722458 ## iter 10 value 195.705189 ## iter 10 value 195.705188 ## iter 10 value 195.705188 ## final value 195.705188 ## converged ## Likelihood ratio test ## ## Model 1: prog ~ ses + write_c ## Model 2: prog ~ ses ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 8 -179.98 ## 2 6 -195.71 -2 31.447 1.484e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Both SES, \\(X^2\\) = 11.058, p = 0.026, and writing score \\(X^2\\) = 31.447, p = 1.484e-07, had significant effects on the program. Logistic regression does not have a direct R-squared statistic like OLS does (the proportion of variance explained by the model). However, there are some analogs, called pseudo R-squared. You’ll encounter three pseudo R-squared measures: Cox and Snell, Nagelkerke, and McFadden. DescTools::PseudoR2(cs2$fit_VGAM, which = c(&quot;CoxSnell&quot;, &quot;Nagelkerke&quot;, &quot;McFadden&quot;)) ## CoxSnell Nagelkerke McFadden ## 0.2142758 0.2462666 0.1181545 Accuracy measures formed by the cross-tabulation of observed and predicted classes is the better model fit diagnostic the the pseudo r-squareds. pred &lt;- predict(cs2$fit_VGAM, type = &quot;response&quot;) %&gt;% as.data.frame() %&gt;% rowwise() %&gt;% mutate( p_max = max(c(general, vocation, academic)), pred_idx = match(p_max, c(general, vocation, academic)), pred = c(&quot;general&quot;, &quot;vocation&quot;, &quot;academic&quot;)[pred_idx] ) %&gt;% pull(pred) (cs2$confusion_matrix &lt;- table( pred = factor(pred, levels = levels(cs2$dat$prog)), truth = factor(cs2$dat$prog, levels = levels(cs2$dat$prog)) ) %&gt;% caret::confusionMatrix()) ## Confusion Matrix and Statistics ## ## truth ## pred academic general vocation ## academic 92 27 23 ## general 4 7 4 ## vocation 9 11 23 ## ## Overall Statistics ## ## Accuracy : 0.61 ## 95% CI : (0.5387, 0.678) ## No Information Rate : 0.525 ## P-Value [Acc &gt; NIR] : 0.009485 ## ## Kappa : 0.2993 ## ## Mcnemar&#39;s Test P-Value : 7.654e-06 ## ## Statistics by Class: ## ## Class: academic Class: general Class: vocation ## Sensitivity 0.8762 0.1556 0.4600 ## Specificity 0.4737 0.9484 0.8667 ## Pos Pred Value 0.6479 0.4667 0.5349 ## Neg Pred Value 0.7759 0.7946 0.8280 ## Prevalence 0.5250 0.2250 0.2500 ## Detection Rate 0.4600 0.0350 0.1150 ## Detection Prevalence 0.7100 0.0750 0.2150 ## Balanced Accuracy 0.6749 0.5520 0.6633 The model accuracy, 61.0%, is the percent of observations correctly classified. The sensitivities are the accuracy with regard to predicting positive cases in each level of the dependent variable. The specificities are the accuracy with regard to predicting negative cases. The prevalences are the proportion of cases that were positive. Finally, plot the gain curve or ROC curve. In the gain curve, the x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulatively summed true outcome. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicts the response well. The gray area is the “gain” over a random prediction. bind_cols(cs2$dat, .fitted = predict(cs2$fit_VGAM, type = &quot;response&quot;)) %&gt;% # event_level = &quot;second&quot; sets the second level as success yardstick::gain_curve(prog, .fitted, event_level = &quot;second&quot;) %&gt;% autoplot() + labs(title = &quot;Gain Curve&quot;) 105 of the 200 participants were in the academic program. The gain curve encountered 52 academic programs (50%) within the first 72 observations (36%). It encountered all 105 cases on the 189th observation. The bottom of the grey area is the outcome of a random model. Only half the academic program cases would be observed within 50% of the observations. The top of the grey area is the outcome of the perfect model, the “wizard curve”. Half the academic program cases would be observed in 52.5/200=26.25% of the observations. The ROC (Receiver Operating Characteristics) curve plots sensitivity vs specificity at varying cut-off values for the probability ranging from 0 to 1. Ideally, you want very little trade-off between sensitivity and specificity, with a curve hugging the left and top axes. bind_cols(cs2$dat, .fitted = predict(cs2$fit_VGAM, type = &quot;response&quot;)) %&gt;% # event_level = &quot;second&quot; sets the second level as success yardstick::roc_curve(prog, .fitted, event_level = &quot;second&quot;) %&gt;% autoplot() + labs(title = &quot;ROC Curve&quot;) Reporting A multinomial logistic regression was performed to ascertain the effects of socioeconomic status (ses) and writing score on the likelihood that participants are enrolled in an academic, general, or vocation program. Linearity of the continuous variables with respect to the logit of the dependent variable was assessed via the Box-Tidwell (1962) procedure. A Bonferroni correction was applied using all eight terms in the model resulting in statistical significance being accepted when p &lt; 0.00625 (Tabachnick &amp; Fidell, 2014). Based on this assessment, all continuous independent variables were found to be linearly related to the logit of the dependent variable. There were two standardized residuals with value of 2.01 and 2.27 standard deviations, which were kept in the analysis. The logistic regression model was statistically significant, χ2(4) = 27.40, p &lt; .001. The model explained 33.0% (Nagelkerke R2) of the variance in heart disease and correctly classified 71.0% of cases. Sensitivity was 45.7%, specificity was 84.6%, positive predictive value was 61.5% and negative predictive value was 74.3%. Of the five predictor variables only three were statistically significant: age, gender and VO2max (as shown in Table 1). Females had 0.14 times lower odds to exhibit heart disease than males. Increasing age was associated with an increased likelihood of exhibiting heart disease, but increasing VO2max was associated with a reduction in the likelihood of exhibiting heart disease. gtsummary::tbl_regression( cs2$fit_nnet, exponentiate = TRUE ) #muwimfsbvd table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #muwimfsbvd thead, #muwimfsbvd tbody, #muwimfsbvd tfoot, #muwimfsbvd tr, #muwimfsbvd td, #muwimfsbvd th { border-style: none; } #muwimfsbvd p { margin: 0; padding: 0; } #muwimfsbvd .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #muwimfsbvd .gt_caption { padding-top: 4px; padding-bottom: 4px; } #muwimfsbvd .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #muwimfsbvd .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #muwimfsbvd .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #muwimfsbvd .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #muwimfsbvd .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #muwimfsbvd .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #muwimfsbvd .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #muwimfsbvd .gt_column_spanner_outer:first-child { padding-left: 0; } #muwimfsbvd .gt_column_spanner_outer:last-child { padding-right: 0; } #muwimfsbvd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #muwimfsbvd .gt_spanner_row { border-bottom-style: hidden; } #muwimfsbvd .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #muwimfsbvd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #muwimfsbvd .gt_from_md > :first-child { margin-top: 0; } #muwimfsbvd .gt_from_md > :last-child { margin-bottom: 0; } #muwimfsbvd .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #muwimfsbvd .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #muwimfsbvd .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #muwimfsbvd .gt_row_group_first td { border-top-width: 2px; } #muwimfsbvd .gt_row_group_first th { border-top-width: 2px; } #muwimfsbvd .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #muwimfsbvd .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #muwimfsbvd .gt_first_summary_row.thick { border-top-width: 2px; } #muwimfsbvd .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #muwimfsbvd .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #muwimfsbvd .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #muwimfsbvd .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #muwimfsbvd .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #muwimfsbvd .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #muwimfsbvd .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #muwimfsbvd .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #muwimfsbvd .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #muwimfsbvd .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #muwimfsbvd .gt_left { text-align: left; } #muwimfsbvd .gt_center { text-align: center; } #muwimfsbvd .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #muwimfsbvd .gt_font_normal { font-weight: normal; } #muwimfsbvd .gt_font_bold { font-weight: bold; } #muwimfsbvd .gt_font_italic { font-style: italic; } #muwimfsbvd .gt_super { font-size: 65%; } #muwimfsbvd .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #muwimfsbvd .gt_asterisk { font-size: 100%; vertical-align: 0; } #muwimfsbvd .gt_indent_1 { text-indent: 5px; } #muwimfsbvd .gt_indent_2 { text-indent: 10px; } #muwimfsbvd .gt_indent_3 { text-indent: 15px; } #muwimfsbvd .gt_indent_4 { text-indent: 20px; } #muwimfsbvd .gt_indent_5 { text-indent: 25px; } Characteristic OR1 95% CI1 p-value general ses     low — —     middle 0.59 0.25, 1.40 0.2     high 0.31 0.11, 0.86 0.024 write_c 0.58 0.39, 0.86 0.007 vocation ses     low — —     middle 1.34 0.53, 3.40 0.5     high 0.37 0.12, 1.20 0.10 write_c 0.34 0.23, 0.51 1 OR = Odds Ratio, CI = Confidence Interval 2.3 Ordinal Logistic Regression Ordinal logistic regression, also call cumulative link model (CLM), is a generalized linear model (GZLM), an extension of the general linear model (GLM) to non-continuous outcome variables. There are many approaches to ordinal logistic regression, including cumulative, adjacent, and continuation categories, but the most popular is the cumulative odds ordinal logistic regression with proportional odds. These notes rely on UVA, PSU, Laerd, and the CLM package article vignette. The model for ordinal response random variable \\(Y_i\\) with \\(J\\) levels is \\[\\gamma_{ij} = F(\\eta_{ij}), \\hspace{5 mm} \\eta_{ij} = \\theta_j - x_i^\\mathrm{T}\\beta, \\hspace{5 mm} i = 1, \\ldots, n, \\hspace{5 mm} j = 1, \\ldots, J-1\\] where \\(\\gamma_{ij} = P(Y_i \\le j) = \\pi_{i1} + \\cdots + \\pi_{ij}\\). \\(\\eta_{ij}\\) is a linear predictor with \\(J-1\\) intercepts. \\(F\\) is the inverse link function. The regression models the logit link function of \\(\\gamma_{ij}\\). \\[\\mathrm{logit}(\\gamma_{ij}) = \\log \\left[\\frac{P(Y_i \\le j)}{P(Y_i \\gt j)} \\right] = \\theta_j - x_i^\\mathrm{T}\\beta\\] The cumulative logit is the log-odds of the cumulative probabilities that the response is in category \\(\\le j\\) versus \\(\\gt j\\). \\(\\theta_j\\) is the log-odds when \\(x_i^\\mathrm{T}=0\\) and \\(\\beta\\) is the increase in the log odds attributed to a one unit increase in \\(x_i^\\mathrm{T}\\). Notice \\(\\beta\\) is the same for all \\(j\\). Once you fit the model, you will either generate predicted values or evaluate the coefficient estimators. The predicted value is a log-odds by default (useless), so you will at least take exponential to get the odds. From there you can solve for the probability, \\[P(Y_i \\gt j) = \\frac{\\mathrm{exp}(\\hat{y}_i)}{1 + \\mathrm{exp}(\\hat{y}_i)}.\\] The exponential of \\(\\beta\\) is the odds ratio of \\(x_1^\\mathrm{T} - x_0^\\mathrm{T}\\). You can solve for the odds ratio \\[\\mathrm{OR} = \\frac{\\mathrm{exp}(\\theta_j - x_1^\\mathrm{T}\\beta)}{\\mathrm{exp}(\\theta_j - x_2^\\mathrm{T}\\beta)} = \\mathrm{exp}(\\beta(x_1^\\mathrm{T} - x_0^\\mathrm{T}))\\] If \\(x\\) is a binary factor factor, then \\(exp(\\beta)\\) is the odds ratio of \\(x=1\\) vs \\(x=0\\). Thus the odds-ratio is proportional to the difference between values of \\(x\\) and \\(\\beta\\) is the constant of proportionality. The model is estimated with a regularized Newton-Raphson algorithm with step-halving (line search) using analytic expressions for the gradient and Hessian of the negative log-likelihood function. What this means is beyond me right now, but the upshot is that the estimation is an iterative maximization exercise, not a formulaic matrix algebra process. It is possible for the model estimation to fail to converge on a maximum. You will sometimes encounter discussion about the latent variable. That is just the underlying quality you are trying to measure. If you rate something a 4 on a 5-level likert scale, 4 is the expression of your valuation, the latent variable. Your precise valuation is somewhere between 3 and 5 on a continuous scale. The link function defines the distribution of the latent variable. There are variations on the ordinal model. Structured thresholds impose restrictions on \\(\\theta_j\\), for example requiring equal distances between levels. Partial proportional odds allow \\(\\theta_j\\) to vary with nominal predictors. You can also use link functions other than logit. There are two conditions to ordinal logistic regression: (a) no multicollinearity, and (b) proportional odds. Case Study 3 192 participants in a study respond to the statement “Taxes are too high” on a 4-level likert scale (tax_too_high, Strongly Disagree, Disagree, Agree, Strongly Agree). Participant attributes include business owner (biz_owner, Y|N), age (age), and political affiliation (politics, Liberal, Conservative, Labor). tax$gt &lt;- tbl_summary( tax$dat %&gt;% select(-age), by = politics, statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;) ) tax$gt #lzjzpoqsnm table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #lzjzpoqsnm thead, #lzjzpoqsnm tbody, #lzjzpoqsnm tfoot, #lzjzpoqsnm tr, #lzjzpoqsnm td, #lzjzpoqsnm th { border-style: none; } #lzjzpoqsnm p { margin: 0; padding: 0; } #lzjzpoqsnm .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #lzjzpoqsnm .gt_caption { padding-top: 4px; padding-bottom: 4px; } #lzjzpoqsnm .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #lzjzpoqsnm .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #lzjzpoqsnm .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lzjzpoqsnm .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lzjzpoqsnm .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lzjzpoqsnm .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #lzjzpoqsnm .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #lzjzpoqsnm .gt_column_spanner_outer:first-child { padding-left: 0; } #lzjzpoqsnm .gt_column_spanner_outer:last-child { padding-right: 0; } #lzjzpoqsnm .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #lzjzpoqsnm .gt_spanner_row { border-bottom-style: hidden; } #lzjzpoqsnm .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #lzjzpoqsnm .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #lzjzpoqsnm .gt_from_md > :first-child { margin-top: 0; } #lzjzpoqsnm .gt_from_md > :last-child { margin-bottom: 0; } #lzjzpoqsnm .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #lzjzpoqsnm .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #lzjzpoqsnm .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #lzjzpoqsnm .gt_row_group_first td { border-top-width: 2px; } #lzjzpoqsnm .gt_row_group_first th { border-top-width: 2px; } #lzjzpoqsnm .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lzjzpoqsnm .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #lzjzpoqsnm .gt_first_summary_row.thick { border-top-width: 2px; } #lzjzpoqsnm .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lzjzpoqsnm .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lzjzpoqsnm .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #lzjzpoqsnm .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #lzjzpoqsnm .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #lzjzpoqsnm .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lzjzpoqsnm .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lzjzpoqsnm .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #lzjzpoqsnm .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lzjzpoqsnm .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #lzjzpoqsnm .gt_left { text-align: left; } #lzjzpoqsnm .gt_center { text-align: center; } #lzjzpoqsnm .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #lzjzpoqsnm .gt_font_normal { font-weight: normal; } #lzjzpoqsnm .gt_font_bold { font-weight: bold; } #lzjzpoqsnm .gt_font_italic { font-style: italic; } #lzjzpoqsnm .gt_super { font-size: 65%; } #lzjzpoqsnm .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #lzjzpoqsnm .gt_asterisk { font-size: 100%; vertical-align: 0; } #lzjzpoqsnm .gt_indent_1 { text-indent: 5px; } #lzjzpoqsnm .gt_indent_2 { text-indent: 10px; } #lzjzpoqsnm .gt_indent_3 { text-indent: 15px; } #lzjzpoqsnm .gt_indent_4 { text-indent: 20px; } #lzjzpoqsnm .gt_indent_5 { text-indent: 25px; } Characteristic Lab, N = 621 Lib, N = 541 Con, N = 761 biz_owner 36 (58%) 26 (48%) 47 (62%) income 36 (8) 35 (6) 37 (7) tax_too_high     Strongly Disagree 10 (16%) 10 (19%) 4 (5.3%)     Disagree 13 (21%) 16 (30%) 9 (12%)     Agree 28 (45%) 21 (39%) 42 (55%)     Strongly Agree 11 (18%) 7 (13%) 21 (28%) 1 n (%); Mean (SD) tax$dat %&gt;% mutate(income = case_when(income &lt; quantile(income, .25) ~ &quot;low income&quot;, income &lt; quantile(income, .75) ~ &quot;med income&quot;, TRUE ~ &quot;high income&quot;), income = factor(income, levels = c(&quot;low income&quot;, &quot;med income&quot;, &quot;high income&quot;))) %&gt;% count(tax_too_high, biz_owner, politics, income) %&gt;% ggplot(aes(x = tax_too_high, y = n, fill = biz_owner)) + geom_col(position = position_dodge2(preserve = &quot;single&quot;)) + facet_grid(rows = vars(income), cols = vars(politics), space = &quot;free&quot;) + scale_x_discrete(labels = function (x) str_wrap(x, width = 10)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + labs(title = &quot;Taxes too high?&quot;, subtitle = &quot;Reponse count by business owner, income level, and party.&quot;) Fit a cumulative link model for the cumulative probability of the \\(i\\)th response falling in \\(j\\)th category or below where \\(i\\) indexes the (\\(n = 192\\)) responses, \\(j = 1, \\ldots, J\\) indexes the (\\(J = 4\\)) response categories, and \\(\\theta_j\\) is the threshold for the \\(j\\)th cumulative logit. \\[\\mathrm{logit}(P(Y_i \\le j)) = \\theta_j - \\beta_1(\\mathrm{politics}_i) - \\beta_2(\\mathrm{biz\\_owner}_i) - \\beta_3(\\mathrm{age}_i)\\] Fit the Model tax$fmla &lt;- formula(tax_too_high ~ biz_owner + age + politics) tax$clm &lt;- clm(tax$fmla, data = tax$dat) summary(tax$clm) ## formula: tax_too_high ~ biz_owner + age + politics ## data: tax$dat ## ## link threshold nobs logLik AIC niter max.grad cond.H ## logit flexible 192 -197.62 409.23 6(0) 3.14e-12 3.2e+05 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## biz_ownerYes 0.66462 0.28894 2.300 0.021435 * ## age 0.24189 0.03260 7.421 1.17e-13 *** ## politicsLib 0.03695 0.36366 0.102 0.919072 ## politicsCon 1.16142 0.34554 3.361 0.000776 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Threshold coefficients: ## Estimate Std. Error z value ## Strongly Disagree|Disagree 7.026 1.166 6.024 ## Disagree|Agree 8.766 1.231 7.119 ## Agree|Strongly Agree 11.653 1.357 8.590 The summary table shows two fit statistics at the top: the log-likelihood and the AIC. The log-likelihood is the sum of the likelihoods for each observation that the predicted value correctly predicts the observed value. Its value ranges from \\(-\\infty\\) to \\(+\\infty\\). It’s value increases with observations, additional variables, and fit quality. I think you just use it to compare alternative model formulations. Akaike Information Criterion (AIC) is a measure of the relative quality of a statistical model. Again, I think the value is only useful as a comparison benchmark between alternative model fits. You’d want the model with the lowest AIC. The Coefficients table is the familiar parameter estimates. E.g., for biz_ownerYes, the coefficient estimate is 0.665 with standard error 0.289. The z-value is the ratio \\(z = \\hat{\\beta} / se =\\) 2.300 with p-value equal to \\(2 \\cdot P(Z&gt;z) =\\) 0.021. Some programs (e.g., SPSS) also show the Wald chi-squared statistic, the square of the z statistic, \\(z^2 =\\), 5.291. The square of a normal variable has a chi-square distribution, so the p value for the Wald chi-squared statistic is the pchisq(z^2, df = 1) \\(=\\) 0.021. The Threshold coefficients table are the intercepts, or cut-points. The first cut-point is log-odds of a response level Strongly Disagree (or less) vs greater than Strongly Disagree when all factor variables are at their reference level and the continuous vars are at 0. There may be interaction effects between biz_owner and politics. Fit a saturate model, then compare their log likelihoods with a likelihood ratio test. tax$fmla_sat &lt;- formula(tax_too_high ~ biz_owner*politics + age) tax$clm_sat &lt;- clm(tax$fmla_sat, data = tax$dat) tax$sat_anova &lt;- anova(tax$clm, tax$clm_sat) tax$sat_anova ## Likelihood ratio tests of cumulative link models: ## ## formula: link: threshold: ## tax$clm tax$fmla logit flexible ## tax$clm_sat tax$fmla_sat logit flexible ## ## no.par AIC logLik LR.stat df Pr(&gt;Chisq) ## tax$clm 7 409.23 -197.62 ## tax$clm_sat 9 411.75 -196.87 1.4805 2 0.477 The likelihood ratio test indicates the main-effects model fits about the same in comparison to the saturated model (\\(\\chi^2(\\) 2 $) = $ LR = 1.48, p = 0.477) Verify Assumptions Cumulative odds ordinal logistic regression with proportional odds models require a) no multicollinearity, and b) proportional odds. Multicollinearity Multicollinearity occurs when two or more independent variables are highly correlated so that they do not provide unique or independent information in the regression model. Multicollinearity inflates the variances of the estimated coefficients, resulting in larger confidence intervals. The usual interpretation of a slope coefficient as the change in the mean response per unit increase in the predictor when all the other predictors are held constant breaks down because changing one predictor necessarily changes other predictors. Test for multicollinearity with variance inflation factors (VIF). The VIF is the inflation percentage of the parameter variance due to multicollinearity. E.g., a VIF of 1.9 means the parameter variance is 90% larger than what it would be if it was not correlated with other predictors. Predictor \\(K\\)’s variance, \\(Var(\\hat{\\beta_k})\\), is inflated by a factor of \\[VIF_k = \\frac{1}{1 - R_k^2}\\] due to collinearity with other predictors, where \\(R_k^2\\) is the \\(R^2\\) of a regression of the \\(k^{th}\\) predictor on the remaining predictors. If there is zero relationship between predictor \\(k\\) and the other variables, \\(R_k^2 = 0\\) and \\(VIF = 1\\) (no variance inflation). If 100% of the variance in predictor \\(k\\) is explained by the other predictors, then \\(R_k^2 = 1\\) and \\(VIF = \\infty\\). A good rule of thumb is that \\(VIF \\le 5\\) is acceptable. # Cannot use CLM model with vif(). Re-express as a linear model. tax$vif &lt;- lm(as.numeric(tax_too_high) ~ politics + biz_owner + age, dat = tax$dat) %&gt;% car::vif() tax$vif ## GVIF Df GVIF^(1/(2*Df)) ## politics 1.035831 2 1.008840 ## biz_owner 1.023642 1 1.011752 ## age 1.036491 1 1.018082 The VIFs in column GVIF are all below 5, so this model is not compromised by multicollinearity. Proportional Odds The assumption of proportional odds means the independent variable effects are constant across each cumulative split of the ordinal dependent variable. Test for proportional odds using a full likelihood ratio test comparing the proportional odds model with a multinomial logit model, also called an unconstrained baseline logit model. This is also called the test of parallel lines. The multinomial logit model fits a slope to each of the \\(J – 1\\) levels. The proportional odds model is nested within the multinomial model, so you can use a likelihood ratio test to see if the models are statistically different. Fit the proportional odds model and a multinomial model using VGAM::vglm() and capture the log likelihoods and degrees of freedom. Perform a likelihood ratio test on the differences in log likelihoods, \\(D = -2 \\mathrm{loglik}(\\beta)\\). tax$vglm_ordinal &lt;- vglm(tax$fmla, propodds, data = tax$dat) tax$vglm_multinomial &lt;- vglm(tax$fmla, cumulative, data = tax$dat) tax$po_lrt &lt;- VGAM::lrtest(tax$vglm_multinomial, tax$vglm_ordinal) tax$po_lrt ## Likelihood ratio test ## ## Model 1: tax_too_high ~ biz_owner + age + politics ## Model 2: tax_too_high ~ biz_owner + age + politics ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 561 -193.31 ## 2 569 -197.62 8 8.6197 0.3754 The assumption of proportional odds was met, as assessed by a full likelihood ratio test comparing the fit of the proportional odds model to a model with varying location parameters, \\(\\chi^2\\)(8) = 8.620, p = 0.375. Another option is the partial proportional odds test. This test locates specific variables causing the rejection of proportional odds. tax$po_lrt2 &lt;- clm(tax$fmla, data = tax$dat) %&gt;% nominal_test() tax$po_lrt2 ## Tests of nominal effects ## ## formula: tax_too_high ~ biz_owner + age + politics ## Df logLik AIC LRT Pr(&gt;Chi) ## &lt;none&gt; -197.62 409.23 ## biz_owner 2 -197.34 412.67 0.55974 0.7559 ## age ## politics 4 -196.20 414.40 2.83415 0.5860 The assumption of proportional odds was met, as assessed by a full likelihood ratio test comparing the fit of the proportional odds model to a model with varying location parameters for business owner, \\(\\chi^2\\)(2) = 0.560, p = 0.756 and politics, \\(\\chi^2\\)(4) = 2.834, p = 0.586. Assess the Model Fit There are three ways to assess overall model fit: The Deviance and Pearson goodness-of-fit tests of the overall model fit; the Cox and Snell, Nagelkerke, and McFadden pseudo R measures of explained variance; and the likelihood ratio test comparing the model fit to the intercept-only model. Deviance and Pearson However, these tests rely on large frequencies in each cell, that is, each possible combination of predictor values. Overall goodness-of-fit statistics should be treated with suspicion when a continuous independent variable is present and/or there are a large number of cells with zero frequency.The Pearson goodness-of-fit statistic is \\(X^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\) where \\(i\\) is the observation number and \\(j\\) is the response variable level. It is a summary of the Pearson residuals, the difference between the observed and expected cell counts, \\(O_{ij} - E_{ij}\\). The deviance goodness-of-fit statistic is the difference in fit between the model and a full model; a full model being a model that fits the data perfectly, \\(G^2 = 2 \\sum_{ij} O_{ij} \\log \\left( \\frac{O_{ij}}{E_{ij}} \\right)\\). Neither of these tests are reliable if there are many cells with zero frequencies and/or small expected frequencies and are generally not recommended. Generally, the chi-squared test requires a frequency count of at least 5 per cell. # Observed combinations of model vars tax$cell_patterns &lt;- tax$dat %&gt;% count(biz_owner, age, politics, tax_too_high) %&gt;% nrow() # Observed combinations of predictor vars * levels of response var tax$covariate_patterns &lt;- tax$dat %&gt;% count(biz_owner, age, politics) %&gt;% nrow() tax$possible_cells &lt;- tax$covariate_patterns * length(levels(tax$dat$tax_too_high)) # 1 - ratio of observed to possible tax$pct_freq_zero &lt;- 1 - tax$cell_patterns / tax$possible_cells There are 137 observed combinations of model variables (predictors), and 372 possible combinations (predictors * outcome levels), so 63.2% of cells have zero frequencies. Ideally, zero frequencies should be less than 20%, so if you were to use the deviance or Pearson tests, you would need to report this. The results below are contradictory and bogus. I think you’d only use this test if you didn’t have continuous predictor variables. observed &lt;- tax$dat %&gt;% count(biz_owner, age, politics, tax_too_high) %&gt;% pivot_wider(names_from = tax_too_high, values_from = n) %&gt;% replace_na(list(`Strongly Disagree` = 0, Disagree = 0, Agree = 0, `Strongly Agree` = 0)) %&gt;% pivot_longer(cols = `Strongly Disagree`:`Strongly Agree`, names_to = &quot;outcome&quot;, values_to = &quot;observed&quot;) expected &lt;- bind_cols( tax$dat, predict(tax$clm, subset(tax$dat, select = -tax_too_high))$fit %&gt;% data.frame() ) %&gt;% rename(c(&quot;Strongly Disagree&quot; = &quot;Strongly.Disagree&quot;, &quot;Strongly Agree&quot; = &quot;Strongly.Agree&quot;)) %&gt;% group_by(biz_owner, age, politics) %&gt;% summarize(.groups = &quot;drop&quot;, across(`Strongly Disagree`:`Strongly Agree`, sum)) %&gt;% pivot_longer(cols = `Strongly Disagree`:`Strongly Agree`, names_to = &quot;outcome&quot;, values_to = &quot;expected&quot;) obs_exp &lt;- observed %&gt;% inner_join(expected, by = c(&quot;politics&quot;, &quot;biz_owner&quot;, &quot;age&quot;, &quot;outcome&quot;)) %&gt;% mutate(epsilon_sq = (observed - expected)^2, chi_sq = epsilon_sq / expected, g_sq = 2 * observed * log((observed+.0001) / expected) ) tax$chisq &lt;- list() tax$chisq$X2 = sum(obs_exp$chi_sq) tax$chisq$G2 = sum(obs_exp$g_sq) tax$chisq$df = tax$covariate_patterns * (length(levels(tax$dat$tax_too_high)) - 1) - 7 tax$chisq$X2_p.value = pchisq(tax$chisq$X2, df = tax$chisq$df, lower.tail = FALSE) tax$chisq$G2_p.value = pchisq(tax$chisq$G2, df = tax$chisq$df, lower.tail = FALSE) The Pearson goodness-of-fit test indicated that the model was not a good fit to the observed data, \\(\\chi^2\\)(272) = 745.4, p &lt; .001$. The deviance goodness-of-fit test indicated that the model was a good fit to the observed data, \\(G^2\\)(272) = 232.6, p = 0.960. Pseudo-R2 Measures There are a number of measures in ordinal regression that attempt to provide a similar “variance explained” measure as that provided in ordinary least-squares linear regression. However, these measures do not have the direct interpretation that they do in ordinary linear regression and are often, therefore, referred to as “pseudo” R2 measures. The three most common measures (Cox and Snell, Nagelkerke, and McFadden) are not particularly good and not universally used. It is presented in the SPSS output, so you might encounter it in published work. tax$nagelkerke &lt;- rcompanion::nagelkerke(tax$clm) tax$nagelkerke$Pseudo.R.squared.for.model.vs.null ## Pseudo.R.squared ## McFadden 0.181957 ## Cox and Snell (ML) 0.367369 ## Nagelkerke (Cragg and Uhler) 0.399641 Likelihood Ratio Test The best way to assess model fit is the likelihood ratio test comparing the model to an intercept-only model. The difference in the -2 log likelihood between the models has a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in the number of parameters. intercept_only &lt;- clm(tax_too_high ~ 1, data = tax$dat) tax$lrt &lt;- anova(tax$clm, intercept_only) tax$lrt ## Likelihood ratio tests of cumulative link models: ## ## formula: link: threshold: ## intercept_only tax_too_high ~ 1 logit flexible ## tax$clm tax$fmla logit flexible ## ## no.par AIC logLik LR.stat df Pr(&gt;Chisq) ## intercept_only 3 489.14 -241.57 ## tax$clm 7 409.23 -197.62 87.911 4 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The table shows the log likelihoods of the two models. LR.stat is the difference between 2 * the logLik values. The final model statistically significantly predicted the dependent variable over and above the intercept-only model, \\(\\chi^2(4)\\) = 87.9, p = 0.000. Interpret Results Return to the model summary. tidy(tax$clm) ## # A tibble: 7 × 6 ## term estimate std.error statistic p.value coef.type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Strongly Disagree|Disagree 7.03 1.17 6.02 1.70e- 9 intercept ## 2 Disagree|Agree 8.77 1.23 7.12 1.08e-12 intercept ## 3 Agree|Strongly Agree 11.7 1.36 8.59 8.72e-18 intercept ## 4 biz_ownerYes 0.665 0.289 2.30 2.14e- 2 location ## 5 age 0.242 0.0326 7.42 1.17e-13 location ## 6 politicsLib 0.0369 0.364 0.102 9.19e- 1 location ## 7 politicsCon 1.16 0.346 3.36 7.76e- 4 location The coefficients for biz_owner, age, and politics are positive. Positive parameters increase the likelihood of stronger agreement with the statement. In this case, discontent with taxes are higher for business owners, increase with age, and are higher for Liberal Democrats and Conservatives relative to the Labor Party. The expected cumulative log-odds of declaring \\(\\le j\\) level of agreement with the statement for the baseline group (biz_ownerNo, age = 0, politicsLib) is 7.026 for \\(j = 1\\) (Strongly Disagree), 8.766 for \\(j = 2\\) (Disagree), and 11.653 for \\(j = 3\\) (Agree). You could solve the logit equation for \\[\\pi_j = \\frac{\\mathrm{exp}(Y_i)} {1 + \\mathrm{exp}(Y_i)}\\] to get the cumulative probabilities for each level. That’s what predict(type = \"cum.prob\") does. But it might be more intuitive to work with individual probabilities, the lagged differences to get the individual probabilities for each \\(j\\). That’s what predict(type = \"prob\") does. I like to play with predicted values to get a sense of the outcome distributions. In this case, I’ll take the median age, and each combination of biz_owner and politics. new_data &lt;- tax$dat %&gt;% mutate(age = median(tax$dat$age)) %&gt;% expand(age, politics, biz_owner) preds &lt;- predict(tax$clm, newdata = new_data, type = &quot;prob&quot;)[[&quot;fit&quot;]] %&gt;% as.data.frame() bind_cols(new_data, preds) %&gt;% pivot_longer(cols = `Strongly Disagree`:`Strongly Agree`) %&gt;% mutate(name = factor(name, levels = levels(tax$dat$tax_too_high))) %&gt;% ggplot(aes(y = politics, x = value, fill = fct_rev(name))) + geom_col() + geom_text(aes(label = scales::percent(value, accuracy = 1)), size = 3, position = position_stack(vjust=0.5)) + facet_grid(~paste(&quot;Bus Owner = &quot;, biz_owner)) + scale_fill_grey(start = 0.5, end = 0.8) + theme_bw() + theme(legend.position = &quot;top&quot;, axis.text.x = element_blank(), axis.ticks.x = element_blank()) + guides(fill = guide_legend(reverse = TRUE)) + labs(title = &quot;Taxes too High for Conservative Business Owners?&quot;, x = NULL, fill = NULL) You will want to establish whether politics is statistically significant overall before exploring any specific contrasts. The ANOVA procedure with type I test reports an overall test of significance for each variable entered into the model. (tax$anovaI &lt;- anova(tax$clm, type = &quot;I&quot;)) ## Type I Analysis of Deviance Table with Wald chi-square tests ## ## Df Chisq Pr(&gt;Chisq) ## biz_owner 1 13.201 0.0002798 *** ## age 1 57.413 3.533e-14 *** ## politics 2 14.636 0.0006635 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The political party last voted for has a statistically significant effect on the prediction of whether tax is thought to be too high, Wald \\(\\chi^2\\)(2) = 14.6, p = 0.001. The best way to work with the data is with the tidy(exponentiate = TRUE) version. tax$tidy &lt;- tax$clm %&gt;% tidy(conf.int = TRUE, exponentiate = TRUE) tax$tidy ## # A tibble: 7 × 8 ## term estimate std.error statistic p.value conf.low conf.high coef.type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Strongly D… 1.13e3 1.17 6.02 1.70e- 9 NA NA intercept ## 2 Disagree|A… 6.41e3 1.23 7.12 1.08e-12 NA NA intercept ## 3 Agree|Stro… 1.15e5 1.36 8.59 8.72e-18 NA NA intercept ## 4 biz_ownerY… 1.94e0 0.289 2.30 2.14e- 2 1.11 3.44 location ## 5 age 1.27e0 0.0326 7.42 1.17e-13 1.20 1.36 location ## 6 politicsLib 1.04e0 0.364 0.102 9.19e- 1 0.508 2.12 location ## 7 politicsCon 3.19e0 0.346 3.36 7.76e- 4 1.63 6.35 location Then you can summarize the table in words. The odds of business owners considering tax to be too high was 1.944 (95% CI, 1.107 to 3.443) times that of non-business owners, a statistically significant effect, z = 2.300, p = 0.021. The odds of Conservative voters considering tax to be too high was 3.194 (95% CI, 1.635 to 6.351) times that of Labour voters, a statistically significant effect, z = 3.361, p = 0.001. The odds of Liberal Democrat voters considering tax to be too high was similar to that of Labour voters (odds ratio of 1.038 (95% CI, 0.508 to 2.121), p = 0.919. An increase in age (expressed in years) was associated with an increase in the odds of considering tax too high, with an odds ratio of 1.274 (95% CI, 1.197 to 1.360), z = 7.421, p = 0.000. Reporting Here is the complete write-up. A cumulative odds ordinal logistic regression with proportional odds was run to determine the effect of business ownership, political party voted for, and age, on the belief that taxes are too high. There were proportional odds, as assessed by a full likelihood ratio test comparing the fitted model to a model with varying location parameters, \\(\\chi^2\\)(8) = 8.620, p = 0.375. The final model statistically significantly predicted the dependent variable over and above the intercept-only model, \\(\\chi^2(4)\\) = 87.9, p = 0.000. The odds of business owners considering tax to be too high was 1.944 (95% CI, 1.107 to 3.443) times that of non-business owners, a statistically significant effect, z = 2.300, p = 0.021. The political party last voted for has a statistically significant effect on the prediction of whether tax is thought to be too high, Wald \\(\\chi^2\\)(2) = 14.6, p = 0.001. The odds of Conservative voters considering tax to be too high was 3.194 (95% CI, 1.635 to 6.351) times that of Labour voters, a statistically significant effect, z = 3.361, p = 0.001. The odds of Liberal Democrat voters considering tax to be too high was similar to that of Labour voters (odds ratio of 1.038 (95% CI, 0.508 to 2.121), p = 0.919. An increase in age (expressed in years) was associated with an increase in the odds of considering tax too high, with an odds ratio of 1.274 (95% CI, 1.197 to 1.360), z = 7.421, p = 0.000. Package gtsummary shows a nice summary table. tbl_regression(tax$clm) #axptjscvqs table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #axptjscvqs thead, #axptjscvqs tbody, #axptjscvqs tfoot, #axptjscvqs tr, #axptjscvqs td, #axptjscvqs th { border-style: none; } #axptjscvqs p { margin: 0; padding: 0; } #axptjscvqs .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #axptjscvqs .gt_caption { padding-top: 4px; padding-bottom: 4px; } #axptjscvqs .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #axptjscvqs .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #axptjscvqs .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #axptjscvqs .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #axptjscvqs .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #axptjscvqs .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #axptjscvqs .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #axptjscvqs .gt_column_spanner_outer:first-child { padding-left: 0; } #axptjscvqs .gt_column_spanner_outer:last-child { padding-right: 0; } #axptjscvqs .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #axptjscvqs .gt_spanner_row { border-bottom-style: hidden; } #axptjscvqs .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #axptjscvqs .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #axptjscvqs .gt_from_md > :first-child { margin-top: 0; } #axptjscvqs .gt_from_md > :last-child { margin-bottom: 0; } #axptjscvqs .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #axptjscvqs .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #axptjscvqs .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #axptjscvqs .gt_row_group_first td { border-top-width: 2px; } #axptjscvqs .gt_row_group_first th { border-top-width: 2px; } #axptjscvqs .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #axptjscvqs .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #axptjscvqs .gt_first_summary_row.thick { border-top-width: 2px; } #axptjscvqs .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #axptjscvqs .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #axptjscvqs .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #axptjscvqs .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #axptjscvqs .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #axptjscvqs .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #axptjscvqs .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #axptjscvqs .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #axptjscvqs .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #axptjscvqs .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #axptjscvqs .gt_left { text-align: left; } #axptjscvqs .gt_center { text-align: center; } #axptjscvqs .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #axptjscvqs .gt_font_normal { font-weight: normal; } #axptjscvqs .gt_font_bold { font-weight: bold; } #axptjscvqs .gt_font_italic { font-style: italic; } #axptjscvqs .gt_super { font-size: 65%; } #axptjscvqs .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #axptjscvqs .gt_asterisk { font-size: 100%; vertical-align: 0; } #axptjscvqs .gt_indent_1 { text-indent: 5px; } #axptjscvqs .gt_indent_2 { text-indent: 10px; } #axptjscvqs .gt_indent_3 { text-indent: 15px; } #axptjscvqs .gt_indent_4 { text-indent: 20px; } #axptjscvqs .gt_indent_5 { text-indent: 25px; } Characteristic log(OR)1 95% CI1 p-value biz_owner     No — —     Yes 0.66 0.10, 1.2 0.021 age 0.24 0.18, 0.31 politics     Lab — —     Lib 0.04 -0.68, 0.75 >0.9     Con 1.2 0.49, 1.8 1 OR = Odds Ratio, CI = Confidence Interval 2.4 Poisson Regression Poisson models count data, like “traffic tickets per day”, or “website hits per day”. The response is an expected rate or intensity. For count data, specify the generalized model, this time with family = poisson or family = quasipoisson. Recall that the probability of achieving a count \\(y\\) when the expected rate is \\(\\lambda\\) is distributed \\[P(Y = y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^y}{y!}.\\] The poisson regression model is \\[\\lambda = \\exp(X \\beta).\\] You can solve this for \\(y\\) to get \\[y = X\\beta = \\ln(\\lambda).\\] That is, the model predicts the log of the response rate. For a sample of size n, the likelihood function is \\[L(\\beta; y, X) = \\prod_{i=1}^n \\frac{e^{-\\exp({X_i\\beta})}\\exp({X_i\\beta})^{y_i}}{y_i!}.\\] The log-likelihood is \\[l(\\beta) = \\sum_{i=1}^n (y_i X_i \\beta - \\sum_{i=1}^n\\exp(X_i\\beta) - \\sum_{i=1}^n\\log(y_i!).\\] Maximizing the log-likelihood has no closed-form solution, so the coefficient estimates are found through interatively reweighted least squares. Poisson processes assume the variance of the response variable equals its mean. “Equals” means the mean and variance are of a similar order of magnitude. If that assumption does not hold, use the quasi-poisson. Use Poisson regression for large datasets. If the predicted counts are much greater than zero (&gt;30), the linear regression will work fine. Whereas RMSE is not useful for logistic models, it is a good metric in Poisson. Case Study 4 Dataset fire contains response variable injuries counting the number of injuries during the month and one explanatory variable, the month mo. fire &lt;- read_csv(file = &quot;C:/Users/mpfol/OneDrive/Documents/Data Science/Data/CivilInjury_0.csv&quot;) fire &lt;- fire %&gt;% mutate(mo = as.POSIXlt(`Injury Date`)$mon + 1) %&gt;% rename(dt = `Injury Date`, injuries = `Total Injuries`) str(fire) In a situation like this where there the relationship is bivariate, start with a visualization. ggplot(fire, aes(x = mo, y = injuries)) + geom_jitter() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;poisson&quot;)) + labs(title = &quot;Injuries by Month&quot;) Fit a poisson regression in R using glm(formula, data, family = poisson). But first, check whether the mean and variance of injuries are the same magnitude? If not, then use family = quasipoisson. mean(fire$injuries) var(fire$injuries) They are of the same magnitude, so fit the regression with family = poisson. m2 &lt;- glm(injuries ~ mo, family = poisson, data = fire) summary(m2) The predicted value \\(\\hat{y}\\) is the estimated log of the response variable, \\[\\hat{y} = X \\hat{\\beta} = \\ln (\\lambda).\\] Suppose mo is January (mo = ), then the log ofinjuries` is \\(\\hat{y} = 0.323787\\). Or, more intuitively, the expected count of injuries is \\(\\exp(0.323787) = 1.38\\) predict(m2, newdata = data.frame(mo=1)) predict(m2, newdata = data.frame(mo=1), type = &quot;response&quot;) Here is a plot of the predicted counts in red. augment(m2, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = mo, y = injuries)) + geom_point() + geom_point(aes(y = .fitted), color = &quot;red&quot;) + scale_y_continuous(limits = c(0, NA)) + labs(x = &quot;Month&quot;, y = &quot;Injuries&quot;, title = &quot;Poisson Fitted Line Plot&quot;) Evaluate a logistic model fit with an analysis of deviance. (perf &lt;- glance(m2)) (pseudoR2 &lt;- 1 - perf$deviance / perf$null.deviance) The deviance of the null model (no regressors) is 139.9. The deviance of the full model is 132.2. The psuedo-R2 is very low at .05. How about the RMSE? RMSE(pred = predict(m2, type = &quot;response&quot;), obs = fire$injuries) The average prediction error is about 0.99. That’s almost as much as the variance of injuries - i.e., just predicting the mean of injuries would be almost as good! Use the GainCurvePlot() function to plot the gain curve. augment(m2, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = injuries, y = .fitted)) + geom_point() + geom_smooth(method =&quot;lm&quot;) + labs(x = &quot;Actual&quot;, y = &quot;Predicted&quot;, title = &quot;Poisson Fitted vs Actual&quot;) augment(m2) %&gt;% data.frame() %&gt;% GainCurvePlot(xvar = &quot;.fitted&quot;, truthVar = &quot;injuries&quot;, title = &quot;Poisson Model&quot;) It seems that mo was a poor predictor of injuries. References "],["regularization.html", "Chapter 3 Regularization 3.1 Ridge 3.2 Lasso 3.3 Elastic Net Model Summary", " Chapter 3 Regularization These notes are from this tutorial on DataCamp, the Machine Learning Toolbox DataCamp class, and Interpretable Machine Learning (Molnar 2020). Regularization is a set of methods that manage the bias-variance trade-off problem in linear regression. The linear regression model is \\(Y = X \\beta + \\epsilon\\), where \\(\\epsilon \\sim N(0, \\sigma^2)\\). OLS estimates the coefficients by minimizing the loss function \\[L = \\sum_{i = 1}^n \\left(y_i - x_i^{&#39;} \\hat\\beta \\right)^2.\\] The resulting estimate for the coefficients is \\[\\hat{\\beta} = \\left(X&#39;X\\right)^{-1}\\left(X&#39;Y\\right).\\] There are two important characteristics of any estimator: its bias and its variance. For OLS, these are \\[Bias(\\hat{\\beta}) = E(\\hat{\\beta}) - \\beta = 0\\] and \\[Var(\\hat{\\beta}) = \\sigma^2(X&#39;X)^{-1}\\] where the unknown population variance \\(\\sigma^2\\) is estimated from the residuals \\[\\hat\\sigma^2 = \\frac{\\epsilon&#39; \\epsilon}{n - k}.\\] The OLS estimator is unbiased, but can have a large variance when the predictor variables are highly correlated with each other, or when there are many predictors (notice how \\(\\hat{\\sigma}^2\\) increases as \\(k \\rightarrow n\\)). Stepwise selection balances the trade-off by eliminating variables, but this throws away information. Regularization keeps all the predictors, but reduces coefficient magnitudes to reduce variance at the expense of some bias. In the sections below, I’ll use the mtcars data set to predict mpg from the other variables using the caret::glmnet() function. glmnet() uses penalized maximum likelihood to fit generalized linear models such as ridge, lasso, and elastic net. I’ll compare the model performances by creating a training and validation set, and a common trainControl object to make sure the models use the same observations in the cross-validation folds. library(tidyverse) library(caret) data(&quot;mtcars&quot;) set.seed(123) partition &lt;- createDataPartition(mtcars$mpg, p = 0.8, list = FALSE) training &lt;- mtcars[partition, ] testing &lt;- mtcars[-partition, ] train_control &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 5, repeats = 5, savePredictions = &quot;final&quot; # saves predictions from optimal tuning parameters ) 3.1 Ridge Ridge regression estimates the linear model coefficients by minimizing an augmented loss function which includes a term, \\(\\lambda\\), that penalizes the magnitude of the coefficient estimates, \\[L = \\sum_{i = 1}^n \\left(y_i - x_i^{&#39;} \\hat\\beta \\right)^2 + \\lambda \\sum_{j=1}^k \\hat{\\beta}_j^2.\\] The resulting estimate for the coefficients is \\[\\hat{\\beta} = \\left(X&#39;X + \\lambda I\\right)^{-1}\\left(X&#39;Y \\right).\\] As \\(\\lambda \\rightarrow 0\\), ridge regression approaches OLS. The bias and variance for the ridge estimator are \\[Bias(\\hat{\\beta}) = -\\lambda \\left(X&#39;X + \\lambda I \\right)^{-1} \\beta\\] \\[Var(\\hat{\\beta}) = \\sigma^2 \\left(X&#39;X + \\lambda I \\right)^{-1}X&#39;X \\left(X&#39;X + \\lambda I \\right)^{-1}\\] The estimator bias increases with \\(\\lambda\\) and the estimator variance decreases with \\(\\lambda\\). The optimal level for \\(\\lambda\\) is the one that minimizes the root mean squared error (RMSE) or the Akaike or Bayesian Information Criterion (AIC or BIC), or R-squared. Example Specify alpha = 0 in a tuning grid for ridge regression (the following sections reveal how alpha distinguishes ridge, lasso, and elastic net). Note that I standardize the predictors in the preProcess step - ridge regression requires standardization. set.seed(1234) mdl_ridge &lt;- train( mpg ~ ., data = training, method = &quot;glmnet&quot;, metric = &quot;RMSE&quot;, # Choose from RMSE, RSquared, AIC, BIC, ...others? preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid( .alpha = 0, # optimize a ridge regression .lambda = seq(0, 5, length.out = 101) ), trControl = train_control ) mdl_ridge ## glmnet ## ## 28 samples ## 10 predictors ## ## Pre-processing: centered (10), scaled (10) ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 21, 24, 22, 21, 24, 21, ... ## Resampling results across tuning parameters: ## ## lambda RMSE Rsquared MAE ## 0.00 2.554751 0.8788756 2.234470 ## 0.05 2.554751 0.8788756 2.234470 ## 0.10 2.554751 0.8788756 2.234470 ## 0.15 2.554751 0.8788756 2.234470 ## 0.20 2.554751 0.8788756 2.234470 ## 0.25 2.554751 0.8788756 2.234470 ## 0.30 2.554751 0.8788756 2.234470 ## 0.35 2.554751 0.8788756 2.234470 ## 0.40 2.554751 0.8788756 2.234470 ## 0.45 2.554594 0.8789255 2.234683 ## 0.50 2.552197 0.8791114 2.233333 ## 0.55 2.541833 0.8793654 2.225176 ## 0.60 2.529913 0.8798195 2.215643 ## 0.65 2.519126 0.8803007 2.206810 ## 0.70 2.509440 0.8807077 2.198506 ## 0.75 2.500730 0.8810505 2.190795 ## 0.80 2.492807 0.8813491 2.183807 ## 0.85 2.485563 0.8816140 2.177294 ## 0.90 2.478997 0.8818405 2.171425 ## 0.95 2.472966 0.8820389 2.165789 ## 1.00 2.467383 0.8822118 2.160407 ## 1.05 2.462264 0.8823596 2.155277 ## 1.10 2.457491 0.8824959 2.150329 ## 1.15 2.453096 0.8826143 2.145577 ## 1.20 2.448994 0.8827201 2.141010 ## 1.25 2.445283 0.8828075 2.136689 ## 1.30 2.441821 0.8828813 2.132518 ## 1.35 2.438632 0.8829440 2.128488 ## 1.40 2.435595 0.8829971 2.124567 ## 1.45 2.432766 0.8830499 2.120776 ## 1.50 2.430196 0.8830912 2.117202 ## 1.55 2.427779 0.8831246 2.113911 ## 1.60 2.425544 0.8831528 2.110735 ## 1.65 2.423506 0.8831710 2.107696 ## 1.70 2.421585 0.8831843 2.104720 ## 1.75 2.419833 0.8831973 2.101850 ## 1.80 2.418232 0.8832041 2.099074 ## 1.85 2.416707 0.8832052 2.096356 ## 1.90 2.415308 0.8832049 2.093724 ## 1.95 2.414077 0.8831966 2.091421 ## 2.00 2.412915 0.8831858 2.089232 ## 2.05 2.411832 0.8831731 2.087084 ## 2.10 2.410862 0.8831612 2.084999 ## 2.15 2.410002 0.8831449 2.082982 ## 2.20 2.409207 0.8831253 2.081117 ## 2.25 2.408493 0.8831033 2.079633 ## 2.30 2.407877 0.8830816 2.078216 ## 2.35 2.407362 0.8830560 2.076947 ## 2.40 2.406888 0.8830284 2.075738 ## 2.45 2.406463 0.8829988 2.074538 ## 2.50 2.406109 0.8829717 2.073390 ## 2.55 2.405864 0.8829408 2.072316 ## 2.60 2.405693 0.8829069 2.071291 ## 2.65 2.405542 0.8828717 2.070256 ## 2.70 2.405448 0.8828350 2.069254 ## 2.75 2.405415 0.8827999 2.068291 ## 2.80 2.405452 0.8827624 2.067369 ## 2.85 2.405539 0.8827235 2.066542 ## 2.90 2.405637 0.8826837 2.065770 ## 2.95 2.405780 0.8826424 2.065242 ## 3.00 2.405964 0.8826039 2.064761 ## 3.05 2.406229 0.8825637 2.064336 ## 3.10 2.406541 0.8825222 2.063945 ## 3.15 2.406876 0.8824799 2.063540 ## 3.20 2.407231 0.8824370 2.063120 ## 3.25 2.407628 0.8823943 2.062722 ## 3.30 2.408062 0.8823537 2.062348 ## 3.35 2.408562 0.8823110 2.062118 ## 3.40 2.409100 0.8822673 2.062037 ## 3.45 2.409655 0.8822228 2.061944 ## 3.50 2.410216 0.8821782 2.061828 ## 3.55 2.410808 0.8821331 2.061723 ## 3.60 2.411432 0.8820899 2.061639 ## 3.65 2.412102 0.8820470 2.061577 ## 3.70 2.412809 0.8820035 2.061543 ## 3.75 2.413551 0.8819589 2.061568 ## 3.80 2.414295 0.8819141 2.061655 ## 3.85 2.415055 0.8818687 2.061734 ## 3.90 2.415843 0.8818234 2.062062 ## 3.95 2.416658 0.8817793 2.062424 ## 4.00 2.417511 0.8817359 2.062798 ## 4.05 2.418400 0.8816915 2.063183 ## 4.10 2.419316 0.8816465 2.063584 ## 4.15 2.420238 0.8816012 2.063970 ## 4.20 2.421163 0.8815562 2.064598 ## 4.25 2.422110 0.8815102 2.065248 ## 4.30 2.423081 0.8814658 2.065910 ## 4.35 2.424072 0.8814226 2.066657 ## 4.40 2.425104 0.8813791 2.067521 ## 4.45 2.426155 0.8813356 2.068385 ## 4.50 2.427230 0.8812914 2.069250 ## 4.55 2.428308 0.8812470 2.070099 ## 4.60 2.429387 0.8812029 2.070929 ## 4.65 2.430482 0.8811578 2.071757 ## 4.70 2.431595 0.8811139 2.072827 ## 4.75 2.432731 0.8810710 2.073935 ## 4.80 2.433886 0.8810292 2.075036 ## 4.85 2.435074 0.8809864 2.076150 ## 4.90 2.436276 0.8809438 2.077259 ## 4.95 2.437498 0.8809006 2.078371 ## 5.00 2.438716 0.8808576 2.079462 ## ## Tuning parameter &#39;alpha&#39; was held constant at a value of 0 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were alpha = 0 and lambda = 2.75. The model printout shows the RMSE, R-Squared, and mean absolute error (MAE) values at each lambda specified in the tuning grid. The final three lines summarize what happened. It did not tune alpha because I held it at 0 for ridge regression; it optimized using RMSE; and the optimal tuning values (at the minimum RMSE) were alpha = 0 and lambda = 2.75. You plot the model to see the tuning results. ggplot(mdl_ridge) + labs(title = &quot;Ridge Regression Parameter Tuning&quot;, x = &quot;lambda&quot;) varImp() ranks the predictors by the absolute value of the coefficients in the tuned model. The most important variables here were wt, disp, and am. plot(varImp(mdl_ridge)) 3.2 Lasso Lasso stands for “least absolute shrinkage and selection operator”. Like ridge, lasso adds a penalty for coefficients, but instead of penalizing the sum of squared coefficients (L2 penalty), lasso penalizes the sum of absolute values (L1 penalty). As a result, for high values of \\(\\lambda\\), coefficients can be zeroed under lasso. The loss function for lasso is \\[L = \\sum_{i = 1}^n \\left(y_i - x_i^{&#39;} \\hat\\beta \\right)^2 + \\lambda \\sum_{j=1}^k \\left| \\hat{\\beta}_j \\right|.\\] Example Continuing with prediction of mpg from the other variables in the mtcars data set, follow the same steps as before, but with ridge regression. This time specify parameter alpha = 1 for ridge regression (it was 0 for ridge, and for elastic net it will be something in between and require optimization). set.seed(1234) mdl_lasso &lt;- train( mpg ~ ., data = training, method = &quot;glmnet&quot;, metric = &quot;RMSE&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid( .alpha = 1, # optimize a lasso regression .lambda = seq(0, 5, length.out = 101) ), trControl = train_control ) mdl_lasso$bestTune ## alpha lambda ## 14 1 0.65 The summary output shows the model did not tune alpha because I held it at 1 for lasso regression. The optimal tuning values (at the minimum RMSE) were alpha = 1 and lambda = 0.65. You can see the RMSE minimum on the the plot. ggplot(mdl_ridge) + labs(title = &quot;Lasso Regression Parameter Tuning&quot;, x = &quot;lambda&quot;) 3.3 Elastic Net Elastic Net combines the penalties of ridge and lasso to get the best of both worlds. The loss function for elastic net is \\[L = \\frac{\\sum_{i = 1}^n \\left(y_i - x_i^{&#39;} \\hat\\beta \\right)^2}{2n} + \\lambda \\frac{1 - \\alpha}{2}\\sum_{j=1}^k \\hat{\\beta}_j^2 + \\lambda \\alpha\\left| \\hat{\\beta}_j \\right|.\\] In this loss function, new parameter \\(\\alpha\\) is a “mixing” parameter that balances the two approaches. If \\(\\alpha\\) is zero, you are back to ridge regression, and if \\(\\alpha\\) is one, you are back to lasso. Example Continuing with prediction of mpg from the other variables in the mtcars data set, follow the same steps as before, but with elastic net regression there are two parameters to optimize: \\(\\lambda\\) and \\(\\alpha\\). set.seed(1234) mdl_elnet &lt;- train( mpg ~ ., data = training, method = &quot;glmnet&quot;, metric = &quot;RMSE&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid( .alpha = seq(0, 1, length.out = 10), # optimize an elnet regression .lambda = seq(0, 5, length.out = 101) ), trControl = train_control ) mdl_elnet$bestTune ## alpha lambda ## 56 0 2.75 The optimal tuning values (at the mininum RMSE) were alpha = 0.0 and lambda = 2.75, so the mix is 100% ridge, 0% lasso. You can see the RMSE minimum on the the plot. Alpha is on the horizontal axis and the different lambdas are shown as separate series. ggplot(mdl_elnet) + labs(title = &quot;Elastic Net Regression Parameter Tuning&quot;, x = &quot;lambda&quot;) Model Summary Make predictions on the validation data set for each of the three models. pr_ridge &lt;- postResample(pred = predict(mdl_ridge, newdata = testing), obs = testing$mpg) pr_lasso &lt;- postResample(pred = predict(mdl_lasso, newdata = testing), obs = testing$mpg) pr_elnet &lt;- postResample(pred = predict(mdl_elnet, newdata = testing), obs = testing$mpg) rbind(pr_ridge, pr_lasso, pr_elnet) ## RMSE Rsquared MAE ## pr_ridge 3.745350 0.9007955 2.761726 ## pr_lasso 4.032868 0.9734421 3.009135 ## pr_elnet 3.745350 0.9007955 2.761726 It looks like ridge/elnet was the big winner today based on RMSE and MAE. Lasso had the best Rsquared though. On average, ridge/elnet will miss the true value of mpg by 3.75 mpg (RMSE) or 2.76 mpg (MAE). The model explains about 90% of the variation in mpg. You can also compare the models by resampling. model.resamples &lt;- resamples(list(Ridge = mdl_ridge, Lasso = mdl_lasso, ELNet = mdl_elnet)) summary(model.resamples) ## ## Call: ## summary.resamples(object = model.resamples) ## ## Models: Ridge, Lasso, ELNet ## Number of resamples: 25 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Ridge 0.6614059 1.587531 2.179524 2.068291 2.523411 3.527409 0 ## Lasso 0.8148722 1.862340 2.237722 2.272930 2.596078 3.985098 0 ## ELNet 0.6614059 1.587531 2.179524 2.068291 2.523411 3.527409 0 ## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Ridge 0.7325538 1.935884 2.462086 2.405415 2.820892 4.264528 0 ## Lasso 0.9083703 2.138883 2.521894 2.623880 2.934442 4.506856 0 ## ELNet 0.7325538 1.935884 2.462086 2.405415 2.820892 4.264528 0 ## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Ridge 0.6919630 0.8357446 0.8904019 0.8827999 0.9382490 0.9791672 0 ## Lasso 0.6263831 0.8100574 0.8656579 0.8593787 0.9407846 0.9955706 0 ## ELNet 0.6919630 0.8357446 0.8904019 0.8827999 0.9382490 0.9791672 0 You want the smallest mean RMSE, and a small range of RMSEs. Ridge/elnet had the smallest mean, and a relatively small range. Boxplots are a common way to visualize this information. bwplot(model.resamples, metric = &quot;RMSE&quot;, main = &quot;Model Comparison on Resamples&quot;) Now that you have identified the optimal model, capture its tuning parameters and refit the model to the entire data set. set.seed(123) mdl_final &lt;- train( mpg ~ ., data = training, method = &quot;glmnet&quot;, metric = &quot;RMSE&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = data.frame( .alpha = mdl_ridge$bestTune$alpha, # optimized hyperparameters .lambda = mdl_ridge$bestTune$lambda), # optimized hyperparameters trControl = train_control ) mdl_final ## glmnet ## ## 28 samples ## 10 predictors ## ## Pre-processing: centered (10), scaled (10) ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 22, 22, 23, 22, 23, 23, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 2.441704 0.8884221 2.115503 ## ## Tuning parameter &#39;alpha&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;lambda&#39; was held constant at a value of 2.75 The model is ready to predict on new data! Here are some final conclusions on the models. Lasso can set some coefficients to zero, thus performing variable selection. Lasso and Ridge address multicollinearity differently: in ridge regression, the coefficients of correlated predictors are similar; In lasso, one of the correlated predictors has a larger coefficient, while the rest are (nearly) zeroed. Lasso tends to do well if there are a small number of significant parameters and the others are close to zero. Ridge tends to work well if there are many large parameters of about the same value. In practice, you don’t know which will be best, so run cross-validation pick the best. References "],["decision-trees.html", "Chapter 4 Decision Trees 4.1 Classification Tree 4.2 Regression Tree 4.3 Bagged Trees 4.4 Random Forests 4.5 Gradient Boosting 4.6 Summary", " Chapter 4 Decision Trees These notes rely on PSU STAT 508. Decision trees, also known as classification and regression tree (CART) models, are tree-based methods for supervised machine learning. Simple classification trees and regression trees are easy to use and interpret, but are not competitive with the best machine learning methods. However, they form the foundation for ensemble models such as bagged trees, random forests, and boosted trees, which although less interpretable, are very accurate. CART models segment the predictor space into \\(K\\) non-overlapping terminal nodes (leaves). Each node is described by a set of rules which can be used to predict new responses. The predicted value \\(\\hat{y}\\) for each node is the mode (classification) or mean (regression). CART models define the nodes through a top-down greedy process called recursive binary splitting. The process is top-down because it begins at the top of the tree with all observations in a single region and successively splits the predictor space. It is greedy because at each splitting step, the best split is made at that particular step without consideration to subsequent splits. The best split is the predictor variable and cutpoint that minimizes a cost function. The most common cost function for regression trees is the sum of squared residuals, \\[RSS = \\sum_{k=1}^K\\sum_{i \\in A_k}{\\left(y_i - \\hat{y}_{A_k} \\right)^2}.\\] For classification trees, it is the Gini index, \\[G = \\sum_{c=1}^C{\\hat{p}_{kc}(1 - \\hat{p}_{kc})},\\] and the entropy (aka information statistic) \\[D = - \\sum_{c=1}^C{\\hat{p}_{kc} \\log \\hat{p}_{kc}}\\] where \\(\\hat{p}_{kc}\\) is the proportion of training observations in node \\(k\\) that are class \\(c\\). A completely pure node in a binary tree would have \\(\\hat{p} \\in \\{ 0, 1 \\}\\) and \\(G = D = 0\\). A completely impure node in a binary tree would have \\(\\hat{p} = 0.5\\) and \\(G = 0.5^2 \\cdot 2 = 0.25\\) and \\(D = -(0.5 \\log(0.5)) \\cdot 2 = 0.69\\). CART repeats the splitting process for each child node until a stopping criterion is satisfied, usually when no node size surpasses a predefined maximum, or continued splitting does not improve the model significantly. CART may also impose a minimum number of observations in each node. The resulting tree likely over-fits the training data and therefore does not generalize well to test data, so CART prunes the tree, minimizing the cross-validated prediction error. Rather than cross-validating every possible subtree to find the one with minimum error, CART uses cost-complexity pruning. Cost-complexity is the tradeoff between error (cost) and tree size (complexity) where the tradeoff is quantified with cost-complexity parameter \\(c_p\\). The cost complexity of the tree, \\(R_{c_p}(T)\\), is the sum of its risk (error) plus a “cost complexity” factor \\(c_p\\) multiple of the tree size \\(|T|\\). \\[R_{c_p}(T) = R(T) + c_p|T|\\] \\(c_p\\) can take on any value from \\([0..\\infty]\\), but it turns out there is an optimal tree for ranges of \\(c_p\\) values, so there are only a finite set of interesting values for \\(c_p\\) (James et al. 2013) (Therneau and Atkinson 2019) (Kuhn and Johnson 2016). A parametric algorithm identifies the interesting \\(c_p\\) values and their associated pruned trees, \\(T_{c_p}\\). CART uses cross-validation to determine which \\(c_p\\) is optimal. 4.1 Classification Tree You don’t usually build a simple classification tree on its own, but it is a good way to build understanding, and the ensemble models build on the logic. I’ll learn by example, using the ISLR::OJ data set to predict which brand of orange juice, Citrus Hill (CH) or Minute Maid = (MM), customers Purchase from its 17 predictor variables. library(tidyverse) library(caret) library(rpart) # classification and regression trees library(rpart.plot) # better formatted plots than the ones in rpart oj_dat &lt;- ISLR::OJ skimr::skim(oj_dat) Table 4.1: Data summary Name oj_dat Number of rows 1070 Number of columns 18 _______________________ Column type frequency: factor 2 numeric 16 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Purchase 0 1 FALSE 2 CH: 653, MM: 417 Store7 0 1 FALSE 2 No: 714, Yes: 356 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist WeekofPurchase 0 1 254.38 15.56 227.00 240.00 257.00 268.00 278.00 ▆▅▅▇▇ StoreID 0 1 3.96 2.31 1.00 2.00 3.00 7.00 7.00 ▇▅▃▁▇ PriceCH 0 1 1.87 0.10 1.69 1.79 1.86 1.99 2.09 ▅▂▇▆▁ PriceMM 0 1 2.09 0.13 1.69 1.99 2.09 2.18 2.29 ▂▁▃▇▆ DiscCH 0 1 0.05 0.12 0.00 0.00 0.00 0.00 0.50 ▇▁▁▁▁ DiscMM 0 1 0.12 0.21 0.00 0.00 0.00 0.23 0.80 ▇▁▂▁▁ SpecialCH 0 1 0.15 0.35 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▂ SpecialMM 0 1 0.16 0.37 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▂ LoyalCH 0 1 0.57 0.31 0.00 0.33 0.60 0.85 1.00 ▅▃▆▆▇ SalePriceMM 0 1 1.96 0.25 1.19 1.69 2.09 2.13 2.29 ▁▂▂▂▇ SalePriceCH 0 1 1.82 0.14 1.39 1.75 1.86 1.89 2.09 ▂▁▇▇▅ PriceDiff 0 1 0.15 0.27 -0.67 0.00 0.23 0.32 0.64 ▁▂▃▇▂ PctDiscMM 0 1 0.06 0.10 0.00 0.00 0.00 0.11 0.40 ▇▁▂▁▁ PctDiscCH 0 1 0.03 0.06 0.00 0.00 0.00 0.00 0.25 ▇▁▁▁▁ ListPriceDiff 0 1 0.22 0.11 0.00 0.14 0.24 0.30 0.44 ▂▃▆▇▁ STORE 0 1 1.63 1.43 0.00 0.00 2.00 3.00 4.00 ▇▃▅▅▃ I’ll split oj_dat (n = 1,070) into oj_train (80%, n = 857) to fit various models, and oj_test (20%, n = 213) to compare their performance on new data. set.seed(12345) partition &lt;- createDataPartition(y = oj_dat$Purchase, p = 0.8, list = FALSE) oj_train &lt;- oj_dat[partition, ] oj_test &lt;- oj_dat[-partition, ] Function rpart::rpart() builds a full tree, minimizing the Gini index \\(G\\) by default (parms = list(split = \"gini\")), until the stopping criterion is satisfied. The default stopping criterion is only attempt a split if the current node has at least minsplit = 20 observations, and only accept a split if the resulting nodes have at least minbucket = round(minsplit/3) observations, and the resulting overall fit improves by cp = 0.01 (i.e., \\(\\Delta G &lt;= 0.01\\)). # Use method = &quot;class&quot; for classification, method = &quot;anova&quot; for regression set.seed(123) oj_mdl_cart_full &lt;- rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;) print(oj_mdl_cart_full) ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 334 CH (0.61026838 0.38973162) ## 2) LoyalCH&gt;=0.48285 537 94 CH (0.82495345 0.17504655) ## 4) LoyalCH&gt;=0.7648795 271 13 CH (0.95202952 0.04797048) * ## 5) LoyalCH&lt; 0.7648795 266 81 CH (0.69548872 0.30451128) ## 10) PriceDiff&gt;=-0.165 226 50 CH (0.77876106 0.22123894) * ## 11) PriceDiff&lt; -0.165 40 9 MM (0.22500000 0.77500000) * ## 3) LoyalCH&lt; 0.48285 320 80 MM (0.25000000 0.75000000) ## 6) LoyalCH&gt;=0.2761415 146 58 MM (0.39726027 0.60273973) ## 12) SalePriceMM&gt;=2.04 71 31 CH (0.56338028 0.43661972) * ## 13) SalePriceMM&lt; 2.04 75 18 MM (0.24000000 0.76000000) * ## 7) LoyalCH&lt; 0.2761415 174 22 MM (0.12643678 0.87356322) * The output starts with the root node. The predicted class at the root is CH and this prediction produces 334 errors on the 857 observations for a success rate (accuracy) of 61% (0.61026838) and an error rate of 39% (0.38973162). The child nodes of node “x” are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5). Terminal nodes are labeled with an asterisk (*). Surprisingly, only 3 of the 17 features were used the in full tree: LoyalCH (Customer brand loyalty for CH), PriceDiff (relative price of MM over CH), and SalePriceMM (absolute price of MM). The first split is at LoyalCH = 0.48285. Here is a diagram of the full (unpruned) tree. rpart.plot(oj_mdl_cart_full, yesno = TRUE) The boxes show the node classification (based on mode), the proportion of observations that are not CH, and the proportion of observations included in the node. rpart() not only grew the full tree, it identified the set of cost complexity parameters, and measured the model performance of each corresponding tree using cross-validation. printcp() displays the candidate \\(c_p\\) values. You can use this table to decide how to prune the tree. printcp(oj_mdl_cart_full) ## ## Classification tree: ## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] LoyalCH PriceDiff SalePriceMM ## ## Root node error: 334/857 = 0.38973 ## ## n= 857 ## ## CP nsplit rel error xerror xstd ## 1 0.479042 0 1.00000 1.00000 0.042745 ## 2 0.032934 1 0.52096 0.54192 0.035775 ## 3 0.013473 3 0.45509 0.47006 0.033905 ## 4 0.010000 5 0.42814 0.46407 0.033736 There are 4 \\(c_p\\) values in this model. The model with the smallest complexity parameter allows the most splits (nsplit). The highest complexity parameter corresponds to a tree with just a root node. rel error is the error rate relative to the root node. The root node absolute error is 0.38973162 (the proportion of MM), so its rel error is 0.38973162/0.38973162 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.42814 * 0.38973162 = 0.1669. You can verify that by calculating the error rate of the predicted values: data.frame(pred = predict(oj_mdl_cart_full, newdata = oj_train, type = &quot;class&quot;)) %&gt;% mutate(obs = oj_train$Purchase, err = if_else(pred != obs, 1, 0)) %&gt;% summarize(mean_err = mean(err)) ## mean_err ## 1 0.1668611 Finishing the CP table tour, xerror is the relative cross-validated error rate and xstd is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative CV error, \\(c_p\\) = 0.01. If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative error. The CP table is not super-helpful for finding that tree, so add a column to find it. oj_mdl_cart_full$cptable %&gt;% data.frame() %&gt;% mutate( min_idx = which.min(oj_mdl_cart_full$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = oj_mdl_cart_full$cptable[min_idx, &quot;xerror&quot;] + oj_mdl_cart_full$cptable[min_idx, &quot;xstd&quot;], eval = case_when(rownum == min_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;) ) %&gt;% select(-rownum, -min_idx) ## CP nsplit rel.error xerror xstd xerror_cap eval ## 1 0.47904192 0 1.0000000 1.0000000 0.04274518 0.4978082 ## 2 0.03293413 1 0.5209581 0.5419162 0.03577468 0.4978082 ## 3 0.01347305 3 0.4550898 0.4700599 0.03390486 0.4978082 under cap ## 4 0.01000000 5 0.4281437 0.4640719 0.03373631 0.4978082 min xerror The simplest tree using the 1-SE rule is $c_p = 0.01347305, CV error = 0.1832). Fortunately, plotcp() presents a nice graphical representation of the relationship between xerror and cp. plotcp(oj_mdl_cart_full, upper = &quot;splits&quot;) The dashed line is set at the minimum xerror + xstd. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The figure suggests I should prune to 5 or 3 splits. I see this curve never really hits a minimum - it is still decreasing at 5 splits. The default tuning parameter value cp = 0.01 may be too large, so I’ll set it to cp = 0.001 and start over. set.seed(123) oj_mdl_cart_full &lt;- rpart( formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, cp = 0.001 ) print(oj_mdl_cart_full) ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 334 CH (0.61026838 0.38973162) ## 2) LoyalCH&gt;=0.48285 537 94 CH (0.82495345 0.17504655) ## 4) LoyalCH&gt;=0.7648795 271 13 CH (0.95202952 0.04797048) * ## 5) LoyalCH&lt; 0.7648795 266 81 CH (0.69548872 0.30451128) ## 10) PriceDiff&gt;=-0.165 226 50 CH (0.77876106 0.22123894) ## 20) ListPriceDiff&gt;=0.255 115 11 CH (0.90434783 0.09565217) * ## 21) ListPriceDiff&lt; 0.255 111 39 CH (0.64864865 0.35135135) ## 42) PriceMM&gt;=2.155 19 2 CH (0.89473684 0.10526316) * ## 43) PriceMM&lt; 2.155 92 37 CH (0.59782609 0.40217391) ## 86) DiscCH&gt;=0.115 7 0 CH (1.00000000 0.00000000) * ## 87) DiscCH&lt; 0.115 85 37 CH (0.56470588 0.43529412) ## 174) ListPriceDiff&gt;=0.215 45 15 CH (0.66666667 0.33333333) * ## 175) ListPriceDiff&lt; 0.215 40 18 MM (0.45000000 0.55000000) ## 350) LoyalCH&gt;=0.527571 28 13 CH (0.53571429 0.46428571) ## 700) WeekofPurchase&lt; 266.5 21 8 CH (0.61904762 0.38095238) * ## 701) WeekofPurchase&gt;=266.5 7 2 MM (0.28571429 0.71428571) * ## 351) LoyalCH&lt; 0.527571 12 3 MM (0.25000000 0.75000000) * ## 11) PriceDiff&lt; -0.165 40 9 MM (0.22500000 0.77500000) * ## 3) LoyalCH&lt; 0.48285 320 80 MM (0.25000000 0.75000000) ## 6) LoyalCH&gt;=0.2761415 146 58 MM (0.39726027 0.60273973) ## 12) SalePriceMM&gt;=2.04 71 31 CH (0.56338028 0.43661972) ## 24) LoyalCH&lt; 0.303104 7 0 CH (1.00000000 0.00000000) * ## 25) LoyalCH&gt;=0.303104 64 31 CH (0.51562500 0.48437500) ## 50) WeekofPurchase&gt;=246.5 52 22 CH (0.57692308 0.42307692) ## 100) PriceCH&lt; 1.94 35 11 CH (0.68571429 0.31428571) ## 200) StoreID&lt; 1.5 9 1 CH (0.88888889 0.11111111) * ## 201) StoreID&gt;=1.5 26 10 CH (0.61538462 0.38461538) ## 402) LoyalCH&lt; 0.410969 17 4 CH (0.76470588 0.23529412) * ## 403) LoyalCH&gt;=0.410969 9 3 MM (0.33333333 0.66666667) * ## 101) PriceCH&gt;=1.94 17 6 MM (0.35294118 0.64705882) * ## 51) WeekofPurchase&lt; 246.5 12 3 MM (0.25000000 0.75000000) * ## 13) SalePriceMM&lt; 2.04 75 18 MM (0.24000000 0.76000000) ## 26) SpecialCH&gt;=0.5 14 6 CH (0.57142857 0.42857143) * ## 27) SpecialCH&lt; 0.5 61 10 MM (0.16393443 0.83606557) * ## 7) LoyalCH&lt; 0.2761415 174 22 MM (0.12643678 0.87356322) ## 14) LoyalCH&gt;=0.035047 117 21 MM (0.17948718 0.82051282) ## 28) WeekofPurchase&lt; 273.5 104 21 MM (0.20192308 0.79807692) ## 56) PriceCH&gt;=1.875 20 9 MM (0.45000000 0.55000000) ## 112) WeekofPurchase&gt;=252.5 12 5 CH (0.58333333 0.41666667) * ## 113) WeekofPurchase&lt; 252.5 8 2 MM (0.25000000 0.75000000) * ## 57) PriceCH&lt; 1.875 84 12 MM (0.14285714 0.85714286) * ## 29) WeekofPurchase&gt;=273.5 13 0 MM (0.00000000 1.00000000) * ## 15) LoyalCH&lt; 0.035047 57 1 MM (0.01754386 0.98245614) * This is a much larger tree. Did I find a cp value that produces a local min? plotcp(oj_mdl_cart_full, upper = &quot;splits&quot;) Yes, the min is at CP = 0.011 with 5 splits. The min + 1 SE is at CP = 0.021 with 3 splits. I’ll prune the tree to 3 splits. oj_mdl_cart &lt;- prune( oj_mdl_cart_full, cp = oj_mdl_cart_full$cptable[oj_mdl_cart_full$cptable[, 2] == 3, &quot;CP&quot;] ) rpart.plot(oj_mdl_cart, yesno = TRUE) The most “important” indicator of Purchase appears to be LoyalCH. From the rpart vignette (page 12), “An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate.” Surrogates refer to alternative features for a node to handle missing data. For each split, CART evaluates a variety of alternative “surrogate” splits to use when the feature value for the primary split is NA. Surrogate splits are splits that produce results similar to the original split. A variable’s importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. Here is the variable importance for this model. oj_mdl_cart$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Variable Importance with Simple Classication&quot;) LoyalCH is by far the most important variable, as expected from its position at the top of the tree, and one level down. You can see how the surrogates appear in the model with the summary() function. summary(oj_mdl_cart) ## Call: ## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, ## cp = 0.001) ## n= 857 ## ## CP nsplit rel error xerror xstd ## 1 0.47904192 0 1.0000000 1.0000000 0.04274518 ## 2 0.03293413 1 0.5209581 0.5419162 0.03577468 ## 3 0.01347305 3 0.4550898 0.4700599 0.03390486 ## ## Variable importance ## LoyalCH PriceDiff SalePriceMM StoreID WeekofPurchase ## 67 9 5 4 4 ## DiscMM PriceMM PctDiscMM PriceCH ## 3 3 3 1 ## ## Node number 1: 857 observations, complexity param=0.4790419 ## predicted class=CH expected loss=0.3897316 P(node) =1 ## class counts: 523 334 ## probabilities: 0.610 0.390 ## left son=2 (537 obs) right son=3 (320 obs) ## Primary splits: ## LoyalCH &lt; 0.48285 to the right, improve=132.56800, (0 missing) ## StoreID &lt; 3.5 to the right, improve= 40.12097, (0 missing) ## PriceDiff &lt; 0.015 to the right, improve= 24.26552, (0 missing) ## ListPriceDiff &lt; 0.255 to the right, improve= 22.79117, (0 missing) ## SalePriceMM &lt; 1.84 to the right, improve= 20.16447, (0 missing) ## Surrogate splits: ## StoreID &lt; 3.5 to the right, agree=0.646, adj=0.053, (0 split) ## PriceMM &lt; 1.89 to the right, agree=0.638, adj=0.031, (0 split) ## WeekofPurchase &lt; 229.5 to the right, agree=0.632, adj=0.016, (0 split) ## DiscMM &lt; 0.77 to the left, agree=0.629, adj=0.006, (0 split) ## SalePriceMM &lt; 1.385 to the right, agree=0.629, adj=0.006, (0 split) ## ## Node number 2: 537 observations, complexity param=0.03293413 ## predicted class=CH expected loss=0.1750466 P(node) =0.6266044 ## class counts: 443 94 ## probabilities: 0.825 0.175 ## left son=4 (271 obs) right son=5 (266 obs) ## Primary splits: ## LoyalCH &lt; 0.7648795 to the right, improve=17.669310, (0 missing) ## PriceDiff &lt; 0.015 to the right, improve=15.475200, (0 missing) ## SalePriceMM &lt; 1.84 to the right, improve=13.951730, (0 missing) ## ListPriceDiff &lt; 0.255 to the right, improve=11.407560, (0 missing) ## DiscMM &lt; 0.15 to the left, improve= 7.795122, (0 missing) ## Surrogate splits: ## WeekofPurchase &lt; 257.5 to the right, agree=0.594, adj=0.180, (0 split) ## PriceCH &lt; 1.775 to the right, agree=0.590, adj=0.173, (0 split) ## StoreID &lt; 3.5 to the right, agree=0.587, adj=0.165, (0 split) ## PriceMM &lt; 2.04 to the right, agree=0.587, adj=0.165, (0 split) ## SalePriceMM &lt; 2.04 to the right, agree=0.587, adj=0.165, (0 split) ## ## Node number 3: 320 observations ## predicted class=MM expected loss=0.25 P(node) =0.3733956 ## class counts: 80 240 ## probabilities: 0.250 0.750 ## ## Node number 4: 271 observations ## predicted class=CH expected loss=0.04797048 P(node) =0.3162194 ## class counts: 258 13 ## probabilities: 0.952 0.048 ## ## Node number 5: 266 observations, complexity param=0.03293413 ## predicted class=CH expected loss=0.3045113 P(node) =0.3103851 ## class counts: 185 81 ## probabilities: 0.695 0.305 ## left son=10 (226 obs) right son=11 (40 obs) ## Primary splits: ## PriceDiff &lt; -0.165 to the right, improve=20.84307, (0 missing) ## ListPriceDiff &lt; 0.235 to the right, improve=20.82404, (0 missing) ## SalePriceMM &lt; 1.84 to the right, improve=16.80587, (0 missing) ## DiscMM &lt; 0.15 to the left, improve=10.05120, (0 missing) ## PctDiscMM &lt; 0.0729725 to the left, improve=10.05120, (0 missing) ## Surrogate splits: ## SalePriceMM &lt; 1.585 to the right, agree=0.906, adj=0.375, (0 split) ## DiscMM &lt; 0.57 to the left, agree=0.895, adj=0.300, (0 split) ## PctDiscMM &lt; 0.264375 to the left, agree=0.895, adj=0.300, (0 split) ## WeekofPurchase &lt; 274.5 to the left, agree=0.872, adj=0.150, (0 split) ## SalePriceCH &lt; 2.075 to the left, agree=0.857, adj=0.050, (0 split) ## ## Node number 10: 226 observations ## predicted class=CH expected loss=0.2212389 P(node) =0.2637106 ## class counts: 176 50 ## probabilities: 0.779 0.221 ## ## Node number 11: 40 observations ## predicted class=MM expected loss=0.225 P(node) =0.04667445 ## class counts: 9 31 ## probabilities: 0.225 0.775 I’ll evaluate the predictions and record the accuracy (correct classification percentage) for comparison to other models. Two ways to evaluate the model are the confusion matrix, and the ROC curve. 4.1.1 Measuring Performance 4.1.1.1 Confusion Matrix Print the confusion matrix with caret::confusionMatrix() to see how well does this model performs against the holdout set. oj_preds_cart &lt;- bind_cols( predict(oj_mdl_cart, newdata = oj_test, type = &quot;prob&quot;), predicted = predict(oj_mdl_cart, newdata = oj_test, type = &quot;class&quot;), actual = oj_test$Purchase ) oj_cm_cart &lt;- confusionMatrix(oj_preds_cart$predicted, reference = oj_preds_cart$actual) oj_cm_cart ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 113 13 ## MM 17 70 ## ## Accuracy : 0.8592 ## 95% CI : (0.8051, 0.9029) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 1.265e-15 ## ## Kappa : 0.7064 ## ## Mcnemar&#39;s Test P-Value : 0.5839 ## ## Sensitivity : 0.8692 ## Specificity : 0.8434 ## Pos Pred Value : 0.8968 ## Neg Pred Value : 0.8046 ## Prevalence : 0.6103 ## Detection Rate : 0.5305 ## Detection Prevalence : 0.5915 ## Balanced Accuracy : 0.8563 ## ## &#39;Positive&#39; Class : CH ## The confusion matrix is at the top. It also includes a lot of statistics. It’s worth getting familiar with the stats. The model accuracy and 95% CI are calculated from the binomial test. binom.test(x = 113 + 70, n = 213) ## ## Exact binomial test ## ## data: 113 + 70 and 213 ## number of successes = 183, number of trials = 213, p-value &lt; 2.2e-16 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.8050785 0.9029123 ## sample estimates: ## probability of success ## 0.8591549 The “No Information Rate” (NIR) statistic is the class rate for the largest class. In this case CH is the largest class, so NIR = 130/213 = 0.6103. “P-Value [Acc &gt; NIR]” is the binomial test that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH). binom.test(x = 113 + 70, n = 213, p = 130/213, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 113 + 70 and 213 ## number of successes = 183, number of trials = 213, p-value = 1.265e-15 ## alternative hypothesis: true probability of success is greater than 0.6103286 ## 95 percent confidence interval: ## 0.8138446 1.0000000 ## sample estimates: ## probability of success ## 0.8591549 The “Accuracy” statistic indicates the model predicts 0.8590 of the observations correctly. That’s good, but less impressive when you consider the prevalence of CH is 0.6103 - you could achieve 61% accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen’s kappa statistic. The kappa statistic is explained here. It compares the accuracy to the accuracy of a “random system”. It is defined as \\[\\kappa = \\frac{Acc - RA}{1-RA}\\] where \\[RA = \\frac{ActFalse \\times PredFalse + ActTrue \\times PredTrue}{Total \\times Total}\\] is the hypothetical probability of a chance agreement. ActFalse will be the number of “MM” (13 + 70 = 83) and actual true will be the number of “CH” (113 + 17 = 130). The predicted counts are table(oj_preds_cart$predicted) ## ## CH MM ## 126 87 So, \\(RA = (83*87 + 130*126) / 213^2 = 0.5202\\) and \\(\\kappa = (0.8592 - 0.5202)/(1 - 0.5202) = 0.7064\\). The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations. In this case, a kappa statistic of 0.7064 is “substantial”. See chart here. The other measures from the confusionMatrix() output are various proportions and you can remind yourself of their definitions in the documentation with ?confusionMatrix. Visuals are almost always helpful. Here is a plot of the confusion matrix. plot(oj_preds_cart$actual, oj_preds_cart$predicted, main = &quot;Simple Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) 4.1.1.2 ROC Curve The ROC (receiver operating characteristics) curve (Fawcett 2005) is another measure of accuracy. The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold. precrec::evalmod() calculates the confusion matrix values from the model using the holdout data set. The AUC on the holdout set is 0.8848. pRoc::plot.roc(), plotROC::geom_roc(), and yardstick::roc_curve() are all options for plotting a ROC curve. mdl_auc &lt;- Metrics::auc(actual = oj_preds_cart$actual == &quot;CH&quot;, oj_preds_cart$CH) yardstick::roc_curve(oj_preds_cart, actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ CART ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) A few points on the ROC space are helpful for understanding how to use it. The lower left point (0, 0) is the result of always predicting “negative” or in this case “MM” if “CH” is taken as the default class. No false positives, but no true positives either. The upper right point (1, 1) is the result of always predicting “positive” (“CH” here). You catch all true positives, but miss all the true negatives. The upper left point (0, 1) is the result of perfect accuracy. The lower right point (1, 0) is the result of perfect imbecility. You made the exact wrong prediction every time. The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time. If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc. The goal is for all nodes to bunch up in the upper left. Points to the left of the diagonal with a low TPR can be thought of as “conservative” predictors - they only make positive (CH) predictions with strong evidence. Points to the left of the diagonal with a high TPR can be thought of as “liberal” predictors - they make positive (CH) predictions with weak evidence. 4.1.1.3 Gain Curve The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The gray area is the “gain” over a random prediction. 130 of the 213 consumers in the holdout set purchased CH. The gain curve encountered 77 CH purchasers (59%) within the first 79 observations (37%). It encountered all 130 CH purchasers on the 213th observation (100%). The bottom of the gray area is the outcome of a random model. Only half the CH purchasers would be observed within 50% of the observations. The top of the gray area is the outcome of the perfect model, the “wizard curve”. Half the CH purchasers would be observed in 65/213=31% of the observations. yardstick::gain_curve(oj_preds_cart, actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ CART Gain Curve&quot; ) 4.1.2 Training with Caret I can also fit the model with caret::train(). There are two ways to tune hyperparameters in train(): set the number of tuning parameter values to consider by setting tuneLength, or set particular values to consider for each parameter by defining a tuneGrid. I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP. If you have no idea what is the optimal tuning parameter, start with tuneLength to get close, then fine-tune with tuneGrid. That’s what I’ll do. I’ll create a training control object that I can re-use in other model builds. oj_trControl = trainControl( method = &quot;cv&quot;, number = 10, savePredictions = &quot;final&quot;, # save preds for the optimal tuning parameter classProbs = TRUE, # class probs in addition to preds summaryFunction = twoClassSummary ) Now fit the model. set.seed(1234) oj_mdl_cart2 &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneLength = 5, metric = &quot;ROC&quot;, trControl = oj_trControl ) caret built a full tree using rpart’s default parameters: gini splitting index, at least 20 observations in a node in order to consider splitting it, and at least 6 observations in each node. Caret then calculated the accuracy for each candidate value of \\(\\alpha\\). Here is the results. print(oj_mdl_cart2) ## CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.005988024 0.8539885 0.8605225 0.7274510 ## 0.008982036 0.8502309 0.8568578 0.7334225 ## 0.013473054 0.8459290 0.8473149 0.7397504 ## 0.032934132 0.7776483 0.8509071 0.6796791 ## 0.479041916 0.5878764 0.9201379 0.2556150 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.005988024. The second cp (0.008982036) produced the highest accuracy. I can drill into the best value of cp using a tuning grid. set.seed(1234) oj_mdl_cart2 &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneGrid = expand.grid(cp = seq(from = 0.001, to = 0.010, length = 11)), metric = &quot;ROC&quot;, trControl = oj_trControl ) print(oj_mdl_cart2) ## CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.0010 0.8513056 0.8529390 0.7182709 ## 0.0019 0.8528471 0.8529753 0.7213012 ## 0.0028 0.8524435 0.8510522 0.7302139 ## 0.0037 0.8533529 0.8510522 0.7421569 ## 0.0046 0.8540042 0.8491292 0.7333333 ## 0.0055 0.8543820 0.8567126 0.7334225 ## 0.0064 0.8539885 0.8605225 0.7274510 ## 0.0073 0.8521076 0.8625181 0.7335116 ## 0.0082 0.8521076 0.8625181 0.7335116 ## 0.0091 0.8502309 0.8568578 0.7334225 ## 0.0100 0.8507262 0.8510885 0.7424242 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.0055. The best model is at cp = 0.0082. Here are the cross-validated accuracies for the candidate cp values. plot(oj_mdl_cart2) Here are the rules in the final model. oj_mdl_cart2$finalModel ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 334 CH (0.61026838 0.38973162) ## 2) LoyalCH&gt;=0.48285 537 94 CH (0.82495345 0.17504655) ## 4) LoyalCH&gt;=0.7648795 271 13 CH (0.95202952 0.04797048) * ## 5) LoyalCH&lt; 0.7648795 266 81 CH (0.69548872 0.30451128) ## 10) PriceDiff&gt;=-0.165 226 50 CH (0.77876106 0.22123894) * ## 11) PriceDiff&lt; -0.165 40 9 MM (0.22500000 0.77500000) * ## 3) LoyalCH&lt; 0.48285 320 80 MM (0.25000000 0.75000000) ## 6) LoyalCH&gt;=0.2761415 146 58 MM (0.39726027 0.60273973) ## 12) SalePriceMM&gt;=2.04 71 31 CH (0.56338028 0.43661972) ## 24) LoyalCH&lt; 0.303104 7 0 CH (1.00000000 0.00000000) * ## 25) LoyalCH&gt;=0.303104 64 31 CH (0.51562500 0.48437500) ## 50) WeekofPurchase&gt;=246.5 52 22 CH (0.57692308 0.42307692) ## 100) PriceCH&lt; 1.94 35 11 CH (0.68571429 0.31428571) * ## 101) PriceCH&gt;=1.94 17 6 MM (0.35294118 0.64705882) * ## 51) WeekofPurchase&lt; 246.5 12 3 MM (0.25000000 0.75000000) * ## 13) SalePriceMM&lt; 2.04 75 18 MM (0.24000000 0.76000000) ## 26) SpecialCH&gt;=0.5 14 6 CH (0.57142857 0.42857143) * ## 27) SpecialCH&lt; 0.5 61 10 MM (0.16393443 0.83606557) * ## 7) LoyalCH&lt; 0.2761415 174 22 MM (0.12643678 0.87356322) * rpart.plot(oj_mdl_cart2$finalModel) Let’s look at the performance on the holdout data set. oj_preds_cart2 &lt;- bind_cols( predict(oj_mdl_cart2, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_cart2, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_cart2 &lt;- confusionMatrix(oj_preds_cart2$Predicted, oj_preds_cart2$Actual) oj_cm_cart2 ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 117 18 ## MM 13 65 ## ## Accuracy : 0.8545 ## 95% CI : (0.7998, 0.8989) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 4.83e-15 ## ## Kappa : 0.6907 ## ## Mcnemar&#39;s Test P-Value : 0.4725 ## ## Sensitivity : 0.9000 ## Specificity : 0.7831 ## Pos Pred Value : 0.8667 ## Neg Pred Value : 0.8333 ## Prevalence : 0.6103 ## Detection Rate : 0.5493 ## Detection Prevalence : 0.6338 ## Balanced Accuracy : 0.8416 ## ## &#39;Positive&#39; Class : CH ## The accuracy is 0.8451 - a little worse than the 0.8592 from the direct method. The AUC is 0.9102. mdl_auc &lt;- Metrics::auc(actual = oj_preds_cart2$Actual == &quot;CH&quot;, oj_preds_cart2$CH) yardstick::roc_curve(oj_preds_cart2, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ CART ROC Curve (caret)&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_cart2, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ CART Gain Curve (caret)&quot;) Finally, here is the variable importance plot. Brand loyalty is most important, followed by price difference. plot(varImp(oj_mdl_cart2), main=&quot;Variable Importance with CART (caret)&quot;) Looks like the manual effort fared best. Here is a summary the accuracy rates of the two models. oj_scoreboard &lt;- rbind( data.frame(Model = &quot;Single Tree&quot;, Accuracy = oj_cm_cart$overall[&quot;Accuracy&quot;]), data.frame(Model = &quot;Single Tree (caret)&quot;, Accuracy = oj_cm_cart2$overall[&quot;Accuracy&quot;]) ) %&gt;% arrange(desc(Accuracy)) scoreboard(oj_scoreboard) .cl-98620f4c{}.cl-985a59dc{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-985dd4a4{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-985dd4ae{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-985de8f4{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-985de8fe{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-985de8ff{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-985de908{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-985de909{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-985de912{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelAccuracySingle Tree0.8591549Single Tree (caret)0.8544601 4.2 Regression Tree A simple regression tree is built in a manner similar to a simple classification tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic. I’ll learn by example again. Using the ISLR::Carseats data set, and predict Sales using from the 10 feature variables. cs_dat &lt;- ISLR::Carseats skimr::skim(cs_dat) Table 4.2: Data summary Name cs_dat Number of rows 400 Number of columns 11 _______________________ Column type frequency: factor 3 numeric 8 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts ShelveLoc 0 1 FALSE 3 Med: 219, Bad: 96, Goo: 85 Urban 0 1 FALSE 2 Yes: 282, No: 118 US 0 1 FALSE 2 Yes: 258, No: 142 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Sales 0 1 7.50 2.82 0 5.39 7.49 9.32 16.27 ▁▆▇▃▁ CompPrice 0 1 124.97 15.33 77 115.00 125.00 135.00 175.00 ▁▅▇▃▁ Income 0 1 68.66 27.99 21 42.75 69.00 91.00 120.00 ▇▆▇▆▅ Advertising 0 1 6.64 6.65 0 0.00 5.00 12.00 29.00 ▇▃▃▁▁ Population 0 1 264.84 147.38 10 139.00 272.00 398.50 509.00 ▇▇▇▇▇ Price 0 1 115.80 23.68 24 100.00 117.00 131.00 191.00 ▁▂▇▆▁ Age 0 1 53.32 16.20 25 39.75 54.50 66.00 80.00 ▇▆▇▇▇ Education 0 1 13.90 2.62 10 12.00 14.00 16.00 18.00 ▇▇▃▇▇ Split careseats_dat (n = 400) into cs_train (80%, n = 321) and cs_test (20%, n = 79). set.seed(12345) partition &lt;- createDataPartition(y = cs_dat$Sales, p = 0.8, list = FALSE) cs_train &lt;- cs_dat[partition, ] cs_test &lt;- cs_dat[-partition, ] The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp). The only difference here is the rpart() parameter method = \"anova\" to produce a regression tree. set.seed(1234) cs_mdl_cart_full &lt;- rpart(Sales ~ ., cs_train, method = &quot;anova&quot;) print(cs_mdl_cart_full) ## n= 321 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 321 2567.76800 7.535950 ## 2) ShelveLoc=Bad,Medium 251 1474.14100 6.770359 ## 4) Price&gt;=105.5 168 719.70630 5.987024 ## 8) ShelveLoc=Bad 50 165.70160 4.693600 ## 16) Population&lt; 201.5 20 48.35505 3.646500 * ## 17) Population&gt;=201.5 30 80.79922 5.391667 * ## 9) ShelveLoc=Medium 118 434.91370 6.535085 ## 18) Advertising&lt; 11.5 88 290.05490 6.113068 ## 36) CompPrice&lt; 142 69 193.86340 5.769420 ## 72) Price&gt;=132.5 16 50.75440 4.455000 * ## 73) Price&lt; 132.5 53 107.12060 6.166226 * ## 37) CompPrice&gt;=142 19 58.45118 7.361053 * ## 19) Advertising&gt;=11.5 30 83.21323 7.773000 * ## 5) Price&lt; 105.5 83 442.68920 8.355904 ## 10) Age&gt;=63.5 32 153.42300 6.922500 ## 20) Price&gt;=85 25 66.89398 6.160800 ## 40) ShelveLoc=Bad 9 18.39396 4.772222 * ## 41) ShelveLoc=Medium 16 21.38544 6.941875 * ## 21) Price&lt; 85 7 20.22194 9.642857 * ## 11) Age&lt; 63.5 51 182.26350 9.255294 ## 22) Income&lt; 57.5 12 28.03042 7.707500 * ## 23) Income&gt;=57.5 39 116.63950 9.731538 ## 46) Age&gt;=50.5 14 21.32597 8.451429 * ## 47) Age&lt; 50.5 25 59.52474 10.448400 * ## 3) ShelveLoc=Good 70 418.98290 10.281140 ## 6) Price&gt;=107.5 49 242.58730 9.441633 ## 12) Advertising&lt; 13.5 41 162.47820 8.926098 ## 24) Age&gt;=61 17 53.37051 7.757647 * ## 25) Age&lt; 61 24 69.45776 9.753750 * ## 13) Advertising&gt;=13.5 8 13.36599 12.083750 * ## 7) Price&lt; 107.5 21 61.28200 12.240000 * The predicted Sales at the root is the mean Sales for the training data set, 7.535950 (values are $000s). The deviance at the root is the SSE, 2567.768. The first split is at ShelveLoc = [Bad, Medium] vs Good. Here is the unpruned tree diagram. rpart.plot(cs_mdl_cart_full, yesno = TRUE) The boxes show the node predicted value (mean) and the proportion of observations that are in the node (or child nodes). rpart() grew the full tree, and used cross-validation to test the performance of the possible complexity hyperparameters. printcp() displays the candidate cp values. You can use this table to decide how to prune the tree. printcp(cs_mdl_cart_full) ## ## Regression tree: ## rpart(formula = Sales ~ ., data = cs_train, method = &quot;anova&quot;) ## ## Variables actually used in tree construction: ## [1] Advertising Age CompPrice Income Population Price ## [7] ShelveLoc ## ## Root node error: 2567.8/321 = 7.9993 ## ## n= 321 ## ## CP nsplit rel error xerror xstd ## 1 0.262736 0 1.00000 1.00635 0.076664 ## 2 0.121407 1 0.73726 0.74888 0.058981 ## 3 0.046379 2 0.61586 0.65278 0.050839 ## 4 0.044830 3 0.56948 0.67245 0.051638 ## 5 0.041671 4 0.52465 0.66230 0.051065 ## 6 0.025993 5 0.48298 0.62345 0.049368 ## 7 0.025823 6 0.45698 0.61980 0.048026 ## 8 0.024007 7 0.43116 0.62058 0.048213 ## 9 0.015441 8 0.40715 0.58061 0.041738 ## 10 0.014698 9 0.39171 0.56413 0.041368 ## 11 0.014641 10 0.37701 0.56277 0.041271 ## 12 0.014233 11 0.36237 0.56081 0.041097 ## 13 0.014015 12 0.34814 0.55647 0.038308 ## 14 0.013938 13 0.33413 0.55647 0.038308 ## 15 0.010560 14 0.32019 0.57110 0.038872 ## 16 0.010000 15 0.30963 0.56676 0.038090 There were 16 possible cp values in this model. The model with the smallest complexity parameter allows the most splits (nsplit). The highest complexity parameter corresponds to a tree with just a root node. rel error is the SSE relative to the root node. The root node SSE is 2567.76800, so its rel error is 2567.76800/2567.76800 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.30963 * 2567.76800 = 795.058. You can verify that by calculating the SSE of the model predicted values: data.frame(pred = predict(cs_mdl_cart_full, newdata = cs_train)) %&gt;% mutate(obs = cs_train$Sales, sq_err = (obs - pred)^2) %&gt;% summarize(sse = sum(sq_err)) ## sse ## 1 795.0525 Finishing the CP table tour, xerror is the cross-validated SSE and xstd is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative SSE (xerror). If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative SSE. The CP table is not super-helpful for finding that tree. I’ll add a column to find it. cs_mdl_cart_full$cptable %&gt;% data.frame() %&gt;% mutate(min_xerror_idx = which.min(cs_mdl_cart_full$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = cs_mdl_cart_full$cptable[min_xerror_idx, &quot;xerror&quot;] + cs_mdl_cart_full$cptable[min_xerror_idx, &quot;xstd&quot;], eval = case_when(rownum == min_xerror_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;)) %&gt;% select(-rownum, -min_xerror_idx) ## CP nsplit rel.error xerror xstd xerror_cap eval ## 1 0.26273578 0 1.0000000 1.0063530 0.07666355 0.5947744 ## 2 0.12140705 1 0.7372642 0.7488767 0.05898146 0.5947744 ## 3 0.04637919 2 0.6158572 0.6527823 0.05083938 0.5947744 ## 4 0.04483023 3 0.5694780 0.6724529 0.05163819 0.5947744 ## 5 0.04167149 4 0.5246478 0.6623028 0.05106530 0.5947744 ## 6 0.02599265 5 0.4829763 0.6234457 0.04936799 0.5947744 ## 7 0.02582284 6 0.4569836 0.6198034 0.04802643 0.5947744 ## 8 0.02400748 7 0.4311608 0.6205756 0.04821332 0.5947744 ## 9 0.01544139 8 0.4071533 0.5806072 0.04173785 0.5947744 under cap ## 10 0.01469771 9 0.3917119 0.5641331 0.04136793 0.5947744 under cap ## 11 0.01464055 10 0.3770142 0.5627713 0.04127139 0.5947744 under cap ## 12 0.01423309 11 0.3623736 0.5608073 0.04109662 0.5947744 under cap ## 13 0.01401541 12 0.3481405 0.5564663 0.03830810 0.5947744 min xerror ## 14 0.01393771 13 0.3341251 0.5564663 0.03830810 0.5947744 under cap ## 15 0.01055959 14 0.3201874 0.5710951 0.03887227 0.5947744 under cap ## 16 0.01000000 15 0.3096278 0.5667561 0.03808991 0.5947744 under cap Okay, so the simplest tree is the one with CP = 0.02599265 (5 splits). Fortunately, plotcp() presents a nice graphical representation of the relationship between xerror and cp. plotcp(cs_mdl_cart_full, upper = &quot;splits&quot;) The dashed line is set at the minimum xerror + xstd. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The smallest relative error is at CP = 0.01000000 (15 splits), but the maximum CP below the dashed line (one standard deviation above the minimum error) is at CP = 0.02599265 (5 splits). Use the prune() function to prune the tree by specifying the associated cost-complexity cp. cs_mdl_cart &lt;- prune( cs_mdl_cart_full, cp = cs_mdl_cart_full$cptable[cs_mdl_cart_full$cptable[, 2] == 5, &quot;CP&quot;] ) rpart.plot(cs_mdl_cart, yesno = TRUE) The most “important” indicator of Sales is ShelveLoc. Here are the importance values from the model. cs_mdl_cart$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Variable Importance with Simple Regression&quot;) The most important indicator of Sales is ShelveLoc, then Price, then Age, all of which appear in the final model. CompPrice was also important. The last step is to make predictions on the validation data set. The root mean squared error (\\(RMSE = \\sqrt{(1/2) \\sum{(actual - pred)^2}})\\) and mean absolute error (\\(MAE = (1/n) \\sum{|actual - pred|}\\)) are the two most common measures of predictive accuracy. The key difference is that RMSE punishes large errors more harshly. For a regression tree, set argument type = \"vector\" (or do not specify at all). cs_preds_cart &lt;- predict(cs_mdl_cart, cs_test, type = &quot;vector&quot;) cs_rmse_cart &lt;- RMSE( pred = cs_preds_cart, obs = cs_test$Sales ) cs_rmse_cart ## [1] 2.363202 The pruning process leads to an average prediction error of 2.363 in the test data set. Not too bad considering the standard deviation of Sales is 2.801. Here is a predicted vs actual plot. data.frame(Predicted = cs_preds_cart, Actual = cs_test$Sales) %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth() + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats CART, Predicted vs Actual&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; The 6 possible predicted values do a decent job of binning the observations. 4.2.1 Training with Caret I can also fit the model with caret::train(), specifying method = \"rpart\". I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP. cs_trControl = trainControl( method = &quot;cv&quot;, number = 10, savePredictions = &quot;final&quot; # save predictions for the optimal tuning parameter ) I’ll let the model look for the best CP tuning parameter with tuneLength to get close, then fine-tune with tuneGrid. set.seed(1234) cs_mdl_cart2 = train( Sales ~ ., data = cs_train, method = &quot;rpart&quot;, tuneLength = 5, metric = &quot;RMSE&quot;, trControl = cs_trControl ) print(cs_mdl_cart2) ## CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.04167149 2.209383 0.4065251 1.778797 ## 0.04483023 2.243618 0.3849728 1.805027 ## 0.04637919 2.275563 0.3684309 1.808814 ## 0.12140705 2.400455 0.2942663 1.936927 ## 0.26273578 2.692867 0.1898998 2.192774 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0.04167149. The first cp (0.04167149) produced the smallest RMSE. I can drill into the best value of cp using a tuning grid. I’ll try that now. set.seed(1234) cs_mdl_cart2 = train( Sales ~ ., data = cs_train, method = &quot;rpart&quot;, tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.01)), metric = &quot;RMSE&quot;, trControl = cs_trControl ) print(cs_mdl_cart2) ## CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.00 2.055676 0.5027431 1.695453 ## 0.01 2.135096 0.4642577 1.745937 ## 0.02 2.095767 0.4733269 1.699235 ## 0.03 2.131246 0.4534544 1.690453 ## 0.04 2.146886 0.4411380 1.712705 ## 0.05 2.284937 0.3614130 1.837782 ## 0.06 2.265498 0.3709523 1.808319 ## 0.07 2.282630 0.3597216 1.836227 ## 0.08 2.282630 0.3597216 1.836227 ## 0.09 2.282630 0.3597216 1.836227 ## 0.10 2.282630 0.3597216 1.836227 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0. It looks like the best performing tree is the unpruned one. plot(cs_mdl_cart2) Let’s see the final model. rpart.plot(cs_mdl_cart2$finalModel) What were the most important variables? plot(varImp(cs_mdl_cart2), main=&quot;Variable Importance with Simple Regression&quot;) Evaluate the model by making predictions with the test data set. cs_preds_cart2 &lt;- predict(cs_mdl_cart2, cs_test, type = &quot;raw&quot;) data.frame(Actual = cs_test$Sales, Predicted = cs_preds_cart2) %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats CART, Predicted vs Actual (caret)&quot;) The model over-estimates at the low end and underestimates at the high end. Calculate the test data set RMSE. (cs_rmse_cart2 &lt;- RMSE(pred = cs_preds_cart2, obs = cs_test$Sales)) ## [1] 2.298331 Caret performed better in this model. Here is a summary the RMSE values of the two models. cs_scoreboard &lt;- rbind( data.frame(Model = &quot;Single Tree&quot;, RMSE = cs_rmse_cart), data.frame(Model = &quot;Single Tree (caret)&quot;, RMSE = cs_rmse_cart2) ) %&gt;% arrange(RMSE) scoreboard(cs_scoreboard) .cl-99f24f20{}.cl-99ea8fb0{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-99ed04f2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-99ed04fc{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-99ed1d52{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-99ed1d5c{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-99ed1d66{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-99ed1d70{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-99ed1d7a{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-99ed1d84{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSESingle Tree (caret)2.298331Single Tree2.363202 4.3 Bagged Trees One drawback of decision trees is that they are high-variance estimators. A small number of additional training observations can dramatically alter the prediction performance of a learned tree. Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method. The algorithm constructs B regression trees using B bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging the B trees reduces the variance. The predicted value for an observation is the mode (classification) or mean (regression) of the trees. B usually equals ~25. To test the model accuracy, the out-of-bag observations are predicted from the models. For a training set of size n, each tree is composed of \\(\\sim (1 - e^{-1})n = .632n\\) unique observations in-bag and \\(.368n\\) out-of-bag. For each tree in the ensemble, bagging makes predictions on the tree’s out-of-bag observations. I think (see page 197 of (Kuhn and Johnson 2016)) bagging measures the performance (RMSE, Accuracy, ROC, etc.) of each tree in the ensemble and averages them to produce an overall performance estimate. (This makes no sense to me. If each tree has poor performance, then the average performance of many trees will still be poor. An ensemble of B trees will produce \\(\\sim .368 B\\) predictions per unique observation. Seems like you should take the mean/mode of each observation’s prediction as the final prediction. Then you have n predictions to compare to n actuals, and you assess performance on that.) The downside to bagging is that there is no single tree with a set of rules to interpret. It becomes unclear which variables are more important than others. The next section explains how bagged trees are a special case of random forests. 4.3.1 Bagged Classification Tree Leaning by example, I’ll predict Purchase from the OJ data set again, this time using the bagging method by specifying method = \"treebag\". Caret has no hyperparameters to tune with this model, so I won’t set tuneLegth or tuneGrid. The ensemble size defaults to nbagg = 25, but you can override it (I didn’t). set.seed(1234) oj_mdl_bag &lt;- train( Purchase ~ ., data = oj_train, method = &quot;treebag&quot;, trControl = oj_trControl, metric = &quot;ROC&quot; ) oj_mdl_bag$finalModel ## ## Bagging classification trees with 25 bootstrap replications oj_mdl_bag ## Bagged CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results: ## ## ROC Sens Spec ## 0.8553731 0.8432511 0.7186275 # summary(oj_mdl_bag) If you review the summary(oj_mdl_bag), you’ll see that caret built B = 25 trees from 25 bootstrapped training sets of 857 samples (the size of oj_train). I think caret started by splitting the training set into 10 folds, then using 9 of the folds to run the bagging algorithm and collect performance measures on the hold-out fold. After repeating the process for all 10 folds, it averaged the performance measures to produce the resampling results shown above. Had there been hyperparameters to tune, caret would have repeated this process for all hyperparameter combinations and the resampling results above would be from the best performing combination. Then caret ran the bagging algorithm again on the entire data set, and the trees you see in summary(oj_mdl_bag) are what it produces. (It seems inefficient to cross-validate a bagging algorithm given that the out-of-bag samples are there for performance testing.) Let’s look at the performance on the holdout data set. oj_preds_bag &lt;- bind_cols( predict(oj_mdl_bag, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_bag, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_bag &lt;- confusionMatrix(oj_preds_bag$Predicted, reference = oj_preds_bag$Actual) oj_cm_bag ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 113 16 ## MM 17 67 ## ## Accuracy : 0.8451 ## 95% CI : (0.7894, 0.8909) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 6.311e-14 ## ## Kappa : 0.675 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.8692 ## Specificity : 0.8072 ## Pos Pred Value : 0.8760 ## Neg Pred Value : 0.7976 ## Prevalence : 0.6103 ## Detection Rate : 0.5305 ## Detection Prevalence : 0.6056 ## Balanced Accuracy : 0.8382 ## ## &#39;Positive&#39; Class : CH ## The accuracy is 0.8451 - surprisingly worse than the 0.85915 of the single tree model, but that is a difference of three predictions in a set of 213. Here are the ROC and gain curves. mdl_auc &lt;- Metrics::auc(actual = oj_preds_bag$Actual == &quot;CH&quot;, oj_preds_bag$CH) yardstick::roc_curve(oj_preds_bag, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ Bagging ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_bag, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ Bagging Gain Curve&quot;) Let’s see what are the most important variables. plot(varImp(oj_mdl_bag), main=&quot;Variable Importance with Bagging&quot;) Finally, let’s check out the scoreboard. Bagging fared worse than the single tree models. oj_scoreboard &lt;- rbind(oj_scoreboard, data.frame(Model = &quot;Bagging&quot;, Accuracy = oj_cm_bag$overall[&quot;Accuracy&quot;]) ) %&gt;% arrange(desc(Accuracy)) scoreboard(oj_scoreboard) .cl-9c1add08{}.cl-9c1198e2{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9c145bcc{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9c145bd6{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9c1469f0{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c1469fa{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c1469fb{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c1469fc{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c146a04{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c146a05{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelAccuracySingle Tree0.8591549Single Tree (caret)0.8544601Bagging0.8450704 4.3.2 Bagging Regression Tree I’ll predict Sales from the Carseats data set again, this time using the bagging method by specifying method = \"treebag\". set.seed(1234) cs_mdl_bag &lt;- train( Sales ~ ., data = cs_train, method = &quot;treebag&quot;, trControl = cs_trControl ) cs_mdl_bag ## Bagged CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.681889 0.675239 1.343427 Let’s look at the performance on the holdout data set. The RMSE is 1.9185, but the model over-predicts at low end of Sales and under-predicts at high end. cs_preds_bag &lt;- bind_cols( Predicted = predict(cs_mdl_bag, newdata = cs_test), Actual = cs_test$Sales ) (cs_rmse_bag &lt;- RMSE(pred = cs_preds_bag$Predicted, obs = cs_preds_bag$Actual)) ## [1] 1.918473 cs_preds_bag %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats Bagging, Predicted vs Actual (caret)&quot;) Now the variable importance. plot(varImp(cs_mdl_bag), main=&quot;Variable Importance with Bagging&quot;) Before moving on, check in with the scoreboard. cs_scoreboard &lt;- rbind(cs_scoreboard, data.frame(Model = &quot;Bagging&quot;, RMSE = cs_rmse_bag) ) %&gt;% arrange(RMSE) scoreboard(cs_scoreboard) .cl-9d3fb47e{}.cl-9d354674{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9d386b10{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9d386b1a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9d387880{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d38788a{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d387894{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d387895{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d38789e{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d38789f{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d3878a8{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d3878a9{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEBagging1.918473Single Tree (caret)2.298331Single Tree2.363202 4.4 Random Forests Random forests improve bagged trees by way of a small tweak that de-correlates the trees. As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of mtry predictors is chosen as split candidates from the full set of p predictors. A fresh sample of mtry predictors is taken at each split. Typically \\(mtry \\sim \\sqrt{p}\\). Bagged trees are thus a special case of random forests where mtry = p. 4.4.0.1 Random Forest Classification Tree Now I’ll try it with the random forest method by specifying method = \"rf\". Hyperparameter mtry can take any value from 1 to 17 (the number of predictors) and I expect the best value to be near \\(\\sqrt{17} \\sim 4\\). set.seed(1234) oj_mdl_rf &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rf&quot;, metric = &quot;ROC&quot;, tuneGrid = expand.grid(mtry = 1:10), # searching around mtry=4 trControl = oj_trControl ) oj_mdl_rf ## Random Forest ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## mtry ROC Sens Spec ## 1 0.8419091 0.9024673 0.5479501 ## 2 0.8625832 0.8756531 0.6976827 ## 3 0.8667189 0.8623004 0.7214795 ## 4 0.8680957 0.8507983 0.7183601 ## 5 0.8678322 0.8469521 0.7091800 ## 6 0.8687425 0.8431785 0.7273619 ## 7 0.8690552 0.8394049 0.7213904 ## 8 0.8673816 0.8432148 0.7212121 ## 9 0.8675878 0.8317489 0.7182709 ## 10 0.8653655 0.8412917 0.7152406 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 7. The largest ROC score was at mtry = 7 - higher than I expected. plot(oj_mdl_rf) Use the model to make predictions on the test set. oj_preds_rf &lt;- bind_cols( predict(oj_mdl_rf, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_rf, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_rf &lt;- confusionMatrix(oj_preds_rf$Predicted, reference = oj_preds_rf$Actual) oj_cm_rf ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 110 16 ## MM 20 67 ## ## Accuracy : 0.831 ## 95% CI : (0.7738, 0.8787) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 2.296e-12 ## ## Kappa : 0.6477 ## ## Mcnemar&#39;s Test P-Value : 0.6171 ## ## Sensitivity : 0.8462 ## Specificity : 0.8072 ## Pos Pred Value : 0.8730 ## Neg Pred Value : 0.7701 ## Prevalence : 0.6103 ## Detection Rate : 0.5164 ## Detection Prevalence : 0.5915 ## Balanced Accuracy : 0.8267 ## ## &#39;Positive&#39; Class : CH ## The accuracy on the holdout set is 0.8310. The AUC is 0.9244. Here are the ROC and gain curves. # AUC is 0.9190 mdl_auc &lt;- Metrics::auc(actual = oj_preds_rf$Actual == &quot;CH&quot;, oj_preds_rf$CH) yardstick::roc_curve(oj_preds_rf, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ Random Forest ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_rf, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ Random Forest Gain Curve&quot;) What are the most important variables? plot(varImp(oj_mdl_rf), main=&quot;Variable Importance with Random Forest&quot;) Let’s update the scoreboard. The bagging and random forest models did pretty well, but the manual classification tree is still in first place. There’s still gradient boosting to investigate! oj_scoreboard &lt;- rbind(oj_scoreboard, data.frame(Model = &quot;Random Forest&quot;, Accuracy = oj_cm_rf$overall[&quot;Accuracy&quot;]) ) %&gt;% arrange(desc(Accuracy)) scoreboard(oj_scoreboard) .cl-b39569da{}.cl-b38c9198{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-b390ab5c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b390ab70{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-b390dafa{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b390db0e{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b390db18{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b390db22{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b390db2c{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b390db36{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b390db4a{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-b390db54{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelAccuracySingle Tree0.8591549Single Tree (caret)0.8544601Bagging0.8450704Random Forest0.8309859 4.4.0.2 Random Forest Regression Tree Now I’ll try it with the random forest method by specifying method = \"rf\". Hyperparameter mtry can take any value from 1 to 10 (the number of predictors) and I expect the best value to be near \\(\\sqrt{10} \\sim 3\\). set.seed(1234) cs_mdl_rf &lt;- train( Sales ~ ., data = cs_train, method = &quot;rf&quot;, tuneGrid = expand.grid(mtry = 1:10), # searching around mtry=3 trControl = cs_trControl ) cs_mdl_rf ## Random Forest ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 1 2.170362 0.6401338 1.739791 ## 2 1.806516 0.7281537 1.444411 ## 3 1.661626 0.7539811 1.320989 ## 4 1.588878 0.7531926 1.259214 ## 5 1.539960 0.7580374 1.222062 ## 6 1.526479 0.7536928 1.211417 ## 7 1.515426 0.7541277 1.205956 ## 8 1.523217 0.7456623 1.215768 ## 9 1.521271 0.7447813 1.217091 ## 10 1.527277 0.7380014 1.218469 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 7. The minimum RMSE is at mtry = 7. plot(cs_mdl_rf) Make predictions on the test set. Like the bagged tree model, this one also over-predicts at low end of Sales and under-predicts at high end. The RMSE of 1.7184 is better than bagging’s 1.9185. cs_preds_rf &lt;- bind_cols( Predicted = predict(cs_mdl_rf, newdata = cs_test), Actual = cs_test$Sales ) (cs_rmse_rf &lt;- RMSE(pred = cs_preds_rf$Predicted, obs = cs_preds_rf$Actual)) ## [1] 1.718358 cs_preds_rf %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats Random Forest, Predicted vs Actual (caret)&quot;) plot(varImp(cs_mdl_rf), main=&quot;Variable Importance with Random Forest&quot;) Let’s check in with the scoreboard. cs_scoreboard &lt;- rbind(cs_scoreboard, data.frame(Model = &quot;Random Forest&quot;, RMSE = cs_rmse_rf) ) %&gt;% arrange(RMSE) scoreboard(cs_scoreboard) .cl-c0b60340{}.cl-c0aea2ee{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c0b1256e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c0b12582{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c0b1337e{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0b1337f{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0b13388{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0b13392{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0b13393{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0b13394{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0b1339c{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0b1339d{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0b1339e{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0b133a6{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSERandom Forest1.718358Bagging1.918473Single Tree (caret)2.298331Single Tree2.363202 The bagging and random forest models did very well - they took over the top positions! 4.5 Gradient Boosting Note: I learned gradient boosting from explained.ai. Gradient boosting machine (GBM) is an additive modeling algorithm that gradually builds a composite model by iteratively adding M weak sub-models based on the performance of the prior iteration’s composite, \\[F_M(x) = \\sum_m^M f_m(x).\\] The idea is to fit a weak model, then replace the response values with the residuals from that model, and fit another model. Adding the residual prediction model to the original response prediction model produces a more accurate model. GBM repeats this process over and over, running new models to predict the residuals of the previous composite models, and adding the results to produce new composites. With each iteration, the model becomes stronger and stronger. The successive trees are usually weighted to slow down the learning rate. “Shrinkage” reduces the influence of each individual tree and leaves space for future trees to improve the model. \\[F_M(x) = f_0 + \\eta\\sum_{m = 1}^M f_m(x).\\] The smaller the learning rate, \\(\\eta\\), the larger the number of trees, \\(M\\). \\(\\eta\\) and \\(M\\) are hyperparameters. Other constraints to the trees are usually applied as additional hyperparameters, including, tree depth, number of nodes, minimum observations per split, and minimum improvement to loss. The name “gradient boosting” refers to the boosting of a model with a gradient. Each round of training builds a weak learner and uses the residuals to calculate a gradient, the partial derivative of the loss function. Gradient boosting “descends the gradient” to adjust the model parameters to reduce the error in the next round of training. In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error. GBM continues until it reaches maximum number of trees or an acceptable error level. 4.5.0.1 Gradient Boosting Classification Tree In addition to the gradient boosting machine algorithm, implemented in caret with method = gbm, there is a variable called Extreme Gradient Boosting, XGBoost, which frankly I don’t know anything about other than it is supposed to work extremely well. Let’s try them both! 4.5.0.1.1 GBM I’ll predict Purchase from the OJ data set again, this time using the GBM method by specifying method = \"gbm\". gbm has the following tuneable hyperparameters (see modelLookup(\"gbm\")). n.trees: number of boosting iterations, \\(M\\) interaction.depth: maximum tree depth shrinkage: shrinkage, \\(\\eta\\) n.minobsinnode: minimum terminal node size I’ll use tuneLength = 5. set.seed(1234) garbage &lt;- capture.output( oj_mdl_gbm &lt;- train( Purchase ~ ., data = oj_train, method = &quot;gbm&quot;, metric = &quot;ROC&quot;, tuneLength = 5, trControl = oj_trControl )) oj_mdl_gbm ## Stochastic Gradient Boosting ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees ROC Sens Spec ## 1 50 0.8838502 0.8701016 0.7155971 ## 1 100 0.8851784 0.8719521 0.7427807 ## 1 150 0.8843982 0.8738389 0.7548128 ## 1 200 0.8828378 0.8738389 0.7487522 ## 1 250 0.8812937 0.8719884 0.7367201 ## 2 50 0.8843060 0.8718795 0.7546346 ## 2 100 0.8865391 0.8681422 0.7546346 ## 2 150 0.8830249 0.8642961 0.7456328 ## 2 200 0.8822619 0.8642598 0.7515152 ## 2 250 0.8771918 0.8529028 0.7515152 ## 3 50 0.8874290 0.8681422 0.7606061 ## 3 100 0.8828219 0.8605588 0.7726381 ## 3 150 0.8806565 0.8566038 0.7634581 ## 3 200 0.8732572 0.8661829 0.7695187 ## 3 250 0.8711321 0.8604499 0.7604278 ## 4 50 0.8828612 0.8489840 0.7515152 ## 4 100 0.8792110 0.8604862 0.7606061 ## 4 150 0.8723941 0.8527939 0.7695187 ## 4 200 0.8690015 0.8546444 0.7605169 ## 4 250 0.8683316 0.8451016 0.7512478 ## 5 50 0.8893367 0.8604499 0.7636364 ## 5 100 0.8818969 0.8546807 0.7426025 ## 5 150 0.8762509 0.8490203 0.7574866 ## 5 200 0.8739284 0.8470247 0.7426025 ## 5 250 0.8713918 0.8413643 0.7455437 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 50, interaction.depth = ## 5, shrinkage = 0.1 and n.minobsinnode = 10. train() tuned n.trees ($M) and interaction.depth, holding shrinkage = 0.1 (), and n.minobsinnode = 10. The optimal hyperparameter values were n.trees = 50, and interaction.depth = 5. You can see from the tuning plot that accuracy is maximized at \\(M=50\\) for tree depth of 5, but \\(M=50\\) with tree depth of 3 worked nearly as well. plot(oj_mdl_gbm) Let’s see how the model performed on the holdout set. The accuracy was 0.8451. oj_preds_gbm &lt;- bind_cols( predict(oj_mdl_gbm, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_gbm, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_gbm &lt;- confusionMatrix(oj_preds_gbm$Predicted, reference = oj_preds_gbm$Actual) oj_cm_gbm ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 113 16 ## MM 17 67 ## ## Accuracy : 0.8451 ## 95% CI : (0.7894, 0.8909) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 6.311e-14 ## ## Kappa : 0.675 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.8692 ## Specificity : 0.8072 ## Pos Pred Value : 0.8760 ## Neg Pred Value : 0.7976 ## Prevalence : 0.6103 ## Detection Rate : 0.5305 ## Detection Prevalence : 0.6056 ## Balanced Accuracy : 0.8382 ## ## &#39;Positive&#39; Class : CH ## AUC was 0.9386. Here are the ROC and gain curves. mdl_auc &lt;- Metrics::auc(actual = oj_preds_gbm$Actual == &quot;CH&quot;, oj_preds_gbm$CH) yardstick::roc_curve(oj_preds_gbm, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ GBM ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_gbm, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ GBM Gain Curve&quot;) Now the variable importance. Just a few variables. LoyalCH is at the top again. #plot(varImp(oj_mdl_gbm), main=&quot;Variable Importance with Gradient Boosting&quot;) 4.5.0.1.2 XGBoost I’ll predict Purchase from the OJ data set again, this time using the XGBoost method by specifying method = \"xgbTree\". xgbTree has the following tuneable hyperparameters (see modelLookup(\"xgbTree\")). The first three are the same as xgb. nrounds: number of boosting iterations, \\(M\\) max_depth: maximum tree depth eta: shrinkage, \\(\\eta\\) gamma: minimum loss reduction colsamle_bytree: subsample ratio of columns min_child_weight: minimum size of instance weight substample: subsample percentage I’ll use tuneLength = 5 again. set.seed(1234) garbage &lt;- capture.output( oj_mdl_xgb &lt;- train( Purchase ~ ., data = oj_train, method = &quot;xgbTree&quot;, metric = &quot;ROC&quot;, tuneLength = 5, trControl = oj_trControl )) oj_mdl_xgb ## eXtreme Gradient Boosting ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## eta max_depth colsample_bytree subsample nrounds ROC Sens ## 0.3 1 0.6 0.500 50 0.8840766 0.8834180 ## 0.3 1 0.6 0.500 100 0.8790561 0.8662192 ## 0.3 1 0.6 0.500 150 0.8766781 0.8701379 ## 0.3 1 0.6 0.500 200 0.8776000 0.8624093 ## 0.3 1 0.6 0.500 250 0.8757948 0.8624456 ## 0.3 1 0.6 0.625 50 0.8859505 0.8834180 ## 0.3 1 0.6 0.625 100 0.8835632 0.8719158 ## 0.3 1 0.6 0.625 150 0.8795049 0.8623730 ## 0.3 1 0.6 0.625 200 0.8779641 0.8623730 ## 0.3 1 0.6 0.625 250 0.8750637 0.8604499 ## 0.3 1 0.6 0.750 50 0.8888085 0.8796081 ## 0.3 1 0.6 0.750 100 0.8821363 0.8701016 ## 0.3 1 0.6 0.750 150 0.8788971 0.8758708 ## 0.3 1 0.6 0.750 200 0.8795012 0.8682148 ## 0.3 1 0.6 0.750 250 0.8776496 0.8643324 ## 0.3 1 0.6 0.875 50 0.8873750 0.8699927 ## 0.3 1 0.6 0.875 100 0.8842872 0.8738752 ## 0.3 1 0.6 0.875 150 0.8836644 0.8643687 ## 0.3 1 0.6 0.875 200 0.8815095 0.8643687 ## 0.3 1 0.6 0.875 250 0.8810575 0.8585994 ## 0.3 1 0.6 1.000 50 0.8901207 0.8814949 ## 0.3 1 0.6 1.000 100 0.8873808 0.8814949 ## 0.3 1 0.6 1.000 150 0.8850800 0.8777213 ## 0.3 1 0.6 1.000 200 0.8839367 0.8777213 ## 0.3 1 0.6 1.000 250 0.8830755 0.8701016 ## 0.3 1 0.8 0.500 50 0.8813948 0.8661466 ## 0.3 1 0.8 0.500 100 0.8758169 0.8605225 ## 0.3 1 0.8 0.500 150 0.8775673 0.8719884 ## 0.3 1 0.8 0.500 200 0.8705204 0.8452104 ## 0.3 1 0.8 0.500 250 0.8708827 0.8623730 ## 0.3 1 0.8 0.625 50 0.8837989 0.8680697 ## 0.3 1 0.8 0.625 100 0.8798634 0.8642598 ## 0.3 1 0.8 0.625 150 0.8771115 0.8681785 ## 0.3 1 0.8 0.625 200 0.8767673 0.8643687 ## 0.3 1 0.8 0.625 250 0.8757636 0.8662917 ## 0.3 1 0.8 0.750 50 0.8873982 0.8776851 ## 0.3 1 0.8 0.750 100 0.8845928 0.8663280 ## 0.3 1 0.8 0.750 150 0.8812139 0.8796807 ## 0.3 1 0.8 0.750 200 0.8810481 0.8662192 ## 0.3 1 0.8 0.750 250 0.8791379 0.8605225 ## 0.3 1 0.8 0.875 50 0.8860863 0.8699927 ## 0.3 1 0.8 0.875 100 0.8824399 0.8757983 ## 0.3 1 0.8 0.875 150 0.8800854 0.8720247 ## 0.3 1 0.8 0.875 200 0.8781308 0.8643324 ## 0.3 1 0.8 0.875 250 0.8760868 0.8623730 ## 0.3 1 0.8 1.000 50 0.8885750 0.8738389 ## 0.3 1 0.8 1.000 100 0.8864060 0.8738752 ## 0.3 1 0.8 1.000 150 0.8846236 0.8738752 ## 0.3 1 0.8 1.000 200 0.8834568 0.8777213 ## 0.3 1 0.8 1.000 250 0.8828532 0.8701016 ## 0.3 2 0.6 0.500 50 0.8827826 0.8604499 ## 0.3 2 0.6 0.500 100 0.8738138 0.8623367 ## 0.3 2 0.6 0.500 150 0.8712299 0.8509071 ## 0.3 2 0.6 0.500 200 0.8628896 0.8489840 ## 0.3 2 0.6 0.500 250 0.8588455 0.8432874 ## 0.3 2 0.6 0.625 50 0.8852692 0.8566763 ## 0.3 2 0.6 0.625 100 0.8763552 0.8432148 ## 0.3 2 0.6 0.625 150 0.8707889 0.8355225 ## 0.3 2 0.6 0.625 200 0.8673676 0.8317489 ## 0.3 2 0.6 0.625 250 0.8648451 0.8393687 ## 0.3 2 0.6 0.750 50 0.8820965 0.8662554 ## 0.3 2 0.6 0.750 100 0.8763104 0.8623730 ## 0.3 2 0.6 0.750 150 0.8705862 0.8623004 ## 0.3 2 0.6 0.750 200 0.8665872 0.8566401 ## 0.3 2 0.6 0.750 250 0.8646026 0.8528302 ## 0.3 2 0.6 0.875 50 0.8854970 0.8604499 ## 0.3 2 0.6 0.875 100 0.8756684 0.8546807 ## 0.3 2 0.6 0.875 150 0.8718297 0.8546807 ## 0.3 2 0.6 0.875 200 0.8662731 0.8489115 ## 0.3 2 0.6 0.875 250 0.8657367 0.8432511 ## 0.3 2 0.6 1.000 50 0.8851624 0.8662554 ## 0.3 2 0.6 1.000 100 0.8790797 0.8547170 ## 0.3 2 0.6 1.000 150 0.8764584 0.8547170 ## 0.3 2 0.6 1.000 200 0.8731986 0.8585269 ## 0.3 2 0.6 1.000 250 0.8691992 0.8470247 ## 0.3 2 0.8 0.500 50 0.8795484 0.8546807 ## 0.3 2 0.8 0.500 100 0.8725858 0.8585269 ## 0.3 2 0.8 0.500 150 0.8668489 0.8490929 ## 0.3 2 0.8 0.500 200 0.8627999 0.8317852 ## 0.3 2 0.8 0.500 250 0.8604951 0.8260160 ## 0.3 2 0.8 0.625 50 0.8798383 0.8585994 ## 0.3 2 0.8 0.625 100 0.8731001 0.8546807 ## 0.3 2 0.8 0.625 150 0.8727083 0.8547170 ## 0.3 2 0.8 0.625 200 0.8705551 0.8489478 ## 0.3 2 0.8 0.625 250 0.8648865 0.8432511 ## 0.3 2 0.8 0.750 50 0.8807836 0.8528302 ## 0.3 2 0.8 0.750 100 0.8778557 0.8413280 ## 0.3 2 0.8 0.750 150 0.8710876 0.8489840 ## 0.3 2 0.8 0.750 200 0.8675137 0.8527213 ## 0.3 2 0.8 0.750 250 0.8669419 0.8432511 ## 0.3 2 0.8 0.875 50 0.8830350 0.8585994 ## 0.3 2 0.8 0.875 100 0.8768979 0.8528302 ## 0.3 2 0.8 0.875 150 0.8703421 0.8585631 ## 0.3 2 0.8 0.875 200 0.8675309 0.8547170 ## 0.3 2 0.8 0.875 250 0.8631085 0.8470972 ## 0.3 2 0.8 1.000 50 0.8816738 0.8623730 ## 0.3 2 0.8 1.000 100 0.8754326 0.8546444 ## 0.3 2 0.8 1.000 150 0.8727592 0.8508345 ## 0.3 2 0.8 1.000 200 0.8716418 0.8470247 ## 0.3 2 0.8 1.000 250 0.8671499 0.8451379 ## 0.3 3 0.6 0.500 50 0.8784097 0.8585269 ## 0.3 3 0.6 0.500 100 0.8657922 0.8393324 ## 0.3 3 0.6 0.500 150 0.8546087 0.8412554 ## 0.3 3 0.6 0.500 200 0.8508247 0.8392235 ## 0.3 3 0.6 0.500 250 0.8466136 0.8374093 ## 0.3 3 0.6 0.625 50 0.8769267 0.8566038 ## 0.3 3 0.6 0.625 100 0.8669206 0.8393687 ## 0.3 3 0.6 0.625 150 0.8619305 0.8393324 ## 0.3 3 0.6 0.625 200 0.8574598 0.8451379 ## 0.3 3 0.6 0.625 250 0.8540881 0.8470247 ## 0.3 3 0.6 0.750 50 0.8757081 0.8528302 ## 0.3 3 0.6 0.750 100 0.8631662 0.8394049 ## 0.3 3 0.6 0.750 150 0.8589792 0.8413643 ## 0.3 3 0.6 0.750 200 0.8583321 0.8413280 ## 0.3 3 0.6 0.750 250 0.8534161 0.8394049 ## 0.3 3 0.6 0.875 50 0.8728172 0.8566763 ## 0.3 3 0.6 0.875 100 0.8656370 0.8508345 ## 0.3 3 0.6 0.875 150 0.8615151 0.8490566 ## 0.3 3 0.6 0.875 200 0.8578709 0.8432511 ## 0.3 3 0.6 0.875 250 0.8577331 0.8451379 ## 0.3 3 0.6 1.000 50 0.8776702 0.8605225 ## 0.3 3 0.6 1.000 100 0.8731200 0.8586357 ## 0.3 3 0.6 1.000 150 0.8680546 0.8566401 ## 0.3 3 0.6 1.000 200 0.8652595 0.8432511 ## 0.3 3 0.6 1.000 250 0.8626786 0.8451742 ## 0.3 3 0.8 0.500 50 0.8704410 0.8431785 ## 0.3 3 0.8 0.500 100 0.8611050 0.8470247 ## 0.3 3 0.8 0.500 150 0.8574252 0.8432511 ## 0.3 3 0.8 0.500 200 0.8478274 0.8279390 ## 0.3 3 0.8 0.500 250 0.8415372 0.8260160 ## 0.3 3 0.8 0.625 50 0.8752167 0.8661829 ## 0.3 3 0.8 0.625 100 0.8665248 0.8508345 ## 0.3 3 0.8 0.625 150 0.8670495 0.8547170 ## 0.3 3 0.8 0.625 200 0.8552427 0.8431785 ## 0.3 3 0.8 0.625 250 0.8534494 0.8355951 ## 0.3 3 0.8 0.750 50 0.8763169 0.8585269 ## 0.3 3 0.8 0.750 100 0.8684903 0.8470610 ## 0.3 3 0.8 0.750 150 0.8634290 0.8354862 ## 0.3 3 0.8 0.750 200 0.8598103 0.8393687 ## 0.3 3 0.8 0.750 250 0.8560082 0.8354499 ## 0.3 3 0.8 0.875 50 0.8756224 0.8585631 ## 0.3 3 0.8 0.875 100 0.8691344 0.8508708 ## 0.3 3 0.8 0.875 150 0.8613913 0.8489115 ## 0.3 3 0.8 0.875 200 0.8589936 0.8414006 ## 0.3 3 0.8 0.875 250 0.8583912 0.8395501 ## 0.3 3 0.8 1.000 50 0.8785875 0.8546444 ## 0.3 3 0.8 1.000 100 0.8697357 0.8584906 ## 0.3 3 0.8 1.000 150 0.8663404 0.8470247 ## 0.3 3 0.8 1.000 200 0.8647051 0.8489840 ## 0.3 3 0.8 1.000 250 0.8613289 0.8412917 ## 0.3 4 0.6 0.500 50 0.8684512 0.8469884 ## 0.3 4 0.6 0.500 100 0.8548516 0.8374456 ## 0.3 4 0.6 0.500 150 0.8564383 0.8355225 ## 0.3 4 0.6 0.500 200 0.8531487 0.8374819 ## 0.3 4 0.6 0.500 250 0.8503835 0.8546807 ## 0.3 4 0.6 0.625 50 0.8725837 0.8528302 ## 0.3 4 0.6 0.625 100 0.8650128 0.8490566 ## 0.3 4 0.6 0.625 150 0.8594804 0.8318215 ## 0.3 4 0.6 0.625 200 0.8570231 0.8317852 ## 0.3 4 0.6 0.625 250 0.8551338 0.8432511 ## 0.3 4 0.6 0.750 50 0.8726290 0.8547170 ## 0.3 4 0.6 0.750 100 0.8622128 0.8451016 ## 0.3 4 0.6 0.750 150 0.8628721 0.8412917 ## 0.3 4 0.6 0.750 200 0.8565407 0.8297533 ## 0.3 4 0.6 0.750 250 0.8516140 0.8336357 ## 0.3 4 0.6 0.875 50 0.8690627 0.8585269 ## 0.3 4 0.6 0.875 100 0.8598471 0.8510160 ## 0.3 4 0.6 0.875 150 0.8531149 0.8451742 ## 0.3 4 0.6 0.875 200 0.8522017 0.8355951 ## 0.3 4 0.6 0.875 250 0.8490782 0.8394775 ## 0.3 4 0.6 1.000 50 0.8757086 0.8585269 ## 0.3 4 0.6 1.000 100 0.8648118 0.8469884 ## 0.3 4 0.6 1.000 150 0.8605393 0.8450653 ## 0.3 4 0.6 1.000 200 0.8559972 0.8317126 ## 0.3 4 0.6 1.000 250 0.8533540 0.8241292 ## 0.3 4 0.8 0.500 50 0.8599347 0.8507983 ## 0.3 4 0.8 0.500 100 0.8520126 0.8451379 ## 0.3 4 0.8 0.500 150 0.8495768 0.8450653 ## 0.3 4 0.8 0.500 200 0.8497165 0.8374456 ## 0.3 4 0.8 0.500 250 0.8446128 0.8336357 ## 0.3 4 0.8 0.625 50 0.8729510 0.8490566 ## 0.3 4 0.8 0.625 100 0.8644594 0.8490203 ## 0.3 4 0.8 0.625 150 0.8562968 0.8393687 ## 0.3 4 0.8 0.625 200 0.8513927 0.8336357 ## 0.3 4 0.8 0.625 250 0.8466296 0.8337446 ## 0.3 4 0.8 0.750 50 0.8670178 0.8431785 ## 0.3 4 0.8 0.750 100 0.8608809 0.8432511 ## 0.3 4 0.8 0.750 150 0.8557375 0.8394775 ## 0.3 4 0.8 0.750 200 0.8545252 0.8375181 ## 0.3 4 0.8 0.750 250 0.8516413 0.8338171 ## 0.3 4 0.8 0.875 50 0.8665294 0.8374819 ## 0.3 4 0.8 0.875 100 0.8624626 0.8374456 ## 0.3 4 0.8 0.875 150 0.8600795 0.8337446 ## 0.3 4 0.8 0.875 200 0.8542276 0.8298984 ## 0.3 4 0.8 0.875 250 0.8509514 0.8261248 ## 0.3 4 0.8 1.000 50 0.8712199 0.8546807 ## 0.3 4 0.8 1.000 100 0.8622010 0.8566401 ## 0.3 4 0.8 1.000 150 0.8616357 0.8527939 ## 0.3 4 0.8 1.000 200 0.8570352 0.8414006 ## 0.3 4 0.8 1.000 250 0.8555653 0.8432511 ## 0.3 5 0.6 0.500 50 0.8568017 0.8526488 ## 0.3 5 0.6 0.500 100 0.8473738 0.8354862 ## 0.3 5 0.6 0.500 150 0.8491211 0.8277939 ## 0.3 5 0.6 0.500 200 0.8471236 0.8240929 ## 0.3 5 0.6 0.500 250 0.8459383 0.8145864 ## 0.3 5 0.6 0.625 50 0.8589184 0.8432148 ## 0.3 5 0.6 0.625 100 0.8553874 0.8412917 ## 0.3 5 0.6 0.625 150 0.8517198 0.8393687 ## 0.3 5 0.6 0.625 200 0.8527430 0.8469884 ## 0.3 5 0.6 0.625 250 0.8462826 0.8354862 ## 0.3 5 0.6 0.750 50 0.8633934 0.8412554 ## 0.3 5 0.6 0.750 100 0.8575012 0.8298258 ## 0.3 5 0.6 0.750 150 0.8516705 0.8222061 ## 0.3 5 0.6 0.750 200 0.8499948 0.8240566 ## 0.3 5 0.6 0.750 250 0.8484495 0.8297170 ## 0.3 5 0.6 0.875 50 0.8642104 0.8450653 ## 0.3 5 0.6 0.875 100 0.8571687 0.8355588 ## 0.3 5 0.6 0.875 150 0.8535070 0.8375181 ## 0.3 5 0.6 0.875 200 0.8498702 0.8337083 ## 0.3 5 0.6 0.875 250 0.8503518 0.8242017 ## 0.3 5 0.6 1.000 50 0.8654518 0.8450653 ## 0.3 5 0.6 1.000 100 0.8606506 0.8375181 ## 0.3 5 0.6 1.000 150 0.8579745 0.8375907 ## 0.3 5 0.6 1.000 200 0.8539322 0.8299347 ## 0.3 5 0.6 1.000 250 0.8525294 0.8280116 ## 0.3 5 0.8 0.500 50 0.8611009 0.8373367 ## 0.3 5 0.8 0.500 100 0.8525752 0.8412554 ## 0.3 5 0.8 0.500 150 0.8529361 0.8373367 ## 0.3 5 0.8 0.500 200 0.8490953 0.8354862 ## 0.3 5 0.8 0.500 250 0.8459399 0.8279390 ## 0.3 5 0.8 0.625 50 0.8616727 0.8546444 ## 0.3 5 0.8 0.625 100 0.8583265 0.8432874 ## 0.3 5 0.8 0.625 150 0.8550993 0.8279753 ## 0.3 5 0.8 0.625 200 0.8523346 0.8318940 ## 0.3 5 0.8 0.625 250 0.8526050 0.8298984 ## 0.3 5 0.8 0.750 50 0.8615628 0.8451379 ## 0.3 5 0.8 0.750 100 0.8543987 0.8353774 ## 0.3 5 0.8 0.750 150 0.8495081 0.8240203 ## 0.3 5 0.8 0.750 200 0.8433454 0.8334906 ## 0.3 5 0.8 0.750 250 0.8398512 0.8182874 ## 0.3 5 0.8 0.875 50 0.8695798 0.8470610 ## 0.3 5 0.8 0.875 100 0.8576054 0.8413280 ## 0.3 5 0.8 0.875 150 0.8529901 0.8355225 ## 0.3 5 0.8 0.875 200 0.8505461 0.8240929 ## 0.3 5 0.8 0.875 250 0.8486020 0.8260160 ## 0.3 5 0.8 1.000 50 0.8683535 0.8452104 ## 0.3 5 0.8 1.000 100 0.8608524 0.8452830 ## 0.3 5 0.8 1.000 150 0.8576962 0.8433962 ## 0.3 5 0.8 1.000 200 0.8535975 0.8415094 ## 0.3 5 0.8 1.000 250 0.8513341 0.8415094 ## 0.4 1 0.6 0.500 50 0.8819773 0.8624093 ## 0.4 1 0.6 0.500 100 0.8781782 0.8585631 ## 0.4 1 0.6 0.500 150 0.8732982 0.8529028 ## 0.4 1 0.6 0.500 200 0.8717570 0.8604862 ## 0.4 1 0.6 0.500 250 0.8731235 0.8509434 ## 0.4 1 0.6 0.625 50 0.8850621 0.8661103 ## 0.4 1 0.6 0.625 100 0.8838074 0.8624456 ## 0.4 1 0.6 0.625 150 0.8764260 0.8528665 ## 0.4 1 0.6 0.625 200 0.8740510 0.8490929 ## 0.4 1 0.6 0.625 250 0.8769566 0.8605588 ## 0.4 1 0.6 0.750 50 0.8816833 0.8738389 ## 0.4 1 0.6 0.750 100 0.8794957 0.8700290 ## 0.4 1 0.6 0.750 150 0.8800637 0.8662192 ## 0.4 1 0.6 0.750 200 0.8782589 0.8661829 ## 0.4 1 0.6 0.750 250 0.8765927 0.8585631 ## 0.4 1 0.6 0.875 50 0.8859525 0.8815312 ## 0.4 1 0.6 0.875 100 0.8805408 0.8815312 ## 0.4 1 0.6 0.875 150 0.8770713 0.8720247 ## 0.4 1 0.6 0.875 200 0.8755091 0.8681422 ## 0.4 1 0.6 0.875 250 0.8736719 0.8624819 ## 0.4 1 0.6 1.000 50 0.8892252 0.8719521 ## 0.4 1 0.6 1.000 100 0.8862227 0.8700653 ## 0.4 1 0.6 1.000 150 0.8834724 0.8681785 ## 0.4 1 0.6 1.000 200 0.8827532 0.8681422 ## 0.4 1 0.6 1.000 250 0.8817585 0.8681422 ## 0.4 1 0.8 0.500 50 0.8819568 0.8796081 ## 0.4 1 0.8 0.500 100 0.8771383 0.8682148 ## 0.4 1 0.8 0.500 150 0.8704260 0.8604862 ## 0.4 1 0.8 0.500 200 0.8702556 0.8566401 ## 0.4 1 0.8 0.500 250 0.8672987 0.8510160 ## 0.4 1 0.8 0.625 50 0.8827328 0.8777213 ## 0.4 1 0.8 0.625 100 0.8796493 0.8700653 ## 0.4 1 0.8 0.625 150 0.8781007 0.8604862 ## 0.4 1 0.8 0.625 200 0.8704065 0.8527939 ## 0.4 1 0.8 0.625 250 0.8726450 0.8547896 ## 0.4 1 0.8 0.750 50 0.8850542 0.8680334 ## 0.4 1 0.8 0.750 100 0.8813316 0.8624093 ## 0.4 1 0.8 0.750 150 0.8808412 0.8566401 ## 0.4 1 0.8 0.750 200 0.8787724 0.8642961 ## 0.4 1 0.8 0.750 250 0.8749581 0.8547533 ## 0.4 1 0.8 0.875 50 0.8874193 0.8758345 ## 0.4 1 0.8 0.875 100 0.8836916 0.8719521 ## 0.4 1 0.8 0.875 150 0.8809541 0.8642961 ## 0.4 1 0.8 0.875 200 0.8762188 0.8681785 ## 0.4 1 0.8 0.875 250 0.8747890 0.8603774 ## 0.4 1 0.8 1.000 50 0.8878571 0.8680697 ## 0.4 1 0.8 1.000 100 0.8842316 0.8777213 ## 0.4 1 0.8 1.000 150 0.8821720 0.8757983 ## 0.4 1 0.8 1.000 200 0.8810133 0.8662554 ## 0.4 1 0.8 1.000 250 0.8800336 0.8681422 ## 0.4 2 0.6 0.500 50 0.8766361 0.8566401 ## 0.4 2 0.6 0.500 100 0.8741177 0.8508708 ## 0.4 2 0.6 0.500 150 0.8709445 0.8470972 ## 0.4 2 0.6 0.500 200 0.8626755 0.8509071 ## 0.4 2 0.6 0.500 250 0.8578641 0.8469884 ## 0.4 2 0.6 0.625 50 0.8751713 0.8527939 ## 0.4 2 0.6 0.625 100 0.8693670 0.8509434 ## 0.4 2 0.6 0.625 150 0.8659128 0.8450653 ## 0.4 2 0.6 0.625 200 0.8591888 0.8373730 ## 0.4 2 0.6 0.625 250 0.8580717 0.8259071 ## 0.4 2 0.6 0.750 50 0.8719918 0.8566038 ## 0.4 2 0.6 0.750 100 0.8693202 0.8604862 ## 0.4 2 0.6 0.750 150 0.8668895 0.8431422 ## 0.4 2 0.6 0.750 200 0.8619742 0.8450290 ## 0.4 2 0.6 0.750 250 0.8609928 0.8527213 ## 0.4 2 0.6 0.875 50 0.8797380 0.8490203 ## 0.4 2 0.6 0.875 100 0.8725045 0.8451379 ## 0.4 2 0.6 0.875 150 0.8652448 0.8489115 ## 0.4 2 0.6 0.875 200 0.8670949 0.8412554 ## 0.4 2 0.6 0.875 250 0.8633280 0.8507620 ## 0.4 2 0.6 1.000 50 0.8831165 0.8604136 ## 0.4 2 0.6 1.000 100 0.8728979 0.8489115 ## 0.4 2 0.6 1.000 150 0.8677760 0.8393324 ## 0.4 2 0.6 1.000 200 0.8659265 0.8393324 ## 0.4 2 0.6 1.000 250 0.8646603 0.8451016 ## 0.4 2 0.8 0.500 50 0.8748110 0.8528665 ## 0.4 2 0.8 0.500 100 0.8683823 0.8489115 ## 0.4 2 0.8 0.500 150 0.8618886 0.8565675 ## 0.4 2 0.8 0.500 200 0.8546334 0.8431785 ## 0.4 2 0.8 0.500 250 0.8560853 0.8354862 ## 0.4 2 0.8 0.625 50 0.8716244 0.8547533 ## 0.4 2 0.8 0.625 100 0.8660106 0.8470972 ## 0.4 2 0.8 0.625 150 0.8583216 0.8432874 ## 0.4 2 0.8 0.625 200 0.8545955 0.8413280 ## 0.4 2 0.8 0.625 250 0.8523242 0.8394049 ## 0.4 2 0.8 0.750 50 0.8747039 0.8528665 ## 0.4 2 0.8 0.750 100 0.8720468 0.8470972 ## 0.4 2 0.8 0.750 150 0.8632754 0.8412917 ## 0.4 2 0.8 0.750 200 0.8601299 0.8488752 ## 0.4 2 0.8 0.750 250 0.8551371 0.8317126 ## 0.4 2 0.8 0.875 50 0.8808220 0.8681422 ## 0.4 2 0.8 0.875 100 0.8722082 0.8470610 ## 0.4 2 0.8 0.875 150 0.8654225 0.8470247 ## 0.4 2 0.8 0.875 200 0.8665308 0.8547896 ## 0.4 2 0.8 0.875 250 0.8624247 0.8509071 ## 0.4 2 0.8 1.000 50 0.8767032 0.8546444 ## 0.4 2 0.8 1.000 100 0.8713492 0.8489115 ## 0.4 2 0.8 1.000 150 0.8683053 0.8470610 ## 0.4 2 0.8 1.000 200 0.8666212 0.8470610 ## 0.4 2 0.8 1.000 250 0.8633723 0.8451742 ## 0.4 3 0.6 0.500 50 0.8640470 0.8489840 ## 0.4 3 0.6 0.500 100 0.8623627 0.8395138 ## 0.4 3 0.6 0.500 150 0.8600232 0.8450290 ## 0.4 3 0.6 0.500 200 0.8545515 0.8278665 ## 0.4 3 0.6 0.500 250 0.8486454 0.8412917 ## 0.4 3 0.6 0.625 50 0.8761459 0.8585631 ## 0.4 3 0.6 0.625 100 0.8622783 0.8469884 ## 0.4 3 0.6 0.625 150 0.8568207 0.8336357 ## 0.4 3 0.6 0.625 200 0.8552490 0.8373730 ## 0.4 3 0.6 0.625 250 0.8535823 0.8411829 ## 0.4 3 0.6 0.750 50 0.8718950 0.8489478 ## 0.4 3 0.6 0.750 100 0.8651224 0.8450290 ## 0.4 3 0.6 0.750 150 0.8638079 0.8317126 ## 0.4 3 0.6 0.750 200 0.8565601 0.8469521 ## 0.4 3 0.6 0.750 250 0.8571380 0.8355588 ## 0.4 3 0.6 0.875 50 0.8746308 0.8431060 ## 0.4 3 0.6 0.875 100 0.8665481 0.8566038 ## 0.4 3 0.6 0.875 150 0.8606995 0.8507620 ## 0.4 3 0.6 0.875 200 0.8578503 0.8374093 ## 0.4 3 0.6 0.875 250 0.8524245 0.8412554 ## 0.4 3 0.6 1.000 50 0.8754352 0.8604499 ## 0.4 3 0.6 1.000 100 0.8665944 0.8489840 ## 0.4 3 0.6 1.000 150 0.8630463 0.8509071 ## 0.4 3 0.6 1.000 200 0.8594737 0.8394775 ## 0.4 3 0.6 1.000 250 0.8553279 0.8355951 ## 0.4 3 0.8 0.500 50 0.8651490 0.8432511 ## 0.4 3 0.8 0.500 100 0.8569925 0.8529390 ## 0.4 3 0.8 0.500 150 0.8477096 0.8375181 ## 0.4 3 0.8 0.500 200 0.8457458 0.8279028 ## 0.4 3 0.8 0.500 250 0.8426879 0.8318940 ## 0.4 3 0.8 0.625 50 0.8658222 0.8451742 ## 0.4 3 0.8 0.625 100 0.8572253 0.8297896 ## 0.4 3 0.8 0.625 150 0.8534702 0.8279390 ## 0.4 3 0.8 0.625 200 0.8520551 0.8221335 ## 0.4 3 0.8 0.625 250 0.8469493 0.8317852 ## 0.4 3 0.8 0.750 50 0.8699383 0.8566763 ## 0.4 3 0.8 0.750 100 0.8621005 0.8433237 ## 0.4 3 0.8 0.750 150 0.8543705 0.8451016 ## 0.4 3 0.8 0.750 200 0.8552381 0.8355951 ## 0.4 3 0.8 0.750 250 0.8539085 0.8451016 ## 0.4 3 0.8 0.875 50 0.8688165 0.8585269 ## 0.4 3 0.8 0.875 100 0.8602130 0.8470972 ## 0.4 3 0.8 0.875 150 0.8547113 0.8336720 ## 0.4 3 0.8 0.875 200 0.8528785 0.8414006 ## 0.4 3 0.8 0.875 250 0.8518811 0.8337083 ## 0.4 3 0.8 1.000 50 0.8709665 0.8489115 ## 0.4 3 0.8 1.000 100 0.8653586 0.8412554 ## 0.4 3 0.8 1.000 150 0.8624485 0.8337446 ## 0.4 3 0.8 1.000 200 0.8594047 0.8355951 ## 0.4 3 0.8 1.000 250 0.8559894 0.8317852 ## 0.4 4 0.6 0.500 50 0.8582166 0.8413280 ## 0.4 4 0.6 0.500 100 0.8477981 0.8318215 ## 0.4 4 0.6 0.500 150 0.8487107 0.8279753 ## 0.4 4 0.6 0.500 200 0.8442558 0.8357402 ## 0.4 4 0.6 0.500 250 0.8408989 0.8413280 ## 0.4 4 0.6 0.625 50 0.8586802 0.8413280 ## 0.4 4 0.6 0.625 100 0.8515141 0.8355225 ## 0.4 4 0.6 0.625 150 0.8489226 0.8317126 ## 0.4 4 0.6 0.625 200 0.8401825 0.8201742 ## 0.4 4 0.6 0.625 250 0.8382434 0.8279390 ## 0.4 4 0.6 0.750 50 0.8631389 0.8412554 ## 0.4 4 0.6 0.750 100 0.8550658 0.8356676 ## 0.4 4 0.6 0.750 150 0.8514552 0.8279028 ## 0.4 4 0.6 0.750 200 0.8511109 0.8317852 ## 0.4 4 0.6 0.750 250 0.8472358 0.8299347 ## 0.4 4 0.6 0.875 50 0.8712762 0.8508708 ## 0.4 4 0.6 0.875 100 0.8631429 0.8413280 ## 0.4 4 0.6 0.875 150 0.8551864 0.8355588 ## 0.4 4 0.6 0.875 200 0.8514942 0.8337083 ## 0.4 4 0.6 0.875 250 0.8505960 0.8336357 ## 0.4 4 0.6 1.000 50 0.8682160 0.8412917 ## 0.4 4 0.6 1.000 100 0.8611073 0.8317852 ## 0.4 4 0.6 1.000 150 0.8565338 0.8260160 ## 0.4 4 0.6 1.000 200 0.8536485 0.8202830 ## 0.4 4 0.6 1.000 250 0.8517281 0.8126270 ## 0.4 4 0.8 0.500 50 0.8630090 0.8528665 ## 0.4 4 0.8 0.500 100 0.8568123 0.8355588 ## 0.4 4 0.8 0.500 150 0.8511253 0.8355225 ## 0.4 4 0.8 0.500 200 0.8483375 0.8298621 ## 0.4 4 0.8 0.500 250 0.8452968 0.8335994 ## 0.4 4 0.8 0.625 50 0.8582869 0.8431422 ## 0.4 4 0.8 0.625 100 0.8525613 0.8335631 ## 0.4 4 0.8 0.625 150 0.8488468 0.8335631 ## 0.4 4 0.8 0.625 200 0.8445724 0.8222424 ## 0.4 4 0.8 0.625 250 0.8473872 0.8184688 ## 0.4 4 0.8 0.750 50 0.8655905 0.8452104 ## 0.4 4 0.8 0.750 100 0.8547722 0.8451016 ## 0.4 4 0.8 0.750 150 0.8490490 0.8260522 ## 0.4 4 0.8 0.750 200 0.8457003 0.8298258 ## 0.4 4 0.8 0.750 250 0.8460344 0.8221698 ## 0.4 4 0.8 0.875 50 0.8636225 0.8469884 ## 0.4 4 0.8 0.875 100 0.8538044 0.8432874 ## 0.4 4 0.8 0.875 150 0.8503236 0.8374456 ## 0.4 4 0.8 0.875 200 0.8494653 0.8222061 ## 0.4 4 0.8 0.875 250 0.8481847 0.8298258 ## 0.4 4 0.8 1.000 50 0.8682368 0.8507983 ## 0.4 4 0.8 1.000 100 0.8628753 0.8508708 ## 0.4 4 0.8 1.000 150 0.8578768 0.8471698 ## 0.4 4 0.8 1.000 200 0.8554612 0.8356313 ## 0.4 4 0.8 1.000 250 0.8525602 0.8394775 ## 0.4 5 0.6 0.500 50 0.8559203 0.8412554 ## 0.4 5 0.6 0.500 100 0.8451355 0.8375907 ## 0.4 5 0.6 0.500 150 0.8441891 0.8337083 ## 0.4 5 0.6 0.500 200 0.8413874 0.8394049 ## 0.4 5 0.6 0.500 250 0.8393260 0.8433237 ## 0.4 5 0.6 0.625 50 0.8598324 0.8337808 ## 0.4 5 0.6 0.625 100 0.8536449 0.8336720 ## 0.4 5 0.6 0.625 150 0.8495237 0.8355588 ## 0.4 5 0.6 0.625 200 0.8479709 0.8374456 ## 0.4 5 0.6 0.625 250 0.8438965 0.8374819 ## 0.4 5 0.6 0.750 50 0.8574996 0.8411829 ## 0.4 5 0.6 0.750 100 0.8486574 0.8469158 ## 0.4 5 0.6 0.750 150 0.8449496 0.8279390 ## 0.4 5 0.6 0.750 200 0.8449407 0.8374456 ## 0.4 5 0.6 0.750 250 0.8446452 0.8279390 ## 0.4 5 0.6 0.875 50 0.8635692 0.8470610 ## 0.4 5 0.6 0.875 100 0.8574420 0.8414369 ## 0.4 5 0.6 0.875 150 0.8545774 0.8337808 ## 0.4 5 0.6 0.875 200 0.8512760 0.8357039 ## 0.4 5 0.6 0.875 250 0.8476474 0.8185051 ## 0.4 5 0.6 1.000 50 0.8628838 0.8412554 ## 0.4 5 0.6 1.000 100 0.8549096 0.8317489 ## 0.4 5 0.6 1.000 150 0.8515179 0.8164731 ## 0.4 5 0.6 1.000 200 0.8466769 0.8222424 ## 0.4 5 0.6 1.000 250 0.8449488 0.8241292 ## 0.4 5 0.8 0.500 50 0.8664443 0.8451016 ## 0.4 5 0.8 0.500 100 0.8564837 0.8451379 ## 0.4 5 0.8 0.500 150 0.8505925 0.8355588 ## 0.4 5 0.8 0.500 200 0.8472447 0.8355951 ## 0.4 5 0.8 0.500 250 0.8430956 0.8317126 ## 0.4 5 0.8 0.625 50 0.8583130 0.8508345 ## 0.4 5 0.8 0.625 100 0.8554109 0.8356676 ## 0.4 5 0.8 0.625 150 0.8512384 0.8394049 ## 0.4 5 0.8 0.625 200 0.8506076 0.8298984 ## 0.4 5 0.8 0.625 250 0.8482805 0.8336720 ## 0.4 5 0.8 0.750 50 0.8609450 0.8527576 ## 0.4 5 0.8 0.750 100 0.8500522 0.8374819 ## 0.4 5 0.8 0.750 150 0.8457927 0.8299347 ## 0.4 5 0.8 0.750 200 0.8441071 0.8203193 ## 0.4 5 0.8 0.750 250 0.8414376 0.8223149 ## 0.4 5 0.8 0.875 50 0.8625080 0.8527939 ## 0.4 5 0.8 0.875 100 0.8535023 0.8336720 ## 0.4 5 0.8 0.875 150 0.8487509 0.8317852 ## 0.4 5 0.8 0.875 200 0.8464541 0.8318578 ## 0.4 5 0.8 0.875 250 0.8457981 0.8298984 ## 0.4 5 0.8 1.000 50 0.8697430 0.8509071 ## 0.4 5 0.8 1.000 100 0.8605089 0.8451742 ## 0.4 5 0.8 1.000 150 0.8562087 0.8414006 ## 0.4 5 0.8 1.000 200 0.8521614 0.8414006 ## 0.4 5 0.8 1.000 250 0.8500181 0.8376270 ## Spec ## 0.7456328 ## 0.7518717 ## 0.7337790 ## 0.7308378 ## 0.7397504 ## 0.7545455 ## 0.7576649 ## 0.7456328 ## 0.7365419 ## 0.7336007 ## 0.7486631 ## 0.7455437 ## 0.7245989 ## 0.7336898 ## 0.7395722 ## 0.7456328 ## 0.7426025 ## 0.7484848 ## 0.7395722 ## 0.7455437 ## 0.7457219 ## 0.7366310 ## 0.7425134 ## 0.7515152 ## 0.7485740 ## 0.7485740 ## 0.7365419 ## 0.7394831 ## 0.7364528 ## 0.7395722 ## 0.7364528 ## 0.7635472 ## 0.7426025 ## 0.7395722 ## 0.7305704 ## 0.7425134 ## 0.7426916 ## 0.7367201 ## 0.7454545 ## 0.7363636 ## 0.7398396 ## 0.7365419 ## 0.7396613 ## 0.7516043 ## 0.7336898 ## 0.7426025 ## 0.7426025 ## 0.7515152 ## 0.7457219 ## 0.7516043 ## 0.7604278 ## 0.7515152 ## 0.7517825 ## 0.7396613 ## 0.7275401 ## 0.7396613 ## 0.7574866 ## 0.7606952 ## 0.7606952 ## 0.7426916 ## 0.7726381 ## 0.7545455 ## 0.7396613 ## 0.7335116 ## 0.7394831 ## 0.7694296 ## 0.7397504 ## 0.7575758 ## 0.7546346 ## 0.7455437 ## 0.7635472 ## 0.7573975 ## 0.7514260 ## 0.7485740 ## 0.7487522 ## 0.7394831 ## 0.7486631 ## 0.7366310 ## 0.7425134 ## 0.7486631 ## 0.7605169 ## 0.7573084 ## 0.7574866 ## 0.7573084 ## 0.7394831 ## 0.7604278 ## 0.7426025 ## 0.7456328 ## 0.7487522 ## 0.7427807 ## 0.7456328 ## 0.7516043 ## 0.7515152 ## 0.7456328 ## 0.7425134 ## 0.7635472 ## 0.7542781 ## 0.7484848 ## 0.7455437 ## 0.7367201 ## 0.7576649 ## 0.7303922 ## 0.7154189 ## 0.7155971 ## 0.7275401 ## 0.7637255 ## 0.7394831 ## 0.7277184 ## 0.7335116 ## 0.7364528 ## 0.7543672 ## 0.7422460 ## 0.7333333 ## 0.7363636 ## 0.7395722 ## 0.7696078 ## 0.7545455 ## 0.7546346 ## 0.7457219 ## 0.7486631 ## 0.7574866 ## 0.7484848 ## 0.7366310 ## 0.7485740 ## 0.7308378 ## 0.7545455 ## 0.7334225 ## 0.7098039 ## 0.7364528 ## 0.7126560 ## 0.7515152 ## 0.7304813 ## 0.7393048 ## 0.7426916 ## 0.7246881 ## 0.7453654 ## 0.7245989 ## 0.7336007 ## 0.7276292 ## 0.7247772 ## 0.7426025 ## 0.7394831 ## 0.7455437 ## 0.7485740 ## 0.7397504 ## 0.7544563 ## 0.7515152 ## 0.7394831 ## 0.7453654 ## 0.7454545 ## 0.7393048 ## 0.7420677 ## 0.7304813 ## 0.7154189 ## 0.7153298 ## 0.7514260 ## 0.7362745 ## 0.7483957 ## 0.7512478 ## 0.7424242 ## 0.7364528 ## 0.7184492 ## 0.7483957 ## 0.7275401 ## 0.7243316 ## 0.7458111 ## 0.7395722 ## 0.7306595 ## 0.7278075 ## 0.7218360 ## 0.7574866 ## 0.7335116 ## 0.7306595 ## 0.7306595 ## 0.7217469 ## 0.7186275 ## 0.7187166 ## 0.7278966 ## 0.7217469 ## 0.7304813 ## 0.7602496 ## 0.7393939 ## 0.7124777 ## 0.7245098 ## 0.7094474 ## 0.7483066 ## 0.7395722 ## 0.7275401 ## 0.7187166 ## 0.7185383 ## 0.7542781 ## 0.7365419 ## 0.7336007 ## 0.7456328 ## 0.7395722 ## 0.7425134 ## 0.7304813 ## 0.7277184 ## 0.7306595 ## 0.7217469 ## 0.7304813 ## 0.7064171 ## 0.7155080 ## 0.7272727 ## 0.7184492 ## 0.7333333 ## 0.7213012 ## 0.7272727 ## 0.7213904 ## 0.7034759 ## 0.7483066 ## 0.7275401 ## 0.7277184 ## 0.7126560 ## 0.7097148 ## 0.7452763 ## 0.7424242 ## 0.7273619 ## 0.7186275 ## 0.7307487 ## 0.7393048 ## 0.7212121 ## 0.7274510 ## 0.7274510 ## 0.7334225 ## 0.7426025 ## 0.7303922 ## 0.7305704 ## 0.7246881 ## 0.7097148 ## 0.7241533 ## 0.7390374 ## 0.7245098 ## 0.7274510 ## 0.7215686 ## 0.7484848 ## 0.7304813 ## 0.7245989 ## 0.7187166 ## 0.7066845 ## 0.7455437 ## 0.7364528 ## 0.7247772 ## 0.7218360 ## 0.7308378 ## 0.7333333 ## 0.7484848 ## 0.7364528 ## 0.7249554 ## 0.7159537 ## 0.7424242 ## 0.7187166 ## 0.7397504 ## 0.7336007 ## 0.7426025 ## 0.7306595 ## 0.7457219 ## 0.7485740 ## 0.7456328 ## 0.7395722 ## 0.7399287 ## 0.7459002 ## 0.7577540 ## 0.7337790 ## 0.7427807 ## 0.7426025 ## 0.7485740 ## 0.7396613 ## 0.7307487 ## 0.7397504 ## 0.7426025 ## 0.7365419 ## 0.7455437 ## 0.7456328 ## 0.7426025 ## 0.7453654 ## 0.7394831 ## 0.7245989 ## 0.7336007 ## 0.7277184 ## 0.7485740 ## 0.7425134 ## 0.7665775 ## 0.7277184 ## 0.7486631 ## 0.7545455 ## 0.7365419 ## 0.7334225 ## 0.7336898 ## 0.7336007 ## 0.7365419 ## 0.7484848 ## 0.7426025 ## 0.7457219 ## 0.7426916 ## 0.7487522 ## 0.7454545 ## 0.7366310 ## 0.7425134 ## 0.7456328 ## 0.7546346 ## 0.7603387 ## 0.7514260 ## 0.7122103 ## 0.7155080 ## 0.7574866 ## 0.7488414 ## 0.7606952 ## 0.7396613 ## 0.7336898 ## 0.7663993 ## 0.7304813 ## 0.7575758 ## 0.7548128 ## 0.7336007 ## 0.7486631 ## 0.7663102 ## 0.7366310 ## 0.7368093 ## 0.7396613 ## 0.7694296 ## 0.7397504 ## 0.7305704 ## 0.7394831 ## 0.7394831 ## 0.7364528 ## 0.7216578 ## 0.7278075 ## 0.7067736 ## 0.7005348 ## 0.7545455 ## 0.7517825 ## 0.7547237 ## 0.7276292 ## 0.7277184 ## 0.7515152 ## 0.7457219 ## 0.7456328 ## 0.7277184 ## 0.7393939 ## 0.7484848 ## 0.7395722 ## 0.7334225 ## 0.7364528 ## 0.7395722 ## 0.7483957 ## 0.7365419 ## 0.7394831 ## 0.7455437 ## 0.7484848 ## 0.7425134 ## 0.7514260 ## 0.7276292 ## 0.7365419 ## 0.7216578 ## 0.7575758 ## 0.7277184 ## 0.7182709 ## 0.7219251 ## 0.7123886 ## 0.7394831 ## 0.7335116 ## 0.7186275 ## 0.7125668 ## 0.7213904 ## 0.7516043 ## 0.7485740 ## 0.7334225 ## 0.7336898 ## 0.7246881 ## 0.7574866 ## 0.7336898 ## 0.7397504 ## 0.7368093 ## 0.7160428 ## 0.7398396 ## 0.7362745 ## 0.7302139 ## 0.7245098 ## 0.7158645 ## 0.7426025 ## 0.7336898 ## 0.7247772 ## 0.7216578 ## 0.7156863 ## 0.7276292 ## 0.7335116 ## 0.7304813 ## 0.7246881 ## 0.7187166 ## 0.7394831 ## 0.7512478 ## 0.7424242 ## 0.7543672 ## 0.7275401 ## 0.7306595 ## 0.7393048 ## 0.7424242 ## 0.7367201 ## 0.7276292 ## 0.7515152 ## 0.7513369 ## 0.7304813 ## 0.7336007 ## 0.7426025 ## 0.7216578 ## 0.7337790 ## 0.7274510 ## 0.7186275 ## 0.7125668 ## 0.7458111 ## 0.7396613 ## 0.7397504 ## 0.7338681 ## 0.7219251 ## 0.7332442 ## 0.7543672 ## 0.7247772 ## 0.7336007 ## 0.7245989 ## 0.7304813 ## 0.7274510 ## 0.7276292 ## 0.7187166 ## 0.7156863 ## 0.7275401 ## 0.7215686 ## 0.7126560 ## 0.7065954 ## 0.7065954 ## 0.7664884 ## 0.7271836 ## 0.7242424 ## 0.7333333 ## 0.7274510 ## 0.7220143 ## 0.7215686 ## 0.7244207 ## 0.7123886 ## 0.7245989 ## 0.7394831 ## 0.7395722 ## 0.7393048 ## 0.7333333 ## 0.7333333 ## 0.7546346 ## 0.7308378 ## 0.7099822 ## 0.7191622 ## 0.7042781 ## 0.7484848 ## 0.7242424 ## 0.7242424 ## 0.7123886 ## 0.7214795 ## 0.7213904 ## 0.7336007 ## 0.7394831 ## 0.7245989 ## 0.7245098 ## 0.7182709 ## 0.7094474 ## 0.7036542 ## 0.7125668 ## 0.7064171 ## 0.7335116 ## 0.7336007 ## 0.7336898 ## 0.7278075 ## 0.7246881 ## 0.7423351 ## 0.7395722 ## 0.7337790 ## 0.7216578 ## 0.7188057 ## 0.7127451 ## 0.7127451 ## 0.7158645 ## 0.7275401 ## 0.7008021 ## 0.7274510 ## 0.7277184 ## 0.7363636 ## 0.7303030 ## 0.7273619 ## 0.7364528 ## 0.7244207 ## 0.7184492 ## 0.7216578 ## 0.7155971 ## 0.7335116 ## 0.7365419 ## 0.7336898 ## 0.7309269 ## 0.7279857 ## 0.7216578 ## 0.7098039 ## 0.7247772 ## 0.7249554 ## 0.7190731 ## ## Tuning parameter &#39;gamma&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;min_child_weight&#39; was held constant at a value of 1 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were nrounds = 50, max_depth = 1, eta ## = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample ## = 1. train() tuned eta (\\(\\eta\\)), max_depth, colsample_bytree, subsample, and nrounds, holding gamma = 0, and min_child_weight = 1. The optimal hyperparameter values were eta = 0.3, max_depth - 1, colsample_bytree = 0.6, subsample = 1, and nrounds = 50. With so many hyperparameters, the tuning plot is nearly unreadable. #plot(oj_mdl_xgb) Let’s see how the model performed on the holdout set. The accuracy was 0.8732 - much better than the 0.8451 from regular gradient boosting. oj_preds_xgb &lt;- bind_cols( predict(oj_mdl_xgb, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_xgb, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_xgb &lt;- confusionMatrix(oj_preds_xgb$Predicted, reference = oj_preds_xgb$Actual) oj_cm_xgb ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 120 17 ## MM 10 66 ## ## Accuracy : 0.8732 ## 95% CI : (0.821, 0.9148) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.7294 ## ## Mcnemar&#39;s Test P-Value : 0.2482 ## ## Sensitivity : 0.9231 ## Specificity : 0.7952 ## Pos Pred Value : 0.8759 ## Neg Pred Value : 0.8684 ## Prevalence : 0.6103 ## Detection Rate : 0.5634 ## Detection Prevalence : 0.6432 ## Balanced Accuracy : 0.8591 ## ## &#39;Positive&#39; Class : CH ## AUC was 0.9386 for gradient boosting, and here it is 0.9323. Here are the ROC and gain curves. mdl_auc &lt;- Metrics::auc(actual = oj_preds_xgb$Actual == &quot;CH&quot;, oj_preds_xgb$CH) yardstick::roc_curve(oj_preds_xgb, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ XGBoost ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_xgb, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ XGBoost Gain Curve&quot;) Now the variable importance. Nothing jumps out at me here. It’s the same top variables as regular gradient boosting. plot(varImp(oj_mdl_xgb), main=&quot;Variable Importance with XGBoost&quot;) Okay, let’s check in with the leader board. Wow, XGBoost is extreme. oj_scoreboard &lt;- rbind(oj_scoreboard, data.frame(Model = &quot;Gradient Boosting&quot;, Accuracy = oj_cm_gbm$overall[&quot;Accuracy&quot;])) %&gt;% rbind(data.frame(Model = &quot;XGBoost&quot;, Accuracy = oj_cm_xgb$overall[&quot;Accuracy&quot;])) %&gt;% arrange(desc(Accuracy)) scoreboard(oj_scoreboard) .cl-3ec08080{}.cl-3eba48e6{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3ebca1b8{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3ebca1cc{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3ebcafc8{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ebcafd2{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ebcafd3{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ebcafdc{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ebcafe6{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ebcaff0{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ebcaff1{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ebcaff2{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ebcaffa{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ebcaffb{width:1.007in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelAccuracyXGBoost0.8732394Single Tree0.8591549Single Tree (caret)0.8544601Bagging0.8450704Gradient Boosting0.8450704Random Forest0.8309859 4.5.0.2 Gradient Boosting Regression Tree 4.5.0.2.1 GBM I’ll predict Sales from the Carseats data set again, this time using the bagging method by specifying method = \"gbm\" set.seed(1234) garbage &lt;- capture.output( cs_mdl_gbm &lt;- train( Sales ~ ., data = cs_train, method = &quot;gbm&quot;, tuneLength = 5, trControl = cs_trControl )) cs_mdl_gbm ## Stochastic Gradient Boosting ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees RMSE Rsquared MAE ## 1 50 1.842468 0.6718370 1.4969754 ## 1 100 1.516967 0.7823612 1.2407807 ## 1 150 1.309295 0.8277888 1.0639501 ## 1 200 1.216079 0.8429002 0.9866820 ## 1 250 1.161540 0.8488463 0.9384418 ## 2 50 1.527454 0.7801995 1.2207991 ## 2 100 1.240990 0.8381156 1.0063802 ## 2 150 1.187603 0.8415216 0.9616681 ## 2 200 1.174303 0.8425011 0.9527720 ## 2 250 1.172116 0.8403490 0.9500902 ## 3 50 1.390969 0.8071393 1.1316570 ## 3 100 1.227525 0.8321632 0.9888203 ## 3 150 1.201264 0.8345775 0.9694065 ## 3 200 1.214462 0.8282833 0.9761625 ## 3 250 1.232145 0.8221405 0.9882254 ## 4 50 1.341893 0.8128778 1.0949502 ## 4 100 1.252282 0.8230712 0.9907410 ## 4 150 1.243045 0.8229433 0.9860813 ## 4 200 1.258093 0.8162033 0.9947218 ## 4 250 1.271058 0.8114156 1.0144873 ## 5 50 1.318251 0.8128033 1.0552929 ## 5 100 1.250053 0.8226441 0.9958713 ## 5 150 1.248402 0.8214824 0.9888330 ## 5 200 1.263445 0.8158033 1.0106345 ## 5 250 1.273024 0.8124672 1.0213099 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were n.trees = 250, interaction.depth = ## 1, shrinkage = 0.1 and n.minobsinnode = 10. The optimal tuning parameters were at \\(M = 250\\) and interation.depth = 1. plot(cs_mdl_gbm) Here is the holdout set performance. cs_preds_gbm &lt;- bind_cols( Predicted = predict(cs_mdl_gbm, newdata = cs_test), Actual = cs_test$Sales ) # Model over-predicts at low end of Sales and under-predicts at high end cs_preds_gbm %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats GBM, Predicted vs Actual&quot;) #plot(varImp(cs_mdl_gbm), main=&quot;Variable Importance with GBM&quot;) The RMSE is 1.438 - the best of the bunch. cs_rmse_gbm &lt;- RMSE(pred = cs_preds_gbm$Predicted, obs = cs_preds_gbm$Actual) cs_scoreboard &lt;- rbind(cs_scoreboard, data.frame(Model = &quot;GBM&quot;, RMSE = cs_rmse_gbm) ) %&gt;% arrange(RMSE) scoreboard(cs_scoreboard) .cl-40435b6c{}.cl-403bc3de{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-403f2524{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-403f252e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-403f3596{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-403f35a0{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-403f35a1{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-403f35aa{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-403f35b4{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-403f35b5{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-403f35be{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-403f35bf{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-403f35c8{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-403f35c9{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-403f35d2{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-403f35d3{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEGBM1.438058Random Forest1.718358Bagging1.918473Single Tree (caret)2.298331Single Tree2.363202 4.5.0.2.2 XGBoost I’ll predict Sales from the Carseats data set again, this time using the bagging method by specifying method = \"xgb\" set.seed(1234) garbage &lt;- capture.output( cs_mdl_xgb &lt;- train( Sales ~ ., data = cs_train, method = &quot;xgbTree&quot;, tuneLength = 5, trControl = cs_trControl )) cs_mdl_xgb ## eXtreme Gradient Boosting ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## eta max_depth colsample_bytree subsample nrounds RMSE Rsquared ## 0.3 1 0.6 0.500 50 1.389281 0.8016720 ## 0.3 1 0.6 0.500 100 1.170297 0.8433732 ## 0.3 1 0.6 0.500 150 1.148507 0.8459571 ## 0.3 1 0.6 0.500 200 1.132626 0.8503489 ## 0.3 1 0.6 0.500 250 1.137985 0.8480528 ## 0.3 1 0.6 0.625 50 1.380563 0.8058802 ## 0.3 1 0.6 0.625 100 1.175902 0.8451644 ## 0.3 1 0.6 0.625 150 1.134595 0.8519167 ## 0.3 1 0.6 0.625 200 1.140584 0.8494365 ## 0.3 1 0.6 0.625 250 1.132677 0.8506928 ## 0.3 1 0.6 0.750 50 1.385163 0.8075073 ## 0.3 1 0.6 0.750 100 1.187364 0.8431270 ## 0.3 1 0.6 0.750 150 1.138854 0.8496572 ## 0.3 1 0.6 0.750 200 1.142356 0.8476610 ## 0.3 1 0.6 0.750 250 1.143148 0.8464807 ## 0.3 1 0.6 0.875 50 1.400731 0.8084364 ## 0.3 1 0.6 0.875 100 1.173470 0.8511850 ## 0.3 1 0.6 0.875 150 1.118825 0.8588693 ## 0.3 1 0.6 0.875 200 1.114795 0.8577995 ## 0.3 1 0.6 0.875 250 1.115795 0.8561051 ## 0.3 1 0.6 1.000 50 1.415297 0.8061370 ## 0.3 1 0.6 1.000 100 1.181820 0.8526590 ## 0.3 1 0.6 1.000 150 1.115363 0.8611355 ## 0.3 1 0.6 1.000 200 1.094207 0.8631220 ## 0.3 1 0.6 1.000 250 1.091394 0.8623621 ## 0.3 1 0.8 0.500 50 1.397701 0.7954989 ## 0.3 1 0.8 0.500 100 1.197568 0.8360239 ## 0.3 1 0.8 0.500 150 1.196533 0.8353681 ## 0.3 1 0.8 0.500 200 1.173332 0.8379905 ## 0.3 1 0.8 0.500 250 1.168466 0.8387034 ## 0.3 1 0.8 0.625 50 1.362801 0.8117822 ## 0.3 1 0.8 0.625 100 1.182354 0.8452858 ## 0.3 1 0.8 0.625 150 1.146062 0.8496697 ## 0.3 1 0.8 0.625 200 1.134710 0.8519798 ## 0.3 1 0.8 0.625 250 1.153457 0.8449150 ## 0.3 1 0.8 0.750 50 1.383044 0.8082881 ## 0.3 1 0.8 0.750 100 1.173819 0.8472920 ## 0.3 1 0.8 0.750 150 1.126809 0.8561366 ## 0.3 1 0.8 0.750 200 1.129222 0.8533769 ## 0.3 1 0.8 0.750 250 1.126619 0.8531121 ## 0.3 1 0.8 0.875 50 1.413715 0.8041976 ## 0.3 1 0.8 0.875 100 1.201149 0.8438494 ## 0.3 1 0.8 0.875 150 1.144407 0.8518164 ## 0.3 1 0.8 0.875 200 1.142756 0.8500161 ## 0.3 1 0.8 0.875 250 1.136565 0.8510828 ## 0.3 1 0.8 1.000 50 1.419007 0.8073250 ## 0.3 1 0.8 1.000 100 1.189885 0.8509704 ## 0.3 1 0.8 1.000 150 1.124194 0.8600179 ## 0.3 1 0.8 1.000 200 1.101172 0.8620828 ## 0.3 1 0.8 1.000 250 1.099683 0.8608583 ## 0.3 2 0.6 0.500 50 1.293173 0.8098524 ## 0.3 2 0.6 0.500 100 1.305913 0.7994696 ## 0.3 2 0.6 0.500 150 1.334434 0.7910863 ## 0.3 2 0.6 0.500 200 1.351337 0.7889761 ## 0.3 2 0.6 0.500 250 1.399503 0.7733491 ## 0.3 2 0.6 0.625 50 1.292586 0.8063764 ## 0.3 2 0.6 0.625 100 1.263679 0.8152226 ## 0.3 2 0.6 0.625 150 1.293964 0.8062317 ## 0.3 2 0.6 0.625 200 1.315392 0.7997792 ## 0.3 2 0.6 0.625 250 1.334901 0.7937768 ## 0.3 2 0.6 0.750 50 1.231750 0.8307135 ## 0.3 2 0.6 0.750 100 1.226186 0.8265136 ## 0.3 2 0.6 0.750 150 1.255043 0.8162509 ## 0.3 2 0.6 0.750 200 1.283946 0.8074366 ## 0.3 2 0.6 0.750 250 1.303713 0.8010811 ## 0.3 2 0.6 0.875 50 1.258933 0.8251014 ## 0.3 2 0.6 0.875 100 1.241878 0.8212522 ## 0.3 2 0.6 0.875 150 1.251549 0.8181272 ## 0.3 2 0.6 0.875 200 1.274938 0.8100110 ## 0.3 2 0.6 0.875 250 1.303343 0.8005471 ## 0.3 2 0.6 1.000 50 1.256803 0.8262636 ## 0.3 2 0.6 1.000 100 1.238776 0.8266330 ## 0.3 2 0.6 1.000 150 1.265238 0.8165886 ## 0.3 2 0.6 1.000 200 1.304089 0.8038062 ## 0.3 2 0.6 1.000 250 1.313278 0.7993978 ## 0.3 2 0.8 0.500 50 1.250846 0.8239134 ## 0.3 2 0.8 0.500 100 1.283386 0.8104486 ## 0.3 2 0.8 0.500 150 1.321724 0.7960073 ## 0.3 2 0.8 0.500 200 1.318019 0.7962636 ## 0.3 2 0.8 0.500 250 1.359167 0.7854175 ## 0.3 2 0.8 0.625 50 1.274641 0.8145409 ## 0.3 2 0.8 0.625 100 1.296241 0.8030477 ## 0.3 2 0.8 0.625 150 1.331523 0.7916076 ## 0.3 2 0.8 0.625 200 1.348491 0.7869901 ## 0.3 2 0.8 0.625 250 1.370067 0.7812792 ## 0.3 2 0.8 0.750 50 1.281282 0.8138115 ## 0.3 2 0.8 0.750 100 1.322843 0.7977326 ## 0.3 2 0.8 0.750 150 1.361090 0.7835828 ## 0.3 2 0.8 0.750 200 1.390268 0.7741471 ## 0.3 2 0.8 0.750 250 1.417393 0.7651649 ## 0.3 2 0.8 0.875 50 1.244801 0.8257320 ## 0.3 2 0.8 0.875 100 1.247462 0.8192641 ## 0.3 2 0.8 0.875 150 1.281509 0.8065895 ## 0.3 2 0.8 0.875 200 1.314502 0.7961421 ## 0.3 2 0.8 0.875 250 1.328593 0.7903572 ## 0.3 2 0.8 1.000 50 1.246988 0.8235627 ## 0.3 2 0.8 1.000 100 1.254742 0.8156077 ## 0.3 2 0.8 1.000 150 1.272987 0.8089693 ## 0.3 2 0.8 1.000 200 1.299068 0.7995842 ## 0.3 2 0.8 1.000 250 1.314005 0.7939137 ## 0.3 3 0.6 0.500 50 1.453313 0.7482614 ## 0.3 3 0.6 0.500 100 1.443955 0.7490196 ## 0.3 3 0.6 0.500 150 1.460086 0.7444962 ## 0.3 3 0.6 0.500 200 1.459021 0.7453958 ## 0.3 3 0.6 0.500 250 1.461777 0.7433961 ## 0.3 3 0.6 0.625 50 1.429503 0.7548261 ## 0.3 3 0.6 0.625 100 1.461512 0.7435043 ## 0.3 3 0.6 0.625 150 1.481450 0.7365952 ## 0.3 3 0.6 0.625 200 1.500054 0.7299785 ## 0.3 3 0.6 0.625 250 1.501585 0.7290868 ## 0.3 3 0.6 0.750 50 1.421454 0.7630552 ## 0.3 3 0.6 0.750 100 1.433825 0.7563111 ## 0.3 3 0.6 0.750 150 1.457657 0.7462889 ## 0.3 3 0.6 0.750 200 1.469307 0.7416679 ## 0.3 3 0.6 0.750 250 1.472062 0.7403402 ## 0.3 3 0.6 0.875 50 1.356252 0.7851032 ## 0.3 3 0.6 0.875 100 1.364665 0.7796411 ## 0.3 3 0.6 0.875 150 1.375880 0.7742680 ## 0.3 3 0.6 0.875 200 1.389415 0.7695483 ## 0.3 3 0.6 0.875 250 1.391074 0.7688236 ## 0.3 3 0.6 1.000 50 1.384287 0.7794592 ## 0.3 3 0.6 1.000 100 1.381510 0.7782446 ## 0.3 3 0.6 1.000 150 1.391995 0.7738100 ## 0.3 3 0.6 1.000 200 1.401096 0.7709089 ## 0.3 3 0.6 1.000 250 1.408173 0.7684265 ## 0.3 3 0.8 0.500 50 1.466576 0.7443588 ## 0.3 3 0.8 0.500 100 1.470374 0.7459076 ## 0.3 3 0.8 0.500 150 1.496380 0.7370927 ## 0.3 3 0.8 0.500 200 1.499560 0.7367047 ## 0.3 3 0.8 0.500 250 1.505484 0.7347325 ## 0.3 3 0.8 0.625 50 1.375329 0.7744059 ## 0.3 3 0.8 0.625 100 1.428242 0.7555375 ## 0.3 3 0.8 0.625 150 1.457636 0.7477745 ## 0.3 3 0.8 0.625 200 1.467691 0.7445208 ## 0.3 3 0.8 0.625 250 1.472929 0.7429084 ## 0.3 3 0.8 0.750 50 1.312568 0.7989913 ## 0.3 3 0.8 0.750 100 1.354787 0.7842243 ## 0.3 3 0.8 0.750 150 1.374144 0.7780788 ## 0.3 3 0.8 0.750 200 1.387497 0.7738911 ## 0.3 3 0.8 0.750 250 1.394445 0.7713237 ## 0.3 3 0.8 0.875 50 1.356074 0.7840423 ## 0.3 3 0.8 0.875 100 1.383648 0.7740281 ## 0.3 3 0.8 0.875 150 1.397579 0.7695578 ## 0.3 3 0.8 0.875 200 1.409785 0.7656689 ## 0.3 3 0.8 0.875 250 1.416674 0.7632181 ## 0.3 3 0.8 1.000 50 1.412655 0.7687839 ## 0.3 3 0.8 1.000 100 1.424012 0.7620757 ## 0.3 3 0.8 1.000 150 1.439479 0.7566077 ## 0.3 3 0.8 1.000 200 1.451548 0.7523297 ## 0.3 3 0.8 1.000 250 1.458618 0.7497903 ## 0.3 4 0.6 0.500 50 1.569280 0.7094530 ## 0.3 4 0.6 0.500 100 1.580197 0.7042346 ## 0.3 4 0.6 0.500 150 1.592367 0.7008648 ## 0.3 4 0.6 0.500 200 1.597278 0.6995339 ## 0.3 4 0.6 0.500 250 1.597937 0.6994128 ## 0.3 4 0.6 0.625 50 1.530974 0.7297969 ## 0.3 4 0.6 0.625 100 1.535797 0.7254395 ## 0.3 4 0.6 0.625 150 1.538343 0.7241281 ## 0.3 4 0.6 0.625 200 1.537639 0.7242823 ## 0.3 4 0.6 0.625 250 1.538275 0.7241182 ## 0.3 4 0.6 0.750 50 1.547119 0.7180914 ## 0.3 4 0.6 0.750 100 1.556740 0.7129408 ## 0.3 4 0.6 0.750 150 1.561608 0.7106459 ## 0.3 4 0.6 0.750 200 1.564089 0.7096162 ## 0.3 4 0.6 0.750 250 1.564431 0.7094983 ## 0.3 4 0.6 0.875 50 1.471640 0.7500857 ## 0.3 4 0.6 0.875 100 1.479754 0.7467273 ## 0.3 4 0.6 0.875 150 1.485605 0.7443475 ## 0.3 4 0.6 0.875 200 1.487136 0.7437162 ## 0.3 4 0.6 0.875 250 1.487416 0.7435870 ## 0.3 4 0.6 1.000 50 1.510171 0.7346207 ## 0.3 4 0.6 1.000 100 1.515788 0.7313073 ## 0.3 4 0.6 1.000 150 1.520840 0.7293296 ## 0.3 4 0.6 1.000 200 1.523764 0.7280906 ## 0.3 4 0.6 1.000 250 1.523985 0.7279481 ## 0.3 4 0.8 0.500 50 1.585758 0.6955732 ## 0.3 4 0.8 0.500 100 1.585807 0.6973966 ## 0.3 4 0.8 0.500 150 1.585594 0.6976266 ## 0.3 4 0.8 0.500 200 1.585817 0.6977294 ## 0.3 4 0.8 0.500 250 1.586836 0.6972025 ## 0.3 4 0.8 0.625 50 1.464957 0.7408598 ## 0.3 4 0.8 0.625 100 1.472085 0.7365817 ## 0.3 4 0.8 0.625 150 1.474420 0.7359572 ## 0.3 4 0.8 0.625 200 1.475394 0.7355419 ## 0.3 4 0.8 0.625 250 1.475935 0.7352571 ## 0.3 4 0.8 0.750 50 1.479284 0.7447946 ## 0.3 4 0.8 0.750 100 1.480732 0.7436399 ## 0.3 4 0.8 0.750 150 1.483650 0.7426801 ## 0.3 4 0.8 0.750 200 1.484418 0.7425073 ## 0.3 4 0.8 0.750 250 1.484841 0.7423579 ## 0.3 4 0.8 0.875 50 1.458562 0.7546509 ## 0.3 4 0.8 0.875 100 1.472183 0.7483962 ## 0.3 4 0.8 0.875 150 1.477248 0.7464561 ## 0.3 4 0.8 0.875 200 1.479051 0.7457234 ## 0.3 4 0.8 0.875 250 1.479591 0.7455318 ## 0.3 4 0.8 1.000 50 1.457976 0.7535251 ## 0.3 4 0.8 1.000 100 1.467507 0.7493580 ## 0.3 4 0.8 1.000 150 1.470959 0.7482185 ## 0.3 4 0.8 1.000 200 1.471814 0.7479342 ## 0.3 4 0.8 1.000 250 1.472068 0.7477930 ## 0.3 5 0.6 0.500 50 1.700753 0.6592397 ## 0.3 5 0.6 0.500 100 1.692083 0.6615261 ## 0.3 5 0.6 0.500 150 1.693283 0.6606029 ## 0.3 5 0.6 0.500 200 1.693456 0.6605135 ## 0.3 5 0.6 0.500 250 1.693228 0.6605998 ## 0.3 5 0.6 0.625 50 1.568624 0.7057694 ## 0.3 5 0.6 0.625 100 1.575393 0.7021118 ## 0.3 5 0.6 0.625 150 1.575092 0.7020730 ## 0.3 5 0.6 0.625 200 1.574565 0.7023055 ## 0.3 5 0.6 0.625 250 1.574676 0.7022546 ## 0.3 5 0.6 0.750 50 1.610183 0.6850206 ## 0.3 5 0.6 0.750 100 1.608108 0.6858285 ## 0.3 5 0.6 0.750 150 1.608374 0.6857579 ## 0.3 5 0.6 0.750 200 1.608359 0.6857570 ## 0.3 5 0.6 0.750 250 1.608364 0.6857550 ## 0.3 5 0.6 0.875 50 1.594728 0.7085554 ## 0.3 5 0.6 0.875 100 1.596690 0.7075459 ## 0.3 5 0.6 0.875 150 1.597451 0.7071851 ## 0.3 5 0.6 0.875 200 1.597613 0.7071016 ## 0.3 5 0.6 0.875 250 1.597619 0.7071010 ## 0.3 5 0.6 1.000 50 1.587431 0.7090162 ## 0.3 5 0.6 1.000 100 1.585906 0.7091964 ## 0.3 5 0.6 1.000 150 1.585726 0.7091546 ## 0.3 5 0.6 1.000 200 1.585890 0.7090935 ## 0.3 5 0.6 1.000 250 1.585890 0.7090935 ## 0.3 5 0.8 0.500 50 1.453342 0.7480100 ## 0.3 5 0.8 0.500 100 1.465671 0.7438649 ## 0.3 5 0.8 0.500 150 1.465946 0.7439473 ## 0.3 5 0.8 0.500 200 1.466345 0.7437855 ## 0.3 5 0.8 0.500 250 1.466461 0.7437409 ## 0.3 5 0.8 0.625 50 1.483848 0.7411989 ## 0.3 5 0.8 0.625 100 1.491508 0.7387221 ## 0.3 5 0.8 0.625 150 1.492587 0.7381294 ## 0.3 5 0.8 0.625 200 1.492728 0.7380536 ## 0.3 5 0.8 0.625 250 1.492720 0.7380556 ## 0.3 5 0.8 0.750 50 1.499538 0.7367183 ## 0.3 5 0.8 0.750 100 1.500264 0.7355592 ## 0.3 5 0.8 0.750 150 1.501033 0.7352749 ## 0.3 5 0.8 0.750 200 1.501054 0.7352573 ## 0.3 5 0.8 0.750 250 1.501037 0.7352629 ## 0.3 5 0.8 0.875 50 1.552811 0.7203387 ## 0.3 5 0.8 0.875 100 1.555674 0.7192573 ## 0.3 5 0.8 0.875 150 1.556165 0.7190813 ## 0.3 5 0.8 0.875 200 1.556231 0.7190557 ## 0.3 5 0.8 0.875 250 1.556231 0.7190558 ## 0.3 5 0.8 1.000 50 1.520495 0.7326256 ## 0.3 5 0.8 1.000 100 1.523499 0.7309717 ## 0.3 5 0.8 1.000 150 1.523813 0.7308277 ## 0.3 5 0.8 1.000 200 1.523831 0.7308251 ## 0.3 5 0.8 1.000 250 1.523831 0.7308251 ## 0.4 1 0.6 0.500 50 1.305965 0.8084091 ## 0.4 1 0.6 0.500 100 1.178563 0.8377337 ## 0.4 1 0.6 0.500 150 1.199335 0.8295587 ## 0.4 1 0.6 0.500 200 1.196994 0.8320221 ## 0.4 1 0.6 0.500 250 1.202046 0.8291259 ## 0.4 1 0.6 0.625 50 1.265496 0.8248607 ## 0.4 1 0.6 0.625 100 1.149913 0.8479978 ## 0.4 1 0.6 0.625 150 1.147030 0.8459638 ## 0.4 1 0.6 0.625 200 1.171456 0.8383996 ## 0.4 1 0.6 0.625 250 1.182452 0.8350858 ## 0.4 1 0.6 0.750 50 1.287414 0.8269098 ## 0.4 1 0.6 0.750 100 1.136248 0.8538553 ## 0.4 1 0.6 0.750 150 1.127024 0.8538154 ## 0.4 1 0.6 0.750 200 1.123760 0.8536378 ## 0.4 1 0.6 0.750 250 1.137872 0.8493174 ## 0.4 1 0.6 0.875 50 1.315689 0.8209103 ## 0.4 1 0.6 0.875 100 1.171731 0.8443010 ## 0.4 1 0.6 0.875 150 1.142831 0.8488703 ## 0.4 1 0.6 0.875 200 1.146449 0.8481317 ## 0.4 1 0.6 0.875 250 1.145700 0.8487642 ## 0.4 1 0.6 1.000 50 1.292197 0.8314805 ## 0.4 1 0.6 1.000 100 1.132869 0.8579057 ## 0.4 1 0.6 1.000 150 1.099252 0.8619553 ## 0.4 1 0.6 1.000 200 1.097694 0.8605919 ## 0.4 1 0.6 1.000 250 1.098242 0.8594757 ## 0.4 1 0.8 0.500 50 1.251767 0.8274638 ## 0.4 1 0.8 0.500 100 1.188002 0.8355258 ## 0.4 1 0.8 0.500 150 1.165220 0.8431987 ## 0.4 1 0.8 0.500 200 1.202049 0.8323099 ## 0.4 1 0.8 0.500 250 1.195515 0.8340015 ## 0.4 1 0.8 0.625 50 1.255466 0.8291206 ## 0.4 1 0.8 0.625 100 1.201771 0.8318582 ## 0.4 1 0.8 0.625 150 1.198833 0.8323405 ## 0.4 1 0.8 0.625 200 1.210606 0.8296304 ## 0.4 1 0.8 0.625 250 1.215722 0.8282653 ## 0.4 1 0.8 0.750 50 1.264434 0.8282491 ## 0.4 1 0.8 0.750 100 1.149053 0.8492606 ## 0.4 1 0.8 0.750 150 1.158742 0.8448716 ## 0.4 1 0.8 0.750 200 1.157622 0.8437963 ## 0.4 1 0.8 0.750 250 1.170919 0.8413468 ## 0.4 1 0.8 0.875 50 1.284380 0.8279936 ## 0.4 1 0.8 0.875 100 1.156331 0.8476284 ## 0.4 1 0.8 0.875 150 1.148054 0.8476783 ## 0.4 1 0.8 0.875 200 1.147374 0.8474017 ## 0.4 1 0.8 0.875 250 1.155491 0.8437466 ## 0.4 1 0.8 1.000 50 1.257657 0.8427955 ## 0.4 1 0.8 1.000 100 1.114307 0.8632200 ## 0.4 1 0.8 1.000 150 1.089528 0.8646780 ## 0.4 1 0.8 1.000 200 1.092569 0.8621330 ## 0.4 1 0.8 1.000 250 1.090890 0.8618759 ## 0.4 2 0.6 0.500 50 1.382846 0.7732921 ## 0.4 2 0.6 0.500 100 1.380633 0.7748210 ## 0.4 2 0.6 0.500 150 1.427312 0.7656735 ## 0.4 2 0.6 0.500 200 1.472135 0.7498858 ## 0.4 2 0.6 0.500 250 1.483114 0.7450315 ## 0.4 2 0.6 0.625 50 1.297364 0.8041826 ## 0.4 2 0.6 0.625 100 1.325380 0.7940477 ## 0.4 2 0.6 0.625 150 1.342430 0.7908894 ## 0.4 2 0.6 0.625 200 1.359174 0.7852144 ## 0.4 2 0.6 0.625 250 1.393512 0.7742469 ## 0.4 2 0.6 0.750 50 1.296944 0.7991756 ## 0.4 2 0.6 0.750 100 1.337103 0.7838766 ## 0.4 2 0.6 0.750 150 1.366138 0.7730353 ## 0.4 2 0.6 0.750 200 1.407434 0.7601698 ## 0.4 2 0.6 0.750 250 1.415998 0.7579753 ## 0.4 2 0.6 0.875 50 1.281847 0.8123070 ## 0.4 2 0.6 0.875 100 1.274670 0.8103222 ## 0.4 2 0.6 0.875 150 1.314215 0.7964751 ## 0.4 2 0.6 0.875 200 1.332842 0.7898352 ## 0.4 2 0.6 0.875 250 1.348344 0.7847896 ## 0.4 2 0.6 1.000 50 1.249770 0.8241132 ## 0.4 2 0.6 1.000 100 1.273365 0.8129780 ## 0.4 2 0.6 1.000 150 1.291020 0.8062827 ## 0.4 2 0.6 1.000 200 1.319222 0.7968750 ## 0.4 2 0.6 1.000 250 1.336921 0.7909964 ## 0.4 2 0.8 0.500 50 1.375613 0.7758602 ## 0.4 2 0.8 0.500 100 1.435903 0.7624594 ## 0.4 2 0.8 0.500 150 1.466381 0.7509388 ## 0.4 2 0.8 0.500 200 1.524180 0.7337150 ## 0.4 2 0.8 0.500 250 1.560392 0.7221041 ## 0.4 2 0.8 0.625 50 1.306547 0.8010212 ## 0.4 2 0.8 0.625 100 1.311037 0.7949561 ## 0.4 2 0.8 0.625 150 1.347385 0.7829079 ## 0.4 2 0.8 0.625 200 1.365435 0.7772369 ## 0.4 2 0.8 0.625 250 1.365802 0.7752091 ## 0.4 2 0.8 0.750 50 1.330693 0.7941363 ## 0.4 2 0.8 0.750 100 1.359145 0.7830225 ## 0.4 2 0.8 0.750 150 1.410332 0.7680722 ## 0.4 2 0.8 0.750 200 1.451177 0.7547243 ## 0.4 2 0.8 0.750 250 1.449425 0.7560458 ## 0.4 2 0.8 0.875 50 1.300123 0.7999244 ## 0.4 2 0.8 0.875 100 1.339805 0.7834316 ## 0.4 2 0.8 0.875 150 1.368530 0.7747647 ## 0.4 2 0.8 0.875 200 1.407303 0.7623229 ## 0.4 2 0.8 0.875 250 1.435079 0.7528887 ## 0.4 2 0.8 1.000 50 1.260959 0.8130258 ## 0.4 2 0.8 1.000 100 1.286146 0.8047402 ## 0.4 2 0.8 1.000 150 1.330098 0.7909328 ## 0.4 2 0.8 1.000 200 1.364343 0.7802868 ## 0.4 2 0.8 1.000 250 1.390332 0.7713007 ## 0.4 3 0.6 0.500 50 1.459147 0.7540519 ## 0.4 3 0.6 0.500 100 1.489147 0.7448597 ## 0.4 3 0.6 0.500 150 1.511626 0.7343949 ## 0.4 3 0.6 0.500 200 1.517107 0.7318242 ## 0.4 3 0.6 0.500 250 1.518259 0.7316891 ## 0.4 3 0.6 0.625 50 1.426459 0.7524073 ## 0.4 3 0.6 0.625 100 1.477395 0.7344770 ## 0.4 3 0.6 0.625 150 1.497116 0.7286593 ## 0.4 3 0.6 0.625 200 1.501331 0.7274881 ## 0.4 3 0.6 0.625 250 1.503481 0.7272506 ## 0.4 3 0.6 0.750 50 1.407118 0.7688565 ## 0.4 3 0.6 0.750 100 1.455873 0.7531430 ## 0.4 3 0.6 0.750 150 1.471028 0.7468242 ## 0.4 3 0.6 0.750 200 1.478463 0.7438433 ## 0.4 3 0.6 0.750 250 1.480622 0.7430397 ## 0.4 3 0.6 0.875 50 1.389217 0.7670554 ## 0.4 3 0.6 0.875 100 1.437512 0.7503580 ## 0.4 3 0.6 0.875 150 1.457816 0.7429962 ## 0.4 3 0.6 0.875 200 1.464680 0.7407345 ## 0.4 3 0.6 0.875 250 1.469581 0.7391226 ## 0.4 3 0.6 1.000 50 1.404112 0.7752231 ## 0.4 3 0.6 1.000 100 1.413625 0.7703729 ## 0.4 3 0.6 1.000 150 1.419060 0.7666844 ## 0.4 3 0.6 1.000 200 1.423849 0.7645119 ## 0.4 3 0.6 1.000 250 1.427611 0.7631356 ## 0.4 3 0.8 0.500 50 1.560611 0.7128458 ## 0.4 3 0.8 0.500 100 1.613221 0.6920922 ## 0.4 3 0.8 0.500 150 1.624152 0.6892352 ## 0.4 3 0.8 0.500 200 1.633569 0.6859070 ## 0.4 3 0.8 0.500 250 1.634972 0.6857252 ## 0.4 3 0.8 0.625 50 1.430450 0.7632358 ## 0.4 3 0.8 0.625 100 1.473931 0.7513469 ## 0.4 3 0.8 0.625 150 1.496736 0.7441335 ## 0.4 3 0.8 0.625 200 1.504713 0.7414591 ## 0.4 3 0.8 0.625 250 1.507800 0.7395685 ## 0.4 3 0.8 0.750 50 1.470519 0.7450424 ## 0.4 3 0.8 0.750 100 1.543300 0.7205907 ## 0.4 3 0.8 0.750 150 1.567029 0.7129915 ## 0.4 3 0.8 0.750 200 1.575830 0.7102099 ## 0.4 3 0.8 0.750 250 1.579058 0.7089863 ## 0.4 3 0.8 0.875 50 1.380954 0.7786713 ## 0.4 3 0.8 0.875 100 1.378215 0.7799626 ## 0.4 3 0.8 0.875 150 1.395653 0.7733847 ## 0.4 3 0.8 0.875 200 1.404123 0.7698944 ## 0.4 3 0.8 0.875 250 1.404418 0.7695174 ## 0.4 3 0.8 1.000 50 1.328849 0.7937076 ## 0.4 3 0.8 1.000 100 1.366506 0.7819128 ## 0.4 3 0.8 1.000 150 1.382197 0.7768419 ## 0.4 3 0.8 1.000 200 1.387384 0.7748091 ## 0.4 3 0.8 1.000 250 1.389670 0.7740772 ## 0.4 4 0.6 0.500 50 1.575142 0.7056286 ## 0.4 4 0.6 0.500 100 1.579966 0.7049851 ## 0.4 4 0.6 0.500 150 1.585896 0.7032349 ## 0.4 4 0.6 0.500 200 1.587252 0.7024304 ## 0.4 4 0.6 0.500 250 1.588391 0.7020538 ## 0.4 4 0.6 0.625 50 1.572469 0.6969080 ## 0.4 4 0.6 0.625 100 1.580685 0.6950103 ## 0.4 4 0.6 0.625 150 1.584498 0.6933922 ## 0.4 4 0.6 0.625 200 1.585625 0.6929754 ## 0.4 4 0.6 0.625 250 1.585385 0.6930482 ## 0.4 4 0.6 0.750 50 1.577369 0.7047530 ## 0.4 4 0.6 0.750 100 1.589654 0.7007150 ## 0.4 4 0.6 0.750 150 1.590692 0.7000641 ## 0.4 4 0.6 0.750 200 1.590921 0.6998228 ## 0.4 4 0.6 0.750 250 1.591109 0.6997410 ## 0.4 4 0.6 0.875 50 1.566603 0.7227653 ## 0.4 4 0.6 0.875 100 1.585162 0.7157663 ## 0.4 4 0.6 0.875 150 1.587427 0.7151324 ## 0.4 4 0.6 0.875 200 1.588393 0.7147788 ## 0.4 4 0.6 0.875 250 1.588491 0.7147416 ## 0.4 4 0.6 1.000 50 1.575535 0.7063202 ## 0.4 4 0.6 1.000 100 1.588724 0.6991125 ## 0.4 4 0.6 1.000 150 1.591885 0.6977843 ## 0.4 4 0.6 1.000 200 1.592291 0.6975743 ## 0.4 4 0.6 1.000 250 1.592368 0.6975165 ## 0.4 4 0.8 0.500 50 1.704180 0.6554199 ## 0.4 4 0.8 0.500 100 1.742700 0.6421643 ## 0.4 4 0.8 0.500 150 1.749431 0.6394268 ## 0.4 4 0.8 0.500 200 1.751546 0.6384905 ## 0.4 4 0.8 0.500 250 1.751423 0.6385508 ## 0.4 4 0.8 0.625 50 1.596006 0.6900303 ## 0.4 4 0.8 0.625 100 1.611045 0.6831381 ## 0.4 4 0.8 0.625 150 1.612709 0.6827260 ## 0.4 4 0.8 0.625 200 1.613116 0.6826293 ## 0.4 4 0.8 0.625 250 1.613228 0.6825674 ## 0.4 4 0.8 0.750 50 1.562654 0.7217163 ## 0.4 4 0.8 0.750 100 1.576654 0.7154934 ## 0.4 4 0.8 0.750 150 1.578169 0.7149825 ## 0.4 4 0.8 0.750 200 1.578445 0.7148381 ## 0.4 4 0.8 0.750 250 1.578533 0.7148008 ## 0.4 4 0.8 0.875 50 1.556560 0.7109702 ## 0.4 4 0.8 0.875 100 1.563499 0.7078701 ## 0.4 4 0.8 0.875 150 1.566407 0.7067507 ## 0.4 4 0.8 0.875 200 1.566620 0.7066863 ## 0.4 4 0.8 0.875 250 1.566623 0.7066944 ## 0.4 4 0.8 1.000 50 1.570160 0.7094464 ## 0.4 4 0.8 1.000 100 1.581473 0.7054040 ## 0.4 4 0.8 1.000 150 1.584587 0.7043297 ## 0.4 4 0.8 1.000 200 1.585064 0.7041741 ## 0.4 4 0.8 1.000 250 1.585156 0.7041437 ## 0.4 5 0.6 0.500 50 1.898602 0.5890063 ## 0.4 5 0.6 0.500 100 1.909160 0.5864831 ## 0.4 5 0.6 0.500 150 1.909618 0.5862836 ## 0.4 5 0.6 0.500 200 1.909594 0.5862960 ## 0.4 5 0.6 0.500 250 1.909600 0.5862945 ## 0.4 5 0.6 0.625 50 1.679796 0.6716745 ## 0.4 5 0.6 0.625 100 1.689155 0.6683289 ## 0.4 5 0.6 0.625 150 1.689804 0.6681862 ## 0.4 5 0.6 0.625 200 1.689905 0.6681524 ## 0.4 5 0.6 0.625 250 1.689917 0.6681456 ## 0.4 5 0.6 0.750 50 1.668344 0.6622040 ## 0.4 5 0.6 0.750 100 1.673505 0.6599669 ## 0.4 5 0.6 0.750 150 1.673395 0.6599716 ## 0.4 5 0.6 0.750 200 1.673394 0.6599678 ## 0.4 5 0.6 0.750 250 1.673396 0.6599668 ## 0.4 5 0.6 0.875 50 1.719785 0.6472494 ## 0.4 5 0.6 0.875 100 1.724025 0.6453718 ## 0.4 5 0.6 0.875 150 1.724172 0.6452740 ## 0.4 5 0.6 0.875 200 1.724179 0.6452709 ## 0.4 5 0.6 0.875 250 1.724182 0.6452696 ## 0.4 5 0.6 1.000 50 1.626033 0.6812134 ## 0.4 5 0.6 1.000 100 1.625980 0.6813039 ## 0.4 5 0.6 1.000 150 1.626088 0.6812283 ## 0.4 5 0.6 1.000 200 1.626088 0.6812283 ## 0.4 5 0.6 1.000 250 1.626088 0.6812283 ## 0.4 5 0.8 0.500 50 1.643016 0.6892097 ## 0.4 5 0.8 0.500 100 1.642470 0.6911773 ## 0.4 5 0.8 0.500 150 1.643028 0.6910391 ## 0.4 5 0.8 0.500 200 1.643019 0.6910702 ## 0.4 5 0.8 0.500 250 1.643060 0.6910535 ## 0.4 5 0.8 0.625 50 1.582925 0.6894788 ## 0.4 5 0.8 0.625 100 1.589245 0.6876139 ## 0.4 5 0.8 0.625 150 1.589230 0.6876971 ## 0.4 5 0.8 0.625 200 1.589220 0.6877053 ## 0.4 5 0.8 0.625 250 1.589219 0.6877055 ## 0.4 5 0.8 0.750 50 1.651746 0.6920814 ## 0.4 5 0.8 0.750 100 1.654738 0.6912944 ## 0.4 5 0.8 0.750 150 1.654761 0.6913242 ## 0.4 5 0.8 0.750 200 1.654754 0.6913269 ## 0.4 5 0.8 0.750 250 1.654762 0.6913237 ## 0.4 5 0.8 0.875 50 1.567924 0.7103048 ## 0.4 5 0.8 0.875 100 1.570973 0.7095698 ## 0.4 5 0.8 0.875 150 1.570945 0.7095782 ## 0.4 5 0.8 0.875 200 1.570942 0.7095796 ## 0.4 5 0.8 0.875 250 1.570943 0.7095789 ## 0.4 5 0.8 1.000 50 1.616367 0.6908239 ## 0.4 5 0.8 1.000 100 1.620053 0.6895021 ## 0.4 5 0.8 1.000 150 1.620264 0.6894186 ## 0.4 5 0.8 1.000 200 1.620264 0.6894186 ## 0.4 5 0.8 1.000 250 1.620264 0.6894187 ## MAE ## 1.1099093 ## 0.9245570 ## 0.9057356 ## 0.9006313 ## 0.9015553 ## 1.1286382 ## 0.9572945 ## 0.9132657 ## 0.9165966 ## 0.9051690 ## 1.1384398 ## 0.9673060 ## 0.9222307 ## 0.9184807 ## 0.9176063 ## 1.1252644 ## 0.9495338 ## 0.8948260 ## 0.8857622 ## 0.8895029 ## 1.1500644 ## 0.9595836 ## 0.9044162 ## 0.8831874 ## 0.8768802 ## 1.1376807 ## 0.9555981 ## 0.9498461 ## 0.9381643 ## 0.9299478 ## 1.1119232 ## 0.9512827 ## 0.9129568 ## 0.8984065 ## 0.9179026 ## 1.1151192 ## 0.9515504 ## 0.9118137 ## 0.9031139 ## 0.9021885 ## 1.1526422 ## 0.9645017 ## 0.9148279 ## 0.9116130 ## 0.9037894 ## 1.1540039 ## 0.9646393 ## 0.9148862 ## 0.8898829 ## 0.8862980 ## 1.0383184 ## 1.0434678 ## 1.0556689 ## 1.0522642 ## 1.0938811 ## 1.0221279 ## 0.9938015 ## 1.0194798 ## 1.0427383 ## 1.0538485 ## 0.9766369 ## 0.9584202 ## 0.9760734 ## 0.9976644 ## 1.0183018 ## 1.0169940 ## 0.9964500 ## 0.9902019 ## 1.0033306 ## 1.0271238 ## 1.0213600 ## 1.0052175 ## 1.0120136 ## 1.0415740 ## 1.0429269 ## 0.9958239 ## 1.0269493 ## 1.0597648 ## 1.0522737 ## 1.0815983 ## 1.0170641 ## 1.0154035 ## 1.0495998 ## 1.0640598 ## 1.0843005 ## 1.0037579 ## 1.0221312 ## 1.0589777 ## 1.0849650 ## 1.1073361 ## 0.9974183 ## 0.9899920 ## 1.0068758 ## 1.0310381 ## 1.0401662 ## 1.0094042 ## 1.0109649 ## 1.0196011 ## 1.0406794 ## 1.0528244 ## 1.1600687 ## 1.1550581 ## 1.1642241 ## 1.1647047 ## 1.1654880 ## 1.1180829 ## 1.1492295 ## 1.1686545 ## 1.1775221 ## 1.1774666 ## 1.1318122 ## 1.1317376 ## 1.1467040 ## 1.1571830 ## 1.1571273 ## 1.0870837 ## 1.0863042 ## 1.0880827 ## 1.0985641 ## 1.0982434 ## 1.0604554 ## 1.0502054 ## 1.0649334 ## 1.0720000 ## 1.0786414 ## 1.1531217 ## 1.1781169 ## 1.1895009 ## 1.1949642 ## 1.1928002 ## 1.0831388 ## 1.1216113 ## 1.1423254 ## 1.1455745 ## 1.1495573 ## 1.0508424 ## 1.0928082 ## 1.1134835 ## 1.1209380 ## 1.1263917 ## 1.0839660 ## 1.1043721 ## 1.1181377 ## 1.1286702 ## 1.1348692 ## 1.1231898 ## 1.1332555 ## 1.1440117 ## 1.1532247 ## 1.1589587 ## 1.2329020 ## 1.2546853 ## 1.2665031 ## 1.2722046 ## 1.2732580 ## 1.1870184 ## 1.1862673 ## 1.1890758 ## 1.1900052 ## 1.1905861 ## 1.2216724 ## 1.2273659 ## 1.2314963 ## 1.2330391 ## 1.2332628 ## 1.1645393 ## 1.1662377 ## 1.1719584 ## 1.1729362 ## 1.1728675 ## 1.1981096 ## 1.2005868 ## 1.2054635 ## 1.2069324 ## 1.2072017 ## 1.2296038 ## 1.2428200 ## 1.2455443 ## 1.2476924 ## 1.2481663 ## 1.1865587 ## 1.1959459 ## 1.1976635 ## 1.1989303 ## 1.1994840 ## 1.2033866 ## 1.2095661 ## 1.2095744 ## 1.2099511 ## 1.2101850 ## 1.1314935 ## 1.1479773 ## 1.1505861 ## 1.1515814 ## 1.1520882 ## 1.1588330 ## 1.1635980 ## 1.1673839 ## 1.1681092 ## 1.1684456 ## 1.3196434 ## 1.3213752 ## 1.3209509 ## 1.3213930 ## 1.3212481 ## 1.2464763 ## 1.2549354 ## 1.2541472 ## 1.2538593 ## 1.2539534 ## 1.2745961 ## 1.2701806 ## 1.2706173 ## 1.2705911 ## 1.2705875 ## 1.2265959 ## 1.2292165 ## 1.2304741 ## 1.2305428 ## 1.2305490 ## 1.2379645 ## 1.2409163 ## 1.2409590 ## 1.2411754 ## 1.2411754 ## 1.1800773 ## 1.1922281 ## 1.1921384 ## 1.1923749 ## 1.1924936 ## 1.1526669 ## 1.1590582 ## 1.1593375 ## 1.1593076 ## 1.1592939 ## 1.2044167 ## 1.2043236 ## 1.2052006 ## 1.2052030 ## 1.2051872 ## 1.2367109 ## 1.2391399 ## 1.2394807 ## 1.2395397 ## 1.2395384 ## 1.1737210 ## 1.1752562 ## 1.1751172 ## 1.1751438 ## 1.1751438 ## 1.0425901 ## 0.9485953 ## 0.9543805 ## 0.9636615 ## 0.9611337 ## 1.0181251 ## 0.9083673 ## 0.9102624 ## 0.9310842 ## 0.9377569 ## 1.0440464 ## 0.9150213 ## 0.8970604 ## 0.8981324 ## 0.9034928 ## 1.0859945 ## 0.9524923 ## 0.9247408 ## 0.9302482 ## 0.9250008 ## 1.0405790 ## 0.9154058 ## 0.8873516 ## 0.8861441 ## 0.8868640 ## 1.0069705 ## 0.9614287 ## 0.9448433 ## 0.9625522 ## 0.9482393 ## 1.0157218 ## 0.9679074 ## 0.9624926 ## 0.9666938 ## 0.9673617 ## 1.0284019 ## 0.9200545 ## 0.9226820 ## 0.9315137 ## 0.9354305 ## 1.0394873 ## 0.9224046 ## 0.9150609 ## 0.9204421 ## 0.9292643 ## 1.0275614 ## 0.9086050 ## 0.8827241 ## 0.8835899 ## 0.8801720 ## 1.1206259 ## 1.1239517 ## 1.1574685 ## 1.1906529 ## 1.1961089 ## 1.0290172 ## 1.0590727 ## 1.0598536 ## 1.0669336 ## 1.0989374 ## 1.0292234 ## 1.0631206 ## 1.0814107 ## 1.1179271 ## 1.1210283 ## 1.0240166 ## 1.0116956 ## 1.0339817 ## 1.0479879 ## 1.0574304 ## 1.0056519 ## 1.0265502 ## 1.0477290 ## 1.0716691 ## 1.0805170 ## 1.0870670 ## 1.1339179 ## 1.1576223 ## 1.1821358 ## 1.2156837 ## 1.0610358 ## 1.0469250 ## 1.0770923 ## 1.0846618 ## 1.0878712 ## 1.0435288 ## 1.0654351 ## 1.1044403 ## 1.1306804 ## 1.1273219 ## 1.0514433 ## 1.0789669 ## 1.0893827 ## 1.1166410 ## 1.1366952 ## 1.0156490 ## 1.0304714 ## 1.0695846 ## 1.0970003 ## 1.1166430 ## 1.1808794 ## 1.2019858 ## 1.2127665 ## 1.2206288 ## 1.2185911 ## 1.1391184 ## 1.1780851 ## 1.1898540 ## 1.1888889 ## 1.1894795 ## 1.1182876 ## 1.1590121 ## 1.1741198 ## 1.1823603 ## 1.1834221 ## 1.0701272 ## 1.1100681 ## 1.1229393 ## 1.1260757 ## 1.1299649 ## 1.1070961 ## 1.1132061 ## 1.1187244 ## 1.1222459 ## 1.1260774 ## 1.2496636 ## 1.2905973 ## 1.3011820 ## 1.3091368 ## 1.3084402 ## 1.1283284 ## 1.1556825 ## 1.1763817 ## 1.1836317 ## 1.1845571 ## 1.1804629 ## 1.2231376 ## 1.2379055 ## 1.2462197 ## 1.2481402 ## 1.0979053 ## 1.1041015 ## 1.1175119 ## 1.1246292 ## 1.1247143 ## 1.0663160 ## 1.0997386 ## 1.1117569 ## 1.1165799 ## 1.1180343 ## 1.2521192 ## 1.2639941 ## 1.2654975 ## 1.2653663 ## 1.2669387 ## 1.2600469 ## 1.2692797 ## 1.2719550 ## 1.2720294 ## 1.2717640 ## 1.2671093 ## 1.2752578 ## 1.2764972 ## 1.2770011 ## 1.2772723 ## 1.2549843 ## 1.2712944 ## 1.2740373 ## 1.2749654 ## 1.2750516 ## 1.2320308 ## 1.2421188 ## 1.2429720 ## 1.2430056 ## 1.2429580 ## 1.3749276 ## 1.4210401 ## 1.4246909 ## 1.4257667 ## 1.4254044 ## 1.2398914 ## 1.2516431 ## 1.2535345 ## 1.2539249 ## 1.2538262 ## 1.2088494 ## 1.2224488 ## 1.2239978 ## 1.2240628 ## 1.2241538 ## 1.2226143 ## 1.2289841 ## 1.2318370 ## 1.2319651 ## 1.2319454 ## 1.2440424 ## 1.2516369 ## 1.2536159 ## 1.2536638 ## 1.2537733 ## 1.5166460 ## 1.5268671 ## 1.5267661 ## 1.5269790 ## 1.5269727 ## 1.3461198 ## 1.3549655 ## 1.3552709 ## 1.3553762 ## 1.3553749 ## 1.2971210 ## 1.3002670 ## 1.3000096 ## 1.2999752 ## 1.2999876 ## 1.3658948 ## 1.3689985 ## 1.3691218 ## 1.3691295 ## 1.3691316 ## 1.3019971 ## 1.2992931 ## 1.2993410 ## 1.2993410 ## 1.2993410 ## 1.2721764 ## 1.2710665 ## 1.2716482 ## 1.2717113 ## 1.2717652 ## 1.2646809 ## 1.2674157 ## 1.2673933 ## 1.2673961 ## 1.2673972 ## 1.2926777 ## 1.2970154 ## 1.2970959 ## 1.2970875 ## 1.2970958 ## 1.2374779 ## 1.2387755 ## 1.2387794 ## 1.2387733 ## 1.2387769 ## 1.2654542 ## 1.2674219 ## 1.2675772 ## 1.2675772 ## 1.2675771 ## ## Tuning parameter &#39;gamma&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;min_child_weight&#39; was held constant at a value of 1 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were nrounds = 150, max_depth = 1, eta ## = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample ## = 1. The optimal tuning parameters were at \\(M = 250\\) and interation.depth = 1. train() tuned eta (\\(\\eta\\)), max_depth, gamma, colsample_bytree, subsample, and nrounds, holding gamma = 0, and min_child_weight = 1. The optimal hyperparameter values were eta = 0.4, max_depth = 1, gamma = 0, colsample_bytree = 0.8, subsample = 1, and nrounds = 150. With so many hyperparameters, the tuning plot is nearly unreadable. #plot(cs_mdl_xgb) Here is the holdout set performance. cs_preds_xgb &lt;- bind_cols( Predicted = predict(cs_mdl_xgb, newdata = cs_test), Actual = cs_test$Sales ) # Model over-predicts at low end of Sales and under-predicts at high end cs_preds_xgb %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats XGBoost, Predicted vs Actual&quot;) plot(varImp(cs_mdl_xgb), main=&quot;Variable Importance with XGBoost&quot;) The RMSE is 1.438 - the best of the bunch. cs_rmse_xgb &lt;- RMSE(pred = cs_preds_xgb$Predicted, obs = cs_preds_xgb$Actual) cs_scoreboard &lt;- rbind(cs_scoreboard, data.frame(Model = &quot;XGBoost&quot;, RMSE = cs_rmse_xgb) ) %&gt;% arrange(RMSE) scoreboard(cs_scoreboard) .cl-a4356fc0{}.cl-a42f5806{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a431bbbe{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a431bbc8{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a431c99c{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a431c9a6{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a431c9b0{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a431c9ba{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a431c9bb{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a431c9c4{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a431c9c5{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a431c9ce{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a431c9cf{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a431c9d8{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a431c9d9{width:1.55in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a431c9e2{width:0.922in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ModelRMSEXGBoost1.395152GBM1.438058Random Forest1.718358Bagging1.918473Single Tree (caret)2.298331Single Tree2.363202 4.6 Summary I created four classification trees using the ISLR::OJ data set and four regression trees using the ISLR::Carseats data set. Let’s compare their performance. 4.6.1 Classification Trees The caret::resamples() function summarizes the resampling performance on the final model produced in train(). It creates summary statistics (mean, min, max, etc.) for each performance metric (ROC, RMSE, etc.) for a list of models. oj_resamples &lt;- resamples(list(&quot;Single Tree (caret)&quot; = oj_mdl_cart2, &quot;Bagged&quot; = oj_mdl_bag, &quot;Random Forest&quot; = oj_mdl_rf, &quot;GBM&quot; = oj_mdl_gbm, &quot;XGBoost&quot; = oj_mdl_xgb)) summary(oj_resamples) ## ## Call: ## summary.resamples(object = oj_resamples) ## ## Models: Single Tree (caret), Bagged, Random Forest, GBM, XGBoost ## Number of resamples: 10 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## Single Tree (caret) 0.7444639 0.8436740 0.8607833 0.8543820 0.8840991 0.9166667 ## Bagged 0.7922494 0.8399071 0.8625566 0.8553731 0.8784410 0.8933824 ## Random Forest 0.8004079 0.8664461 0.8792635 0.8690552 0.8936234 0.9105478 ## GBM 0.8167249 0.8721867 0.9069656 0.8893367 0.9119447 0.9386792 ## XGBoost 0.8155594 0.8823227 0.9022693 0.8901207 0.9177244 0.9405594 ## NA&#39;s ## Single Tree (caret) 0 ## Bagged 0 ## Random Forest 0 ## GBM 0 ## XGBoost 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## Single Tree (caret) 0.7692308 0.8461538 0.8476052 0.8567126 0.8948657 0.9230769 ## Bagged 0.7692308 0.8269231 0.8301887 0.8432511 0.8483309 0.9230769 ## Random Forest 0.7884615 0.8269231 0.8301887 0.8394049 0.8483309 0.9230769 ## GBM 0.7884615 0.8509615 0.8679245 0.8604499 0.8846154 0.8846154 ## XGBoost 0.8076923 0.8679245 0.8857039 0.8814949 0.9038462 0.9230769 ## NA&#39;s ## Single Tree (caret) 0 ## Bagged 0 ## Random Forest 0 ## GBM 0 ## XGBoost 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## Single Tree (caret) 0.6060606 0.6299020 0.7647059 0.7334225 0.8181818 0.8484848 ## Bagged 0.6176471 0.6691176 0.7121212 0.7186275 0.7629234 0.8181818 ## Random Forest 0.5757576 0.6519608 0.7352941 0.7213904 0.8106061 0.8484848 ## GBM 0.6060606 0.6838235 0.7909982 0.7636364 0.8422460 0.9090909 ## XGBoost 0.6060606 0.6397059 0.7762923 0.7457219 0.8221925 0.8787879 ## NA&#39;s ## Single Tree (caret) 0 ## Bagged 0 ## Random Forest 0 ## GBM 0 ## XGBoost 0 The mean ROC value is the performance measure I used to evaluate the models in train() (you can compare the Mean column to each model’s object (e.g., print(oj_mdl_cart2))). The best performing model on resamples based on the mean ROC score was XGBoost. It also had the highest mean sensitivity. GBM had the highest specificity. Here is a box plot of the distributions. bwplot(oj_resamples) One way to evaluate the box plots is with a post-hoc test of differences. The single tree was about the same as the random forest and bagged models. GBM and XGBoost were also statistically equivalent. (oj_diff &lt;- diff(oj_resamples)) ## ## Call: ## diff.resamples(x = oj_resamples) ## ## Models: Single Tree (caret), Bagged, Random Forest, GBM, XGBoost ## Metrics: ROC, Sens, Spec ## Number of differences: 10 ## p-value adjustment: bonferroni # summary(oj_diff) dotplot(oj_diff) 4.6.2 Regression Trees Here is the summary of resampling metrics for the Carseats models. cs_resamples &lt;- resamples(list(&quot;Single Tree (caret)&quot; = cs_mdl_cart2, &quot;Bagged&quot; = cs_mdl_bag, &quot;Random Forest&quot; = cs_mdl_rf, &quot;GBM&quot; = cs_mdl_gbm, &quot;XGBoost&quot; = cs_mdl_xgb)) summary(cs_resamples) ## ## Call: ## summary.resamples(object = cs_resamples) ## ## Models: Single Tree (caret), Bagged, Random Forest, GBM, XGBoost ## Number of resamples: 10 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## Single Tree (caret) 1.3654588 1.5880059 1.7313313 1.6954527 1.7997916 2.064407 ## Bagged 0.9303614 1.1254345 1.3464588 1.3434268 1.5712597 1.737744 ## Random Forest 0.8519036 1.0374544 1.2649349 1.2059561 1.3923549 1.473589 ## GBM 0.7158107 0.8573375 0.9156746 0.9384418 1.0435786 1.113679 ## XGBoost 0.6569093 0.7931956 0.8997581 0.8827241 0.9598644 1.070442 ## NA&#39;s ## Single Tree (caret) 0 ## Bagged 0 ## Random Forest 0 ## GBM 0 ## XGBoost 0 ## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## Single Tree (caret) 1.6948538 1.9026827 2.038878 2.055676 2.178660 2.496287 ## Bagged 1.2099294 1.4055017 1.693264 1.681889 1.979521 2.069958 ## Random Forest 1.0744194 1.2796154 1.587416 1.515426 1.724651 1.905302 ## GBM 0.9305520 1.0823005 1.159712 1.161540 1.271790 1.390241 ## XGBoost 0.8597657 0.9752211 1.108881 1.089528 1.199456 1.272658 ## NA&#39;s ## Single Tree (caret) 0 ## Bagged 0 ## Random Forest 0 ## GBM 0 ## XGBoost 0 ## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## Single Tree (caret) 0.3858765 0.4977272 0.5185928 0.5027431 0.5272691 0.5852449 ## Bagged 0.5280595 0.6322136 0.6805770 0.6752390 0.7281293 0.7848460 ## Random Forest 0.6820868 0.6970255 0.7583809 0.7541277 0.8027971 0.8308440 ## GBM 0.8176434 0.8319850 0.8536650 0.8488463 0.8628293 0.8721381 ## XGBoost 0.8415200 0.8620120 0.8689647 0.8646780 0.8716127 0.8808527 ## NA&#39;s ## Single Tree (caret) 0 ## Bagged 0 ## Random Forest 0 ## GBM 0 ## XGBoost 0 The best performing model on resamples based on the mean RMSE score was XGBoost. It also had the lowest mean absolute error (MAE) and highest R-squared. Here is a box plot of the distributions. bwplot(cs_resamples) The post-hoc test indicates all models differed from each other except for GBM and XGBoost. (cs_diff &lt;- diff(cs_resamples)) ## ## Call: ## diff.resamples(x = cs_resamples) ## ## Models: Single Tree (caret), Bagged, Random Forest, GBM, XGBoost ## Metrics: MAE, RMSE, Rsquared ## Number of differences: 10 ## p-value adjustment: bonferroni # summary(cs_diff) dotplot(cs_diff) References "],["non-linear-models.html", "Chapter 5 Non-linear Models 5.1 Splines 5.2 MARS 5.3 GAM", " Chapter 5 Non-linear Models Linear methods can model nonlinear relationships by including polynomial terms, interaction effects, and variable transformations. However, it is often difficult to identify how to formulate the model. Nonlinear models may be preferable because you do not need to know the the exact form of the nonlinearity prior to model training. 5.1 Splines A regression spline fits a piecewise polynomial to the range of X partitioned by knots (K knots produce K + 1 piecewise polynomials) James et al (James et al. 2013). The polynomials can be of any degree d, but are usually in the range [0, 3], most commonly 3 (a cubic spline). To avoid discontinuities in the fit, a degree-d spline is constrained to have continuity in derivatives up to degree d−1 at each knot. A cubic spline fit to a data set with K knots, performs least squares regression with an intercept and 3 + K predictors, of the form \\[y_i = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\beta_4h(X, \\xi_1) + \\beta_5h(X, \\xi_2) + \\dots + \\beta_{K+3}h(X, \\xi_K)\\] where \\(\\xi_1, \\dots, \\xi_K\\) are the knots are truncated power basis functions \\(h(X, \\xi) = (X - \\xi)^3\\) if \\(X &gt; \\xi\\), else 0. Splines can have high variance at the outer range of the predictors. A natural spline is a regression spline additionally constrained to be linear at the boundaries. How many knots should there be, and Where should the knots be placed? It is common to place knots in a uniform fashion, with equal numbers of points between each knot. The number of knots is typically chosen by trial and error using cross-validation to minimize the RSS. The number of knots is usually expressed in terms of degrees of freedom. A cubic spline will have K + 3 + 1 degrees of freedom. A natural spline has K + 3 + 1 - 5 degrees of freedom due to the constraints at the endpoints. A further constraint can be added to reduce overfitting by enforcing smoothness in the spline. Instead of minimizing the loss function \\(\\sum{(y - g(x))^2}\\) where \\(g(x)\\) is a natural spline, minimize a loss function with an additional penalty for variability: \\[L = \\sum{(y_i - g(x_i))^2 + \\lambda \\int g&#39;&#39;(t)^2dt}.\\] The function \\(g(x)\\) that minimizes the loss function is a natural cubic spline with knots at each \\(x_1, \\dots, x_n\\). This is called a smoothing spline. The larger g is, the greater the penalty on variation in the spline. In a smoothing spline, you do not optimize the number or location of the knots – there is a knot at each training observation. Instead, you optimize \\(\\lambda\\). One way to optimze \\(\\lambda\\) is cross-validation to minimize RSS. Leave-one-out cross-validation (LOOCV) can be computed efficiently for smoothing splines. 5.2 MARS Multivariate adaptive regression splines (MARS) is a non-parametric algorithm that creates a piecewise linear model to capture nonlinearities and interactions effects. The resulting model is a weighted sum of basis functions \\(B_i(X)\\): \\[\\hat{y} = \\sum_{i=1}^{k}{w_iB_i(x)}\\] The basis functions are either a constant (for the intercept), a hinge function of the form \\(\\max(0, x - x_0)\\) or \\(\\max(0, x_0 - x)\\) (a more concise representation is \\([\\pm(x - x_0)]_+\\)), or products of two or more hinge functions (for interactions). MARS automatically selects which predictors to use and what predictor values to serve as the knots of the hinge functions. MARS builds a model in two phases: the forward pass and the backward pass, similar to growing and pruning of tree models. MARS starts with a model consisting of just the intercept term equaling the mean of the response values. It then asseses every predictor to find a basis function pair consisting of opposing sides of a mirrored hinge function which produces the maximum improvement in the model error. MARS repeats the process until either it reaches a predefined limit of terms or the error improvement reaches a predefined limit. MARS generalizes the model by removing terms according to the generalized cross validation (GCV) criterion. GCV is a form of regularization: it trades off goodness-of-fit against model complexity. The earth::earth() function (documentation) performs the MARS algorithm (the term “MARS” is trademarked, so open-source implementations use “Earth” instead). The caret implementation tunes two parameters: nprune and degree. nprune is the maximum number of terms in the pruned model. degree is the maximum degree of interaction (default is 1 (no interactions)). However, there are other hyperparameters in the model that may improve performance, including minspan which regulates the number of knots in the predictors. Here is an example using the Ames housing data set (following this tutorial. library(tidyverse) library(earth) library(caret) # set up ames &lt;- AmesHousing::make_ames() set.seed(12345) idx &lt;- createDataPartition(ames$Sale_Price, p = 0.80, list = FALSE) ames_train &lt;- ames[idx, ] %&gt;% as.data.frame() ames_test &lt;- ames[-idx, ] m &lt;- train( x = subset(ames_train, select = -Sale_Price), y = ames_train$Sale_Price, method = &quot;earth&quot;, metric = &quot;RMSE&quot;, minspan = -15, trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = expand.grid( degree = 1:3, nprune = seq(2, 100, length.out = 10) %&gt;% floor() ) ) The model plot shows the best tuning parameter combination. plot(m, main = &quot;MARS Parameter Tuning&quot;) m$bestTune ## nprune degree ## 25 45 3 How does this model perform against the holdout data? caret::postResample( pred = log(predict(m, newdata = ames_test)), obs = log(ames_test$Sale_Price) ) ## RMSE Rsquared MAE ## 0.16515620 0.85470300 0.09319503 5.3 GAM Generalized additive models (GAM) allow for non-linear relationships between each feature and the response by replacing each linear component \\(\\beta_j x_{ij}\\) with a nonlinear function \\(f_j(x_{ij})\\). The GAM model is of the form \\[y_i = \\beta_0 + \\sum{f_j(x_{ij})} + \\epsilon_i.\\] It is called an additive model because we calculate a separate \\(f_j\\) for each \\(X_j\\), and then add together all of their contributions. The advantage of GAMs is that they automatically model non-linear relationships so you do not need to manually try out many diﬀerent transformations on each variable individually. And because the model is additive, you can still examine the eﬀect of each \\(X_j\\) on \\(Y\\) individually while holding all of the other variables ﬁxed. The main limitation of GAMs is that the model is restricted to be additive, so important interactions can be missed unless you explicitly add them. References "],["support-vector-machines.html", "Chapter 6 Support Vector Machines 6.1 Maximal Margin Classifier 6.2 Support Vector Classifier 6.3 Support Vector Machines", " Chapter 6 Support Vector Machines These notes rely on (James et al. 2013), (Hastie, Tibshirani, and Friedman 2017), (Kuhn and Johnson 2016), PSU STAT 508, and the e1071 SVM vignette. Support Vector Machines (SVM) is a classification model that maps observations as points in space so that the categories are divided by as wide a gap as possible. New observations can then be mapped into the space for prediction. The SVM algorithm finds the optimal separating hyperplane using a nonlinear mapping. The hyperplane is defined by the observations that lie within a margin optimized by a cost hyperparameter. These observations are called the support vectors. SVM is an extension of the support vector classifier which in turn is a generalization of the simple and intuitive maximal margin classifier. The maximal margin classifier is defined for cases where the data can be separated by a linear boundary (uncommon). The support vector classifier generalizes the maximal margin classifier by introducing a margin which permits some observations to land on wrong side of the hyperplane. The support vector machine generalizes still more by introducing non-linear hyperplanes. The best way to understand SVM is to start with the maximal margin classifier and work up. I’ll learn by example, using the ISLR::Default data set to predict which customers will default on their credit card debt from its 3 predictor variables. I’m using this Dataaspirant tutorial for guidance. The three predictors are student (Yes|No), current credit card balance, and annual income. library(tidyverse) library(caret) library(recipes) library(janitor) # for tabyl() library(tictoc) dat &lt;- ISLR::Default skimr::skim(dat) Table 6.1: Data summary Name dat Number of rows 10000 Number of columns 4 _______________________ Column type frequency: factor 2 numeric 2 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts default 0 1 FALSE 2 No: 9667, Yes: 333 student 0 1 FALSE 2 No: 7056, Yes: 2944 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist balance 0 1 835.37 483.71 0.00 481.73 823.64 1166.31 2654.32 ▆▇▅▁▁ income 0 1 33516.98 13336.64 771.97 21340.46 34552.64 43807.73 73554.23 ▂▇▇▅▁ I’ll build and compare models the customary way, splitting dat (n = 10,000) into dat_train (80%, n = 8001) to fit models, and dat_test (20%, n = 1999) to compare performance on new data. set.seed(123) train_index &lt;- createDataPartition(y = dat$default, p = 0.8, list = FALSE) dat_train &lt;- dat[train_index, ] dat_test &lt;- dat[-train_index, ] createDataPartition() is nice because it creates equal representation of the response variable in the partition. Only 3.3% of applicants default, so this is a difficult prediction problem. tabyl(dat$default) ## dat$default n percent ## No 9667 0.9667 ## Yes 333 0.0333 6.1 Maximal Margin Classifier The maximal margin classifier is the optimal hyperplane defined in the (rare) case where two classes are linearly separable. Given an \\(X_{n \\times p}\\) predictor matrix with a binary response variable \\(y \\in \\{-1, 1\\}\\) it might be possible to define a p-dimensional hyperplane \\(h(x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 \\dots + \\beta_px_p = X_i^{&#39;} \\beta + \\beta_0 = 0\\) such that all of the \\(y_i = -1\\) observations fall on the negative side of the hyperplane and the \\(y_i = +1\\) observations fall on the positive side: \\[y_i \\left(x_i^{&#39;} \\beta + \\beta_0 \\right) &gt; 0\\] This separating hyperplane is a simple classifier, and the magnitude of \\(\\left(x_i^{&#39;} \\beta + \\beta_0 \\right)\\) is an indicator of confidence in the predicted classification. If you constrain \\(\\beta\\) to be a unit vector, \\(||\\beta|| = \\sum\\beta^2 = 1\\), then the products of the hyperplane and response variables, \\(\\left(x_i^{&#39;} \\beta + \\beta_0 \\right)\\), are the positive perpendicular distances from the hyperplane. If a separating hyperplane exists, there are probably an infinite number of possible hyperplanes. One way to evaluate a hyperplane is to measure its margin, \\(M\\), the perpendicular distance to the closest observation. \\[M = \\min \\left\\{y_i (x_i^{&#39;} \\beta + \\beta_0) \\right\\}.\\] The maximal margin classifier is the hyperplane that maximizes \\(M.\\) The figure below (figure 9.3 from (James et al. 2013)) shows a maximal marginal classifier. The three vectors shown in the figure anchor the hyperplane and are called the support vectors. Interestingly, it is only these three observations that factor into the determination of the maximal marginal classifier. FIGURE 9.3 from An Introduction to Statistical Learning So, to put it all together, if a separating hyperplane exists, one could calculate it by maximizing \\(M\\) subject to \\(||\\beta|| = 1\\) and \\(y_i (x_i^{&#39;} \\beta + \\beta_0) \\ge M\\) for all \\(i\\). However, a separating hyperplane rarely exists. In fact, even if a separating hyperplane does exist, its maximal margin classifier is probably undesirably narrow. A maximal margin classifier is sensitive to outliers so it tends to overfit data. 6.2 Support Vector Classifier The maximal margin classifier can be generalized to non-separable cases using a so-called soft margin. The generalization is called the support vector classifier. The soft margin allows some misclassification in the interest of greater robustness to individual observations. The support vector classifier maximizes \\(M\\) subject to \\(||\\beta|| = 1\\) and \\(y_i (x_i^{&#39;} \\beta + \\beta_0) \\ge M(1 - \\xi_i)\\) and \\(\\sum \\xi_i \\le \\Xi\\) for all \\(i\\). The \\(\\xi_i\\) are slack variables whose sum is bounded by some constant tuning parameter \\(\\Xi\\). The slack variable values indicate where the observation lies: \\(\\xi_i = 0\\) observations lie on the correct side of the margin; \\(\\xi_i &gt; 0\\) observation lie on the wrong side of the margin; \\(\\xi_i &gt; 1\\) observations lie on the wrong side of the hyperplane. \\(\\Xi\\) sets the tolerance for margin violation. If \\(\\Xi = 0\\), then all observations must reside on the correct side of the margin, as in the maximal margin classifier. \\(\\Xi\\) controls the bias-variance trade-off: as \\(\\Xi\\) increases, the margin widens and allows more violations, increasing bias and decreasing variance. Similar to the maximal margin classifier, only the observations that are on the margin or that violate the margin factor into the determination of the support vector classifier. These observations are the support vectors. The figure below (figure 9.7 from (James et al. 2013)) shows two support vector classifiers. The one on the left uses a large \\(\\Xi\\) and as a result includes many support vectors. The one on the right uses a smaller \\(\\Xi.\\) FIGURE 9.7 from An Introduction to Statistical Learning As \\(\\Xi\\) increases, the number of violating observations increase, and thus the number of support vectors increases. This property makes the algorithm robust to the extreme observations far away from the hyperplane. The only shortcoming with the algorithm is that it presumes a linear decision boundary. Let’s build a support vector classifier model to predict credit default in the ISLM:Default data set. I’ll build the model in caret with the svmLinear method using 10-fold cross-validation (CV-10) to optimize the hyperparameters. There is only one hyperparameter for this model, the cost parameter: caret::modelLookup(&quot;svmLinear&quot;) ## model parameter label forReg forClass probModel ## 1 svmLinear C Cost TRUE TRUE TRUE I’m not sure if \\(C = Xi\\), but it seems to function the same way. The documentation notes in e1071::svm() say C is the “cost of constraints violation (default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.” CV-10 will fit 10 models for each candidate value of C and keep the model with the best performance on resamples according to our evaluation metric. I can evaluate with “Accuracy”, “Kappa”, or “ROC”. I’ll use ROC, so I need to also set summaryFunction = twoClassSummary and classProbs = TRUE. svmLinear expects the response variable to be a factor with labels that can double as R variable names. This data set is fine because default is a factor with labels “No” and “Yes”. The predictor variables should be of comparable scale, but that is not the case here: student is binary, balance has a range of $0 - $2654, and income has a range of $772 - $73,554. I’ll one-hot encode student and standardize balance and income inside a recipe object. mdl_ctrl &lt;- trainControl( method = &quot;cv&quot;, number = 10, summaryFunction = twoClassSummary, classProbs = TRUE ) rcpe &lt;- recipe(default ~ ., data = dat_train) %&gt;% step_dummy(all_nominal(), -all_outcomes()) %&gt;% step_center(balance, income) %&gt;% step_scale(balance, income) tic() set.seed(1234) capture.output( mdl_svm_linear &lt;- train( rcpe, data = dat_train, method = &quot;svmLinear&quot;, metric = &quot;ROC&quot;, trControl = mdl_ctrl, tuneGrid = expand.grid(C = c(1e-1, 1e0, 1e1, 1e2, 1e3, 1e4)) ) ) ## Loading required namespace: kernlab ## ## Attaching package: &#39;kernlab&#39; ## The following object is masked from &#39;package:tictoc&#39;: ## ## size ## The following object is masked from &#39;package:scales&#39;: ## ## alpha ## The following object is masked from &#39;package:ordinal&#39;: ## ## convergence ## The following object is masked from &#39;package:VGAM&#39;: ## ## nvar ## The following object is masked from &#39;package:purrr&#39;: ## ## cross ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha toc() I experimented with the tuneGrid and found that smaller values for C (C &lt;= 10) produced poorer performance on resamples and on the holdout set. Unfortunately, the time to fit the models increased with C so that expand.grid(C = c(1e-1, 1e0, 1e1, 1e2, 1e3, 1e4) ran ~6 minutes. The cross-validation maximized ROC with C = 10,000. The first three values (.1, 1, and 10) resulted in models that predicted no default every time. mdl_svm_linear ## Support Vector Machines with Linear Kernel ## ## 8001 samples ## 3 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## Recipe steps: dummy, center, scale ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 7201, 7201, 7200, 7201, 7200, 7201, ... ## Resampling results across tuning parameters: ## ## C ROC Sens Spec ## 1e-01 0.7819529 1.0000000 0.00000000 ## 1e+00 0.8545441 1.0000000 0.00000000 ## 1e+01 0.8735454 1.0000000 0.00000000 ## 1e+02 0.9368640 1.0000000 0.02592593 ## 1e+03 0.9375616 0.9949579 0.27207977 ## 1e+04 0.9442658 0.9981890 0.20413105 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was C = 10000. As C increases, the model variance decreases at the expense of more bias. The plot of the optimization results makes you wonder if C = 100 is basically just as good as C = 10,000 at a fraction of the fitting time. ggplot(mdl_svm_linear) Predictions on the holdout set yield 96.8% accuracy. It found 15 of the 66 defaulters (sensitivity = 0.227), and misclassified 13 of the 1933 non-defaulters (specificity = 0.993). preds_svm_linear &lt;- bind_cols( dat_test, predict(mdl_svm_linear, newdata = dat_test, type = &quot;prob&quot;), Predicted = predict(mdl_svm_linear, newdata = dat_test, type = &quot;raw&quot;) ) confusionMatrix(preds_svm_linear$Predicted, reference = preds_svm_linear$default, positive = &quot;Yes&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 1920 51 ## Yes 13 15 ## ## Accuracy : 0.968 ## 95% CI : (0.9593, 0.9753) ## No Information Rate : 0.967 ## P-Value [Acc &gt; NIR] : 0.433 ## ## Kappa : 0.3055 ## ## Mcnemar&#39;s Test P-Value : 3.746e-06 ## ## Sensitivity : 0.227273 ## Specificity : 0.993275 ## Pos Pred Value : 0.535714 ## Neg Pred Value : 0.974125 ## Prevalence : 0.033017 ## Detection Rate : 0.007504 ## Detection Prevalence : 0.014007 ## Balanced Accuracy : 0.610274 ## ## &#39;Positive&#39; Class : Yes ## Metrics::auc() will calculate the confusion matrix values from the model using the holdout data set. The AUC on the holdout set is 0.9541. mdl_svm_linear_auc &lt;- Metrics::auc(actual = preds_svm_linear$default == &quot;Yes&quot;, preds_svm_linear$Yes) yardstick::roc_curve(preds_svm_linear, default, Yes) %&gt;% autoplot() + labs( title = &quot;SVM Linear Model ROC Curve, Test Data&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_svm_linear_auc, 4)) ) There are just a few predictors in this model, so there is a chance I can visualize the model. fits_svm_linear &lt;- bind_cols( dat_train, predict(mdl_svm_linear, newdata = dat_train, type = &quot;prob&quot;), Predicted = predict(mdl_svm_linear, newdata = dat_train, type = &quot;raw&quot;), ) bind_rows( fits_svm_linear %&gt;% mutate(set = &quot;Actual&quot;, outcome = default), fits_svm_linear %&gt;% mutate(set = &quot;Fitted&quot;, outcome = Predicted) ) %&gt;% ggplot(aes(x = balance, y = income, color = outcome)) + geom_point(aes(shape = student), alpha = 0.6) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) + scale_y_continuous(labels=scales::dollar_format()) + scale_x_continuous(labels=scales::dollar_format()) + scale_color_manual(values = list(No = &quot;#B6E2D3&quot;, Yes = &quot;#EF7C8E&quot;)) + labs(title = &quot;Training Set&quot;) + facet_wrap(vars(set)) Looks like the hyperplane slopes slightly right, so high credit card balances are a little less likely to fall into default if income is high. Distinguishing students is difficult, but they are generally at the low end of the income scale, and they seem to exert a positive association with default. Let’s look at the same figure with the training set. bind_rows( preds_svm_linear %&gt;% mutate(set = &quot;Actual&quot;, outcome = default), preds_svm_linear %&gt;% mutate(set = &quot;Fitted&quot;, outcome = Predicted) ) %&gt;% ggplot(aes(x = balance, y = income, color = outcome)) + geom_point(aes(shape = student), alpha = 0.6) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) + scale_y_continuous(labels=scales::dollar_format()) + scale_x_continuous(labels=scales::dollar_format()) + scale_color_manual(values = list(No = &quot;#B6E2D3&quot;, Yes = &quot;#EF7C8E&quot;)) + labs(title = &quot;Test Set&quot;) + facet_wrap(vars(set)) Visually, the model performed consistently between the training and test sets. 6.3 Support Vector Machines Enlarging the feature space of the support vector classifier accommodates nonlinear relationships. Support vector machines do this in a specific way, using kernels. Before diving into kernels, you need to understand (somewhat) the solution to the support vector classifier optimization problem. The linear support vector classifier can be represented as \\[f(x) = \\beta_0 + \\sum_i^n \\alpha_i \\langle x, x_i \\rangle.\\] That is, the classification of test observation \\(x\\) is the sum of the dot products of \\(x\\) with all the \\(n\\) observations in the training set, multiplied by the vector \\(\\alpha\\) (plus the constant \\(\\beta_0\\)). The \\(\\alpha\\) vector is calculated from the \\(n \\choose 2\\) dot products of the training data set. Actually, the classification is simpler than that because \\(\\alpha_i = 0\\) for all observation that are not support vectors, so you can actually represent the solution as \\[f(x) = \\beta_0 + \\sum_{i \\in S} \\alpha_i \\langle x, x_i \\rangle\\] where \\(S\\) is the set of support vector indices. Now, you can generalize the inner dot product with a wrapper function, called a kernel, \\(K(x_i, x_{i^{&#39;}})\\). \\[f(x) = \\beta_0 + \\sum_{i \\in S} \\alpha_i K(x, x_i)\\] To get the the support vector classifier, you’d defined \\(K\\) to be a linear kernel: \\[K(x_i, x_i^{&#39;}) = \\langle x, x_i \\rangle\\] But you could also use other kernels, like the polynomial of degree \\(d\\), \\[K(x, x&#39;) = (1 + \\langle x, x&#39; \\rangle)^d\\] or radial \\[K(x, x&#39;) = \\exp\\{-\\gamma ||x - x&#39;||^2\\}.\\] The figure below (figure 9.9 from (James et al. 2013)) shows two support vector classifiers. The one on the left uses a polynomial kernel and the one on the right uses a radial kernel. FIGURE 9.9 from An Introduction to Statistical Learning The SVM model can be expressed in the familiar “loss + penalty” minimization structure, \\(\\min_{\\beta} \\left \\{ L(X,y,\\beta) + \\lambda P(\\beta) \\right \\}\\) as \\[\\min_\\beta \\left \\{ \\sum_{i=1}^n \\max [0, 1-y_i f(x_i)] + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right \\}\\] Increasing \\(\\lambda\\), shrinks \\(\\beta\\) and more violations to the margin are tolerated, resulting in a lower-variance/higher-bias model. The loss function above is known as a hinge loss. Let’s build a support vector machine model to predict credit default in the ISLM:Default data set again. I’ll try a polynomial kernel with the svmPoly method and a radial kernal with svmRadial. svmPoly has three hyperparameters. What is the Scale parameter? caret::modelLookup(&quot;svmPoly&quot;) ## model parameter label forReg forClass probModel ## 1 svmPoly degree Polynomial Degree TRUE TRUE TRUE ## 2 svmPoly scale Scale TRUE TRUE TRUE ## 3 svmPoly C Cost TRUE TRUE TRUE svmRadial has two hyperparameters: caret::modelLookup(&quot;svmRadial&quot;) ## model parameter label forReg forClass probModel ## 1 svmRadial sigma Sigma TRUE TRUE TRUE ## 2 svmRadial C Cost TRUE TRUE TRUE I’ll use the same trControl object as with the support vector classifier, and I’ll use the same one-hot encoded binaries and scaled and centered data. I fixed scale at its default value and tried polynomials of degree 1-3. I used the same candidate cost values. tic() set.seed(1234) capture.output( mdl_svm_poly &lt;- train( rcpe, data = dat_train, method = &quot;svmPoly&quot;, metric = &quot;ROC&quot;, trControl = mdl_ctrl, tuneGrid = expand.grid( C = c(1e-1, 1e0, 1e1, 1e2, 1e3, 1e4), degree = c(1, 2, 3), scale = 0.001) ) ) toc() The model ran ~3 minutes. The cross-validation maximized ROC with degree = 2 and C = 0.1. mdl_svm_poly ## Support Vector Machines with Polynomial Kernel ## ## 8001 samples ## 3 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## Recipe steps: dummy, center, scale ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 7201, 7201, 7200, 7201, 7200, 7201, ... ## Resampling results across tuning parameters: ## ## C degree ROC Sens Spec ## 1e-01 1 0.9474883 0.9954749 0.36096866 ## 1e-01 2 0.9476254 0.9949582 0.37948718 ## 1e-01 3 0.9473626 0.9967679 0.33504274 ## 1e+00 1 0.9470650 0.9972855 0.24886040 ## 1e+00 2 0.8522411 0.9987080 0.07136752 ## 1e+00 3 0.8646715 0.9976727 0.17692308 ## 1e+01 1 0.9027483 0.9997413 0.01851852 ## 1e+01 2 0.8854205 0.9993533 0.16168091 ## 1e+01 3 0.8923607 0.9976732 0.21039886 ## 1e+02 1 0.9459051 0.9989664 0.04116809 ## 1e+02 2 0.8955601 0.9971556 0.29358974 ## 1e+02 3 0.8952502 0.9978026 0.19259259 ## 1e+03 1 0.9469310 0.9993533 0.14415954 ## 1e+03 2 0.8960461 0.9963798 0.23931624 ## 1e+03 3 0.8954159 0.9963803 0.28162393 ## 1e+04 1 0.9476159 0.9979320 0.23433048 ## 1e+04 2 0.8932654 0.9963806 0.31168091 ## 1e+04 3 0.8907370 0.9965098 0.31168091 ## ## Tuning parameter &#39;scale&#39; was held constant at a value of 0.001 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were degree = 2, scale = 0.001 and C = 0.1. Here is svmRadial. At this point, I do not know how the sigma tuning parameter works, so I expanded around the default value used from tuneLength = 1. tic() set.seed(1234) capture.output( mdl_svm_radial &lt;- train( rcpe, data = dat_train, method = &quot;svmRadial&quot;, metric = &quot;ROC&quot;, trControl = mdl_ctrl, tuneGrid = expand.grid( C = c(1e-1, 1e0, 1e1, 1e2, 1e3), sigma = c(.01, .1, 1.0) ) ) ) toc() This model ran in ~11 minutes and optimized at C = 0.1 and sigma = 0.01. mdl_svm_radial ## Support Vector Machines with Radial Basis Function Kernel ## ## 8001 samples ## 3 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## Recipe steps: dummy, center, scale ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 7201, 7201, 7200, 7201, 7200, 7201, ... ## Resampling results across tuning parameters: ## ## C sigma ROC Sens Spec ## 1e-01 0.01 0.8943575 0.9957344 0.2515670 ## 1e-01 0.10 0.7572569 0.9958631 0.3380342 ## 1e-01 1.00 0.6919219 0.9959923 0.3457265 ## 1e+00 0.01 0.8935443 0.9958640 0.2594017 ## 1e+00 0.10 0.7581334 0.9968976 0.3044160 ## 1e+00 1.00 0.6708154 0.9974147 0.2782051 ## 1e+01 0.01 0.8926542 0.9970259 0.2743590 ## 1e+01 0.10 0.8362050 0.9976729 0.2783476 ## 1e+01 1.00 0.7456079 0.9974147 0.2632479 ## 1e+02 0.01 0.8586855 0.9967684 0.2669516 ## 1e+02 0.10 0.8819563 0.9978023 0.2746439 ## 1e+02 1.00 0.8185661 0.9976731 0.2330484 ## 1e+03 0.01 0.7735879 0.9975435 0.2743590 ## 1e+03 0.10 0.8614572 0.9979315 0.2559829 ## 1e+03 1.00 0.8147071 0.9985776 0.1958689 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.01 and C = 0.1. Here are the optimization plots I used to help tune the models. p1 &lt;- ggplot(mdl_svm_poly) + labs(title = &quot;SVMPoly Tuning&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) p2 &lt;- ggplot(mdl_svm_radial) + labs(title = &quot;SVMRadial Tuning&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) The polynomial model predictions on the holdout set yielded 96.85% accuracy. It found 20 of the 66 defaulters (sensitivity = 0.303), and misclassified 17 of the 1933 non-defaulters (specificity = 0.991). So the polynomial model found a few more defaulters at the expense of a few more mistakes. The radial model predictions on the holdout set yielded 97% accuracy. It found 17 defaulters (sensitivity = 0.258), and misclassified 11 non-defaulters (specificity = 0.994). So the radial was somewhere between the linear and polynomial models. preds_svm_poly &lt;- bind_cols( dat_test, predict(mdl_svm_poly, newdata = dat_test, type = &quot;prob&quot;), Predicted = predict(mdl_svm_poly, newdata = dat_test, type = &quot;raw&quot;) ) confusionMatrix(preds_svm_poly$Predicted, reference = preds_svm_poly$default, positive = &quot;Yes&quot;) preds_svm_radial &lt;- bind_cols( dat_test, predict(mdl_svm_radial, newdata = dat_test, type = &quot;prob&quot;), Predicted = predict(mdl_svm_radial, newdata = dat_test, type = &quot;raw&quot;) ) confusionMatrix(preds_svm_radial$Predicted, reference = preds_svm_radial$default, positive = &quot;Yes&quot;) The AUCs on the holdout set is where 0.9536 for the polynmomial and 0.8836 for the radial. Metrics::auc(actual = preds_svm_poly$default == &quot;Yes&quot;, preds_svm_poly$Yes) ## [1] 0.9536049 Metrics::auc(actual = preds_svm_radial$default == &quot;Yes&quot;, preds_svm_radial$Yes) ## [1] 0.8835849 Let’s see what the two models look like on the training data. fits_svm_poly &lt;- bind_cols( dat_train, predict(mdl_svm_poly, newdata = dat_train, type = &quot;prob&quot;), Predicted = predict(mdl_svm_poly, newdata = dat_train, type = &quot;raw&quot;), ) bind_rows( fits_svm_poly %&gt;% mutate(set = &quot;Actual&quot;, outcome = default), fits_svm_poly %&gt;% mutate(set = &quot;Fitted&quot;, outcome = Predicted) ) %&gt;% ggplot(aes(x = balance, y = income, color = outcome)) + geom_point(aes(shape = student), alpha = 0.6) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) + scale_y_continuous(labels=scales::dollar_format()) + scale_x_continuous(labels=scales::dollar_format()) + scale_color_manual(values = list(No = &quot;#B6E2D3&quot;, Yes = &quot;#EF7C8E&quot;)) + labs(title = &quot;Training Set, Polynomial Kernel&quot;) + facet_wrap(vars(set)) fits_svm_radial &lt;- bind_cols( dat_train, predict(mdl_svm_radial, newdata = dat_train, type = &quot;prob&quot;), Predicted = predict(mdl_svm_radial, newdata = dat_train, type = &quot;raw&quot;), ) bind_rows( fits_svm_radial %&gt;% mutate(set = &quot;Actual&quot;, outcome = default), fits_svm_radial %&gt;% mutate(set = &quot;Fitted&quot;, outcome = Predicted) ) %&gt;% ggplot(aes(x = balance, y = income, color = outcome)) + geom_point(aes(shape = student), alpha = 0.6) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) + scale_y_continuous(labels=scales::dollar_format()) + scale_x_continuous(labels=scales::dollar_format()) + scale_color_manual(values = list(No = &quot;#B6E2D3&quot;, Yes = &quot;#EF7C8E&quot;)) + labs(title = &quot;Training Set, Radial Kernel&quot;) + facet_wrap(vars(set)) You can see the slight curvature in the hyperplane now. The polynomial and the radial models look pretty much identical to me. References "],["EMMs.html", "Chapter 7 Topics: EMMs 7.1 References", " Chapter 7 Topics: EMMs This section is an overview of estimated marginal means, and its implementation in in the emmeans package. pigs ## source percent conc ## 1 fish 9 27.8 ## 2 fish 9 23.7 ## 3 fish 12 31.5 ## 4 fish 12 28.5 ## 5 fish 12 32.8 ## 6 fish 15 34.0 ## 7 fish 15 28.3 ## 8 fish 18 30.6 ## 9 fish 18 32.7 ## 10 fish 18 33.7 ## 11 soy 9 39.3 ## 12 soy 9 34.8 ## 13 soy 9 29.8 ## 14 soy 12 39.8 ## 15 soy 12 40.0 ## 16 soy 12 39.1 ## 17 soy 15 38.5 ## 18 soy 15 39.2 ## 19 soy 15 40.0 ## 20 soy 18 42.9 ## 21 skim 9 40.6 ## 22 skim 9 31.0 ## 23 skim 9 34.6 ## 24 skim 12 42.9 ## 25 skim 12 50.1 ## 26 skim 12 37.4 ## 27 skim 15 59.5 ## 28 skim 15 41.4 ## 29 skim 18 59.8 7.1 References This Very statisticious blog post is helpful. I also worked through the emmeans vignettes on CRAN. "],["BayesRegression.html", "Chapter 8 Bayesian Regression 8.1 Compared to Frequentist Regression 8.2 Model Evaluation", " Chapter 8 Bayesian Regression 8.1 Compared to Frequentist Regression This section is my notes from DataCamp course Bayesian Regression Modeling with rstanarm. The course refers to Andrew Gelman’s book “Bayesian Data Analysis” (Gelman2013?). It is an intuitive approach to Bayesian inference. Frequentist regression estimates fixed population parameters from a random sample of data whose test statistics are random variables. The Bayesian approach to regression works the other direction: Bayesian regression characterizes the distribution of the random variable population parameters from the evidence of a fixed sample of data. The frequentist p-value is the probability of observing a test statistic of equal or greater magnitude if the null hypothesis is true. The 95% confidence interval has a 95% probability of capturing the population value (repeated sampling would produce similar CIs, 95% of which would capture the population value). The 95% credible interval captures the confidence the value falls within the range. There is an important difference here. In the Bayesian framework, you can state the probability the parameter value falls with a specified range, but there is no equivalent in frequentist regression. Bayesian regression uses maximum likelihood to fit the model because the integral in the denominator (the marginal distribution) cannot (or is difficult) to calculate analytically. Instead, the algorithm samples from the posterior distribution in groups (chains). Each chain begins at a random location. Each sample (iteration) moves toward the area where the combination of likelihood and prior indicates a high probability of the true parameter value residing. The more iterations per chain, the larger the total sample size, and more robust the outcome. The chains need to converge to have a stable estimate. The iterations prior to convergence are sometimes referred to as the “warm up” and are not included in the posterior distribution estimate. By default, the rstanarm package estimates 4 chains, each with 2,000 iterations, and the first 1,000 set aside as warm-up. The final posterior combines the chains, so is composed of 4,000 iterations. A Bayesian model estimates each coefficient parameter and model error with a prior*likelihood/marginal dist = posterior framework. library(rstanarm) sink(type = &quot;message&quot;) stan_mdl &lt;- stan_glm(kid_score ~ mom_iq, data = kidiq) ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 6e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.6 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.033 seconds (Warm-up) ## Chain 1: 0.046 seconds (Sampling) ## Chain 1: 0.079 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 7e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.03 seconds (Warm-up) ## Chain 2: 0.046 seconds (Sampling) ## Chain 2: 0.076 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 7e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.03 seconds (Warm-up) ## Chain 3: 0.046 seconds (Sampling) ## Chain 3: 0.076 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 5e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.036 seconds (Warm-up) ## Chain 4: 0.047 seconds (Sampling) ## Chain 4: 0.083 seconds (Total) ## Chain 4: sink(type = &quot;message&quot;) summary(stan_mdl) ## ## Model Info: ## function: stan_glm ## family: gaussian [identity] ## formula: kid_score ~ mom_iq ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(&#39;prior_summary&#39;) ## observations: 434 ## predictors: 2 ## ## Estimates: ## mean sd 10% 50% 90% ## (Intercept) 25.8 5.9 18.3 25.8 33.4 ## mom_iq 0.6 0.1 0.5 0.6 0.7 ## sigma 18.3 0.6 17.5 18.3 19.1 ## ## Fit Diagnostics: ## mean sd 10% 50% 90% ## mean_PPD 86.8 1.2 85.2 86.8 88.4 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.1 1.0 4050 ## mom_iq 0.0 1.0 4022 ## sigma 0.0 1.0 3712 ## mean_PPD 0.0 1.0 3885 ## log-posterior 0.0 1.0 1853 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). broom.mixed::tidy(stan_mdl) ## # A tibble: 2 × 3 ## term estimate std.error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 25.8 5.62 ## 2 mom_iq 0.609 0.0553 posterior_interval(stan_mdl) ## 5% 95% ## (Intercept) 16.0877014 35.5482351 ## mom_iq 0.5142779 0.7068406 ## sigma 17.3141517 19.3641861 By default, rstanarm priors are normal distributions with mean 0 and variance equal to a scaler multiple of the data variance. prior_summary(stan_mdl) ## Priors for model &#39;stan_mdl&#39; ## ------ ## Intercept (after predictors centered) ## Specified prior: ## ~ normal(location = 87, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 87, scale = 51) ## ## Coefficients ## Specified prior: ## ~ normal(location = 0, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 0, scale = 3.4) ## ## Auxiliary (sigma) ## Specified prior: ## ~ exponential(rate = 1) ## Adjusted prior: ## ~ exponential(rate = 0.049) ## ------ ## See help(&#39;prior_summary.stanreg&#39;) for more details # Calculate the adjusted scale for the intercept 2.5 * sd(kidiq$kid_score) ## [1] 51.02672 # Calculate the adjusted scale for `mom_iq` (2.5 / sd(kidiq$mom_iq)) * sd(kidiq$kid_score) ## [1] 3.401781 You can override the priors to set your own: stan_mdl &lt;- stan_glm(kid_score ~ mom_iq, data = kidiq, prior_intercept = normal(location = 0, scale = 10, autoscale = FALSE), prior = normal(location = 0, scale = 2.5, autoscale = FALSE), prior_aux = exponential(rate = 1, autoscale = FALSE)) ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.03 seconds (Warm-up) ## Chain 1: 0.045 seconds (Sampling) ## Chain 1: 0.075 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 7e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.031 seconds (Warm-up) ## Chain 2: 0.046 seconds (Sampling) ## Chain 2: 0.077 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.032 seconds (Warm-up) ## Chain 3: 0.045 seconds (Sampling) ## Chain 3: 0.077 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 8e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.032 seconds (Warm-up) ## Chain 4: 0.046 seconds (Sampling) ## Chain 4: 0.078 seconds (Total) ## Chain 4: Bayesian estimation samples from the posterior distribution, so there is no point estimate and test statistic. 8.2 Model Evaluation The R-squared statistic is not available from the summary object, but you can still calculate it manually from the model data, or use the bayes_R2 function. sse &lt;- var(stan_mdl$residuals) ssr &lt;- var(stan_mdl$fitted.values) sst &lt;- ssr + sse r2 &lt;- 1 - sse/sst # bayes_R2 returns a vector of length equal to the posterior sample size. bayes_R2(stan_mdl) %&gt;% summary() ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1087 0.1847 0.2067 0.2070 0.2287 0.3394 bayes_R2(stan_mdl) %&gt;% quantile(c(.025, .975)) ## 2.5% 97.5% ## 0.1436118 0.2708439 Bayesian regression has other model fit statistics. # Calculate posterior predictive scores predictions &lt;- posterior_linpred(stan_mdl) # Print a summary of the 1st replication summary(predictions[1,]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 72.51 81.80 86.68 87.78 93.20 108.29 Or produce a posterior predictive model check. # peaks of observed data and model are similar places, but there is a second # mode of popularity scores around 10 that is not captured by the model. pp_check(stan_mdl, &quot;dens_overlay&quot;) pp_check(stan_mdl, &quot;stat&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # mean and sd of the observed data in middle of expected distribution - these # two characteristics are recovered well. pp_check(stan_mdl, &quot;stat_2d&quot;) 8.2.1 Model Comparison Use the loo (leave on out) package to compare models. loo approximates cross-validation. In the initial summary, 4000 is the number of iterations in the posterior, and 434 is the number of observations in the data set. ekpd_loo is the LOO estimate; p_loo is the effective number of parameters in the model; and looic is the LOO estimate converted to the deviance scale, -2 * LOO. stan_loo &lt;- loo(stan_mdl) stan_loo ## ## Computed from 4000 by 434 log-likelihood matrix ## ## Estimate SE ## elpd_loo -1879.0 14.9 ## p_loo 3.0 0.3 ## looic 3758.0 29.8 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. The statistics are not useful in isolation - they should be used in comparison to competing models. stan_mdl_2 &lt;- stan_glm(kid_score ~ mom_iq * mom_hs, data = kidiq) Which model has more predictive power? As a rule of thumb, if the absolute value of the difference is greater than the standard error, then the result is significant. In this case, the new model is significantly better. stan_loo_2 &lt;- loo(stan_mdl_2) loo_compare(stan_loo, stan_loo_2) ## elpd_diff se_diff ## stan_mdl_2 0.0 0.0 ## stan_mdl -6.5 4.3 8.2.2 Visualization Use tidy() to pull out the intercept and slope. Use the posterior distributions to create a predicted regression line from each draw in the posterior samples. These lines will show the uncertainty around the overall line. library(broom) stan_tdy_2 &lt;- tidy(stan_mdl_2) draws &lt;- tidybayes::spread_draws(stan_mdl_2, `(Intercept)`, mom_iq) kidiq %&gt;% ggplot(aes(x = mom_iq, y = kid_score)) + geom_point() + geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = mom_iq), size = 0.1, alpha = 0.2, color = &quot;skyblue&quot;) + geom_abline(intercept = stan_tdy_2$estimate[1], slope = stan_tdy_2$estimate[2]) Use the model to get posterior predictions with posterior_predict(). posteriors &lt;- posterior_predict(stan_mdl_2) new_data &lt;- data.frame(mom_iq = 100, mom_hs = c(0, 1)) posteriors_2 &lt;- posterior_predict(stan_mdl_2, newdata = new_data) %&gt;% as.data.frame() colnames(posteriors_2) &lt;- c(&quot;No HS&quot;, &quot;Completed HS&quot;) posteriors_2 %&gt;% pivot_longer(everything(), names_to = &quot;HS&quot;, values_to = &quot;predict&quot;) %&gt;% ggplot(aes(x = predict)) + geom_density() + facet_wrap(~HS, ncol = 1) "],["references-1.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
