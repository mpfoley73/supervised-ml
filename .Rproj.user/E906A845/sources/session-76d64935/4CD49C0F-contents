--- 
title: "Supervised Machine Learning"
subtitle: "My handbook"
author: "Michael Foley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "These are my personal notes related to supervised machine learning techniques."
---
--- 
title: "Supervised Machine Learning"
subtitle: "My handbook"
author: "Michael Foley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "These are my personal notes related to supervised machine learning techniques."
---

# Intro {-}

Machine learning (ML) develops algorithms to identify patterns in data (unsupervised ML) or make predictions and inferences (supervised ML). 

Supervised ML trains the machine to learn from prior examples to *predict* either a categorical outcome (classification) or a numeric outcome (regression), or to *infer* the relationships between the outcome and its explanatory variables.  

Two early forms of supervised ML are *[linear regression (OLS)](http://rpubs.com/mpfoley73/527767)* and *[generalized linear models (GLM)](http://rpubs.com/mpfoley73/527573)* (Poisson and logistic regression).  These methods have been improved with advanced linear methods, including *stepwise selection*, [*regularization*](http://rpubs.com/mpfoley73/521922) (ridge, lasso, elastic net), *principal components regression*, and *partial least squares*.  With greater computing capacity, non-linear models are now in use, including *polynomial regression*, *step functions*, *splines*, and generalized additive models (*GAM*).  [Decision trees](http://rpubs.com/mpfoley73/529130) (*bagging*, *random forests*, and, *boosting*) are additional options for regression and classification, and *support vector machines* is an additional option for classification.

<!--chapter:end:index.Rmd-->


# Ordinary Least Squares

Placeholder


## Linear Regression Model
## Parameter Estimation
#### Example
## Model Assumptions
### Linearity
### Multicollinearity
#### Example
#### Example
### Normality
### Equal Variances
## Prediction
#### Example
#### Example 
## Inference
### *t*-Test
#### Example
### *F*-Test
#### Example
## Interpretation
## Model Validation
### Accuracy Metrics
#### R-Squared
#### RMSE
#### RSE
#### MAE
#### Adjusted R-squared
#### AIC
#### AICc
#### BIC
#### Mallows Cp
##### Example
### Cross-Validation
#### Validation Set
##### Example
#### LOOCV
#### K-fold Cross-Validation
#### Repeated K-fold CV
#### Bootstrapping
### Gain Curve
## OLS Reference

<!--chapter:end:01-linear_regression.Rmd-->


# Generalized Linear Models (GLM)

Placeholder


## Binomial Logistic Regression {#binomiallogistic}
## Case Study 1 {.unnumbered #cs1}
### The Model {-}
### Interpretation {-}
### Assumptions {-}
### Model Fit {-}
### Reporting {-}
## Multinomial Logistic Regression {#multinomiallogistic}
## Case Study 2 {.unnumbered #cs2}
### The Model {-}
### Interpretation {-}
### Assumptions {-}
### Model Fit {-}
#### Lack of Fit {-}
#### Likelihood Ratio Test {-}
### Reporting {-}
## Ordinal Logistic Regression {#ordinallogistic}
## Case Study 3 {.unnumbered #cs3}
### Fit the Model {-}
### Verify Assumptions {-}
#### Multicollinearity {-}
#### Proportional Odds {-}
### Assess the Model Fit {-}
#### Deviance and Pearson {-}
#### Pseudo-R2 Measures {-}
#### Likelihood Ratio Test {-}
### Interpret Results {-}
### Reporting {-}
## Poisson Regression {#poissonregression}
## Case Study 4 {.unnumbered #cs4}

<!--chapter:end:02-generalized_linear_models.Rmd-->


# Regularization

Placeholder


## Ridge
#### Example {-}
## Lasso
#### Example {-}
## Elastic Net
#### Example {-}
## Model Summary {-}

<!--chapter:end:03-regularization.Rmd-->


# Decision Trees

Placeholder


## Classification Tree
### Measuring Performance
#### Confusion Matrix
#### ROC Curve
#### Gain Curve
### Training with Caret
## Regression Tree
### Training with Caret
## Bagged Trees
### Bagged Classification Tree
### Bagging Regression Tree
## Random Forests
#### Random Forest Classification Tree
#### Random Forest Regression Tree
## Gradient Boosting
#### Gradient Boosting Classification Tree
##### GBM
##### XGBoost
#### Gradient Boosting Regression Tree
##### GBM
##### XGBoost
## Summary
### Classification Trees
### Regression Trees

<!--chapter:end:04-decision_trees.Rmd-->


# Non-linear Models

Placeholder


## Splines
## MARS
## GAM

<!--chapter:end:05-nonlinear_regression.Rmd-->


# Support Vector Machines

Placeholder


## Maximal Margin Classifier
## Support Vector Classifier
## Support Vector Machines

<!--chapter:end:06-support_vector_machines.Rmd-->

```{r include=FALSE}
library(tidyverse)
library(emmeans)
```

# Topics: EMMs {#EMMs}

This section is an overview of estimated marginal means, and its implementation in in the **emmeans** package. 

```{r}
pigs
```


## References

This [Very statisticious](https://aosmith.rbind.io/2019/03/25/getting-started-with-emmeans/) blog post is helpful. I also worked through the **emmeans** vignettes on [CRAN](https://cran.r-project.org/web/packages/emmeans/).

<!--chapter:end:07-emmeans.Rmd-->


# Bayesian Regression {#BayesRegression}

Placeholder


## Compared to Frequentist Regression
## Model Evaluation
### Model Comparison
### Visualization

<!--chapter:end:08-bayesian_regression.Rmd-->


# Survival Analysis

Placeholder


## Basics
## Kaplan-Meier
## Log-Rank Test
## Common Distributions
### Exponential
### Weibull
## Cox
## Discrete Time
## Case Study: KM + Cox
### KM
### Cox
### Discrete Time
### Reporting

<!--chapter:end:09-survival_analysis.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:10-references.Rmd-->

